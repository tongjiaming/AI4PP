{"input": "\n from django . utils . translation import ugettext_lazy as _ \n from horizon import tabs \n class NetworkProfileTab ( tabs . Tab ) : \n name = _ ( \"<STR_LIT>\" ) \n slug = \"<STR_LIT>\" \n template_name = '<STR_LIT>' \n def get_context_data ( self , request ) : \n return None \n class PolicyProfileTab ( tabs . Tab ) : \n name = _ ( \"<STR_LIT>\" ) \n slug = \"<STR_LIT>\" \n template_name = '<STR_LIT>' \n preload = False \n class IndexTabs ( tabs . TabGroup ) : \n slug = \"<STR_LIT>\" \n tabs = ( NetworkProfileTab , <mask0> ) \n", "gt": "PolicyProfileTab"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import weakref \n from eventlet import corolocal \n class WeakLocal ( corolocal . local ) : \n def __getattribute__ ( self , attr ) : \n rval = corolocal . local . __getattribute__ ( self , attr ) \n if rval : \n rval = rval ( ) \n return rval \n def __setattr__ ( self , attr , value ) : \n value = weakref . ref ( value ) \n return corolocal . local . __setattr__ ( self , attr , value ) \n store = WeakLocal ( ) \n weak_store = WeakLocal ( ) \n strong_store = corolocal . <mask0> \n", "gt": "local"}
{"input": "\n import eventlet \n eventlet . monkey_patch ( ) \n import contextlib \n import sys \n from oslo . config import cfg \n from openstack_dashboard . openstack . common import log as logging \n from openstack_dashboard . openstack . common import rpc \n from openstack_dashboard . openstack . common . rpc import impl_zmq \n CONF = cfg . CONF \n CONF . register_opts ( rpc . rpc_opts ) \n CONF . register_opts ( impl_zmq . zmq_opts ) \n def main ( ) : \n CONF ( sys . argv [ <NUM_LIT:1> : ] , project = '<STR_LIT>' ) \n logging . setup ( \"<STR_LIT>\" ) \n with contextlib . closing ( impl_zmq . ZmqProxy ( CONF ) ) as reactor : \n reactor . consume_in_thread ( ) \n reactor . <mask0> ( ) \n", "gt": "wait"}
{"input": "\n from openstack_dashboard import api \n from openstack_dashboard . test import helpers as test \n from neutronclient . v2_0 import client \n neutronclient = client . Client \n class VPNaasApiTests ( test . APITestCase ) : \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_vpnservice_create ( self ) : \n vpnservice1 = self . api_vpnservices . first ( ) \n form_data = { \n '<STR_LIT:name>' : vpnservice1 [ '<STR_LIT:name>' ] , \n '<STR_LIT:description>' : vpnservice1 [ '<STR_LIT:description>' ] , \n '<STR_LIT>' : vpnservice1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : vpnservice1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : vpnservice1 [ '<STR_LIT>' ] \n } \n vpnservice = { '<STR_LIT>' : self . api_vpnservices . first ( ) } \n neutronclient . create_vpnservice ( \n { '<STR_LIT>' : form_data } ) . AndReturn ( vpnservice ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . vpnservice_create ( self . request , ** form_data ) \n self . assertIsInstance ( ret_val , api . vpn . VPNService ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_vpnservices_get ( self ) : \n vpnservices = { '<STR_LIT>' : self . vpnservices . list ( ) } \n vpnservices_dict = { '<STR_LIT>' : self . api_vpnservices . list ( ) } \n neutronclient . list_vpnservices ( ) . AndReturn ( vpnservices_dict ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . vpnservices_get ( self . request ) \n for ( v , d ) in zip ( ret_val , vpnservices [ '<STR_LIT>' ] ) : \n self . assertIsInstance ( v , api . vpn . VPNService ) \n self . assertTrue ( v . name , d . name ) \n self . assertTrue ( v . id ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_vpnservice_get ( self ) : \n vpnservice1 = self . api_vpnservices . first ( ) \n vpnservice = { '<STR_LIT>' : vpnservice1 } \n neutronclient . show_vpnservice ( \n vpnservice [ '<STR_LIT>' ] [ '<STR_LIT:id>' ] ) . AndReturn ( vpnservice ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . vpnservice_get ( self . request , \n vpnservice [ '<STR_LIT>' ] [ '<STR_LIT:id>' ] ) \n self . assertIsInstance ( ret_val , api . vpn . VPNService ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_ikepolicy_create ( self ) : \n ikepolicy1 = self . api_ikepolicies . first ( ) \n form_data = { \n '<STR_LIT:name>' : ikepolicy1 [ '<STR_LIT:name>' ] , \n '<STR_LIT:description>' : ikepolicy1 [ '<STR_LIT:description>' ] , \n '<STR_LIT>' : ikepolicy1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ikepolicy1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ikepolicy1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ikepolicy1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ikepolicy1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ikepolicy1 [ '<STR_LIT>' ] \n } \n ikepolicy = { '<STR_LIT>' : self . api_ikepolicies . first ( ) } \n neutronclient . create_ikepolicy ( \n { '<STR_LIT>' : form_data } ) . AndReturn ( ikepolicy ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . ikepolicy_create ( self . request , ** form_data ) \n self . assertIsInstance ( ret_val , api . vpn . IKEPolicy ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_ikepolicies_get ( self ) : \n ikepolicies = { '<STR_LIT>' : self . ikepolicies . list ( ) } \n ikepolicies_dict = { '<STR_LIT>' : self . api_ikepolicies . list ( ) } \n neutronclient . list_ikepolicies ( ) . AndReturn ( ikepolicies_dict ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . ikepolicies_get ( self . request ) \n for ( v , d ) in zip ( ret_val , ikepolicies [ '<STR_LIT>' ] ) : \n self . assertIsInstance ( v , api . vpn . IKEPolicy ) \n self . assertTrue ( v . name , d . name ) \n self . assertTrue ( v . id ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_ikepolicy_get ( self ) : \n ikepolicy1 = self . api_ikepolicies . first ( ) \n ikepolicy = { '<STR_LIT>' : ikepolicy1 } \n neutronclient . show_ikepolicy ( \n ikepolicy [ '<STR_LIT>' ] [ '<STR_LIT:id>' ] ) . AndReturn ( ikepolicy ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . ikepolicy_get ( self . request , \n ikepolicy [ '<STR_LIT>' ] [ '<STR_LIT:id>' ] ) \n self . assertIsInstance ( ret_val , api . vpn . IKEPolicy ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_ipsecpolicy_create ( self ) : \n ipsecpolicy1 = self . api_ipsecpolicies . first ( ) \n form_data = { \n '<STR_LIT:name>' : ipsecpolicy1 [ '<STR_LIT:name>' ] , \n '<STR_LIT:description>' : ipsecpolicy1 [ '<STR_LIT:description>' ] , \n '<STR_LIT>' : ipsecpolicy1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecpolicy1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecpolicy1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecpolicy1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecpolicy1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecpolicy1 [ '<STR_LIT>' ] \n } \n ipsecpolicy = { '<STR_LIT>' : self . api_ipsecpolicies . first ( ) } \n neutronclient . create_ipsecpolicy ( \n { '<STR_LIT>' : form_data } ) . AndReturn ( ipsecpolicy ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . ipsecpolicy_create ( self . request , ** form_data ) \n self . assertIsInstance ( ret_val , api . vpn . IPSecPolicy ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_ipsecpolicies_get ( self ) : \n ipsecpolicies = { '<STR_LIT>' : self . ipsecpolicies . list ( ) } \n ipsecpolicies_dict = { '<STR_LIT>' : self . api_ipsecpolicies . list ( ) } \n neutronclient . list_ipsecpolicies ( ) . AndReturn ( ipsecpolicies_dict ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . ipsecpolicies_get ( self . request ) \n for ( v , d ) in zip ( ret_val , ipsecpolicies [ '<STR_LIT>' ] ) : \n self . assertIsInstance ( v , api . vpn . IPSecPolicy ) \n self . assertTrue ( v . name , d . name ) \n self . assertTrue ( v . id ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_ipsecpolicy_get ( self ) : \n ipsecpolicy1 = self . api_ipsecpolicies . first ( ) \n ipsecpolicy = { '<STR_LIT>' : ipsecpolicy1 } \n neutronclient . show_ipsecpolicy ( \n ipsecpolicy [ '<STR_LIT>' ] [ '<STR_LIT:id>' ] ) . AndReturn ( ipsecpolicy ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . ipsecpolicy_get ( self . request , \n ipsecpolicy [ '<STR_LIT>' ] [ '<STR_LIT:id>' ] ) \n self . assertIsInstance ( ret_val , api . vpn . IPSecPolicy ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_ipsecsiteconnection_create ( self ) : \n ipsecsiteconnection1 = self . api_ipsecsiteconnections . first ( ) \n form_data = { \n '<STR_LIT:name>' : ipsecsiteconnection1 [ '<STR_LIT:name>' ] , \n '<STR_LIT:description>' : ipsecsiteconnection1 [ '<STR_LIT:description>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] , \n '<STR_LIT>' : ipsecsiteconnection1 [ '<STR_LIT>' ] \n } \n ipsecsiteconnection = { '<STR_LIT>' : \n self . api_ipsecsiteconnections . first ( ) } \n neutronclient . create_ipsec_site_connection ( \n { '<STR_LIT>' : \n form_data } ) . AndReturn ( ipsecsiteconnection ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . ipsecsiteconnection_create ( \n self . request , ** form_data ) \n self . assertIsInstance ( ret_val , api . vpn . IPSecSiteConnection ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_ipsecsiteconnections_get ( self ) : \n ipsecsiteconnections = { \n '<STR_LIT>' : self . ipsecsiteconnections . list ( ) } \n ipsecsiteconnections_dict = { \n '<STR_LIT>' : self . api_ipsecsiteconnections . list ( ) } \n neutronclient . list_ipsec_site_connections ( ) . AndReturn ( \n ipsecsiteconnections_dict ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . ipsecsiteconnections_get ( self . request ) \n for ( v , d ) in zip ( ret_val , \n ipsecsiteconnections [ '<STR_LIT>' ] ) : \n self . assertIsInstance ( v , api . vpn . IPSecSiteConnection ) \n self . assertTrue ( v . name , d . name ) \n self . assertTrue ( v . id ) \n @ test . create_stubs ( { neutronclient : ( '<STR_LIT>' , ) } ) \n def test_ipsecsiteconnection_get ( self ) : \n ipsecsiteconnection1 = self . api_ipsecsiteconnections . first ( ) \n ipsecsiteconnection = { '<STR_LIT>' : ipsecsiteconnection1 } \n neutronclient . show_ipsec_site_connection ( \n ipsecsiteconnection [ '<STR_LIT>' ] [ '<STR_LIT:id>' ] ) . AndReturn ( \n ipsecsiteconnection ) \n self . mox . ReplayAll ( ) \n ret_val = api . vpn . ipsecsiteconnection_get ( self . request , \n ipsecsiteconnection [ '<STR_LIT>' ] [ '<STR_LIT:id>' ] ) \n self . assertIsInstance ( ret_val , api . vpn . <mask0> ) \n", "gt": "IPSecSiteConnection"}
{"input": "\n from horizon import tables \n from openstack_dashboard . usage import base \n class UsageView ( tables . DataTableView ) : \n usage_class = None \n show_terminated = True \n def __init__ ( self , * args , ** kwargs ) : \n super ( UsageView , self ) . __init__ ( * args , ** kwargs ) \n if not issubclass ( self . usage_class , base . BaseUsage ) : \n raise AttributeError ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n def get_template_names ( self ) : \n if self . request . GET . get ( '<STR_LIT>' , '<STR_LIT:html>' ) == '<STR_LIT>' : \n return \"<STR_LIT:.>\" . join ( ( self . template_name . rsplit ( '<STR_LIT:.>' , <NUM_LIT:1> ) [ <NUM_LIT:0> ] , '<STR_LIT>' ) ) \n return self . template_name \n def get_content_type ( self ) : \n if self . request . GET . get ( '<STR_LIT>' , '<STR_LIT:html>' ) == '<STR_LIT>' : \n return \"<STR_LIT>\" \n return \"<STR_LIT>\" \n def get_data ( self ) : \n project_id = self . kwargs . get ( '<STR_LIT>' , self . request . user . tenant_id ) \n self . usage = self . usage_class ( self . request , project_id ) \n self . usage . summarize ( * self . usage . get_date_range ( ) ) \n self . usage . get_limits ( ) \n self . kwargs [ '<STR_LIT>' ] = self . usage \n return self . usage . usage_list \n def get_context_data ( self , ** kwargs ) : \n context = super ( UsageView , self ) . get_context_data ( ** kwargs ) \n context [ '<STR_LIT>' ] . kwargs [ '<STR_LIT>' ] = self . usage \n context [ '<STR_LIT>' ] = self . usage . form \n context [ '<STR_LIT>' ] = self . usage \n return context \n def render_to_response ( self , context , ** response_kwargs ) : \n if self . request . GET . get ( '<STR_LIT>' , '<STR_LIT:html>' ) == '<STR_LIT>' : \n render_class = self . csv_response_class \n response_kwargs . setdefault ( \"<STR_LIT:filename>\" , \"<STR_LIT>\" ) \n else : \n render_class = self . response_class \n resp = render_class ( request = self . request , \n template = self . get_template_names ( ) , \n context = context , \n content_type = self . get_content_type ( ) , \n ** response_kwargs ) \n return <mask0> \n", "gt": "resp"}
{"input": "\n from enum import IntEnum \n from . component import Component \n from . object import field \n class ReflectionProbeUsage ( IntEnum ) : \n Off = <NUM_LIT:0> \n BlendProbes = <NUM_LIT:1> \n BlendProbesAndSkybox = <NUM_LIT:2> \n Simple = <NUM_LIT:3> \n class ShadowCastingMode ( IntEnum ) : \n Off = <NUM_LIT:0> \n On = <NUM_LIT:1> \n TwoSided = <NUM_LIT:2> \n ShadowsOnly = <NUM_LIT:3> \n class Renderer ( Component ) : \n enabled = field ( \"<STR_LIT>\" , bool ) \n lightmap_index = field ( \"<STR_LIT>\" ) \n materials = field ( \"<STR_LIT>\" ) \n probe_anchor = field ( \"<STR_LIT>\" ) \n receive_shadows = field ( \"<STR_LIT>\" , bool ) \n reflection_probe_usage = field ( \"<STR_LIT>\" , ReflectionProbeUsage ) \n shadow_casting_mode = field ( \"<STR_LIT>\" , ShadowCastingMode ) \n sorting_layer_id = field ( \"<STR_LIT>\" ) \n sorting_order = field ( \"<STR_LIT>\" ) \n use_light_probes = field ( \"<STR_LIT>\" , bool ) \n lightmap_index_dynamic = field ( \"<STR_LIT>\" ) \n lightmap_tiling_offset = field ( \"<STR_LIT>\" ) \n lightmap_tiling_offset_dynamic = field ( \"<STR_LIT>\" ) \n static_batch_root = field ( \"<STR_LIT>\" ) \n subset_indices = field ( \"<STR_LIT>\" ) \n @ property \n def material ( self ) : \n return self . materials [ <NUM_LIT:0> ] \n class ParticleSystemRenderMode ( IntEnum ) : \n Billboard = <NUM_LIT:0> \n Stretch = <NUM_LIT:1> \n HorizontalBillboard = <NUM_LIT:2> \n VerticalBillboard = <NUM_LIT:3> \n Mesh = <NUM_LIT:4> \n class ParticleSystemSortMode ( IntEnum ) : \n None_ = <NUM_LIT:0> \n Distance = <NUM_LIT:1> \n OldestInFront = <NUM_LIT:2> \n YoungestInFront = <NUM_LIT:3> \n class MeshRenderer ( Component ) : \n pass \n class ParticleRenderer ( Renderer ) : \n camera_velocity_scale = field ( \"<STR_LIT>\" ) \n length_scale = field ( \"<STR_LIT>\" ) \n max_particle_size = field ( \"<STR_LIT>\" ) \n velocity_scale = field ( \"<STR_LIT>\" ) \n stretch_particles = field ( \"<STR_LIT>\" ) \n uv_animation = field ( \"<STR_LIT>\" ) \n class ParticleSystemRenderer ( Renderer ) : \n camera_velocity_scale = field ( \"<STR_LIT>\" ) \n length_scale = field ( \"<STR_LIT>\" ) \n max_particle_size = field ( \"<STR_LIT>\" ) \n mesh = field ( \"<STR_LIT>\" ) \n mesh1 = field ( \"<STR_LIT>\" ) \n mesh2 = field ( \"<STR_LIT>\" ) \n mesh3 = field ( \"<STR_LIT>\" ) \n normal_direction = field ( \"<STR_LIT>\" ) \n render_mode = field ( \"<STR_LIT>\" , ParticleSystemRenderMode ) \n sort_mode = field ( \"<STR_LIT>\" , ParticleSystemSortMode ) \n sorting_fudge = field ( \"<STR_LIT>\" ) \n velocity_scale = <mask0> ( \"<STR_LIT>\" ) \n", "gt": "field"}
{"input": "\n from ConfigParser import * \n from StringIO import * \n from Log import Log \n import datetime \n class Config : \n @ staticmethod \n def LoadConfig ( ) : \n Config . parser = ConfigParser ( ) \n try : \n sconff = open ( CONFIG_FILE , \"<STR_LIT:r>\" ) \n except : \n Log . warn ( \"<STR_LIT>\" ) \n return \n sconf = StringIO ( ) \n sconf . write ( \"<STR_LIT>\" ) \n sconf . write ( sconff . read ( ) ) \n sconf . seek ( <NUM_LIT:0> ) \n Config . parser . readfp ( sconf ) \n sconff . close ( ) \n sconf . close ( ) \n return \n @ staticmethod \n def GetBoardsFile ( ) : \n return BOARDS_FILE \n @ staticmethod \n def GetInt ( name , defval ) : \n if ( Config . parser . has_option ( '<STR_LIT>' , name ) ) : \n return Config . parser . getint ( '<STR_LIT>' , name ) \n else : \n return defval \n @ staticmethod \n def GetString ( name , defval ) : \n if ( Config . parser . has_option ( '<STR_LIT>' , name ) ) : \n val = Config . parser . get ( '<STR_LIT>' , name ) \n if ( val [ <NUM_LIT:0> ] == '<STR_LIT:\">' and val . endswith ( '<STR_LIT:\">' ) ) : \n val = val [ <NUM_LIT:1> : - <NUM_LIT:1> ] \n return val . decode ( '<STR_LIT>' ) \n else : \n return defval \n BBS_ROOT = '<STR_LIT>' \n BBS_XMPP_CERT_FILE = BBS_ROOT + \"<STR_LIT>\" \n BBS_XMPP_KEY_FILE = BBS_ROOT + \"<STR_LIT>\" \n BOARDS_FILE = BBS_ROOT + '<STR_LIT>' \n STRLEN = <NUM_LIT> \n ARTICLE_TITLE_LEN = <NUM_LIT> \n BM_LEN = <NUM_LIT> \n MAXBOARD = <NUM_LIT> \n CONFIG_FILE = BBS_ROOT + '<STR_LIT>' \n FILENAME_LEN = <NUM_LIT:20> \n OWNER_LEN = <NUM_LIT:30> \n SESSIONID_LEN = <NUM_LIT:32> \n REFRESH_TOKEN_LEN = <NUM_LIT> \n NAMELEN = <NUM_LIT> \n IDLEN = <NUM_LIT:12> \n MD5PASSLEN = <NUM_LIT:16> \n OLDPASSLEN = <NUM_LIT> \n MOBILE_NUMBER_LEN = <NUM_LIT> \n MAXCLUB = <NUM_LIT> \n MAXUSERS = <NUM_LIT> \n MAX_MSG_SIZE = <NUM_LIT> \n MAXFRIENDS = <NUM_LIT> \n MAXMESSAGE = <NUM_LIT:5> \n MAXSIGLINES = <NUM_LIT:6> \n IPLEN = <NUM_LIT:16> \n DEFAULTBOARD = \"<STR_LIT>\" \n BLESS_BOARD = \"<STR_LIT>\" \n QUOTED_LINES = <NUM_LIT:10> \n MAXACTIVE = <NUM_LIT> \n USHM_SIZE = MAXACTIVE + <NUM_LIT:10> \n UTMP_HASHSIZE = USHM_SIZE * <NUM_LIT:4> \n UCACHE_SEMLOCK = <NUM_LIT:0> \n LEN_FRIEND_EXP = <NUM_LIT:15> \n REFRESH_TIME = <NUM_LIT:30> \n USER_TITLE_LEN = <NUM_LIT> \n SESSION_TIMEOUT = datetime . timedelta ( <NUM_LIT:30> ) \n SESSION_TIMEOUT_SECONDS = <NUM_LIT> * <NUM_LIT:30> \n XMPP_IDLE_TIME = <NUM_LIT> \n XMPP_LONG_IDLE_TIME = <NUM_LIT> \n XMPP_UPDATE_TIME_INTERVAL = <NUM_LIT:10> \n XMPP_PING_TIME_INTERVAL = <NUM_LIT> \n PUBLIC_SHMKEY = <NUM_LIT> \n MAX_ATTACHSIZE = <NUM_LIT:20> * <NUM_LIT> * <NUM_LIT> \n BMDEL_DECREASE = True \n SYSMAIL_BOARD = \"<STR_LIT>\" \n ADD_EDITMARK = <mask0> \n SEARCH_COUNT_LIMIT = <NUM_LIT:20> \n MAIL_SIZE_LIMIT = - <NUM_LIT:1> \n SEC_DELETED_OLDHOME = <NUM_LIT> * <NUM_LIT> * <NUM_LIT:3> \n SELF_INTRO_MAX_LEN = <NUM_LIT> \n", "gt": "True"}
{"input": "\n import re \n import os \n import stat \n import json \n import struct \n import time \n import Config \n import Board \n import Post \n import BoardManager \n from Util import Util \n from Log import Log \n from errors import * \n DEFAULT_DIGEST_LIST_COUNT = <NUM_LIT:20> \n class DigestItem : \n def __init__ ( self , basepath ) : \n self . basepath = basepath \n self . title = '<STR_LIT>' \n self . host = '<STR_LIT>' \n self . port = <NUM_LIT:0> \n self . attachpos = <NUM_LIT:0> \n self . fname = '<STR_LIT>' \n self . mtitle = '<STR_LIT>' \n self . items = [ ] \n self . update_time = <NUM_LIT:0> \n self . id = <NUM_LIT:0> \n self . sysop_only = <NUM_LIT:0> \n self . bms_only = <NUM_LIT:0> \n self . zixia_only = <NUM_LIT:0> \n def IsDir ( self ) : \n try : \n st = os . stat ( self . realpath ( ) ) \n return stat . S_ISDIR ( st . st_mode ) \n except : \n return False \n def IsFile ( self ) : \n try : \n st = os . stat ( self . realpath ( ) ) \n return stat . S_ISREG ( st . st_mode ) \n except : \n return False \n def GetModTime ( self ) : \n try : \n st = os . stat ( self . realpath ( ) ) \n mtime = st . st_mtime \n except : \n mtime = time . time ( ) \n return mtime \n def names_path ( self ) : \n return \"<STR_LIT>\" % self . realpath ( ) \n def realpath ( self ) : \n return \"<STR_LIT>\" % ( Config . BBS_ROOT , self . path ( ) ) \n def path ( self ) : \n if ( self . fname ) : \n return \"<STR_LIT>\" % ( self . basepath , self . fname ) \n else : \n return self . basepath \n def CheckUpdate ( self ) : \n try : \n stat = os . stat ( self . names_path ( ) ) \n if ( stat . st_mtime > self . update_time ) : \n self . LoadNames ( ) \n except : \n return False \n return True \n def LoadNames ( self ) : \n try : \n f = open ( self . names_path ( ) , \"<STR_LIT:r>\" ) \n except IOError : \n return <NUM_LIT:0> \n stat = os . fstat ( f . fileno ( ) ) \n self . update_time = stat . st_mtime \n item = DigestItem ( self . path ( ) ) \n hostname = '<STR_LIT>' \n _id = <NUM_LIT:0> \n bms_only = <NUM_LIT:0> \n sysop_only = <NUM_LIT:0> \n zixia_only = <NUM_LIT:0> \n while ( True ) : \n line = f . readline ( ) \n if ( line == \"<STR_LIT>\" ) : break \n npos = line . find ( \"<STR_LIT:\\n>\" ) \n if ( npos != - <NUM_LIT:1> ) : line = line [ : npos ] \n if ( line [ : <NUM_LIT:1> ] == '<STR_LIT:#>' ) : \n if ( line [ : <NUM_LIT:8> ] == \"<STR_LIT>\" ) : \n if ( not self . mtitle ) : \n self . mtitle = line [ <NUM_LIT:8> : ] \n result = re . match ( '<STR_LIT>' , line ) \n if ( result ) : \n key = result . group ( <NUM_LIT:1> ) \n value = result . group ( <NUM_LIT:2> ) \n if ( key == \"<STR_LIT:Name>\" ) : \n item . title = value \n item . attachpos = <NUM_LIT:0> \n elif ( key == \"<STR_LIT>\" ) : \n if ( value [ : <NUM_LIT:2> ] == \"<STR_LIT>\" ) : \n item . fname = value [ <NUM_LIT:2> : ] \n else : \n item . fname = value \n if ( item . fname . find ( \"<STR_LIT:..>\" ) != - <NUM_LIT:1> ) : \n continue \n if ( item . title . find ( \"<STR_LIT>\" ) != - <NUM_LIT:1> ) : \n bms_only += <NUM_LIT:1> \n elif ( item . title . find ( \"<STR_LIT>\" ) != - <NUM_LIT:1> ) : \n sysop_only += <NUM_LIT:1> \n elif ( item . title . find ( \"<STR_LIT>\" ) != - <NUM_LIT:1> ) : \n zixia_only += <NUM_LIT:1> \n if ( item . fname . find ( \"<STR_LIT>\" ) != - <NUM_LIT:1> ) : \n parts = re . split ( '<STR_LIT>' , item . fname ) \n newparts = [ ] \n for part in parts : \n if ( part ) : \n newparts += [ part ] \n hostname = newparts [ <NUM_LIT:0> ] \n item . fname = newparts [ <NUM_LIT:1> ] \n try : \n item . port = int ( newparts [ <NUM_LIT:2> ] ) \n except : \n item . port = <NUM_LIT:0> \n item . id = _id \n _id += <NUM_LIT:1> \n item . bms_only = bms_only \n item . sysop_only = sysop_only \n item . zixia_only = zixia_only \n item . host = hostname \n self . items += [ item ] \n item = DigestItem ( self . path ( ) ) \n hostname = '<STR_LIT>' \n elif ( key == \"<STR_LIT>\" ) : \n hostname = value \n elif ( key == \"<STR_LIT>\" ) : \n try : \n item . port = int ( value ) \n except : \n item . port = <NUM_LIT:0> \n elif ( key == \"<STR_LIT>\" ) : \n try : \n item . attachpos = int ( value ) \n except : \n item . attachpos = <NUM_LIT:0> \n f . close ( ) \n return <NUM_LIT:1> \n def GetItem ( self , user , route , has_perm = False , need_perm = False ) : \n self . CheckUpdate ( ) \n if ( self . mtitle . find ( \"<STR_LIT>\" ) != - <NUM_LIT:1> ) : \n if ( Board . Board . IsBM ( user , self . mtitle [ <NUM_LIT:4> : ] , ) or user . IsSysop ( ) ) : \n has_perm = True \n elif ( need_perm and not has_perm ) : \n return None \n if ( self . mtitle . find ( \"<STR_LIT>\" ) != - <NUM_LIT:1> \n or self . mtitle . find ( \"<STR_LIT>\" ) != - <NUM_LIT:1> \n or self . mtitle . find ( \"<STR_LIT>\" ) != - <NUM_LIT:1> ) : \n need_perm = True \n if ( len ( route ) == <NUM_LIT:0> ) : \n return self \n target = route [ <NUM_LIT:0> ] - <NUM_LIT:1> \n _id = target \n if ( _id >= len ( self . items ) ) : \n return None \n while ( self . items [ _id ] . EffectiveId ( user ) < target ) : \n _id += <NUM_LIT:1> \n if ( _id >= len ( self . items ) ) : \n return None \n item = self . items [ _id ] \n item . mtitle = item . title \n if ( len ( route ) == <NUM_LIT:1> ) : \n return item \n else : \n if ( item . IsDir ( ) ) : \n if ( not item . CheckUpdate ( ) ) : \n return None \n return item . GetItem ( user , route [ <NUM_LIT:1> : ] , has_perm , need_perm ) \n else : \n return None \n def GetRange ( self , user , route , start , end , has_perm = False , need_perm = False ) : \n self . CheckUpdate ( ) \n firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n if ( not firstitem ) : \n return [ ] \n parent = self . GetItem ( user , route , has_perm , need_perm ) \n if ( not parent ) : \n return [ ] \n if ( not parent . IsDir ( ) ) : \n return [ ] \n result = [ ] \n _id = start - <NUM_LIT:1> \n for i in range ( start , end + <NUM_LIT:1> ) : \n target = i - <NUM_LIT:1> \n if ( _id >= len ( parent . items ) ) : \n return [ ] \n while ( parent . items [ _id ] . EffectiveId ( user ) < target ) : \n _id += <NUM_LIT:1> \n if ( _id >= len ( parent . items ) ) : \n return result \n item = parent . items [ _id ] \n item . mtitle = item . title \n result += [ item ] \n return result \n def EffectiveId ( self , user ) : \n _id = self . id \n if ( user . IsSysop ( ) ) : \n return _id \n if ( not user . IsSysop ( ) ) : \n _id -= self . sysop_only \n if ( not user . IsBM ( ) ) : \n _id -= self . bms_only \n if ( not user . IsSECANC ( ) ) : \n _id -= self . zixia_only \n return _id \n def GetInfo ( self ) : \n info = { } \n info [ '<STR_LIT>' ] = Util . gbkDec ( self . mtitle ) \n info [ '<STR_LIT:title>' ] = Util . gbkDec ( self . title ) \n info [ '<STR_LIT>' ] = self . attachpos \n if ( self . host != '<STR_LIT>' ) : \n info [ '<STR_LIT:host>' ] = self . host \n info [ '<STR_LIT:port>' ] = self . port \n info [ '<STR_LIT:type>' ] = '<STR_LIT>' \n elif ( self . IsDir ( ) ) : \n info [ '<STR_LIT:type>' ] = '<STR_LIT>' \n elif ( self . IsFile ( ) ) : \n info [ '<STR_LIT:type>' ] = '<STR_LIT:file>' \n else : \n info [ '<STR_LIT:type>' ] = '<STR_LIT>' \n info [ '<STR_LIT>' ] = int ( self . GetModTime ( ) ) \n return info \n def GetInfoForUser ( self , user ) : \n info = self . GetInfo ( ) \n info [ '<STR_LIT:id>' ] = self . EffectiveId ( user ) + <NUM_LIT:1> \n return info \n def GetAttachLink ( self , session ) : \n _hash = Util . HashGen ( self . path ( ) , \"<STR_LIT>\" ) \n filename = '<STR_LIT>' \n for i in range ( <NUM_LIT:2> ) : \n filename += \"<STR_LIT>\" % struct . unpack ( '<STR_LIT>' , _hash [ i * <NUM_LIT:4> : ( i + <NUM_LIT:1> ) * <NUM_LIT:4> ] ) \n link = \"<STR_LIT>\" % ( session . GetMirror ( Config . Config . GetInt ( '<STR_LIT>' , <NUM_LIT> ) ) , filename ) \n linkfile = \"<STR_LIT>\" % ( Config . BBS_ROOT , filename ) \n target = \"<STR_LIT>\" % self . path ( ) \n try : \n os . symlink ( target , linkfile ) \n except : \n pass \n return link \n class Digest : \n root = DigestItem ( \"<STR_LIT>\" ) \n def __init__ ( self , board , path ) : \n self . board = board \n self . path = path \n self . root = DigestItem ( self . path ) \n @ staticmethod \n def GET ( svc , session , params , action ) : \n if ( session is None ) : raise Unauthorized ( '<STR_LIT>' ) \n if not session . CheckScope ( '<STR_LIT>' ) : raise NoPerm ( \"<STR_LIT>\" ) \n user = session . GetUser ( ) \n boardname = svc . get_str ( params , '<STR_LIT>' , '<STR_LIT>' ) \n if ( boardname ) : \n board = BoardManager . BoardManager . GetBoard ( boardname ) \n if ( board is None ) : raise NotFound ( '<STR_LIT>' % boardname ) \n if ( not board . CheckReadPerm ( user ) ) : \n raise NoPerm ( '<STR_LIT>' ) \n basenode = board . digest . root \n has_perm = user . IsDigestMgr ( ) or user . IsSysop ( ) or user . IsSuperBM ( ) \n else : \n basenode = Digest . root \n has_perm = user . IsDigestMgr ( ) \n if ( action == \"<STR_LIT:list>\" ) : \n route = svc . get_str ( params , '<STR_LIT>' ) \n start = svc . get_int ( params , '<STR_LIT:start>' , <NUM_LIT:1> ) \n end = svc . get_int ( params , '<STR_LIT:end>' , start + DEFAULT_DIGEST_LIST_COUNT - <NUM_LIT:1> ) \n Digest . List ( svc , basenode , route , start , end , session , has_perm ) \n return \n elif ( action == \"<STR_LIT>\" ) : \n route = svc . get_str ( params , '<STR_LIT>' ) \n start = svc . get_int ( params , '<STR_LIT:start>' , <NUM_LIT:0> ) \n count = svc . get_int ( params , '<STR_LIT:count>' , <NUM_LIT:0> ) \n Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n return \n else : \n raise WrongArgs ( '<STR_LIT>' % action ) \n @ staticmethod \n def ParseRoute ( route ) : \n ret = [ ] \n items = re . split ( '<STR_LIT:->' , route ) \n items = items [ <NUM_LIT:1> : ] \n for item in items : \n try : \n ret += [ int ( item ) ] \n except : \n raise WrongArgs ( '<STR_LIT>' % item ) \n return ret \n @ staticmethod \n def List ( svc , basenode , route , start , end , session , has_perm ) : \n route_array = Digest . ParseRoute ( route ) \n parent = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n if ( not parent ) : \n raise WrongArgs ( '<STR_LIT>' % route ) \n if ( not parent . IsDir ( ) ) : \n raise WrongArgs ( '<STR_LIT>' % route ) \n items = basenode . GetRange ( session . GetUser ( ) , route_array , start , end , has_perm ) \n result = { } \n result [ '<STR_LIT>' ] = parent . GetInfoForUser ( session . GetUser ( ) ) \n result [ '<STR_LIT:count>' ] = len ( items ) \n result_list = [ ] \n for item in items : \n result_list += [ item . GetInfoForUser ( session . GetUser ( ) ) ] \n result [ '<STR_LIT>' ] = result_list \n svc . writedata ( json . dumps ( result ) ) \n @ staticmethod \n def View ( svc , basenode , route , session , has_perm , start , count ) : \n route_array = Digest . ParseRoute ( route ) \n item = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n if ( not item ) : \n raise WrongArgs ( '<STR_LIT>' % route ) \n if ( not item . IsFile ( ) ) : \n raise WrongArgs ( '<STR_LIT>' % route ) \n result = { } \n result [ '<STR_LIT>' ] = item . GetInfoForUser ( session . GetUser ( ) ) \n postinfo = Post . Post ( item . realpath ( ) , None ) \n ( result [ '<STR_LIT:content>' ] , result [ '<STR_LIT>' ] ) = postinfo . GetContent ( start , count ) \n attachlist = postinfo . GetAttachListByType ( ) \n result [ '<STR_LIT>' ] = attachlist [ <NUM_LIT:0> ] \n result [ '<STR_LIT>' ] = attachlist [ <NUM_LIT:1> ] \n if ( attachlist [ <NUM_LIT:0> ] or attachlist [ <NUM_LIT:1> ] ) : \n result [ '<STR_LIT>' ] = item . GetAttachLink ( session ) \n svc . writedata ( json . dumps ( <mask0> ) ) \n", "gt": "result"}
{"input": "\n import time \n import UserManager \n import UserInfo \n from Session import Session \n from Log import Log \n import UCache \n import Config \n import MsgBox \n import xmpp \n import modes \n import Util \n import traceback \n import os \n from xmpp . features import NoRoute \n __disco_info_ns__ = '<STR_LIT>' \n __disco_items_ns__ = '<STR_LIT>' \n __vcard_ns__ = '<STR_LIT>' \n STEAL_AFTER_SEEN = <NUM_LIT:3> \n def elem_to_str ( elem ) : \n return \"<STR_LIT>\" % ( elem . tag , elem . attrib , elem . text ) \n class XMPPServer ( xmpp . Plugin ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , rosters , host ) : \n self . probed = False \n self . _closed = False \n self . rosters = rosters \n self . _session = None \n self . rosters . set_resources ( self . get_resources ( ) ) \n self . _fixedjid = UCache . UCache . formalize_jid ( unicode ( self . authJID ) ) \n self . _userid = self . _fixedjid . partition ( '<STR_LIT:@>' ) [ <NUM_LIT:0> ] . encode ( \"<STR_LIT>\" ) \n if ( not self . rosters . allow_login ( self . authJID . bare ) ) : \n Log . warn ( \"<STR_LIT>\" % self . _userid ) \n self . stream_error ( '<STR_LIT>' , '<STR_LIT>' ) \n return \n Log . info ( \"<STR_LIT>\" % unicode ( self . authJID ) ) \n if self . authJID . resource [ : - <NUM_LIT:8> ] != \"<STR_LIT>\" and len ( self . authJID . resource ) > <NUM_LIT:8> : \n try : \n routes = self . routes ( self . authJID . bare ) \n for route in routes : \n jid = route [ <NUM_LIT:0> ] \n if jid . resource [ : - <NUM_LIT:8> ] == self . authJID . resource [ : - <NUM_LIT:8> ] : \n if jid . resource != self . authJID . resource : \n Log . info ( \"<STR_LIT>\" % ( jid . full , route [ <NUM_LIT:1> ] ) ) \n route [ <NUM_LIT:1> ] . stream_error ( '<STR_LIT>' , '<STR_LIT>' ) \n else : \n Log . info ( \"<STR_LIT>\" % ( jid . full , route [ <NUM_LIT:1> ] ) ) \n except NoRoute : \n pass \n Log . debug ( \"<STR_LIT>\" % self . authJID . full ) \n self . _user = UserManager . UserManager . LoadUser ( self . _userid ) \n if ( self . _user == None ) : \n raise Exception ( \"<STR_LIT>\" ) \n self . _peer_addr = self . getpeername ( ) \n self . _session = Session ( self . _user , self . _peer_addr [ <NUM_LIT:0> ] ) \n self . _session . RecordLogin ( ) \n self . _userinfo = self . _session . Register ( ) \n self . _loginid = self . _session . utmpent \n self . _hostname = host \n self . bind ( xmpp . ReceivedCloseStream , self . recv_close ) \n self . bind ( xmpp . StreamClosed , self . stream_closed ) \n self . bind ( xmpp . SentCloseStream , self . sent_close ) \n self . rosters . register_conn ( self ) \n msgbox = MsgBox . MsgBox ( self . _userid ) \n if self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) is None : \n self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msgbox . GetMsgCount ( all = False ) - msgbox . GetUnreadCount ( ) ) \n self . check_msg ( ) \n def get_loginid ( self ) : \n return self . _loginid \n def recv_close ( self ) : \n Log . debug ( \"<STR_LIT>\" % self . authJID . full ) \n return self . close ( ) \n def stream_closed ( self ) : \n Log . debug ( \"<STR_LIT>\" % self . authJID . full ) \n return self . close ( ) \n def sent_close ( self ) : \n Log . debug ( \"<STR_LIT>\" % self . authJID . full ) \n return self . close ( ) \n def close ( self ) : \n if ( self . _closed ) : \n Log . debug ( \"<STR_LIT>\" ) \n return \n self . _closed = True \n Log . info ( \"<STR_LIT>\" % unicode ( self . authJID ) ) \n if ( self . _session ) : \n self . _session . Unregister ( ) \n self . unbind_res ( ) \n self . rosters . unregister_conn ( self ) \n @ xmpp . iq ( '<STR_LIT>' ) \n def ping ( self , iq ) : \n \"\"\"<STR_LIT>\"\"\" \n self . refresh ( ) \n return self . iq ( '<STR_LIT:result>' , iq ) \n @ xmpp . stanza ( '<STR_LIT:message>' ) \n def message ( self , elem ) : \n \"\"\"<STR_LIT>\"\"\" \n to_jid = elem . get ( '<STR_LIT:to>' ) \n from_jid = elem . get ( '<STR_LIT>' ) \n if ( from_jid == None ) : \n return \n text_body = None \n for child in elem : \n if ( child . tag . endswith ( '<STR_LIT>' ) ) : \n text_body = child . text \n if ( text_body == None ) : \n return \n ret = self . rosters . send_msg ( from_jid , to_jid , text_body ) \n if ( ret <= <NUM_LIT:0> ) : \n Log . warn ( \"<STR_LIT>\" % ( to_jid , from_jid , ret ) ) \n errors = { \n - <NUM_LIT:1> : \"<STR_LIT>\" , \n - <NUM_LIT:11> : \"<STR_LIT>\" , \n - <NUM_LIT:12> : \"<STR_LIT>\" , \n - <NUM_LIT> : \"<STR_LIT>\" , \n - <NUM_LIT> : \"<STR_LIT>\" , \n - <NUM_LIT:2> : \"<STR_LIT>\" , \n - <NUM_LIT> : \"<STR_LIT>\" } \n if ( ret in errors ) : \n elem = self . E . message ( { '<STR_LIT>' : to_jid , \n '<STR_LIT:to>' : from_jid , \n '<STR_LIT:type>' : '<STR_LIT:error>' } , \n self . E . body ( errors [ ret ] ) ) \n self . recv ( from_jid , elem ) \n def make_jid ( self , userid ) : \n return \"<STR_LIT>\" % ( userid , self . _hostname ) \n def refresh ( self ) : \n self . _userinfo . freshtime = int ( time . time ( ) ) \n self . _userinfo . save ( ) \n def ping_result ( self , iq ) : \n self . refresh ( ) \n def ping_client ( self ) : \n try : \n pingelem = self . E . ping ( xmlns = '<STR_LIT>' ) \n return self . iq ( '<STR_LIT>' , self . ping_result , pingelem ) \n except Exception as e : \n Log . debug ( \"<STR_LIT>\" % ( self . authJID , e ) ) \n Log . debug ( traceback . format_exc ( ) ) \n return False \n def get_uid ( self ) : \n return self . _user . GetUID ( ) \n def recv_msg ( self , from_ , msgtext ) : \n elem = self . E . message ( { '<STR_LIT>' : from_ , '<STR_LIT:to>' : unicode ( self . authJID ) } , \n self . E . body ( msgtext ) ) \n self . recv ( unicode ( self . authJID ) , elem ) \n def check_msg ( self ) : \n Log . debug ( \"<STR_LIT>\" % self . _userid ) \n msgbox = MsgBox . MsgBox ( self . _userid ) \n msg_count = msgbox . GetMsgCount ( all = False ) \n my_pid = os . getpid ( ) \n xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n if xmpp_read > msg_count : \n xmpp_read = <NUM_LIT:0> \n Log . debug ( \"<STR_LIT>\" % ( msg_count , xmpp_read ) ) \n self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msg_count ) \n if xmpp_read < msg_count : \n return xmpp_read \n else : \n return - <NUM_LIT:1> \n def deliver_msg ( self , start ) : \n Log . debug ( \"<STR_LIT>\" % unicode ( self . authJID ) ) \n msgbox = MsgBox . MsgBox ( self . _userid ) \n msg_count = msgbox . GetMsgCount ( all = False ) \n my_pid = os . getpid ( ) \n for i in range ( start , msg_count ) : \n msghead = msgbox . LoadMsgHead ( i , all = False ) \n if msghead . topid == my_pid : \n msgtext = msgbox . LoadMsgText ( msghead ) \n self . recv_msg ( self . make_jid ( msghead . id ) , msgtext ) \n def steal_msg ( self ) : \n Log . debug ( \"<STR_LIT>\" % self . _userid ) \n msgbox = MsgBox . MsgBox ( self . _userid ) \n msg_count = msgbox . GetMsgCount ( all = False ) \n msg_unread = msgbox . GetUnreadCount ( ) \n read_count = msg_count - msg_unread \n my_pid = os . getpid ( ) \n term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n term_stealed = self . rosters . get_term_stealed ( self . get_uid ( ) ) \n all_xmpp = True \n new_unread = { } \n for i in range ( read_count - <NUM_LIT:1> , msg_count ) : \n if i < <NUM_LIT:0> : \n continue \n msghead = msgbox . LoadMsgHead ( i , all = False ) \n if i >= read_count and all_xmpp : \n if msghead . topid == my_pid : \n msgbox . GetUnreadMsg ( ) \n else : \n all_xmpp = False \n if msghead . topid == my_pid : \n continue \n if i < read_count : \n session = self . rosters . find_session ( self . authJID . bare , msghead . topid ) \n if session is None or session . get_mode ( ) != modes . MSG : \n continue \n Log . debug ( \"<STR_LIT>\" % i ) \n if msghead . topid not in new_unread : \n Log . debug ( \"<STR_LIT>\" % ( msghead . topid , i ) ) \n new_unread [ msghead . topid ] = i \n final_unread = { } \n to_steal = { } \n to_steal_begin = msg_count \n for pid in term_read : \n if pid in new_unread : \n if new_unread [ pid ] == term_read [ pid ] [ <NUM_LIT:0> ] : \n final_unread [ pid ] = ( term_read [ pid ] [ <NUM_LIT:0> ] , term_read [ pid ] [ <NUM_LIT:1> ] + <NUM_LIT:1> ) \n Log . debug ( \"<STR_LIT>\" % ( new_unread [ pid ] , pid , term_read [ pid ] [ <NUM_LIT:1> ] + <NUM_LIT:1> ) ) \n if final_unread [ pid ] [ <NUM_LIT:1> ] > STEAL_AFTER_SEEN : \n to_steal [ pid ] = final_unread [ pid ] \n Log . debug ( \"<STR_LIT>\" % ( to_steal [ pid ] [ <NUM_LIT:0> ] , pid ) ) \n if pid in term_stealed : \n steal_begin = max ( final_unread [ pid ] [ <NUM_LIT:0> ] , term_stealed [ pid ] + <NUM_LIT:1> ) \n else : \n steal_begin = final_unread [ pid ] [ <NUM_LIT:0> ] \n if steal_begin < to_steal_begin : \n to_steal_begin = steal_begin \n else : \n final_unread [ pid ] = ( new_unread [ pid ] , <NUM_LIT:1> ) \n Log . debug ( \"<STR_LIT>\" % ( term_read [ pid ] [ <NUM_LIT:0> ] , new_unread [ pid ] , pid ) ) \n else : \n Log . debug ( \"<STR_LIT>\" % pid ) \n pass \n for pid in new_unread : \n if pid not in term_read : \n Log . debug ( \"<STR_LIT>\" % ( new_unread [ pid ] , pid ) ) \n final_unread [ pid ] = ( new_unread [ pid ] , <NUM_LIT:1> ) \n if to_steal : \n Log . debug ( \"<STR_LIT>\" % to_steal_begin ) \n for i in range ( to_steal_begin , msg_count ) : \n msghead = msgbox . LoadMsgHead ( i , all = False ) \n if msghead . topid == my_pid : \n Log . debug ( \"<STR_LIT>\" % ( i , msghead . topid ) ) \n msgbox . GetUnreadMsg ( ) \n elif msghead . topid in to_steal : \n if msghead . topid not in term_stealed or i > term_stealed [ msghead . topid ] : \n Log . debug ( \"<STR_LIT>\" % ( i , msghead . topid ) ) \n msgtext = msgbox . LoadMsgText ( msghead ) \n self . recv_msg ( self . make_jid ( msghead . id ) , msgtext ) \n term_stealed [ msghead . topid ] = i \n else : \n Log . debug ( \"<STR_LIT>\" % ( i , msghead . topid ) ) \n self . rosters . set_term_read ( self . get_uid ( ) , final_unread ) \n @ xmpp . stanza ( '<STR_LIT>' ) \n def presence ( self , elem ) : \n \"\"\"<STR_LIT>\"\"\" \n Log . warn ( \"<STR_LIT>\" % ( self . authJID , elem_to_str ( elem ) ) ) \n if self . authJID == elem . get ( '<STR_LIT>' ) : \n if ( elem . get ( '<STR_LIT:to>' ) == None or ( not self . authJID . match_bare ( elem . get ( '<STR_LIT:to>' ) ) ) ) : \n return self . send_presence ( elem ) \n self . recv_presence ( elem ) \n def send_presence ( self , elem ) : \n Log . warn ( \"<STR_LIT>\" % ( self . authJID , elem_to_str ( elem ) ) ) \n direct = elem . get ( '<STR_LIT:to>' ) \n if not direct : \n self . rosters . broadcast ( self , elem ) \n if elem . get ( '<STR_LIT:type>' ) != '<STR_LIT>' : \n self . recv_presence ( elem ) \n if not self . probed : \n self . probed = True \n self . rosters . probe ( self ) \n elif not self . rosters . send ( self , direct , elem ) : \n self . send ( direct , elem ) \n def recv_presence ( self , elem ) : \n Log . warn ( \"<STR_LIT>\" % ( self . authJID , elem_to_str ( elem ) ) ) \n if not self . rosters . recv ( self , elem ) : \n Log . warn ( \"<STR_LIT>\" ) \n self . write ( elem ) \n @ xmpp . iq ( '<STR_LIT>' ) \n def roster ( self , iq ) : \n \"\"\"<STR_LIT>\"\"\" \n roster = self . rosters . get ( self ) \n method = getattr ( self , '<STR_LIT>' % iq . get ( '<STR_LIT:type>' ) ) \n return method and method ( iq , roster ) \n def get_roster ( self , iq , roster ) : \n query = self . E . query ( { '<STR_LIT>' : '<STR_LIT>' } ) \n for item in roster . items ( ) : \n query . append ( item ) \n return self . iq ( '<STR_LIT:result>' , iq , query ) \n def set_roster ( self , iq , roster ) : \n query = self . E . query ( xmlns = '<STR_LIT>' ) \n for item in iq [ <NUM_LIT:0> ] : \n result = roster . set ( item ) \n if result is not None : \n query . append ( result ) \n if len ( query ) > <NUM_LIT:0> : \n self . push ( roster , query ) \n return self . iq ( '<STR_LIT:result>' , iq ) \n def push ( self , roster , query ) : \n \"\"\"<STR_LIT>\"\"\" \n for jid in roster . requests ( ) : \n for ( to , route ) in self . routes ( jid ) : \n route . iq ( '<STR_LIT>' , self . ignore , query ) \n def ignore ( self , iq ) : \n \"\"\"<STR_LIT>\"\"\" \n @ xmpp . iq ( '<STR_LIT>' ) \n def vcard ( self , iq ) : \n \"\"\"<STR_LIT>\"\"\" \n if iq . get ( '<STR_LIT:type>' ) == '<STR_LIT>' : \n if ( iq . get ( '<STR_LIT:to>' ) == None ) : \n target = iq . get ( '<STR_LIT>' ) \n else : \n target = iq . get ( '<STR_LIT:to>' ) \n form_target = UCache . UCache . formalize_jid ( target ) \n name = form_target . partition ( '<STR_LIT:@>' ) [ <NUM_LIT:0> ] \n user = UserManager . UserManager . LoadUser ( name ) \n info = user . GetInfo ( ) \n desc = '''<STR_LIT>''' % ( info [ '<STR_LIT>' ] , info [ '<STR_LIT>' ] , info [ '<STR_LIT>' ] , \n info [ '<STR_LIT>' ] , info [ '<STR_LIT>' ] , info [ '<STR_LIT>' ] , info [ '<STR_LIT>' ] ) \n if ( '<STR_LIT>' in info ) : \n desc += \"<STR_LIT>\" % ( info [ '<STR_LIT>' ] . replace ( '<STR_LIT:\\n>' , '<STR_LIT:\\r\\n>' ) ) \n vcard = self . E . vCard ( { '<STR_LIT>' : '<STR_LIT>' } , \n self . E ( '<STR_LIT>' , name ) , \n self . E ( '<STR_LIT>' , Util . Util . RemoveTags ( info [ '<STR_LIT>' ] ) ) , \n self . E ( '<STR_LIT>' , Util . Util . RemoveTags ( desc ) ) ) \n if ( iq . get ( '<STR_LIT:to>' ) == None ) : \n return self . iq ( '<STR_LIT:result>' , iq , vcard ) \n else : \n return self . iq ( '<STR_LIT:result>' , iq , vcard , { '<STR_LIT>' : iq . get ( '<STR_LIT:to>' ) } ) \n @ xmpp . iq ( '<STR_LIT>' % __disco_info_ns__ ) \n def disco_info ( self , iq ) : \n \"\"\"<STR_LIT>\"\"\" \n target = iq . get ( '<STR_LIT:to>' ) \n if ( target . find ( '<STR_LIT:@>' ) < <NUM_LIT:0> ) : \n query = self . E . query ( { '<STR_LIT>' : __disco_info_ns__ } , \n self . E . identity ( { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:type>' : '<STR_LIT>' , \n '<STR_LIT:name>' : Config . Config . GetString ( '<STR_LIT>' , '<STR_LIT>' ) , \n } ) ) \n features = [ __disco_info_ns__ , __disco_items_ns__ , __vcard_ns__ ] \n for feature in features : \n query . append ( self . E . feature ( { '<STR_LIT>' : feature } ) ) \n else : \n query = self . E . query ( { '<STR_LIT>' : __disco_info_ns__ } , \n self . E . identity ( { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:type>' : '<STR_LIT>' , \n '<STR_LIT:name>' : Config . Config . GetString ( '<STR_LIT>' , '<STR_LIT>' ) , \n } ) ) \n features = [ __disco_info_ns__ , __disco_items_ns__ , __vcard_ns__ ] \n for feature in features : \n query . append ( self . E . feature ( { '<STR_LIT>' : feature } ) ) \n return self . iq ( '<STR_LIT:result>' , iq , query , { '<STR_LIT>' : target } ) \n @ xmpp . iq ( '<STR_LIT>' % __disco_items_ns__ ) \n def disco_items ( self , iq ) : \n \"\"\"<STR_LIT>\"\"\" \n target = iq . get ( '<STR_LIT:to>' ) \n if ( target . find ( '<STR_LIT:@>' ) < <NUM_LIT:0> ) : \n query = self . E . query ( { '<STR_LIT>' : __disco_items_ns__ } ) \n else : \n query = self . E . query ( { '<STR_LIT>' : __disco_items_ns__ } ) \n return self . iq ( '<STR_LIT:result>' , iq , query , { '<STR_LIT>' : <mask0> } ) \n", "gt": "target"}
{"input": "\n from __future__ import print_function \n from __future__ import unicode_literals \n from __future__ import division \n from __future__ import absolute_import \n from builtins import range \n from future import standard_library \n standard_library . install_aliases ( ) \n import sys \n PYTHON_VERSION = sys . version_info [ : <NUM_LIT:3> ] \n PY2 = ( PYTHON_VERSION [ <NUM_LIT:0> ] == <NUM_LIT:2> ) \n if PY2 : \n if PYTHON_VERSION < ( <NUM_LIT:2> , <NUM_LIT:7> , <NUM_LIT:9> ) : \n raise Exception ( '<STR_LIT>' ) \n elif PYTHON_VERSION < ( <NUM_LIT:3> , <NUM_LIT:4> ) : \n raise Exception ( '<STR_LIT>' ) \n import hpOneView as hpov \n from pprint import pprint \n import json \n from hpOneView . common import uri \n import hpOneView . profile as profile \n def acceptEULA ( con ) : \n con . get_eula_status ( ) \n try : \n if con . get_eula_status ( ) is True : \n print ( '<STR_LIT>' ) \n con . set_eula ( '<STR_LIT>' ) \n except Exception as e : \n print ( '<STR_LIT>' ) \n print ( e ) \n def login ( con , credential ) : \n try : \n con . login ( credential ) \n except : \n print ( '<STR_LIT>' ) \n def get_eg_uri_from_arg ( srv , name ) : \n if srv and name : \n if name . startswith ( '<STR_LIT>' ) and uri [ '<STR_LIT>' ] in name : \n return name \n else : \n egs = srv . get_enclosure_groups ( ) \n for eg in egs : \n if eg [ '<STR_LIT:name>' ] == name : \n return eg [ '<STR_LIT>' ] \n return None \n def get_sht_from_arg ( srv , name ) : \n if srv and name : \n if name . startswith ( '<STR_LIT>' ) and uri [ '<STR_LIT>' ] in name : \n return name \n else : \n shts = srv . get_server_hardware_types ( ) \n for sht in shts : \n if sht [ '<STR_LIT:name>' ] == name : \n return sht \n return None \n def define_profile_template ( \n srv , \n name , \n desc , \n sp_desc , \n server_hwt , \n enc_group , \n affinity , \n hide_flexnics , \n conn_list , \n fw_settings , \n boot , \n bootmode ) : \n if conn_list : \n conn = json . loads ( open ( conn_list ) . read ( ) ) \n else : \n conn = [ ] \n profile_template = srv . create_server_profile_template ( \n name = name , \n description = desc , \n serverProfileDescription = sp_desc , \n serverHardwareTypeUri = server_hwt , \n enclosureGroupUri = enc_group , \n affinity = affinity , \n hideUnusedFlexNics = hide_flexnics , \n profileConnectionV4 = conn , \n firmwareSettingsV3 = fw_settings , \n bootSettings = boot , \n bootModeSetting = bootmode ) \n if '<STR_LIT>' in profile_template : \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT:name>' ] ) \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT:type>' ] ) \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT:description>' ] ) \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' ) \n for connection in profile_template [ '<STR_LIT>' ] : \n print ( '<STR_LIT>' , connection [ '<STR_LIT:name>' ] ) \n print ( '<STR_LIT>' , connection [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , connection [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' ) \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' ) \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' ) \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , profile_template [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT:\\n>' ) \n else : \n pprint ( profile_template ) \n def main ( ) : \n parser = argparse . ArgumentParser ( add_help = True , \n formatter_class = argparse . RawTextHelpFormatter , \n description = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT:host>' , required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT:user>' , required = False , \n default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT:-c>' , dest = '<STR_LIT>' , required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT:name>' , \n required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , \n required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , \n required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , \n required = False , choices = [ '<STR_LIT>' , '<STR_LIT>' ] , \n default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , \n required = False , choices = [ '<STR_LIT:true>' , '<STR_LIT:false>' ] , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , \n required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , \n action = '<STR_LIT:store_true>' , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n nargs = '<STR_LIT:+>' , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n choices = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , \n default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n choices = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' ] , \n default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n args = parser . parse_args ( ) \n credential = { '<STR_LIT>' : args . user , '<STR_LIT:password>' : args . passwd } \n con = hpov . connection ( args . host ) \n srv = hpov . servers ( con ) \n sts = hpov . settings ( con ) \n if args . proxy : \n con . set_proxy ( args . proxy . split ( '<STR_LIT::>' ) [ <NUM_LIT:0> ] , args . proxy . split ( '<STR_LIT::>' ) [ <NUM_LIT:1> ] ) \n if args . cert : \n con . set_trusted_ssl_bundle ( args . cert ) \n login ( con , credential ) \n acceptEULA ( con ) \n eg_uri = get_eg_uri_from_arg ( srv , args . enc_group ) \n sht = get_sht_from_arg ( srv , args . server_hwt ) \n fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n args . boot_order , args . boot_mode , args . pxe ) \n define_profile_template ( srv , \n args . name , \n args . desc , \n args . sp_desc , \n sht [ '<STR_LIT>' ] , \n eg_uri , \n args . affinity , \n args . hide_flexnics , \n args . conn_list , \n fw_settings , \n boot , \n bootmode ) \n if __name__ == '<STR_LIT:__main__>' : \n import argparse \n sys . exit ( <mask0> ( ) ) \n", "gt": "main"}
{"input": "\n from __future__ import print_function \n from __future__ import unicode_literals \n from __future__ import division \n from __future__ import absolute_import \n from builtins import range \n from future import standard_library \n standard_library . install_aliases ( ) \n import sys \n PYTHON_VERSION = sys . version_info [ : <NUM_LIT:3> ] \n PY2 = ( PYTHON_VERSION [ <NUM_LIT:0> ] == <NUM_LIT:2> ) \n if PY2 : \n if PYTHON_VERSION < ( <NUM_LIT:2> , <NUM_LIT:7> , <NUM_LIT:9> ) : \n raise Exception ( '<STR_LIT>' ) \n elif PYTHON_VERSION < ( <NUM_LIT:3> , <NUM_LIT:4> ) : \n raise Exception ( '<STR_LIT>' ) \n import hpOneView as hpov \n from pprint import pprint \n def acceptEULA ( con ) : \n con . get_eula_status ( ) \n try : \n if con . get_eula_status ( ) is True : \n print ( \"<STR_LIT>\" ) \n con . set_eula ( '<STR_LIT>' ) \n except Exception as e : \n print ( '<STR_LIT>' ) \n print ( e ) \n def login ( con , credential ) : \n try : \n con . login ( credential ) \n except : \n print ( '<STR_LIT>' ) \n def get_address_pools ( con , srv , types ) : \n if types == '<STR_LIT>' or types == '<STR_LIT>' : \n vmac = srv . get_vmac_pool ( ) \n print ( ) \n for key in sorted ( vmac ) : \n print ( '<STR_LIT>' . format ( key , vmac [ key ] ) ) \n if '<STR_LIT>' in vmac : \n for uri in vmac [ '<STR_LIT>' ] : \n ranges = con . get ( uri ) \n print ( '<STR_LIT>' , ranges [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , ranges [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , ranges [ '<STR_LIT>' ] ) \n if types == '<STR_LIT>' or types == '<STR_LIT>' : \n vwwn = srv . get_vwwn_pool ( ) \n print ( ) \n for key in sorted ( vwwn ) : \n print ( '<STR_LIT>' . format ( key , vwwn [ key ] ) ) \n if '<STR_LIT>' in vwwn : \n for uri in vwwn [ '<STR_LIT>' ] : \n ranges = con . get ( uri ) \n print ( '<STR_LIT>' , ranges [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , ranges [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , ranges [ '<STR_LIT>' ] ) \n if types == '<STR_LIT>' or types == '<STR_LIT>' : \n vsn = srv . get_vsn_pool ( ) \n print ( ) \n for key in sorted ( vsn ) : \n print ( '<STR_LIT>' . format ( key , vsn [ key ] ) ) \n if '<STR_LIT>' in vsn : \n for uri in vsn [ '<STR_LIT>' ] : \n ranges = con . get ( uri ) \n print ( '<STR_LIT>' , ranges [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , ranges [ '<STR_LIT>' ] ) \n print ( '<STR_LIT>' , ranges [ '<STR_LIT>' ] ) \n def main ( ) : \n parser = argparse . ArgumentParser ( add_help = True , \n formatter_class = argparse . RawTextHelpFormatter , \n description = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT:host>' , required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT:user>' , required = False , \n default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT:-c>' , dest = '<STR_LIT>' , required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n choices = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n args = parser . parse_args ( ) \n credential = { '<STR_LIT>' : args . domain . upper ( ) , '<STR_LIT>' : args . user , '<STR_LIT:password>' : args . passwd } \n con = hpov . connection ( args . host ) \n srv = hpov . servers ( con ) \n if args . proxy : \n con . set_proxy ( args . proxy . split ( '<STR_LIT::>' ) [ <NUM_LIT:0> ] , args . proxy . split ( '<STR_LIT::>' ) [ <NUM_LIT:1> ] ) \n if args . cert : \n con . set_trusted_ssl_bundle ( args . cert ) \n login ( con , credential ) \n acceptEULA ( con ) \n get_address_pools ( con , srv , args . types ) \n if __name__ == '<STR_LIT:__main__>' : \n import sys \n import argparse \n sys . exit ( <mask0> ( ) ) \n", "gt": "main"}
{"input": "\n from __future__ import print_function \n from __future__ import unicode_literals \n from __future__ import division \n from __future__ import absolute_import \n from builtins import range \n from future import standard_library \n standard_library . install_aliases ( ) \n import sys \n import re \n PYTHON_VERSION = sys . version_info [ : <NUM_LIT:3> ] \n PY2 = ( PYTHON_VERSION [ <NUM_LIT:0> ] == <NUM_LIT:2> ) \n if PY2 : \n if PYTHON_VERSION < ( <NUM_LIT:2> , <NUM_LIT:7> , <NUM_LIT:9> ) : \n raise Exception ( '<STR_LIT>' ) \n elif PYTHON_VERSION < ( <NUM_LIT:3> , <NUM_LIT:4> ) : \n raise Exception ( '<STR_LIT>' ) \n import hpOneView as hpov \n from pprint import pprint \n def acceptEULA ( con ) : \n con . get_eula_status ( ) \n try : \n if con . get_eula_status ( ) is True : \n print ( '<STR_LIT>' ) \n con . set_eula ( '<STR_LIT>' ) \n except Exception as e : \n print ( '<STR_LIT>' ) \n print ( e ) \n def login ( con , credential ) : \n try : \n con . login ( credential ) \n except : \n print ( '<STR_LIT>' ) \n def get_managed_sans ( fcs ) : \n sans = fcs . get_managed_sans ( ) \n pprint ( sans ) \n def main ( ) : \n parser = argparse . ArgumentParser ( add_help = True , \n formatter_class = argparse . RawTextHelpFormatter , \n description = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT:host>' , required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT:user>' , required = False , \n default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT:-c>' , dest = '<STR_LIT>' , required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n args = parser . parse_args ( ) \n credential = { '<STR_LIT>' : args . domain . upper ( ) , '<STR_LIT>' : args . user , '<STR_LIT:password>' : args . passwd } \n con = hpov . connection ( args . host ) \n fcs = hpov . fcsans ( con ) \n if args . proxy : \n con . set_proxy ( args . proxy . split ( '<STR_LIT::>' ) [ <NUM_LIT:0> ] , args . proxy . split ( '<STR_LIT::>' ) [ <NUM_LIT:1> ] ) \n if args . cert : \n con . set_trusted_ssl_bundle ( args . cert ) \n login ( con , credential ) \n acceptEULA ( con ) \n get_managed_sans ( fcs ) \n if __name__ == '<STR_LIT:__main__>' : \n import sys \n import argparse \n sys . exit ( <mask0> ( ) ) \n", "gt": "main"}
{"input": "\n from __future__ import print_function \n from __future__ import unicode_literals \n from __future__ import division \n from __future__ import absolute_import \n from builtins import range \n from future import standard_library \n standard_library . install_aliases ( ) \n import sys \n PYTHON_VERSION = sys . version_info [ : <NUM_LIT:3> ] \n PY2 = ( PYTHON_VERSION [ <NUM_LIT:0> ] == <NUM_LIT:2> ) \n if PY2 : \n if PYTHON_VERSION < ( <NUM_LIT:2> , <NUM_LIT:7> , <NUM_LIT:9> ) : \n raise Exception ( '<STR_LIT>' ) \n elif PYTHON_VERSION < ( <NUM_LIT:3> , <NUM_LIT:4> ) : \n raise Exception ( '<STR_LIT>' ) \n import hpOneView as hpov \n from pprint import pprint \n def acceptEULA ( con ) : \n con . get_eula_status ( ) \n try : \n if con . get_eula_status ( ) is True : \n print ( '<STR_LIT>' ) \n con . set_eula ( '<STR_LIT>' ) \n except Exception as e : \n print ( '<STR_LIT>' ) \n print ( e ) \n def login ( con , credential ) : \n try : \n con . login ( credential ) \n except : \n print ( '<STR_LIT>' ) \n def getpolicy ( sts ) : \n policy = sts . get_storage_vol_template_policy ( ) \n print ( policy [ '<STR_LIT:value>' ] ) \n def main ( ) : \n parser = argparse . ArgumentParser ( add_help = True , \n formatter_class = argparse . RawTextHelpFormatter , \n description = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT:host>' , required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT:user>' , required = False , \n default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = True , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT:-c>' , dest = '<STR_LIT>' , required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n help = '''<STR_LIT>''' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , required = False , \n default = '<STR_LIT>' , \n help = '''<STR_LIT>''' ) \n args = parser . parse_args ( ) \n credential = { '<STR_LIT>' : args . domain . upper ( ) , '<STR_LIT>' : args . user , '<STR_LIT:password>' : args . passwd } \n con = hpov . connection ( args . host ) \n sts = hpov . settings ( con ) \n if args . proxy : \n con . set_proxy ( args . proxy . split ( '<STR_LIT::>' ) [ <NUM_LIT:0> ] , args . proxy . split ( '<STR_LIT::>' ) [ <NUM_LIT:1> ] ) \n if args . cert : \n con . set_trusted_ssl_bundle ( args . cert ) \n login ( con , credential ) \n acceptEULA ( con ) \n getpolicy ( sts ) \n if __name__ == '<STR_LIT:__main__>' : \n import sys \n import argparse \n sys . exit ( <mask0> ( ) ) \n", "gt": "main"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import unicode_literals \n from __future__ import print_function \n from __future__ import division \n from __future__ import absolute_import \n from future import standard_library \n standard_library . install_aliases ( ) \n from pprint import pprint \n __title__ = '<STR_LIT>' \n __version__ = '<STR_LIT>' \n __copyright__ = '<STR_LIT>' '<STR_LIT>' \n __license__ = '<STR_LIT>' \n __status__ = '<STR_LIT>' \n from hpOneView . common import * \n from hpOneView . connection import * \n from hpOneView . activity import * \n from hpOneView . exceptions import * \n class servers ( object ) : \n def __init__ ( self , con ) : \n self . _con = con \n self . _activity = activity ( con ) \n def get_connections ( self , filter = '<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n return get_members ( self . _con . get ( uri [ '<STR_LIT>' ] + filter ) ) \n def get_connection ( self , server ) : \n \"\"\"<STR_LIT>\"\"\" \n body = self . _con . get ( server [ '<STR_LIT>' ] ) \n return body \n def get_server_by_bay ( self , baynum ) : \n servers = get_members ( self . _con . get ( uri [ '<STR_LIT>' ] ) ) \n for server in servers : \n if server [ '<STR_LIT>' ] == baynum : \n return server \n def get_server_by_name ( self , name ) : \n servers = get_members ( self . _con . get ( uri [ '<STR_LIT>' ] ) ) \n for server in servers : \n if server [ '<STR_LIT:name>' ] == name : \n return server \n def get_available_servers ( self , server_hardware_type = None , \n enclosure_group = None , server_profile = None ) : \n filters = [ ] \n if server_hardware_type : \n filters . append ( '<STR_LIT>' + server_hardware_type [ '<STR_LIT>' ] ) \n if enclosure_group : \n filters . append ( '<STR_LIT>' + enclosure_group [ '<STR_LIT>' ] ) \n if server_profile : \n filters . append ( '<STR_LIT>' + server_profile [ '<STR_LIT>' ] ) \n query_string = '<STR_LIT>' \n if filters : \n query_string = '<STR_LIT:?>' + '<STR_LIT:&>' . join ( filters ) \n return self . _con . get ( uri [ '<STR_LIT>' ] + query_string ) \n def get_servers ( self ) : \n return get_members ( self . _con . get ( uri [ '<STR_LIT>' ] ) ) \n def get_utilization ( self , server ) : \n \"\"\"<STR_LIT>\"\"\" \n body = self . _con . get ( server [ '<STR_LIT>' ] + '<STR_LIT>' ) \n return body \n def get_env_conf ( self , server ) : \n \"\"\"<STR_LIT>\"\"\" \n body = self . _con . get ( server [ '<STR_LIT>' ] + '<STR_LIT>' ) \n return body \n def set_server_powerstate ( self , server , state , force = False , blocking = True , \n verbose = False ) : \n if state == '<STR_LIT>' and force is True : \n powerRequest = make_powerstate_dict ( '<STR_LIT>' , '<STR_LIT>' ) \n elif state == '<STR_LIT>' and force is False : \n powerRequest = make_powerstate_dict ( '<STR_LIT>' , '<STR_LIT>' ) \n elif state == '<STR_LIT>' : \n powerRequest = make_powerstate_dict ( '<STR_LIT>' , '<STR_LIT>' ) \n elif state == '<STR_LIT>' : \n powerRequest = make_powerstate_dict ( '<STR_LIT>' , '<STR_LIT>' ) \n task , body = self . _con . put ( server [ '<STR_LIT>' ] + '<STR_LIT>' , powerRequest ) \n if blocking is True : \n task = self . _activity . wait4task ( task , tout = <NUM_LIT> , verbose = verbose ) \n return task \n def delete_server ( self , server , force = False , blocking = True , verbose = False ) : \n if force : \n task , body = self . _con . delete ( server [ '<STR_LIT>' ] + '<STR_LIT>' ) \n else : \n task , body = self . _con . delete ( server [ '<STR_LIT>' ] ) \n if blocking is True : \n task = self . _activity . wait4task ( task , tout = <NUM_LIT> , verbose = verbose ) \n return task \n def update_server ( self , server ) : \n task , body = self . _con . put ( server [ '<STR_LIT>' ] , server ) \n return body \n def add_server ( self , server , blocking = True , verbose = False ) : \n task , body = self . _con . post ( uri [ '<STR_LIT>' ] , server ) \n if blocking is True : \n task = self . _activity . wait4task ( task , tout = <NUM_LIT> , verbose = verbose ) \n if '<STR_LIT:type>' in task and task [ '<STR_LIT:type>' ] . startswith ( '<STR_LIT>' ) : \n entity = self . _activity . get_task_associated_resource ( task ) \n server = self . _con . get ( entity [ '<STR_LIT>' ] ) \n return server \n return task \n def get_server_schema ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _con . get ( uri [ '<STR_LIT>' ] + '<STR_LIT>' ) \n def get_bios ( self , server ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _con . get ( server [ '<STR_LIT>' ] + '<STR_LIT>' ) \n def get_ilo_sso_url ( self , server ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _con . get ( server [ '<STR_LIT>' ] + '<STR_LIT>' ) \n def get_java_remote_console_url ( self , server ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _con . get ( server [ '<STR_LIT>' ] + '<STR_LIT>' ) \n def get_remote_console_url ( self , server ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _con . get ( server [ '<STR_LIT>' ] + '<STR_LIT>' ) \n def get_server_hardware_types ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return get_members ( body ) \n def remove_server_hardware_type ( self , server_hardware_type , force = False , blocking = True , verbose = False ) : \n \"\"\"<STR_LIT>\"\"\" \n if force : \n task , body = self . _con . delete ( server_hardware_type [ '<STR_LIT>' ] + '<STR_LIT>' ) \n else : \n task , body = self . _con . delete ( server_hardware_type [ '<STR_LIT>' ] ) \n if blocking is True : \n task = self . _activity . wait4task ( task , tout = <NUM_LIT> , verbose = verbose ) \n return task \n def get_server_type_schema ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _con . get ( uri [ '<STR_LIT>' ] + '<STR_LIT>' ) \n def get_server_hardware_type ( self , server_type ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _con . get ( server_type [ '<STR_LIT>' ] ) \n def set_server_hardware_type ( self , server_hardware_type , name , description ) : \n \"\"\"<STR_LIT>\"\"\" \n request = make_server_type_dict ( name , description ) \n task , body = self . _con . put ( server_hardware_type [ '<STR_LIT>' ] , request ) \n return task \n def create_server_profile ( self , \n affinity = '<STR_LIT>' , \n biosSettings = None , \n bootSettings = None , \n bootModeSetting = None , \n profileConnectionV4 = None , \n description = None , \n firmwareSettingsV3 = None , \n hideUnusedFlexNics = True , \n localStorageSettingsV3 = None , \n macType = '<STR_LIT>' , \n name = None , \n sanStorageV3 = None , \n serialNumber = None , \n serialNumberType = '<STR_LIT>' , \n serverHardwareTypeUri = None , \n serverHardwareUri = None , \n serverProfileTemplateUri = None , \n uuid = None , \n wwnType = '<STR_LIT>' , \n blocking = True , verbose = False ) : \n \"\"\"<STR_LIT>\"\"\" \n profile = make_ServerProfileV5 ( affinity , biosSettings , bootSettings , \n bootModeSetting , profileConnectionV4 , \n description , firmwareSettingsV3 , \n hideUnusedFlexNics , \n localStorageSettingsV3 , macType , name , \n sanStorageV3 , serialNumber , \n serialNumberType , serverHardwareTypeUri , \n serverHardwareUri , \n serverProfileTemplateUri , uuid , wwnType ) \n task , body = self . _con . post ( uri [ '<STR_LIT>' ] , profile ) \n if profile [ '<STR_LIT>' ] is None : \n tout = <NUM_LIT> \n else : \n tout = <NUM_LIT> \n if blocking is True : \n task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n if '<STR_LIT:type>' in task and task [ '<STR_LIT:type>' ] . startswith ( '<STR_LIT>' ) : \n entity = self . _activity . get_task_associated_resource ( task ) \n profile = self . _con . get ( entity [ '<STR_LIT>' ] ) \n return profile \n return task \n def post_server_profile ( self , profile , blocking = True , verbose = False ) : \n \"\"\"<STR_LIT>\"\"\" \n task , body = self . _con . post ( uri [ '<STR_LIT>' ] , profile ) \n if profile [ '<STR_LIT>' ] is None : \n tout = <NUM_LIT> \n else : \n tout = <NUM_LIT> \n if blocking is True : \n task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n if '<STR_LIT:type>' in task and task [ '<STR_LIT:type>' ] . startswith ( '<STR_LIT>' ) : \n entity = self . _activity . get_task_associated_resource ( task ) \n profile = self . _con . get ( entity [ '<STR_LIT>' ] ) \n return profile \n return task \n def remove_server_profile ( self , profile , force = False , blocking = True , verbose = False ) : \n if force : \n task , body = self . _con . delete ( profile [ '<STR_LIT>' ] + '<STR_LIT>' ) \n else : \n task , body = self . _con . delete ( profile [ '<STR_LIT>' ] ) \n if blocking is True : \n task = self . _activity . wait4task ( task , tout = <NUM_LIT> , verbose = verbose ) \n return task \n def get_server_profiles ( self ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return get_members ( body ) \n def update_server_profile ( self , profile , blocking = True , verbose = False ) : \n task , body = self . _con . put ( profile [ '<STR_LIT>' ] , profile ) \n try : \n if profile [ '<STR_LIT>' ] [ '<STR_LIT>' ] is None : \n tout = <NUM_LIT> \n else : \n tout = <NUM_LIT> \n except Exception : \n tout = <NUM_LIT> \n if blocking is True : \n task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n profileResource = self . _activity . get_task_associated_resource ( task ) \n profile = self . _con . get ( profileResource [ '<STR_LIT>' ] ) \n return profile \n def update_server_profile_from_template ( self , profile , blocking = True , verbose = False ) : \n patch_request = [ { '<STR_LIT>' : '<STR_LIT:replace>' , '<STR_LIT:path>' : '<STR_LIT>' , '<STR_LIT:value>' : '<STR_LIT>' } ] \n task , body = self . _con . patch ( profile [ '<STR_LIT>' ] , patch_request ) \n try : \n if profile [ '<STR_LIT>' ] [ '<STR_LIT>' ] is None : \n tout = <NUM_LIT> \n else : \n tout = <NUM_LIT> \n except Exception : \n tout = <NUM_LIT> \n if blocking is True : \n task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n profileResource = self . _activity . get_task_associated_resource ( task ) \n profile = self . _con . get ( profileResource [ '<STR_LIT>' ] ) \n return profile \n def get_server_profile_by_name ( self , name ) : \n body = self . _con . get_entity_byfield ( uri [ '<STR_LIT>' ] , '<STR_LIT:name>' , name ) \n return body \n def get_profile_message ( self , profile ) : \n \"\"\"<STR_LIT>\"\"\" \n message = self . _con . get ( profile [ '<STR_LIT>' ] + '<STR_LIT>' ) \n return message \n def get_profile_compliance_preview ( self , profile ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _con . get ( profile [ '<STR_LIT>' ] + '<STR_LIT>' ) \n def create_server_profile_template ( \n self , \n name = None , \n description = None , \n serverProfileDescription = None , \n serverHardwareTypeUri = None , \n enclosureGroupUri = None , \n affinity = None , \n hideUnusedFlexNics = None , \n profileConnectionV4 = None , \n firmwareSettingsV3 = None , \n bootSettings = None , \n bootModeSetting = None , \n blocking = True , \n verbose = False ) : \n \"\"\"<STR_LIT>\"\"\" \n profile_template = make_ServerProfileTemplateV1 ( name , \n description , \n serverProfileDescription , \n serverHardwareTypeUri , \n enclosureGroupUri , \n affinity , \n hideUnusedFlexNics , \n profileConnectionV4 , \n firmwareSettingsV3 , \n bootSettings , \n bootModeSetting ) \n task , body = self . _con . post ( uri [ '<STR_LIT>' ] , profile_template ) \n tout = <NUM_LIT> \n if blocking is True : \n task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n if '<STR_LIT:type>' in task and task [ '<STR_LIT:type>' ] . startswith ( '<STR_LIT>' ) : \n entity = self . _activity . get_task_associated_resource ( task ) \n profile_template = self . _con . get ( entity [ '<STR_LIT>' ] ) \n return profile_template \n return task \n def remove_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n task , body = self . _con . delete ( profile_template [ '<STR_LIT>' ] ) \n if blocking is True : \n task = self . _activity . wait4task ( task , tout = <NUM_LIT> , verbose = verbose ) \n return task \n return body \n def get_server_profile_templates ( self ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return get_members ( body ) \n def get_server_profile_template_by_name ( self , name ) : \n body = self . _con . get_entity_byfield ( uri [ '<STR_LIT>' ] , '<STR_LIT:name>' , name ) \n return body \n def update_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n task , body = self . _con . put ( profile_template [ '<STR_LIT>' ] , profile_template ) \n tout = <NUM_LIT> \n if blocking is True : \n task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n profileTemplateResource = self . _activity . get_task_associated_resource ( task ) \n profile = self . _con . get ( profileTemplateResource [ '<STR_LIT>' ] ) \n return profile_template \n def get_server_profile_from_template ( self , profile_template ) : \n profile = self . _con . get ( profile_template [ '<STR_LIT>' ] + '<STR_LIT>' ) \n return profile \n def get_enclosures ( self ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return get_members ( body ) \n def add_enclosure ( self , enclosure , blocking = True , verbose = False ) : \n task , body = self . _con . post ( uri [ '<STR_LIT>' ] , enclosure ) \n if enclosure [ '<STR_LIT:state>' ] is '<STR_LIT>' : \n tout = <NUM_LIT> \n elif enclosure [ '<STR_LIT>' ] is None : \n tout = <NUM_LIT> \n else : \n tout = <NUM_LIT> \n if blocking is True : \n task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n if '<STR_LIT:type>' in task and task [ '<STR_LIT:type>' ] . startswith ( '<STR_LIT>' ) : \n entity = self . _activity . get_task_associated_resource ( task ) \n enclosure = self . _con . get ( entity [ '<STR_LIT>' ] ) \n return enclosure \n return task \n def remove_enclosure ( self , enclosure , force = False , blocking = True , \n verbose = False ) : \n if force : \n task , body = self . _con . delete ( enclosure [ '<STR_LIT>' ] + '<STR_LIT>' ) \n else : \n task , body = self . _con . delete ( enclosure [ '<STR_LIT>' ] ) \n if blocking is True : \n task = self . _activity . wait4task ( task , tout = <NUM_LIT> , verbose = verbose ) \n return task \n def create_enclosure_group ( self , associatedLIGs , name , \n powerMode = '<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n task , body = self . _con . post ( uri [ '<STR_LIT>' ] , egroup ) \n return body \n def delete_enclosure_group ( self , egroup ) : \n self . _con . delete ( egroup [ '<STR_LIT>' ] ) \n def get_enclosure_groups ( self ) : \n return get_members ( self . _con . get ( uri [ '<STR_LIT>' ] ) ) \n def update_enclosure_group ( self , enclosuregroup ) : \n task , body = self . _con . put ( enclosuregroup [ '<STR_LIT>' ] , enclosuregroup ) \n return body \n def get_pool ( self , pooltype ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] + '<STR_LIT:/>' + pooltype ) \n return body \n def get_vmac_pool ( self ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return body \n def get_vwwn_pool ( self ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return body \n def get_vsn_pool ( self ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return body \n def get_profile_networks ( self ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return body \n def get_profile_schema ( self ) : \n return self . _con . get ( uri [ '<STR_LIT>' ] ) \n def get_profile_available_servers ( self ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return body \n def get_profile_available_storage_systems ( self ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return body \n def get_profile_ports ( self ) : \n body = self . _con . get ( uri [ '<STR_LIT>' ] ) \n return body \n def allocate_pool_ids ( self , url , count ) : \n allocatorUrl = '<STR_LIT>' % url \n allocatorBody = { '<STR_LIT:count>' : count } \n task , body = self . _con . put ( allocatorUrl , allocatorBody ) \n return body \n def release_pool_ids ( self , url , idList ) : \n collectorUrl = '<STR_LIT>' % url \n collectorBody = { '<STR_LIT>' : idList } \n task , body = self . _con . put ( collectorUrl , collectorBody ) \n return body \n def allocate_range_ids ( self , allocatorUrl , count ) : \n task , body = self . _con . put ( allocatorUrl , { '<STR_LIT:count>' : count } ) \n return body \n def release_range_ids ( self , collectorUrl , idList ) : \n task , body = self . _con . put ( collectorUrl , { '<STR_LIT>' : idList } ) \n return body \n def enable_range ( self , url ) : \n prange = self . _con . get ( url ) \n prange [ '<STR_LIT>' ] = True \n task , body = self . _con . put ( url , prange ) \n return body \n def disable_range ( self , url ) : \n prange = self . _con . get ( url ) \n prange [ '<STR_LIT>' ] = False \n task , body = self . _con . put ( url , prange ) \n return <mask0> \n", "gt": "body"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import os \n import re \n import sys \n import json \n import locale \n import zipfile \n import logging \n import textwrap \n import validictory \n from . sharedtypes import JSONEncoder \n from ilorest . rest . v1_helper import ( RisObject ) \n LOGGER = logging . getLogger ( __name__ ) \n class ValidationError ( Exception ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class SchemaValidationError ( ValidationError ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class RegistryValidationError ( ValidationError ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , msg , regentry = None , selector = None ) : \n super ( RegistryValidationError , self ) . __init__ ( msg ) \n self . reg = regentry \n self . sel = selector \n class UnknownValidatorError ( Exception ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n class ValidationManager ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , local_path , bios_local_path , romfamily = None , biosversion = None , iloversion = None , monolith = None ) : \n super ( ValidationManager , self ) . __init__ ( ) \n defaultilopath = None \n defaultbiospath = None \n schemamainfolder = None \n if float ( iloversion ) < <NUM_LIT> : \n if os . name == '<STR_LIT>' : \n defaultilopath = r\"<STR_LIT>\" \n defaultbiospath = r\"<STR_LIT>\" \n schemamainfolder = os . path . dirname ( sys . executable ) \n else : \n defaultilopath = \"<STR_LIT>\" \n defaultbiospath = \"<STR_LIT>\" \n schemamainfolder = \"<STR_LIT>\" \n if not local_path : \n if not os . path . isdir ( defaultilopath ) : \n ilozip = self . getiloziplocation ( schemamainfolder , iloversion ) \n if ilozip and os . path . exists ( ilozip ) : \n with zipfile . ZipFile ( os . path . join ( schemamainfolder , ilozip ) , \"<STR_LIT:r>\" ) as zfile : \n zfile . extractall ( os . path . join ( schemamainfolder , \"<STR_LIT>\" ) ) \n local_path = os . path . join ( schemamainfolder , u'<STR_LIT>' ) \n else : \n raise SchemaValidationError ( u'<STR_LIT>' '<STR_LIT>' '<STR_LIT>' '<STR_LIT>' ) \n else : \n local_path = defaultilopath \n else : \n if not os . path . isdir ( local_path ) : \n raise SchemaValidationError ( u\"<STR_LIT>\" \n \"<STR_LIT>\" % local_path ) \n if not bios_local_path : \n if not os . path . isdir ( defaultbiospath ) : \n bioszip = self . getbiosziplocation ( romfamily , schemamainfolder , biosversion ) \n if bioszip and os . path . exists ( bioszip ) : \n with zipfile . ZipFile ( \n os . path . join ( schemamainfolder , bioszip ) , \"<STR_LIT:r>\" ) as zfile : \n zfile . extractall ( os . path . join ( schemamainfolder , \"<STR_LIT>\" ) ) \n bios_local_path = os . path . join ( schemamainfolder , u'<STR_LIT>' ) \n else : \n raise SchemaValidationError ( u'<STR_LIT>' '<STR_LIT>' '<STR_LIT>' '<STR_LIT>' ) \n else : \n bios_local_path = defaultbiospath \n else : \n if not os . path . isdir ( bios_local_path ) : \n raise SchemaValidationError ( u\"<STR_LIT>\" \"<STR_LIT>\" % bios_local_path ) \n else : \n if monolith . is_redfish : \n local_path = \"<STR_LIT>\" \n bios_local_path = \"<STR_LIT>\" \n else : \n local_path = \"<STR_LIT>\" \n bios_local_path = \"<STR_LIT>\" \n self . _schema_locations = list ( ) \n self . _classes = list ( ) \n self . _registry_locations = list ( ) \n self . _classes_registry = list ( ) \n self . _bios_schema_locations = list ( ) \n self . _bios_classes = list ( ) \n self . _bios_registry_locations = list ( ) \n self . _bios_classes_registry = list ( ) \n self . _ilo_messages = list ( ) \n self . _base_messages = list ( ) \n self . _hpcommon_messages = list ( ) \n self . _iloevents_messages = list ( ) \n self . _errors = list ( ) \n if monolith . is_redfish : \n self . _schemaid = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n self . _regid = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n else : \n self . _schemaid = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n self . _regid = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n if local_path : \n self . add_location ( schema_path = local_path , monolith = monolith ) \n self . add_location ( registry_path = local_path , monolith = monolith ) \n if bios_local_path : \n self . add_location ( schema_path = bios_local_path , biossection = True , monolith = monolith ) \n self . add_location ( registry_path = bios_local_path , biossection = True , monolith = monolith ) \n def getbiosziplocation ( self , romfamily , schemadir , biosversion ) : \n \"\"\"<STR_LIT>\"\"\" \n foundfile = None \n currentver = None \n tempstr = \"<STR_LIT>\" + romfamily + \"<STR_LIT:->\" + biosversion \n for _ , _ , filenames in os . walk ( schemadir ) : \n for filename in filenames : \n if tempstr in filename : \n regentry = re . compile ( '<STR_LIT>' % tempstr ) \n mentry = regentry . search ( filename ) \n if mentry and currentver : \n if currentver < mentry . group ( <NUM_LIT:1> ) : \n foundfile = filename \n currentver = mentry . group ( <NUM_LIT:1> ) \n elif mentry and not currentver : \n foundfile = filename \n currentver = mentry . group ( <NUM_LIT:1> ) \n if foundfile : \n return os . path . join ( schemadir , foundfile ) \n else : \n return None \n def getiloziplocation ( self , schemadir , iloversion ) : \n \"\"\"<STR_LIT>\"\"\" \n if float ( iloversion ) < <NUM_LIT> : \n iloversion = u'<STR_LIT>' \n tempstr = \"<STR_LIT>\" + iloversion . replace ( \"<STR_LIT:.>\" , \"<STR_LIT>\" ) \n for _ , _ , filenames in os . walk ( schemadir ) : \n for filename in filenames : \n if tempstr in filename : \n return os . path . join ( schemadir , filename ) \n return None \n def add_location ( self , schema_path = None , registry_path = None , \n biossection = False , monolith = None ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n if schema_path : \n if not biossection : \n self . _schema_locations . append ( schema_path ) \n self . _update_location_map ( monolith = monolith ) \n else : \n self . _bios_schema_locations . append ( schema_path ) \n self . _update_location_map ( biossection = True , monolith = monolith ) \n elif registry_path : \n if not biossection : \n self . _registry_locations . append ( registry_path ) \n self . _update_location_map ( registries = True , monolith = monolith ) \n else : \n self . _bios_registry_locations . append ( registry_path ) \n self . _update_location_map ( biossection = True , registries = True , monolith = monolith ) \n else : \n raise ValueError ( u\"<STR_LIT>\" \"<STR_LIT>\" ) \n def _update_location_map ( self , biossection = False , registries = False , \n monolith = None ) : \n \"\"\"<STR_LIT>\"\"\" \n locationslist = list ( ) \n pathjoinstr = None \n if not registries : \n pathjoinstr = \"<STR_LIT>\" \n if not biossection : \n locationslist = self . _schema_locations \n else : \n locationslist = self . _bios_schema_locations \n else : \n pathjoinstr = \"<STR_LIT>\" \n if not biossection : \n locationslist = self . _registry_locations \n else : \n locationslist = self . _bios_registry_locations \n for location in locationslist : \n if monolith : \n self . new_load_file ( monolith , root = location , biossection = biossection , registries = registries ) \n elif self . _is_local ( location ) : \n for root , _ , filenames in os . walk ( os . path . join ( location , \n pathjoinstr ) ) : \n for filename in filenames : \n fqpath = os . path . abspath ( os . path . join ( os . path . normpath ( root ) , filename ) ) \n if self . load_file ( fqpath , root = location , biossection = biossection , registries = registries ) : \n LOGGER . info ( \"<STR_LIT>\" , fqpath ) \n def new_load_file ( self , monolith , root = None , biossection = False , registries = False ) : \n \"\"\"<STR_LIT>\"\"\" \n classesdataholder = [ ] \n for itemtype in monolith . types : \n if itemtype . startswith ( \"<STR_LIT>\" ) or itemtype . startswith ( \"<STR_LIT>\" ) and u'<STR_LIT>' in monolith . types [ itemtype ] : \n for instance in monolith . types [ itemtype ] [ u'<STR_LIT>' ] : \n if self . _schemaid [ <NUM_LIT:0> ] in instance . resp . request . path . lower ( ) or self . _regid [ <NUM_LIT:0> ] in instance . resp . request . path . lower ( ) : \n if not registries and self . _schemaid [ <NUM_LIT:0> ] in instance . resp . request . path . lower ( ) : \n if classesdataholder : \n if self . _schemaid [ <NUM_LIT:1> ] in instance . resp . dict : \n classesdataholder [ <NUM_LIT:0> ] [ self . _schemaid [ <NUM_LIT:1> ] ] . extend ( instance . resp . dict [ self . _schemaid [ <NUM_LIT:1> ] ] ) \n else : \n classesdataholder . append ( instance . resp . dict ) \n elif registries and self . _regid [ <NUM_LIT:0> ] in instance . resp . request . path . lower ( ) : \n if classesdataholder : \n if monolith . is_redfish : \n classesdataholder [ <NUM_LIT:0> ] [ self . _regid [ <NUM_LIT:1> ] ] . extend ( instance . resp . dict [ self . _regid [ <NUM_LIT:1> ] ] ) \n else : \n classesdataholder . append ( instance . resp . dict ) \n if classesdataholder : \n classesdataholder = classesdataholder [ <NUM_LIT:0> ] \n try : \n if monolith . _typestring in classesdataholder and ( '<STR_LIT>' in classesdataholder [ monolith . _typestring ] or ( '<STR_LIT>' in classesdataholder [ monolith . _typestring ] and monolith . is_redfish ) ) : \n newclass = Classes . parse ( classesdataholder ) \n newclass . set_root ( root ) \n if not registries : \n if not biossection : \n self . _classes . append ( newclass ) \n else : \n self . _bios_classes . append ( newclass ) \n else : \n if not biossection : \n self . _classes_registry . append ( newclass ) \n else : \n self . _bios_classes_registry . append ( newclass ) \n except BaseException : \n pass \n else : \n pass \n def load_file ( self , filepath , root = None , biossection = False , \n registries = False , datareturn = False ) : \n \"\"\"<STR_LIT>\"\"\" \n result = False \n if os . path . isfile ( filepath ) : \n try : \n filehand = open ( filepath , '<STR_LIT:r>' ) \n data = json . load ( filehand ) \n if datareturn : \n return data \n if u'<STR_LIT>' in data and data [ u'<STR_LIT>' ] == '<STR_LIT>' : \n if biossection and registries : \n itemsreturn = self . bios_helper_function ( data , root ) \n data [ \"<STR_LIT>\" ] = itemsreturn \n newclass = Classes . parse ( data ) \n newclass . set_root ( root ) \n if not registries : \n if not biossection : \n self . _classes . append ( newclass ) \n else : \n self . _bios_classes . append ( newclass ) \n else : \n if not biossection : \n self . _classes_registry . append ( newclass ) \n else : \n self . _bios_classes_registry . append ( newclass ) \n result = True \n except BaseException : \n pass \n else : \n pass \n finally : \n filehand . close ( ) \n return result \n def bios_helper_function ( self , data , root ) : \n \"\"\"<STR_LIT>\"\"\" \n folderentries = data [ \"<STR_LIT>\" ] \n datareturn = list ( ) \n for entry in folderentries [ \"<STR_LIT>\" ] : \n joinstr = entry [ \"<STR_LIT>\" ] \n if os . name == '<STR_LIT>' and joinstr [ <NUM_LIT:0> ] == \"<STR_LIT:/>\" : \n joinstr = joinstr . replace ( \"<STR_LIT:/>\" , \"<STR_LIT:\\\\>\" ) [ <NUM_LIT:1> : ] \n elif joinstr [ <NUM_LIT:0> ] == \"<STR_LIT:/>\" : \n joinstr = joinstr [ <NUM_LIT:1> : ] \n for root , _ , filenames in os . walk ( os . path . join ( root , joinstr ) ) : \n for filename in filenames : \n fqpath = os . path . abspath ( os . path . join ( os . path . normpath ( root ) , filename ) ) \n datareturn . append ( self . load_file ( fqpath , root = root , biossection = True , registries = True , datareturn = True ) ) \n LOGGER . info ( \"<STR_LIT>\" , fqpath ) \n return datareturn \n def validate ( self , item , selector = None , currdict = None , monolith = None , \n newarg = None , checkall = False , regloc = None ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n if regloc : \n attrreg = RepoRegistryEntry ( regloc ) \n else : \n attrreg = self . find_schema ( schname = item [ monolith . _typestring ] ) \n if attrreg : \n tempvalue = attrreg . validate ( item , self . _errors , selector = selector , \n currdict = currdict , monolith = monolith , \n newarg = newarg , checkall = checkall ) \n if tempvalue is True : \n return False \n elif tempvalue : \n self . _errors = tempvalue \n return True \n def bios_validate ( self , item , regname , selector = None , currdict = None , \n checkall = False , monolith = None ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n attrreg = self . find_bios_registry ( regname = regname ) \n if attrreg : \n tempvalue = attrreg . validate_bios_version ( item , self . _errors , selector = selector , currdict = currdict , checkall = checkall , monolith = monolith ) \n if tempvalue == '<STR_LIT>' : \n return tempvalue \n elif tempvalue == '<STR_LIT>' : \n return tempvalue \n elif tempvalue : \n self . _errors = tempvalue \n return True \n def bios_info ( self , item , regname , selector ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n attrreg = self . find_bios_registry ( regname = regname ) \n if attrreg : \n if attrreg . validate_bios_version ( item , self . _errors , selector = selector ) : \n return False \n return True \n def find_schema ( self , schname ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n for cls in self . _classes : \n found = cls . find_schema ( schname = schname ) \n if found : \n return found \n return None \n def find_registry ( self , regname ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n for cls in self . _classes_registry : \n found = cls . find_registry ( regname = regname ) \n if found : \n return found \n return None \n def find_bios_registry ( self , regname ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n for cls in self . _bios_classes_registry : \n found = cls . find_bios_registry ( regname = regname ) \n if found : \n return found \n return None \n def get_errors ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _errors \n def _is_local ( self , path ) : \n \"\"\"<STR_LIT>\"\"\" \n if u'<STR_LIT>' in path : \n return False \n return True \n class Classes ( RisObject ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , item ) : \n super ( Classes , self ) . __init__ ( item ) \n self . _root = None \n def set_root ( self , newroot ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _root = newroot \n def find_schema ( self , schname ) : \n \"\"\"<STR_LIT>\"\"\" \n result = None \n if hasattr ( self , '<STR_LIT>' ) and isinstance ( self . Items , list ) : \n for entry in self . Items : \n if entry and u'<STR_LIT>' in entry and entry [ u'<STR_LIT>' ] . lower ( ) == schname . lower ( ) : \n regentry = RepoRegistryEntry . parse ( entry ) \n regentry . set_root ( self . _root ) \n result = regentry \n break \n elif hasattr ( self , '<STR_LIT>' ) and isinstance ( self . Members , list ) : \n schname = schname . split ( '<STR_LIT:.>' ) [ - <NUM_LIT:1> ] \n for entry in self . Members : \n schlink = entry [ u'<STR_LIT>' ] . split ( '<STR_LIT:/>' ) \n schlink = schlink [ len ( schlink ) - <NUM_LIT:2> ] \n if schname . lower ( ) == schlink . lower ( ) : \n result = entry \n break \n return result \n def find_registry ( self , regname ) : \n \"\"\"<STR_LIT>\"\"\" \n result = None \n if hasattr ( self , '<STR_LIT>' ) and isinstance ( self . Items , list ) : \n for entry in self . Items : \n if entry and ( u'<STR_LIT>' in entry and \n entry [ u'<STR_LIT>' ] . lower ( ) . startswith ( regname . lower ( ) ) ) : \n regentry = RepoRegistryEntry . parse ( entry ) \n regentry . set_root ( self . _root ) \n result = regentry \n break \n elif hasattr ( self , '<STR_LIT>' ) and isinstance ( self . Members , list ) : \n regname = regname . split ( '<STR_LIT:.>' ) [ - <NUM_LIT:1> ] \n for entry in self . Members : \n reglink = entry [ u'<STR_LIT>' ] . split ( '<STR_LIT:/>' ) \n reglink = reglink [ len ( reglink ) - <NUM_LIT:2> ] \n if regname . lower ( ) == reglink . lower ( ) : \n result = entry \n break \n return result \n def find_bios_schema ( self , schname ) : \n \"\"\"<STR_LIT>\"\"\" \n result = None \n if hasattr ( self , '<STR_LIT>' ) and isinstance ( self . Items , list ) : \n for entry in self . Items : \n if ( u'<STR_LIT>' in entry and entry [ u'<STR_LIT>' ] . lower ( ) == \n schname . lower ( ) ) : \n regentry = RepoRegistryEntry . parse ( entry ) \n regentry . set_root ( self . _root ) \n result = regentry \n break \n return result \n def find_bios_registry ( self , regname ) : \n \"\"\"<STR_LIT>\"\"\" \n result = None \n if hasattr ( self , '<STR_LIT>' ) and isinstance ( self . Items , list ) : \n for entry in self . Items : \n if entry and ( u'<STR_LIT>' in entry and regname . lower ( ) in entry [ u'<STR_LIT>' ] . lower ( ) ) : \n regentry = RepoRegistryEntry . parse ( entry ) \n regentry . set_root ( self . _root ) \n result = regentry \n break \n return result \n class RepoBaseEntry ( RisObject ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , d ) : \n super ( RepoBaseEntry , self ) . __init__ ( d ) \n self . _root = None \n def set_root ( self , newroot ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _root = newroot \n def _read_location_file ( self , currloc , errlist ) : \n \"\"\"<STR_LIT>\"\"\" \n result = None \n if u'<STR_LIT>' in currloc : \n root = os . path . normpath ( self . _root ) \n xref = os . path . normpath ( currloc . Uri . extref ) . lstrip ( os . path . sep ) \n fqpath = os . path . join ( root , xref ) \n if not os . path . isfile ( fqpath ) : \n errlist . append ( SchemaValidationError ( \n u\"<STR_LIT>\" % fqpath ) ) \n else : \n result = None \n if fqpath . endswith ( '<STR_LIT>' ) : \n result = open ( fqpath ) . read ( ) \n return result \n class RepoRegistryEntry ( RepoBaseEntry ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , d ) : \n super ( RepoRegistryEntry , self ) . __init__ ( d ) \n def validate ( self , tdict , errlist = None , selector = None , currdict = None , checkall = False , monolith = None , newarg = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if not errlist : \n errlist = list ( ) \n reg = self . get_registry_model ( errlist = errlist , currdict = currdict , monolith = monolith , newarg = newarg ) \n if reg and not checkall : \n try : \n if reg [ selector ] . readonly : \n return True \n except BaseException : \n pass \n else : \n pass \n results = reg . validate_attribute_values ( tdict ) \n errlist . extend ( results ) \n elif checkall and selector is None : \n results = reg . validate_attribute_values ( tdict ) \n errlist . extend ( results ) \n else : \n errlist . append ( RegistryValidationError ( u'<STR_LIT>' '<STR_LIT>' ) ) \n if errlist : \n return errlist \n def validate_bios_version ( self , tdict , errlist = None , selector = None , checkall = False , currdict = None , monolith = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if not errlist : \n errlist = list ( ) \n reg = self . get_registry_model_bios_version ( errlist = errlist , currdict = currdict , monolith = monolith ) \n if reg and not checkall : \n for item in reg . Attributes : \n if not item [ \"<STR_LIT:Name>\" ] == selector : \n continue \n if item [ \"<STR_LIT>\" ] is True : \n return '<STR_LIT>' \n try : \n if item [ \"<STR_LIT>\" ] is True : \n return '<STR_LIT>' \n except BaseException : \n continue \n else : \n continue \n results = reg . validate_att_val_bios ( tdict ) \n errlist . extend ( results ) \n elif checkall and selector is None : \n results = reg . validate_att_val_bios ( tdict ) \n errlist . extend ( results ) \n else : \n errlist . append ( RegistryValidationError ( u'<STR_LIT>' '<STR_LIT>' ) ) \n if errlist : \n return errlist \n def validate_deprecated ( self , tdict , errlist = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if not errlist : \n errlist = list ( ) \n if not hasattr ( self , u'<STR_LIT>' ) : \n errlist . append ( RegistryValidationError ( u'<STR_LIT>' '<STR_LIT>' ) ) \n return errlist \n currloc = None \n defloc = None \n langcode = '<STR_LIT>' \n for loc in self . Location : \n for loclang in loc . keys ( ) : \n if loclang . lower ( ) == langcode . lower ( ) : \n currloc = loc [ loclang ] \n break \n elif loclang . lower ( ) == u'<STR_LIT:default>' : \n defloc = loc [ loclang ] \n if not currloc : \n currloc = defloc \n if not currloc : \n errlist . append ( RegistryValidationError ( u'<STR_LIT>' '<STR_LIT>' ) ) \n return \n location_file = self . _read_location_file ( currloc , errlist = errlist ) \n if not location_file : \n errlist . append ( RegistryValidationError ( u'<STR_LIT>' ) ) \n else : \n jsonreg = json . loads ( location_file ) \n if u'<STR_LIT>' in jsonreg : \n if u'<STR_LIT>' in jsonreg and jsonreg [ u'<STR_LIT>' ] == u'<STR_LIT>' : \n reg = HpPropertiesRegistry . parse ( jsonreg [ u'<STR_LIT>' ] ) \n results = reg . validate_attribute_values ( tdict ) \n errlist . extend ( results ) \n def get_registry_model ( self , currdict = None , monolith = None , errlist = None , skipcommit = False , searchtype = None , newarg = None , latestschema = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if not errlist : \n errlist = list ( ) \n if not hasattr ( self , u'<STR_LIT>' ) : \n errlist . append ( RegistryValidationError ( \n u'<STR_LIT>' ) ) \n return None \n currloc = None \n defloc = \"<STR_LIT>\" \n langcode = list ( locale . getdefaultlocale ( ) ) \n if not langcode [ <NUM_LIT:0> ] : \n langcode [ <NUM_LIT:0> ] = \"<STR_LIT>\" \n for loc in self . Location : \n locationlanguage = loc [ \"<STR_LIT>\" ] . lower ( ) \n locationlanguage = locationlanguage . replace ( \"<STR_LIT:->\" , \"<STR_LIT:_>\" ) \n if locationlanguage in langcode [ <NUM_LIT:0> ] . lower ( ) : \n currloc = loc \n break \n if not currloc : \n currloc = defloc \n if not currloc : \n errlist . append ( RegistryValidationError ( u'<STR_LIT>' '<STR_LIT:location>' ) ) \n return None \n if not searchtype : \n searchtype = \"<STR_LIT>\" \n location_file = None \n if currdict and monolith : \n for itemtype in monolith . types : \n if itemtype . lower ( ) . startswith ( searchtype . lower ( ) ) and u'<STR_LIT>' in monolith . types [ itemtype ] : \n for instance in monolith . types [ itemtype ] [ u'<STR_LIT>' ] : \n try : \n if monolith . is_redfish : \n currtype = currdict [ instance . _typestring ] . split ( '<STR_LIT:#>' ) [ - <NUM_LIT:1> ] \n currtype = currtype . split ( '<STR_LIT:.>' ) [ <NUM_LIT:0> ] + '<STR_LIT:.>' \n else : \n currtype = currdict [ instance . _typestring ] \n if latestschema : \n currtype = currdict [ instance . _typestring ] . split ( '<STR_LIT:.>' ) [ : <NUM_LIT:1> ] \n insttype = instance . resp . dict [ \"<STR_LIT:title>\" ] . split ( '<STR_LIT:.>' ) [ : <NUM_LIT:1> ] \n if currtype == insttype or currtype == instance . resp . dict [ \"<STR_LIT>\" ] . split ( '<STR_LIT:.>' ) [ : <NUM_LIT:1> ] : \n location_file = instance . resp . dict \n break \n elif searchtype == \"<STR_LIT>\" and instance . resp . dict [ \"<STR_LIT:title>\" ] . startswith ( currtype ) or \"<STR_LIT>\" in instance . resp . dict . keys ( ) and currdict [ instance . _typestring ] == instance . resp . dict [ \"<STR_LIT>\" ] : \n location_file = instance . resp . dict \n break \n elif searchtype != \"<STR_LIT>\" and currdict [ instance . _typestring ] in instance . resp . dict [ \"<STR_LIT>\" ] : \n location_file = instance . resp . dict \n break \n except BaseException : \n pass \n else : \n pass \n if location_file : \n break \n else : \n location_file = self . _read_location_file ( currloc , errlist = errlist ) \n if not location_file : \n errlist . append ( RegistryValidationError ( u'<STR_LIT>' ) ) \n else : \n if currdict and monolith : \n jsonreg = json . loads ( json . dumps ( location_file , indent = <NUM_LIT:2> , cls = JSONEncoder ) ) \n else : \n jsonreg = json . loads ( location_file ) \n if skipcommit : \n return jsonreg [ \"<STR_LIT>\" ] \n if u'<STR_LIT>' in jsonreg : \n regitem = jsonreg [ u'<STR_LIT>' ] \n reg = HpPropertiesRegistry . parse ( regitem ) \n if newarg : \n regcopy = reg \n for arg in newarg [ : - <NUM_LIT:1> ] : \n try : \n if '<STR_LIT>' in regcopy [ arg ] . iterkeys ( ) and ( '<STR_LIT>' in regcopy [ arg ] . iterkeys ( ) ) : \n regcopy [ arg ] [ '<STR_LIT>' ] . update ( regcopy [ arg ] [ '<STR_LIT>' ] ) \n regcopy = regcopy [ arg ] [ \"<STR_LIT>\" ] \n for pattern in regcopy . iterkeys ( ) : \n test = re . compile ( pattern ) \n nextarg = newarg [ newarg . index ( arg ) + <NUM_LIT:1> ] \n match = test . match ( nextarg ) \n if match : \n regcopy [ nextarg ] = regcopy . pop ( pattern ) \n break \n elif '<STR_LIT>' in regcopy [ arg ] : \n oneof = regcopy [ arg ] [ '<STR_LIT>' ] \n for item in oneof : \n regcopy = item [ '<STR_LIT>' ] \n if not arg == newarg [ - <NUM_LIT:1> ] : \n try : \n nextitem = newarg [ newarg . index ( arg ) + <NUM_LIT:1> ] \n regcopy [ nextitem ] \n break \n except Exception : \n continue \n else : \n regcopy = regcopy [ arg ] [ \"<STR_LIT>\" ] \n except Exception : \n try : \n regcopy = regcopy [ arg ] [ '<STR_LIT>' ] \n for pattern in regcopy . iterkeys ( ) : \n test = re . compile ( pattern ) \n nextarg = newarg [ newarg . index ( arg ) + <NUM_LIT:1> ] \n match = test . match ( nextarg ) \n if match : \n patterninfo = regcopy . pop ( pattern ) \n regcopy [ nextarg ] = patterninfo \n except BaseException : \n return None \n reg = regcopy \n return reg \n return None \n def get_registry_model_bios_version ( self , currdict = None , monolith = None , errlist = None ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n if not errlist : \n errlist = list ( ) \n if not hasattr ( self , u'<STR_LIT>' ) : \n errlist . append ( RegistryValidationError ( \n u'<STR_LIT>' ) ) \n return None \n currloc = None \n defloc = \"<STR_LIT>\" \n langcode = list ( locale . getdefaultlocale ( ) ) \n if not langcode [ <NUM_LIT:0> ] : \n langcode [ <NUM_LIT:0> ] = \"<STR_LIT>\" \n for loc in self . Location : \n locationlanguage = loc [ \"<STR_LIT>\" ] . lower ( ) \n locationlanguage = locationlanguage . replace ( \"<STR_LIT:->\" , \"<STR_LIT:_>\" ) \n if locationlanguage in langcode [ <NUM_LIT:0> ] . lower ( ) : \n currloc = loc \n break \n if not currloc : \n currloc = defloc \n if not currloc : \n errlist . append ( RegistryValidationError ( \n u'<STR_LIT>' ) ) \n return None \n location_file = None \n if currdict and monolith : \n for itemtype in monolith . types : \n if \"<STR_LIT>\" in itemtype and u'<STR_LIT>' in monolith . types [ itemtype ] : \n for instance in monolith . types [ itemtype ] [ u'<STR_LIT>' ] : \n location_file = instance . resp . dict \n break \n if location_file : \n break \n else : \n location_file = self . _read_location_file ( currloc , errlist = errlist ) \n if not location_file : \n errlist . append ( RegistryValidationError ( u'<STR_LIT>' ) ) \n else : \n if currdict and monolith : \n jsonreg = json . loads ( json . dumps ( location_file , indent = <NUM_LIT:2> , cls = JSONEncoder ) ) \n else : \n jsonreg = json . loads ( location_file ) \n if u'<STR_LIT>' in jsonreg : \n regitem = jsonreg [ u'<STR_LIT>' ] \n reg = HpPropertiesRegistry . parse ( regitem ) \n return reg \n return None \n class RepoSchemaEntry ( RepoBaseEntry ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , item ) : \n super ( RepoSchemaEntry , self ) . __init__ ( item ) \n self . _root = None \n def set_root ( self , newroot ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _root = newroot \n def _read_location_file ( self , currloc , errlist ) : \n \"\"\"<STR_LIT>\"\"\" \n if u'<STR_LIT>' in currloc and u'<STR_LIT>' in currloc : \n fqpath = os . path . join ( self . _root , currloc . ArchiveUri . xref . lstrip ( os . path . sep ) ) \n if not os . path . isfile ( fqpath ) : \n errlist . append ( SchemaValidationError ( u\"<STR_LIT>\" \"<STR_LIT>\" % fqpath ) ) \n else : \n archive_file = currloc . ArchiveFile \n archive_fh = None \n result = None \n if fqpath . endswith ( '<STR_LIT>' ) : \n archive_fh = zipfile . ZipFile ( fqpath ) \n infolist = archive_fh . infolist ( ) \n for i in infolist : \n if i . filename . lower ( ) == archive_file . lower ( ) : \n jsonsch_fh = archive_fh . open ( i ) \n result = jsonsch_fh . read ( ) \n jsonsch_fh . close ( ) \n archive_fh . close ( ) \n return result \n def validate ( self , tdict , errlist = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if not errlist : \n errlist = list ( ) \n result = list ( ) \n if not hasattr ( self , u'<STR_LIT>' ) : \n result . append ( SchemaValidationError ( u'<STR_LIT>' '<STR_LIT>' ) ) \n return result \n currloc = None \n defloc = None \n langcode = '<STR_LIT>' \n for loc in self . Location : \n for loclang in loc . keys ( ) : \n if loclang . lower ( ) == langcode . lower ( ) : \n currloc = loc [ loclang ] \n break \n elif loclang . lower ( ) == u'<STR_LIT:default>' : \n defloc = loc [ loclang ] \n if not currloc : \n currloc = defloc \n if not currloc : \n result . append ( SchemaValidationError ( \n u'<STR_LIT>' ) ) \n return \n location_file = self . _read_location_file ( currloc , errlist = result ) \n if not location_file : \n result . append ( SchemaValidationError ( u'<STR_LIT>' ) ) \n else : \n jsonsch = json . loads ( location_file ) \n validictory . validate ( tdict , jsonsch ) \n class HpPropertiesRegistry ( RisObject ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , d ) : \n super ( HpPropertiesRegistry , self ) . __init__ ( d ) \n def validate_attribute_values ( self , tdict ) : \n \"\"\"<STR_LIT>\"\"\" \n result = list ( ) \n for tkey in tdict : \n try : \n if self [ tkey ] and hasattr ( self [ tkey ] , \"<STR_LIT:type>\" ) : \n temp = self . validate_attribute ( self [ tkey ] , tdict [ tkey ] , tkey ) \n for err in temp : \n if isinstance ( err , RegistryValidationError ) : \n if err . reg : \n err . sel = tkey \n result . extend ( temp ) \n except Exception : \n pass \n return result \n def validate_att_val_bios ( self , tdict ) : \n \"\"\"<STR_LIT>\"\"\" \n result = list ( ) \n for tkey in tdict : \n for item in self . Attributes : \n try : \n if item [ \"<STR_LIT:Name>\" ] == tkey and hasattr ( item , \"<STR_LIT>\" ) : \n temp = self . validate_attribute ( item , tdict [ tkey ] , tkey ) \n for err in temp : \n if isinstance ( err , RegistryValidationError ) : \n if err . reg : \n err . sel = tkey \n result . extend ( temp ) \n break \n except Exception : \n pass \n return result \n def get_validator ( self , attrname , newargs = None , oneof = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if oneof : \n self = oneof \n if newargs : \n for arg in newargs : \n try : \n self = self [ '<STR_LIT>' ] \n except Exception : \n pass \n if not hasattr ( self , arg ) : \n return None \n elif not arg == newargs [ - <NUM_LIT:1> ] : \n self = self [ arg ] \n if not hasattr ( self , attrname ) : \n return None \n validator = None \n if EnumValidator . is_type ( self [ attrname ] ) : \n validator = EnumValidator . parse ( self [ attrname ] ) \n elif StringValidator . is_type ( self [ attrname ] ) : \n validator = StringValidator . parse ( self [ attrname ] ) \n elif ObjectValidator . is_type ( self [ attrname ] ) : \n validator = ObjectValidator . parse ( self [ attrname ] ) \n elif IntegerValidator . is_type ( self [ attrname ] ) : \n validator = IntegerValidator . parse ( self [ attrname ] ) \n elif BoolValidator . is_type ( self [ attrname ] ) : \n validator = BoolValidator . parse ( self [ attrname ] ) \n elif PasswordValidator . is_type ( self [ attrname ] ) : \n validator = PasswordValidator . parse ( self [ attrname ] ) \n elif u'<STR_LIT>' in self [ attrname ] . keys ( ) : \n for item in self [ attrname ] [ '<STR_LIT>' ] : \n validator = self . get_validator ( attrname , newargs , HpPropertiesRegistry ( { attrname : item } ) ) \n if validator : \n break \n return validator \n def get_validator_bios ( self , attrname ) : \n \"\"\"<STR_LIT>\"\"\" \n for item in self . Attributes : \n if item [ \"<STR_LIT:Name>\" ] == attrname : \n validator = None \n if EnumValidator . is_type ( item ) : \n validator = EnumValidator . parse ( item ) \n elif StringValidator . is_type ( item ) : \n validator = StringValidator . parse ( item ) \n elif IntegerValidator . is_type ( item ) : \n validator = IntegerValidator . parse ( item ) \n elif BoolValidator . is_type ( item ) : \n validator = BoolValidator . parse ( item ) \n elif ObjectValidator . is_type ( item ) : \n validator = ObjectValidator . parse ( item ) \n elif PasswordValidator . is_type ( item ) : \n validator = PasswordValidator . parse ( item ) \n return validator \n return None \n def validate_attribute ( self , attrentry , attrval , name ) : \n \"\"\"<STR_LIT>\"\"\" \n result = list ( ) \n validator = None \n if EnumValidator . is_type ( attrentry ) : \n validator = EnumValidator . parse ( attrentry ) \n elif StringValidator . is_type ( attrentry ) : \n validator = StringValidator . parse ( attrentry ) \n elif IntegerValidator . is_type ( attrentry ) : \n validator = IntegerValidator . parse ( attrentry ) \n elif BoolValidator . is_type ( attrentry ) : \n validator = BoolValidator . parse ( attrentry ) \n elif ObjectValidator . is_type ( attrentry ) : \n validator = ObjectValidator . parse ( attrentry ) \n elif PasswordValidator . is_type ( attrentry ) : \n validator = PasswordValidator . parse ( attrentry ) \n else : \n raise UnknownValidatorError ( attrentry ) \n if validator : \n result . extend ( validator . validate ( attrval , name ) ) \n return result \n class BaseValidator ( RisObject ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , d ) : \n super ( BaseValidator , self ) . __init__ ( d ) \n def validate ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n raise RuntimeError ( u'<STR_LIT>' '<STR_LIT:class>' ) \n class EnumValidator ( BaseValidator ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , d ) : \n super ( EnumValidator , self ) . __init__ ( d ) \n @ staticmethod \n def is_type ( attrentry ) : \n \"\"\"<STR_LIT>\"\"\" \n if u'<STR_LIT:type>' in attrentry : \n if isinstance ( attrentry [ u'<STR_LIT:type>' ] , list ) : \n for item in attrentry [ u'<STR_LIT:type>' ] : \n if item . lower ( ) == u'<STR_LIT>' : \n return True \n elif u'<STR_LIT>' in attrentry and item . lower ( ) == u'<STR_LIT:string>' : \n return True \n elif u'<STR_LIT>' in attrentry and attrentry [ u'<STR_LIT:type>' ] == \"<STR_LIT>\" : \n for key , value in attrentry [ u'<STR_LIT>' ] . iteritems ( ) : \n if key . lower ( ) == \"<STR_LIT:type>\" and value . lower ( ) == u'<STR_LIT:string>' : \n return True \n else : \n if attrentry [ u'<STR_LIT:type>' ] . lower ( ) == u'<STR_LIT>' : \n return True \n elif u'<STR_LIT>' in attrentry and attrentry [ u'<STR_LIT:type>' ] . lower ( ) == u'<STR_LIT:string>' : \n return True \n elif u'<STR_LIT>' in attrentry : \n if attrentry [ u'<STR_LIT>' ] . lower ( ) == u'<STR_LIT>' : \n return True \n return False \n def validate ( self , newval , name ) : \n \"\"\"<STR_LIT>\"\"\" \n result = list ( ) \n try : \n for possibleval in self . enum : \n if possibleval . lower ( ) == newval . lower ( ) : \n return result \n except Exception : \n for possibleval in self . Value : \n if possibleval . ValueName . lower ( ) == str ( newval ) . lower ( ) : \n return result \n result . append ( RegistryValidationError ( u\"<STR_LIT>\" \n \"<STR_LIT>\" % ( newval , name ) , \n regentry = self ) ) \n return result \n def print_help ( self , name , out = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n wrapper = textwrap . TextWrapper ( ) \n wrapper . initial_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n wrapper . subsequent_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT:%s>' % name ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:description>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:type>' in self and isinstance ( self [ u'<STR_LIT:type>' ] , list ) : \n out . write ( u'<STR_LIT>' ) \n for item in self [ u'<STR_LIT:type>' ] : \n out . write ( '<STR_LIT>' % wrapper . fill ( '<STR_LIT:%s>' % item ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT:type>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n out . write ( u'<STR_LIT>' ) \n try : \n for possibleval in self . enum : \n out . write ( '<STR_LIT>' % possibleval ) \n except Exception : \n for possibleval in self . Value : \n out . write ( '<STR_LIT>' % possibleval ) \n out . write ( '<STR_LIT:\\n>' ) \n class BoolValidator ( BaseValidator ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , d ) : \n super ( BoolValidator , self ) . __init__ ( d ) \n @ staticmethod \n def is_type ( attrentry ) : \n \"\"\"<STR_LIT>\"\"\" \n if u'<STR_LIT:type>' in attrentry : \n if isinstance ( attrentry [ u'<STR_LIT:type>' ] , list ) : \n for item in attrentry [ u'<STR_LIT:type>' ] : \n if item . lower ( ) == u'<STR_LIT>' : \n return True \n elif attrentry [ u'<STR_LIT:type>' ] == \"<STR_LIT>\" : \n for key , value in attrentry [ u'<STR_LIT>' ] . iteritems ( ) : \n if key . lower ( ) == \"<STR_LIT:type>\" and value . lower ( ) == u'<STR_LIT>' : \n return True \n else : \n if attrentry [ u'<STR_LIT:type>' ] . lower ( ) == u'<STR_LIT>' : \n return True \n elif u'<STR_LIT>' in attrentry : \n if attrentry [ u'<STR_LIT>' ] . lower ( ) == u'<STR_LIT>' : \n return True \n return False \n def validate ( self , newval , name ) : \n \"\"\"<STR_LIT>\"\"\" \n result = list ( ) \n if newval is False or newval is True : \n return result \n result . append ( \n RegistryValidationError ( \n u\"<STR_LIT>\" % ( newval , name ) , \n regentry = self \n ) \n ) \n return result \n def print_help ( self , name , out = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n wrapper = textwrap . TextWrapper ( ) \n wrapper . initial_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n wrapper . subsequent_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT:%s>' % name ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:description>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:type>' in self and isinstance ( self [ u'<STR_LIT:type>' ] , list ) : \n out . write ( u'<STR_LIT>' ) \n for item in self [ u'<STR_LIT:type>' ] : \n out . write ( '<STR_LIT>' % wrapper . fill ( '<STR_LIT:%s>' % item ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT:type>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT>' ) \n out . write ( '<STR_LIT:\\n>' ) \n class StringValidator ( BaseValidator ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , d ) : \n super ( StringValidator , self ) . __init__ ( d ) \n @ staticmethod \n def is_type ( attrentry ) : \n \"\"\"<STR_LIT>\"\"\" \n if u'<STR_LIT:type>' in attrentry : \n if isinstance ( attrentry [ u'<STR_LIT:type>' ] , list ) : \n for item in attrentry [ u'<STR_LIT:type>' ] : \n if item . lower ( ) == u'<STR_LIT:string>' : \n return True \n elif attrentry [ u'<STR_LIT:type>' ] == \"<STR_LIT>\" : \n for key , value in attrentry [ u'<STR_LIT>' ] . iteritems ( ) : \n if key . lower ( ) == \"<STR_LIT:type>\" and u'<STR_LIT:string>' in value : \n return True \n else : \n if attrentry [ u'<STR_LIT:type>' ] . lower ( ) == u'<STR_LIT:string>' : \n return True \n elif u'<STR_LIT>' in attrentry : \n if attrentry [ u'<STR_LIT>' ] . lower ( ) == u'<STR_LIT:string>' : \n return True \n return False \n def validate ( self , newval , name ) : \n \"\"\"<STR_LIT>\"\"\" \n result = list ( ) \n if u'<STR_LIT>' in self : \n if len ( newval ) < int ( self [ u'<STR_LIT>' ] ) : \n result . append ( RegistryValidationError ( \n u\"<STR_LIT>\" % \n ( self . Name , int ( self [ u'<STR_LIT>' ] ) ) , regentry = self ) ) \n if u'<STR_LIT>' in self : \n if len ( newval ) > int ( self [ u'<STR_LIT>' ] ) : \n result . append ( RegistryValidationError ( \n u\"<STR_LIT>\" % \n ( self . Name , int ( self [ u'<STR_LIT>' ] ) ) , regentry = self ) ) \n if u'<STR_LIT>' in self : \n if self [ u'<STR_LIT>' ] : \n pat = re . compile ( self [ u'<STR_LIT>' ] ) \n if newval and not pat . match ( newval ) : \n result . append ( RegistryValidationError ( \n u\"<STR_LIT>\" \n \"<STR_LIT>\" % ( self ) , regentry = self ) ) \n return result \n def print_help ( self , name , out = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n wrapper = textwrap . TextWrapper ( ) \n wrapper . initial_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n wrapper . subsequent_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT:%s>' % name ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:description>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:type>' in self and isinstance ( self [ u'<STR_LIT:type>' ] , list ) : \n out . write ( u'<STR_LIT>' ) \n for item in self [ u'<STR_LIT:type>' ] : \n out . write ( '<STR_LIT>' % wrapper . fill ( '<STR_LIT:%s>' % item ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT:type>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n class IntegerValidator ( BaseValidator ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , d ) : \n super ( IntegerValidator , self ) . __init__ ( d ) \n @ staticmethod \n def is_type ( attrentry ) : \n \"\"\"<STR_LIT>\"\"\" \n if u'<STR_LIT:type>' in attrentry : \n if isinstance ( attrentry [ u'<STR_LIT:type>' ] , list ) : \n for item in attrentry [ u'<STR_LIT:type>' ] : \n if item . lower ( ) == u'<STR_LIT>' or item . lower ( ) == u'<STR_LIT>' : \n return True \n elif attrentry [ u'<STR_LIT:type>' ] == \"<STR_LIT>\" : \n for key , value in attrentry [ u'<STR_LIT>' ] . iteritems ( ) : \n if key . lower ( ) == \"<STR_LIT:type>\" : \n if value . lower ( ) == u'<STR_LIT>' or value . lower ( ) == u'<STR_LIT>' : \n return True \n else : \n if attrentry [ u'<STR_LIT:type>' ] . lower ( ) == u'<STR_LIT>' or attrentry [ u'<STR_LIT:type>' ] . lower ( ) . lower ( ) == u'<STR_LIT>' : \n return True \n elif u'<STR_LIT>' in attrentry : \n if attrentry [ u'<STR_LIT>' ] . lower ( ) == u'<STR_LIT>' : \n return True \n return False \n def validate ( self , newval , name ) : \n \"\"\"<STR_LIT>\"\"\" \n result = list ( ) \n intval = int ( newval ) \n pat = re . compile ( r'<STR_LIT>' ) \n if newval and not pat . match ( intval ) : \n result . append ( \n RegistryValidationError ( \n u\"<STR_LIT>\" % ( self ) , \n regentry = self \n ) \n ) \n return result \n if u'<STR_LIT>' in self : \n if intval < int ( self [ u'<STR_LIT>' ] ) : \n result . append ( RegistryValidationError ( u\"<STR_LIT>\" \"<STR_LIT>\" % ( self . Name , int ( self [ u'<STR_LIT>' ] ) ) , regentry = self ) ) \n if u'<STR_LIT>' in self : \n if intval > int ( self [ u'<STR_LIT>' ] ) : \n result . append ( RegistryValidationError ( u\"<STR_LIT>\" \"<STR_LIT>\" % ( self . Name , int ( self [ u'<STR_LIT>' ] ) ) , regentry = self ) ) \n return result \n def print_help ( self , name , out = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n wrapper = textwrap . TextWrapper ( ) \n wrapper . initial_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n wrapper . subsequent_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT:%s>' % name ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:description>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:type>' in self and isinstance ( self [ u'<STR_LIT:type>' ] , list ) : \n out . write ( u'<STR_LIT>' ) \n for item in self [ u'<STR_LIT:type>' ] : \n out . write ( '<STR_LIT>' % wrapper . fill ( '<STR_LIT:%s>' % item ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT:type>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n class ObjectValidator ( BaseValidator ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , d ) : \n super ( ObjectValidator , self ) . __init__ ( d ) \n @ staticmethod \n def is_type ( attrentry ) : \n \"\"\"<STR_LIT>\"\"\" \n if u'<STR_LIT:type>' in attrentry : \n if isinstance ( attrentry [ u'<STR_LIT:type>' ] , list ) : \n for item in attrentry [ u'<STR_LIT:type>' ] : \n if item . lower ( ) == u'<STR_LIT:object>' : \n return True \n elif attrentry [ u'<STR_LIT:type>' ] == \"<STR_LIT>\" : \n for key , value in attrentry [ u'<STR_LIT>' ] . iteritems ( ) : \n if key . lower ( ) == \"<STR_LIT:type>\" and value . lower ( ) == u'<STR_LIT:object>' : \n return True \n elif key . lower ( ) == \"<STR_LIT>\" : \n try : \n if value [ <NUM_LIT:0> ] [ u'<STR_LIT:type>' ] == u'<STR_LIT:object>' : \n return True \n except Exception : \n continue \n else : \n if attrentry [ u'<STR_LIT:type>' ] . lower ( ) == u'<STR_LIT:object>' : \n return True \n elif u'<STR_LIT>' in attrentry : \n if attrentry [ u'<STR_LIT>' ] . lower ( ) == u'<STR_LIT:object>' : \n return True \n return False \n def validate ( self , newval , name ) : \n \"\"\"<STR_LIT>\"\"\" \n result = list ( ) \n return result \n def print_help ( self , name , out = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n wrapper = textwrap . TextWrapper ( ) \n wrapper . initial_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n wrapper . subsequent_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT:%s>' % name ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:description>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:type>' in self and isinstance ( self [ u'<STR_LIT:type>' ] , list ) : \n out . write ( u'<STR_LIT>' ) \n for item in self [ u'<STR_LIT:type>' ] : \n out . write ( '<STR_LIT>' % wrapper . fill ( '<STR_LIT:%s>' % item ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT:type>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n class PasswordValidator ( BaseValidator ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , d ) : \n super ( PasswordValidator , self ) . __init__ ( d ) \n @ staticmethod \n def is_type ( attrentry ) : \n \"\"\"<STR_LIT>\"\"\" \n if u'<STR_LIT:type>' in attrentry : \n if isinstance ( attrentry [ u'<STR_LIT:type>' ] , list ) : \n for item in attrentry [ u'<STR_LIT:type>' ] : \n if item . lower ( ) == u'<STR_LIT:password>' : \n return True \n elif attrentry [ u'<STR_LIT:type>' ] == \"<STR_LIT>\" : \n for key , value in attrentry [ u'<STR_LIT>' ] . iteritems ( ) : \n if key . lower ( ) == \"<STR_LIT:type>\" and value . lower ( ) == u'<STR_LIT:password>' : \n return True \n else : \n if attrentry [ u'<STR_LIT:type>' ] . lower ( ) == u'<STR_LIT:password>' : \n return True \n elif u'<STR_LIT>' in attrentry : \n if attrentry [ u'<STR_LIT>' ] . lower ( ) == u'<STR_LIT:password>' : \n return True \n return False \n def validate ( self , newval , name ) : \n \"\"\"<STR_LIT>\"\"\" \n result = list ( ) \n if newval is None : \n return result \n if u'<STR_LIT>' in self : \n if len ( newval ) < int ( self [ u'<STR_LIT>' ] ) : \n result . append ( RegistryValidationError ( u\"<STR_LIT>\" \"<STR_LIT>\" % ( self . Name , int ( self [ u'<STR_LIT>' ] ) ) , regentry = self ) ) \n if u'<STR_LIT>' in self : \n if len ( newval ) > int ( self [ u'<STR_LIT>' ] ) : \n result . append ( RegistryValidationError ( u\"<STR_LIT>\" \"<STR_LIT>\" % ( self . Name , int ( self [ u'<STR_LIT>' ] ) ) , regentry = self ) ) \n if u'<STR_LIT>' in self : \n if self [ u'<STR_LIT>' ] : \n pat = re . compile ( self [ u'<STR_LIT>' ] ) \n if newval and not pat . match ( newval ) : \n result . append ( RegistryValidationError ( u\"<STR_LIT>\" \"<STR_LIT>\" \"<STR_LIT>\" % ( self ) , regentry = self ) ) \n return result \n def print_help ( self , name , out = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n wrapper = textwrap . TextWrapper ( ) \n wrapper . initial_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n wrapper . subsequent_indent = '<STR_LIT:U+0020>' * <NUM_LIT:4> \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT:%s>' % name ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:description>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT:type>' in self and isinstance ( self [ u'<STR_LIT:type>' ] , list ) : \n out . write ( u'<STR_LIT>' ) \n for item in self [ u'<STR_LIT:type>' ] : \n out . write ( '<STR_LIT>' % wrapper . fill ( '<STR_LIT:%s>' % item ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT:type>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n if u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( '<STR_LIT:\\n>' ) \n elif u'<STR_LIT>' in self : \n out . write ( u'<STR_LIT>' ) \n out . write ( '<STR_LIT:%s>' % wrapper . fill ( '<STR_LIT>' % self ) ) \n out . write ( <mask0> ) \n", "gt": "'<STR_LIT:\\n>'"}
{"input": "\n from . constants import MILLI_MICROS , SECOND_MICROS , MINUTE_MICROS \n import calendar \n from datetime import datetime \n from dateutil import parser \n from dateutil . tz import tzlocal \n from . error import TimeConstructionError \n from . sanedelta import SaneDelta \n import pytz \n MICROS_TRANSLATIONS = ( \n ( ( '<STR_LIT:m>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , MINUTE_MICROS ) , \n ( ( '<STR_LIT:s>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , SECOND_MICROS ) , \n ( ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , MILLI_MICROS ) , \n ( ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , <NUM_LIT:1> ) ) \n MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n class SaneTime ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , * args , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n super ( time , self ) . __init__ ( ) \n uss = set ( ) \n tzs = set ( ) \n naive_dt = None \n avoid_localize = False \n for k , v in kwargs . iteritems ( ) : \n if k in ( '<STR_LIT>' , '<STR_LIT>' ) : \n tzs . add ( SaneTime . to_timezone ( v ) ) \n elif k in MICROS_TRANSLATION_HASH : \n uss . add ( MICROS_TRANSLATION_HASH [ k ] * v ) \n else : \n raise TimeConstructionError ( \"<STR_LIT>\" % ( k , v ) ) \n args = list ( args ) \n if len ( args ) > <NUM_LIT:2> and len ( args ) <= <NUM_LIT:8> : \n args = [ datetime ( * args ) ] \n if len ( args ) == <NUM_LIT:2> : \n tzs . add ( SaneTime . to_timezone ( args . pop ( ) ) ) \n if len ( args ) == <NUM_LIT:1> : \n arg = args . pop ( ) \n if hasattr ( arg , '<STR_LIT>' ) : \n uss . add ( int ( arg ) ) \n if hasattr ( arg , '<STR_LIT>' ) : tzs . add ( arg . tz ) \n elif isinstance ( arg , basestring ) : \n parts = arg . strip ( ) . split ( '<STR_LIT:U+0020>' ) \n if len ( parts ) > <NUM_LIT:1> and parts [ - <NUM_LIT:1> ] . startswith ( '<STR_LIT:+>' ) : \n try : \n tzs . add ( SaneTime . to_timezone ( parts [ - <NUM_LIT:1> ] [ <NUM_LIT:1> : ] ) ) \n arg = '<STR_LIT:U+0020>' . join ( parts [ : - <NUM_LIT:1> ] ) \n except : pass \n utc = arg . endswith ( '<STR_LIT>' ) or arg . endswith ( '<STR_LIT>' ) \n arg = parser . parse ( arg ) \n if arg . tzinfo : \n if utc : \n tzs . add ( pytz . utc ) \n arg = arg . replace ( tzinfo = None ) \n elif isinstance ( arg . tzinfo , tzlocal ) : \n arg = arg . replace ( tzinfo = None ) \n else : \n avoid_localize = True \n arg = arg . astimezone ( pytz . utc ) . replace ( tzinfo = None ) \n if type ( arg ) == datetime : \n naive_dt = arg \n if naive_dt . tzinfo : \n tzs . add ( SaneTime . to_timezone ( str ( naive_dt . tzinfo ) ) ) \n naive_dt = naive_dt . replace ( tzinfo = None ) \n if len ( tzs ) > <NUM_LIT:1> : \n raise TimeConstructionError ( \"<STR_LIT>\" % ( tzs ) ) \n self . tz = len ( tzs ) and tzs . pop ( ) or pytz . utc \n if naive_dt : \n if avoid_localize : \n uss . add ( SaneTime . utc_datetime_to_us ( naive_dt ) ) \n else : \n uss . add ( SaneTime . utc_datetime_to_us ( self . tz . localize ( naive_dt ) . astimezone ( pytz . utc ) ) ) \n if len ( uss ) == <NUM_LIT:0> : \n uss . add ( SaneTime . utc_datetime_to_us ( datetime . utcnow ( ) ) ) \n if len ( uss ) > <NUM_LIT:1> : \n raise TimeConstructionError ( \"<STR_LIT>\" % ( uss ) ) \n self . us = uss . pop ( ) \n if len ( args ) > <NUM_LIT:0> : \n raise TimeConstructionError ( \"<STR_LIT>\" ) \n @ property \n def ms ( self ) : return self . us / MILLI_MICROS \n epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n @ property \n def s ( self ) : return self . us / SECOND_MICROS \n epoch_seconds = epoch_secs = seconds = secs = s \n @ property \n def m ( self ) : return self . us / MINUTE_MICROS \n epoch_minutes = epoch_mins = minutes = mins = m \n @ property \n def micros ( self ) : return self . us \n epoch_microseconds = epoch_micros = microseconds = micros \n @ property \n def tz_name ( self ) : return self . tz . zone \n @ property \n def tz_abbr ( self ) : return self . tz . _tzname \n def set_tz ( self , tz ) : \n self . tz = self . __class__ . to_timezone ( tz ) ; return self \n def with_tz ( self , tz ) : \n return self . __class__ ( self . us , tz ) \n @ property \n def _tuple ( self ) : return ( self . us , self . tz ) \n def strftime ( self , * args , ** kwargs ) : return self . datetime . strftime ( * args , ** kwargs ) \n def __cmp__ ( self , other ) : \n if not hasattr ( other , '<STR_LIT>' ) : other = SaneTime ( other ) \n return cmp ( self . us , int ( other ) ) \n def __hash__ ( self ) : return self . us . __hash__ ( ) \n def __add__ ( self , operand ) : \n if not hasattr ( operand , '<STR_LIT>' ) : operand = SaneTime ( operand ) \n return self . __class__ ( self . us + int ( operand ) , tz = self . tz ) \n def __sub__ ( self , operand ) : \n if not hasattr ( operand , '<STR_LIT>' ) : operand = SaneTime ( operand ) \n if isinstance ( operand , SaneTime ) : return SaneDelta ( self . us - int ( operand ) ) \n return self . __add__ ( - int ( operand ) ) \n def __mul__ ( self , operand ) : \n return self . us * int ( operand ) \n def __div__ ( self , operand ) : \n return self . us / int ( operand ) \n def __int__ ( self ) : return int ( self . us ) \n def __long__ ( self ) : return long ( self . us ) \n def __repr__ ( self ) : return u\"<STR_LIT>\" % ( self . us , repr ( self . tz ) ) \n def __str__ ( self ) : return unicode ( self ) . encode ( '<STR_LIT:utf-8>' ) \n def __unicode__ ( self ) : \n dt = self . datetime \n micros = u\"<STR_LIT>\" % dt . microsecond if dt . microsecond else '<STR_LIT>' \n time = u\"<STR_LIT>\" % ( dt . hour , dt . minute , dt . second , micros ) if dt . microsecond or dt . second or dt . minute or dt . hour else '<STR_LIT>' \n return u\"<STR_LIT>\" % ( dt . year , dt . month , dt . day , time , dt . tzinfo . zone ) \n def clone ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . __class__ ( self . us , self . tz ) \n @ property \n def ny_str ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . ny_ndt . strftime ( '<STR_LIT>' ) \n @ property \n def utc_datetime ( self ) : return SaneTime . us_to_utc_datetime ( self . us ) \n utc_dt = utc_datetime \n @ property \n def utc_naive_datetime ( self ) : return self . utc_datetime . replace ( tzinfo = None ) \n utc_ndt = utc_naive_datetime \n def to_timezoned_datetime ( self , tz ) : return self . utc_datetime . astimezone ( SaneTime . to_timezone ( tz ) ) \n def to_timezoned_naive_datetime ( self , tz ) : return self . to_timezoned_datetime ( tz ) . replace ( tzinfo = None ) \n @ property \n def datetime ( self ) : return self . to_timezoned_datetime ( self . tz ) \n dt = datetime \n @ property \n def naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( self . tz ) \n ndt = naive_datetime \n @ property \n def ny_datetime ( self ) : return self . to_timezoned_datetime ( '<STR_LIT>' ) \n ny_dt = ny_datetime \n @ property \n def ny_naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( '<STR_LIT>' ) \n ny_ndt = ny_naive_datetime \n @ property \n def year ( self ) : return self . dt . year \n @ property \n def month ( self ) : return self . dt . month \n @ property \n def day ( self ) : return self . dt . day \n @ property \n def hour ( self ) : return self . dt . hour \n @ property \n def minute ( self ) : return self . dt . minute \n @ property \n def second ( self ) : return self . dt . second \n @ property \n def microsecond ( self ) : return self . dt . microsecond \n @ classmethod \n def utc_datetime_to_us ( kls , dt ) : \n return calendar . timegm ( dt . timetuple ( ) ) * <NUM_LIT:1000> ** <NUM_LIT:2> + dt . microsecond \n @ classmethod \n def us_to_utc_datetime ( kls , us ) : \n return pytz . utc . localize ( datetime . utcfromtimestamp ( us / <NUM_LIT:10> ** <NUM_LIT:6> ) ) . replace ( microsecond = us % <NUM_LIT:10> ** <NUM_LIT:6> ) \n @ classmethod \n def to_timezone ( kls , tz ) : \n if not isinstance ( tz , basestring ) : return tz \n return pytz . timezone ( tz ) \n def ntime ( * args , ** kwargs ) : \n if args : \n if args [ <NUM_LIT:0> ] is None : return None \n elif kwargs : \n if None in [ v for k , v in kwargs . iteritems ( ) if k != '<STR_LIT>' ] : return None \n return SaneTime ( * args , ** kwargs ) \n time = sanetime = SaneTime \n nsanetime = <mask0> \n", "gt": "ntime"}
{"input": "\n from tastypie . authorization import Authorization \n from openpds . authentication import OAuth2Authentication \n from openpds . core . models import Profile , AuditEntry \n import settings \n import pdb \n import traceback \n class PDSAuthorization ( Authorization ) : \n audit_enabled = True \n scope = \"<STR_LIT>\" \n requester_uuid = \"<STR_LIT>\" \n def requester ( self ) : \n return self . requester_uuid \n def trustWrapper ( self , datastore_owner ) : \n print \"<STR_LIT>\" \n def is_authorized ( self , request , object = None ) : \n authenticator = OAuth2Authentication ( self . scope ) \n if \"<STR_LIT>\" in request . REQUEST : \n authorized = True \n token = request . REQUEST [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in request . REQUEST else request . META [ \"<STR_LIT>\" ] \n datastore_owner_uuid = request . REQUEST [ \"<STR_LIT>\" ] \n datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n self . requester_uuid = authenticator . get_userinfo_from_token ( token , self . scope ) \n if self . requester_uuid is False or self . requester_uuid is None or len ( self . requester_uuid ) == <NUM_LIT:0> : \n self . requester_uuid = \"<STR_LIT>\" \n authorized = False \n self . trustWrapper ( datastore_owner ) \n try : \n if ( self . audit_enabled ) : \n audit_entry = AuditEntry ( token = token ) \n audit_entry . method = request . method \n audit_entry . scope = self . scope \n audit_entry . purpose = request . REQUEST [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in request . REQUEST else \"<STR_LIT>\" \n audit_entry . system_entity_toggle = request . REQUEST [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in request . REQUEST else False \n audit_entry . datastore_owner = datastore_owner \n audit_entry . requester , created = Profile . objects . get_or_create ( uuid = self . requester_uuid ) \n audit_entry . script = request . path \n audit_entry . save ( ) \n except Exception as e : \n print e \n authorized = False \n return authorized \n return False \n def __init__ ( self , scope , audit_enabled = True ) : \n self . scope = scope \n self . audit_enabled = <mask0> \n", "gt": "audit_enabled"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from django import template \n register = template . Library ( ) \n class VerbatimNode ( template . Node ) : \n def __init__ ( self , text ) : \n self . text = text \n def render ( self , context ) : \n return self . text \n @ register . tag \n def verbatim ( parser , token ) : \n text = [ ] \n while <NUM_LIT:1> : \n token = parser . tokens . pop ( <NUM_LIT:0> ) \n if token . contents == '<STR_LIT>' : \n break \n if token . token_type == template . TOKEN_VAR : \n text . append ( '<STR_LIT>' ) \n elif token . token_type == template . TOKEN_BLOCK : \n text . append ( '<STR_LIT>' ) \n text . append ( token . contents ) \n if token . token_type == template . TOKEN_VAR : \n text . append ( '<STR_LIT>' ) \n elif token . token_type == template . TOKEN_BLOCK : \n text . append ( '<STR_LIT>' ) \n return VerbatimNode ( '<STR_LIT>' . join ( <mask0> ) ) \n", "gt": "text"}
{"input": "\n from django . shortcuts import render_to_response \n from django . template import RequestContext \n import <mask0> \n", "gt": "pdb"}
{"input": "\n from werkzeug . utils import cached_property \n from base import db , Base \n from cluster import Cluster \n class Proxy ( Base ) : \n __tablename__ = '<STR_LIT>' \n host = db . Column ( db . String ( <NUM_LIT:255> ) , nullable = False ) \n port = db . Column ( db . Integer , nullable = False ) \n eru_container_id = db . Column ( db . String ( <NUM_LIT:64> ) , index = True ) \n cluster_id = db . Column ( db . ForeignKey ( Cluster . id ) , index = True ) \n suppress_alert = db . Column ( db . Integer , nullable = False , default = <NUM_LIT:1> ) \n __table_args__ = ( db . Index ( '<STR_LIT:address>' , '<STR_LIT:host>' , '<STR_LIT:port>' , unique = True ) , ) \n @ cached_property \n def eru_deployed ( self ) : \n return self . eru_container_id is not None \n @ cached_property \n def eru_info ( self ) : \n import eru_utils \n if eru_utils . eru_client is None or not self . eru_deployed : \n return None \n return eru_utils . eru_client . get_container ( self . eru_container_id ) \n @ cached_property \n def cluster ( self ) : \n return Cluster . query . get ( self . cluster_id ) \n def get_by_host_port ( host , port ) : \n return db . session . query ( Proxy ) . filter ( \n Proxy . host == host , Proxy . port == port ) . first ( ) \n def del_by_host_port ( host , port ) : \n return db . session . query ( Proxy ) . filter ( \n Proxy . host == host , Proxy . port == port ) . delete ( ) \n def get_or_create ( host , port , cluster_id = None ) : \n p = db . session . query ( Proxy ) . filter ( \n Proxy . host == host , Proxy . port == port ) . first ( ) \n if p is None : \n p = Proxy ( host = host , port = port , cluster_id = cluster_id ) \n db . session . add ( p ) \n db . session . flush ( ) \n return p \n def create_eru_instance ( host , port , cluster_id , eru_container_id ) : \n node = Proxy ( host = host , port = port , eru_container_id = eru_container_id , \n cluster_id = cluster_id ) \n db . session . add ( node ) \n db . session . flush ( ) \n return node \n def delete_eru_instance ( eru_container_id ) : \n db . session . query ( Proxy ) . filter ( \n Proxy . eru_container_id == eru_container_id ) . delete ( ) \n def get_eru_by_container_id ( eru_container_id ) : \n return db . session . query ( Proxy ) . filter ( \n Proxy . eru_container_id == eru_container_id ) . first ( ) \n def list_all ( ) : \n return db . session . query ( Proxy ) . all ( ) \n def list_eru_proxies ( offset , limit ) : \n return db . session . query ( Proxy ) . filter ( \n Proxy . eru_container_id != None ) . order_by ( \n Proxy . id . desc ( ) ) . offset ( offset ) . limit ( limit ) . all ( ) \n def list_ip ( ) : \n return db . session . query ( Proxy . host , Proxy . port ) . <mask0> ( ) \n", "gt": "all"}
{"input": "\n from ethereum import tester \n import hydrachain . native_contracts as nc \n from fungible_contract import IOU \n import ethereum . slogging as slogging \n log = slogging . get_logger ( '<STR_LIT>' ) \n def test_iou_template ( ) : \n \"\"\"<STR_LIT>\"\"\" \n nc . registry . register ( IOU ) \n state = tester . state ( ) \n logs = [ ] \n issuer_address = tester . a0 \n issuer_key = tester . k0 \n for evt_class in IOU . events : \n nc . listen_logs ( state , evt_class , callback = lambda e : logs . append ( e ) ) \n iou_address = nc . tester_create_native_contract_instance ( state , issuer_key , IOU ) \n iou_as_issuer = nc . tester_nac ( state , issuer_key , iou_address ) \n iou_as_issuer . init ( ) \n assert iou_as_issuer . balanceOf ( issuer_address ) == <NUM_LIT:0> \n amount_issued = <NUM_LIT> \n iou_as_issuer . issue_funds ( amount_issued , '<STR_LIT>' ) \n assert iou_as_issuer . balanceOf ( issuer_address ) == amount_issued \n iou_as_issuer . issue_funds ( amount_issued , '<STR_LIT>' ) \n assert iou_as_issuer . balanceOf ( issuer_address ) == <NUM_LIT:2> * amount_issued \n assert iou_as_issuer . get_issued_amount ( issuer_address ) == <NUM_LIT:2> * amount_issued \n print logs \n while logs and logs . pop ( ) : \n pass \n nc . registry . <mask0> ( IOU ) \n", "gt": "unregister"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import json \n import time \n import urllib2 \n import logging \n from view_controls . view import DrawingTool , Event \n from game_objects . item import Item \n from game_objects . state import TrackerState , TrackerStateEncoder \n from log_parser import LogParser \n from options import Options \n class IsaacTracker ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , logging_level = logging . INFO , read_timer = <NUM_LIT:1> ) : \n self . read_timer = read_timer \n self . file_prefix = \"<STR_LIT>\" \n self . log = logging . getLogger ( \"<STR_LIT>\" ) \n self . log . addHandler ( logging . FileHandler ( self . file_prefix + \"<STR_LIT>\" , mode = '<STR_LIT:w>' ) ) \n self . log . setLevel ( logging_level ) \n with open ( self . file_prefix + \"<STR_LIT>\" , \"<STR_LIT:r>\" ) as items_file : \n Item . items_info = json . load ( items_file ) \n with open ( self . file_prefix + '<STR_LIT>' , '<STR_LIT:r>' ) as f : \n self . tracker_version = f . read ( ) \n Options ( ) . load_options ( self . file_prefix + \"<STR_LIT>\" ) \n def __del__ ( self ) : \n Options ( ) . save_options ( self . file_prefix + \"<STR_LIT>\" ) \n def check_for_update ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n latest = \"<STR_LIT>\" \n github_info_json = urllib2 . urlopen ( latest ) . read ( ) \n info = json . loads ( github_info_json ) \n latest_version = info [ \"<STR_LIT:name>\" ] \n title_text = \"<STR_LIT>\" + self . tracker_version \n if latest_version != self . tracker_version : \n title_text += \"<STR_LIT>\" \n return title_text \n except Exception as e : \n self . log . debug ( \"<STR_LIT>\" + e . message ) \n return \"<STR_LIT>\" \n def run ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n update_notifier = self . check_for_update ( ) \n framecount = <NUM_LIT:0> \n drawing_tool = DrawingTool ( self . file_prefix ) \n drawing_tool . set_window_title ( update_notifier ) \n parser = LogParser ( self . file_prefix , self . tracker_version ) \n opt = Options ( ) \n log = logging . getLogger ( \"<STR_LIT>\" ) \n event_result = None \n state = None \n read_from_server = opt . read_from_server \n write_to_server = opt . write_to_server \n state_version = - <NUM_LIT:1> \n twitch_username = None \n new_states_queue = [ ] \n screen_error_message = None \n while event_result != Event . DONE : \n event_result = drawing_tool . handle_events ( ) \n if opt . read_from_server != read_from_server or opt . twitch_name != twitch_username : \n twitch_username = opt . twitch_name \n read_from_server = opt . read_from_server \n new_states_queue = [ ] \n if read_from_server : \n state_version = - <NUM_LIT:1> \n state = None \n drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) ) \n else : \n drawing_tool . set_window_title ( update_notifier ) \n if opt . write_to_server and opt . write_to_server != write_to_server : \n write_to_server = True \n drawing_tool . set_window_title ( update_notifier , uploading = True ) \n if not opt . write_to_server : \n write_to_server = False \n if opt . read_from_server : \n update_timer = <NUM_LIT:2> \n else : \n update_timer = self . read_timer \n if event_result == Event . OPTIONS_UPDATE : \n framecount = <NUM_LIT:0> \n screen_error_message = None \n if state is not None : \n state . modified = True \n if ( framecount % int ( Options ( ) . framerate_limit * update_timer ) == <NUM_LIT:0> ) : \n if opt . read_from_server : \n base_url = opt . trackerserver_url + \"<STR_LIT>\" + opt . twitch_name \n json_dict = None \n try : \n json_version = urllib2 . urlopen ( base_url + \"<STR_LIT>\" ) . read ( ) \n if int ( json_version ) > state_version : \n json_state = urllib2 . urlopen ( base_url ) . read ( ) \n json_dict = json . loads ( json_state ) \n new_state = TrackerState . from_json ( json_dict ) \n if new_state is None : \n raise Exception \n state_version = int ( json_version ) \n new_states_queue . append ( ( state_version , new_state ) ) \n drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) , read_delay = opt . read_delay ) \n except Exception : \n state = None \n log . error ( \"<STR_LIT>\" ) \n import traceback \n log . error ( traceback . format_exc ( ) ) \n if json_dict is not None : \n their_version = \"<STR_LIT>\" \n if \"<STR_LIT>\" in json_dict : \n their_version = json_dict [ \"<STR_LIT>\" ] \n else : \n their_version = \"<STR_LIT>\" \n if their_version != self . tracker_version : \n screen_error_message = \"<STR_LIT>\" + their_version + \"<STR_LIT>\" + self . tracker_version \n else : \n force_draw = state and state . modified \n state = parser . parse ( ) \n if force_draw : \n state . modified = True \n if write_to_server and not opt . trackerserver_authkey : \n screen_error_message = \"<STR_LIT>\" \n if state is not None and write_to_server and state . modified and screen_error_message is None : \n opener = urllib2 . build_opener ( urllib2 . HTTPHandler ) \n put_url = opt . trackerserver_url + \"<STR_LIT>\" + opt . trackerserver_authkey \n json_string = json . dumps ( state , cls = TrackerStateEncoder , sort_keys = True ) \n request = urllib2 . Request ( put_url , \n data = json_string ) \n request . add_header ( '<STR_LIT:Content-Type>' , '<STR_LIT:application/json>' ) \n request . get_method = lambda : '<STR_LIT>' \n try : \n result = opener . open ( request ) \n result_json = json . loads ( result . read ( ) ) \n updated_user = result_json [ \"<STR_LIT>\" ] \n if updated_user is None : \n screen_error_message = \"<STR_LIT>\" \n else : \n screen_error_message = None \n except Exception as e : \n import traceback \n errmsg = traceback . format_exc ( ) \n log . error ( \"<STR_LIT>\" ) \n log . error ( errmsg ) \n screen_error_message = \"<STR_LIT>\" \n if len ( new_states_queue ) > <NUM_LIT:0> : \n ( state_timestamp , new_state ) = new_states_queue [ <NUM_LIT:0> ] \n current_timestamp = int ( time . time ( ) ) \n if current_timestamp - state_timestamp >= opt . read_delay or state is None : \n state = new_state \n new_states_queue . pop ( <NUM_LIT:0> ) \n drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) , read_delay = opt . read_delay ) \n if state is None and screen_error_message is None : \n if read_from_server : \n screen_error_message = \"<STR_LIT>\" \n else : \n screen_error_message = \"<STR_LIT>\" \n if screen_error_message is not None : \n drawing_tool . write_error_message ( screen_error_message ) \n else : \n drawing_tool . draw_state ( state ) \n drawing_tool . tick ( ) \n framecount += <NUM_LIT:1> \n drawing_tool . save_window_position ( ) \n def main ( ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n rt = IsaacTracker ( ) \n rt . run ( ) \n except Exception : \n import traceback \n errmsg = traceback . format_exc ( ) \n print ( errmsg ) \n logging . getLogger ( \"<STR_LIT>\" ) . error ( errmsg ) \n if __name__ == \"<STR_LIT:__main__>\" : \n <mask0> ( ) \n", "gt": "main"}
{"input": "\n from __future__ import division , print_function , unicode_literals \n from collections import OrderedDict \n from brainstorm . layers . base_layer import Layer \n from brainstorm . structure . buffer_structure import ( BufferStructure , \n StructureTemplate ) \n from brainstorm . structure . construction import ConstructionWrapper \n from brainstorm . utils import flatten_all_but_last \n def BatchNorm ( name = None , decay = <NUM_LIT> , epsilon = <NUM_LIT> ) : \n \"\"\"<STR_LIT>\"\"\" \n return ConstructionWrapper . create ( BatchNormLayerImpl , \n name = name , \n decay = decay , \n epsilon = epsilon ) \n class BatchNormLayerImpl ( Layer ) : \n expected_inputs = { '<STR_LIT:default>' : StructureTemplate ( '<STR_LIT:T>' , '<STR_LIT:B>' , '<STR_LIT>' ) } \n expected_kwargs = { '<STR_LIT>' , '<STR_LIT>' } \n def setup ( self , kwargs , in_shapes ) : \n self . epsilon = kwargs . get ( '<STR_LIT>' , <NUM_LIT> ) \n self . decay = kwargs . get ( '<STR_LIT>' , <NUM_LIT> ) \n assert <NUM_LIT:0.0> <= self . decay <= <NUM_LIT:1.0> , \"<STR_LIT>\" \n outputs = OrderedDict ( ) \n outputs [ '<STR_LIT:default>' ] = in_shapes [ '<STR_LIT:default>' ] \n parameters = OrderedDict ( ) \n buf = BufferStructure ( self . in_shapes [ '<STR_LIT:default>' ] . feature_shape [ - <NUM_LIT:1> ] ) \n parameters [ '<STR_LIT>' ] = buf \n parameters [ '<STR_LIT>' ] = buf \n parameters [ '<STR_LIT>' ] = buf \n parameters [ '<STR_LIT>' ] = buf \n internals = OrderedDict ( ) \n internals [ '<STR_LIT>' ] = buf \n internals [ '<STR_LIT>' ] = self . in_shapes [ '<STR_LIT:default>' ] \n internals [ '<STR_LIT>' ] = self . in_shapes [ '<STR_LIT:default>' ] \n return outputs , parameters , internals \n def forward_pass ( self , buffers , training_pass = True ) : \n _h = self . handler \n sigma_b , centered , x_hat = buffers . internals \n gamma , beta , mu , sigma = buffers . parameters \n inputs = flatten_all_but_last ( buffers . inputs . default ) \n centered = flatten_all_but_last ( centered ) \n x_hat = flatten_all_but_last ( x_hat ) \n out = flatten_all_but_last ( buffers . outputs . default ) \n m = inputs . shape [ <NUM_LIT:0> ] \n if training_pass : \n mu_b = sigma_b \n _h . sum_t ( inputs , <NUM_LIT:0> , mu_b ) \n _h . mult_st ( - <NUM_LIT:1.0> / m , mu_b , mu_b ) \n _h . mult_st ( self . decay , mu , mu ) \n _h . mult_add_st ( <NUM_LIT:1.0> - self . decay , mu_b , mu ) \n mu = mu_b \n _h . add_mv ( inputs , mu . reshape ( ( <NUM_LIT:1> , mu . size ) ) , centered ) \n if training_pass : \n sigma2 = sigma_b \n centered2 = x_hat \n _h . mult_tt ( centered , centered , centered2 ) \n _h . sum_t ( centered2 , <NUM_LIT:0> , sigma2 ) \n _h . mult_st ( <NUM_LIT:1.0> / m , sigma2 , sigma2 ) \n _h . add_st ( self . epsilon , sigma2 , sigma2 ) \n _h . sqrt_t ( sigma2 , sigma_b ) \n _h . mult_st ( self . decay , sigma , sigma ) \n _h . mult_add_st ( <NUM_LIT:1.0> - self . decay , sigma_b , sigma ) \n sigma = sigma_b \n _h . divide_mv ( centered , sigma . reshape ( ( <NUM_LIT:1> , sigma . size ) ) , x_hat ) \n _h . mult_mv ( x_hat , gamma . reshape ( ( <NUM_LIT:1> , gamma . size ) ) , out ) \n _h . add_mv ( out , beta . reshape ( ( <NUM_LIT:1> , beta . size ) ) , out ) \n def backward_pass ( self , buffers ) : \n _h = self . handler \n sigma_b , centered , x_hat = buffers . internals \n gamma = buffers . parameters . gamma \n dgamma = buffers . gradients . gamma \n dbeta = buffers . gradients . beta \n x_hat = flatten_all_but_last ( x_hat ) \n outdeltas = flatten_all_but_last ( buffers . output_deltas . default ) \n indeltas = flatten_all_but_last ( buffers . input_deltas . default ) \n m = outdeltas . shape [ <NUM_LIT:0> ] \n big_tmp = _h . allocate ( x_hat . shape ) \n small_tmp = _h . allocate ( gamma . shape ) \n tmp = big_tmp \n dgamma_tmp = small_tmp \n _h . mult_tt ( outdeltas , x_hat , tmp ) \n _h . sum_t ( tmp , axis = <NUM_LIT:0> , out = dgamma_tmp ) \n _h . add_tt ( dgamma_tmp , dgamma , dgamma ) \n _h . mult_st ( <NUM_LIT:1> / m , dgamma_tmp , dgamma_tmp ) \n term1 = big_tmp \n _h . mult_mv ( x_hat , dgamma_tmp . reshape ( ( <NUM_LIT:1> , gamma . size ) ) , term1 ) \n dbeta_tmp = small_tmp \n _h . sum_t ( outdeltas , axis = <NUM_LIT:0> , out = dbeta_tmp ) \n _h . add_tt ( dbeta_tmp , dbeta , dbeta ) \n _h . mult_st ( <NUM_LIT:1> / m , dbeta_tmp , dbeta_tmp ) \n term2 = big_tmp \n term3 = big_tmp \n _h . subtract_tt ( outdeltas , term1 , term2 ) \n _h . subtract_mv ( term2 , dbeta_tmp . reshape ( ( <NUM_LIT:1> , dbeta . size ) ) , term3 ) \n coeff = small_tmp \n _h . divide_tt ( gamma , sigma_b , coeff ) \n term4 = big_tmp \n _h . mult_mv ( term3 , coeff . reshape ( ( <NUM_LIT:1> , coeff . size ) ) , term4 ) \n _h . add_tt ( term4 , indeltas , <mask0> ) \n", "gt": "indeltas"}
{"input": "\n from __future__ import division , print_function , unicode_literals \n from collections import OrderedDict \n import numpy as np \n from brainstorm . describable import Describable \n class Scorer ( Describable ) : \n def __init__ ( self , out_name = '<STR_LIT>' , targets_name = '<STR_LIT>' , mask_name = '<STR_LIT>' , \n name = None ) : \n self . out_name = out_name \n self . targets_name = targets_name \n self . mask_name = mask_name \n self . __name__ = name if name is not None else self . __class__ . __name__ \n def __call__ ( self , true_labels , predicted , mask = None ) : \n pass \n @ staticmethod \n def aggregate ( errors ) : \n errors = np . array ( errors ) \n assert errors . ndim == <NUM_LIT:2> and errors . shape [ <NUM_LIT:1> ] == <NUM_LIT:2> \n return np . sum ( errors [ : , <NUM_LIT:1> ] ) / np . sum ( errors [ : , <NUM_LIT:0> ] ) \n def gather_losses_and_scores ( net , scorers , scores , out_name = '<STR_LIT>' , \n targets_name = '<STR_LIT>' , mask_name = '<STR_LIT>' ) : \n ls = net . get_loss_values ( ) \n for name , loss in ls . items ( ) : \n scores [ name ] . append ( ( net . _buffer_manager . batch_size , loss ) ) \n for sc in scorers : \n name = sc . __name__ \n predicted = net . get ( sc . out_name or out_name or net . output_name ) \n true_labels = net . get_input ( sc . targets_name ) if sc . targets_name else net . get_input ( targets_name ) \n mask = net . get_input ( sc . mask_name ) if sc . mask_name else ( net . get_input ( mask_name ) if mask_name else None ) \n predicted = _flatten_all_but_last ( predicted ) \n true_labels = _flatten_all_but_last ( true_labels ) \n mask = _flatten_all_but_last ( mask ) \n weight = mask . sum ( ) if mask is not None else predicted . shape [ <NUM_LIT:0> ] \n scores [ name ] . append ( ( weight , sc ( true_labels , predicted , mask ) ) ) \n def aggregate_losses_and_scores ( scores , net , scorers ) : \n results = OrderedDict ( ) \n for name in net . get_loss_values ( ) : \n results [ name ] = _weighted_average ( scores [ name ] ) \n for sc in scorers : \n results [ sc . __name__ ] = sc . aggregate ( scores [ sc . __name__ ] ) \n return results \n class Accuracy ( Scorer ) : \n def __call__ ( self , true_labels , predicted , mask = None ) : \n if predicted . shape [ <NUM_LIT:1> ] > <NUM_LIT:1> : \n predicted = predicted . argmax ( <NUM_LIT:1> ) . reshape ( - <NUM_LIT:1> , <NUM_LIT:1> ) \n correct = ( predicted == true_labels ) . astype ( np . float ) \n if mask is not None : \n correct *= mask \n return np . sum ( correct ) \n class Hamming ( Scorer ) : \n def __init__ ( self , threshold = <NUM_LIT:0.5> , out_name = '<STR_LIT>' , targets_name = '<STR_LIT>' , \n mask_name = '<STR_LIT>' , name = None ) : \n super ( Hamming , self ) . __init__ ( out_name , targets_name , mask_name , name ) \n self . threshold = threshold \n def __call__ ( self , true_labels , predicted , mask = None ) : \n correct = np . logical_xor ( predicted < self . threshold , \n true_labels ) . astype ( np . float ) \n if mask is not None : \n correct *= mask \n return np . sum ( correct ) / true_labels . shape [ <NUM_LIT:1> ] \n class MeanSquaredError ( Scorer ) : \n def __call__ ( self , true_labels , predicted , mask = None ) : \n errors = ( true_labels - predicted ) ** <NUM_LIT:2> \n if mask is not None : \n errors *= mask \n return <NUM_LIT:0.5> * np . sum ( errors ) \n def _flatten_all_but_last ( a ) : \n if a is None : \n return None \n return a . reshape ( - <NUM_LIT:1> , a . shape [ - <NUM_LIT:1> ] ) \n def _weighted_average ( errors ) : \n errors = np . array ( errors ) \n assert errors . ndim == <NUM_LIT:2> and errors . shape [ <NUM_LIT:1> ] == <NUM_LIT:2> \n return np . sum ( errors [ : , <NUM_LIT:1> ] * errors [ : , <NUM_LIT:0> ] / np . sum ( <mask0> [ : , <NUM_LIT:0> ] ) ) \n", "gt": "errors"}
{"input": "\n from __future__ import division , print_function , unicode_literals \n import pytest \n import six \n from brainstorm . training . schedules import Exponential , Linear , MultiStep \n def test_linear ( ) : \n sch = Linear ( initial_value = <NUM_LIT:1.0> , final_value = <NUM_LIT:0.5> , num_changes = <NUM_LIT:5> ) \n epochs = [ <NUM_LIT:0> ] * <NUM_LIT:2> + [ <NUM_LIT:1> ] * <NUM_LIT:2> + [ <NUM_LIT:2> ] * <NUM_LIT:2> + [ <NUM_LIT:3> ] * <NUM_LIT:2> + [ <NUM_LIT:4> ] * <NUM_LIT:2> \n updates = range ( <NUM_LIT:10> ) \n values = [ sch ( epoch , update , '<STR_LIT>' , <NUM_LIT:1> , None , None , None ) \n for epoch , update in six . moves . zip ( epochs , updates ) ] \n assert values == [ <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n values = [ sch ( epoch , update , '<STR_LIT>' , <NUM_LIT:1> , None , None , None ) \n for epoch , update in six . moves . zip ( epochs , updates ) ] \n assert values == [ <NUM_LIT:1.0> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT:0.5> , <NUM_LIT:0.5> , <NUM_LIT:0.5> , <NUM_LIT:0.5> , <NUM_LIT:0.5> ] \n values = [ sch ( epoch , update , '<STR_LIT>' , <NUM_LIT:3> , None , None , None ) \n for epoch , update in six . moves . zip ( epochs , updates ) ] \n assert values == [ <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n def test_exponential ( ) : \n sch = Exponential ( initial_value = <NUM_LIT:1.0> , factor = <NUM_LIT> , minimum = <NUM_LIT> ) \n epochs = [ <NUM_LIT:0> ] * <NUM_LIT:4> + [ <NUM_LIT:1> ] * <NUM_LIT:4> + [ <NUM_LIT:2> ] * <NUM_LIT:4> \n updates = range ( <NUM_LIT:12> ) \n values = [ sch ( epoch , update , '<STR_LIT>' , <NUM_LIT:1> , None , None , None ) \n for epoch , update in six . moves . zip ( epochs , updates ) ] \n assert values == [ <NUM_LIT:1.0> ] * <NUM_LIT:4> + [ <NUM_LIT> ] * <NUM_LIT:4> + [ <NUM_LIT> * <NUM_LIT> ] * <NUM_LIT:4> \n values = [ sch ( epoch , update , '<STR_LIT>' , <NUM_LIT:1> , None , None , None ) \n for epoch , update in six . moves . zip ( epochs , updates ) ] \n assert values == [ <NUM_LIT:1.0> * ( <NUM_LIT> ** x ) for x in range ( <NUM_LIT:4> ) ] + [ <NUM_LIT> ] * <NUM_LIT:8> \n values = [ sch ( epoch , update , '<STR_LIT>' , <NUM_LIT:3> , None , None , None ) \n for epoch , update in six . moves . zip ( epochs , updates ) ] \n assert values == [ <NUM_LIT:1.0> ] * <NUM_LIT:3> + [ <NUM_LIT> ] * <NUM_LIT:3> + [ <NUM_LIT> ] * <NUM_LIT:3> + [ <NUM_LIT> ** <NUM_LIT:3> ] * <NUM_LIT:3> \n def test_multistep ( ) : \n sch = MultiStep ( initial_value = <NUM_LIT:1.0> , steps = [ <NUM_LIT:3> , <NUM_LIT:5> , <NUM_LIT:8> ] , \n values = [ <NUM_LIT:0.1> , <NUM_LIT> , <NUM_LIT> ] ) \n epochs = [ <NUM_LIT:0> ] * <NUM_LIT:2> + [ <NUM_LIT:1> ] * <NUM_LIT:2> + [ <NUM_LIT:2> ] * <NUM_LIT:2> + [ <NUM_LIT:3> ] * <NUM_LIT:2> + [ <NUM_LIT:4> ] * <NUM_LIT:2> \n updates = range ( <NUM_LIT:10> ) \n values = [ sch ( epoch , update , '<STR_LIT>' , <NUM_LIT:1> , None , None , None ) \n for epoch , update in six . moves . zip ( epochs , updates ) ] \n assert values == [ <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:0.1> , <NUM_LIT:0.1> , <NUM_LIT:0.1> , <NUM_LIT:0.1> ] \n values = [ sch ( epoch , update , '<STR_LIT>' , <NUM_LIT:1> , None , None , None ) \n for epoch , update in six . moves . zip ( epochs , updates ) ] \n assert values == [ <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:0.1> , <NUM_LIT:0.1> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n with pytest . raises ( AssertionError ) : \n _ = sch ( <NUM_LIT:0> , <NUM_LIT:0> , '<STR_LIT>' , <NUM_LIT:3> , None , None , <mask0> ) \n", "gt": "None"}
{"input": "\n import os \n import sys \n try : \n from unittest . mock import MagicMock \n except ImportError : \n from mock import Mock as MagicMock \n class Mock ( MagicMock ) : \n @ classmethod \n def __getattr__ ( cls , name ) : \n return Mock ( ) \n MOCK_MODULES = [ '<STR_LIT>' , '<STR_LIT>' ] \n sys . modules . update ( ( mod_name , Mock ( ) ) for mod_name in MOCK_MODULES ) \n cwd = os . getcwd ( ) \n parent = os . path . dirname ( cwd ) \n sys . path . insert ( <NUM_LIT:0> , parent ) \n import brainstorm \n extensions = [ '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' ] \n templates_path = [ '<STR_LIT>' ] \n source_suffix = '<STR_LIT>' \n master_doc = '<STR_LIT:index>' \n project = u'<STR_LIT>' \n copyright = u'<STR_LIT>' \n version = brainstorm . __version__ \n release = brainstorm . __version__ \n exclude_patterns = [ '<STR_LIT>' ] \n pygments_style = '<STR_LIT>' \n on_rtd = os . environ . get ( '<STR_LIT>' , None ) == '<STR_LIT:True>' \n if not on_rtd : \n try : \n import sphinx_rtd_theme \n html_theme = '<STR_LIT>' \n html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] \n except ImportError : \n html_theme = '<STR_LIT>' \n html_static_path = [ '<STR_LIT>' ] \n htmlhelp_basename = '<STR_LIT>' \n latex_elements = { \n } \n latex_documents = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n u'<STR_LIT>' , '<STR_LIT>' ) , \n ] \n man_pages = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n [ u'<STR_LIT>' ] , <NUM_LIT:1> ) \n ] \n texinfo_documents = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n <mask0> , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' ) , \n ] \n", "gt": "u'<STR_LIT>'"}
{"input": "\n from __future__ import division , print_function , unicode_literals \n from sacred . utils import iter_prefixes , join_paths \n class ConfigSummary ( dict ) : \n def __init__ ( self , added = ( ) , modified = ( ) , typechanged = ( ) , \n ignored_fallbacks = ( ) ) : \n super ( ConfigSummary , self ) . __init__ ( ) \n self . added = set ( added ) \n self . modified = set ( modified ) \n self . typechanged = dict ( typechanged ) \n self . ignored_fallbacks = set ( ignored_fallbacks ) \n self . ensure_coherence ( ) \n def update_from ( self , config_mod , path = '<STR_LIT>' ) : \n added = config_mod . added \n updated = config_mod . modified \n typechanged = config_mod . typechanged \n self . added &= { join_paths ( path , a ) for a in added } \n self . modified |= { join_paths ( path , u ) for u in updated } \n self . typechanged . update ( { join_paths ( path , k ) : v \n for k , v in typechanged . items ( ) } ) \n self . ensure_coherence ( ) \n def update_add ( self , config_mod , path = '<STR_LIT>' ) : \n added = config_mod . added \n updated = config_mod . modified \n typechanged = config_mod . typechanged \n self . added |= { join_paths ( path , a ) for a in added } \n self . modified |= { join_paths ( path , u ) for u in updated } \n self . typechanged . update ( { join_paths ( path , k ) : v \n for k , v in typechanged . items ( ) } ) \n self . ensure_coherence ( ) \n def ensure_coherence ( self ) : \n self . modified |= { p for a in self . added for p in iter_prefixes ( a ) } \n self . modified |= { p for u in self . modified for p in iter_prefixes ( u ) } \n self . modified |= { p for t in self . typechanged \n for p in iter_prefixes ( t ) } \n self . added -= set ( self . typechanged . keys ( ) ) \n self . modified -= set ( self . typechanged . keys ( ) ) \n self . modified -= self . <mask0> \n", "gt": "added"}
{"input": "\n from __future__ import division , print_function , unicode_literals \n import pytest \n import sacred . optional as opt \n from sacred . config import ConfigDict \n from sacred . config . custom_containers import DogmaticDict , DogmaticList \n @ pytest . fixture \n def conf_dict ( ) : \n cfg = ConfigDict ( { \n \"<STR_LIT:a>\" : <NUM_LIT:1> , \n \"<STR_LIT:b>\" : <NUM_LIT> , \n \"<STR_LIT:c>\" : True , \n \"<STR_LIT:d>\" : '<STR_LIT:string>' , \n \"<STR_LIT:e>\" : [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] , \n \"<STR_LIT:f>\" : { '<STR_LIT:a>' : '<STR_LIT:b>' , '<STR_LIT:c>' : '<STR_LIT:d>' } , \n } ) \n return cfg \n def test_config_dict_returns_dict ( conf_dict ) : \n assert isinstance ( conf_dict ( ) , dict ) \n def test_config_dict_result_contains_keys ( conf_dict ) : \n cfg = conf_dict ( ) \n assert set ( cfg . keys ( ) ) == { '<STR_LIT:a>' , '<STR_LIT:b>' , '<STR_LIT:c>' , '<STR_LIT:d>' , '<STR_LIT:e>' , '<STR_LIT:f>' } \n assert cfg [ '<STR_LIT:a>' ] == <NUM_LIT:1> \n assert cfg [ '<STR_LIT:b>' ] == <NUM_LIT> \n assert cfg [ '<STR_LIT:c>' ] \n assert cfg [ '<STR_LIT:d>' ] == '<STR_LIT:string>' \n assert cfg [ '<STR_LIT:e>' ] == [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] \n assert cfg [ '<STR_LIT:f>' ] == { '<STR_LIT:a>' : '<STR_LIT:b>' , '<STR_LIT:c>' : '<STR_LIT:d>' } \n def test_fixing_values ( conf_dict ) : \n assert conf_dict ( { '<STR_LIT:a>' : <NUM_LIT:100> } ) [ '<STR_LIT:a>' ] == <NUM_LIT:100> \n @ pytest . mark . parametrize ( \"<STR_LIT:key>\" , [ \"<STR_LIT>\" , \"<STR_LIT>\" , <NUM_LIT:12> , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def test_config_dict_raises_on_invalid_keys ( key ) : \n with pytest . raises ( KeyError ) : \n ConfigDict ( { key : True } ) \n @ pytest . mark . parametrize ( \"<STR_LIT:value>\" , [ lambda x : x , pytest , test_fixing_values ] ) \n def test_config_dict_raises_on_invalid_values ( value ) : \n with pytest . raises ( ValueError ) : \n ConfigDict ( { \"<STR_LIT>\" : value } ) \n def test_fixing_nested_dicts ( conf_dict ) : \n cfg = conf_dict ( { '<STR_LIT:f>' : { '<STR_LIT:c>' : '<STR_LIT:t>' } } ) \n assert cfg [ '<STR_LIT:f>' ] [ '<STR_LIT:a>' ] == '<STR_LIT:b>' \n assert cfg [ '<STR_LIT:f>' ] [ '<STR_LIT:c>' ] == '<STR_LIT:t>' \n def test_adding_values ( conf_dict ) : \n cfg = conf_dict ( { '<STR_LIT:g>' : <NUM_LIT> , '<STR_LIT:h>' : { '<STR_LIT:i>' : <NUM_LIT:10> } } ) \n assert cfg [ '<STR_LIT:g>' ] == <NUM_LIT> \n assert cfg [ '<STR_LIT:h>' ] == { '<STR_LIT:i>' : <NUM_LIT:10> } \n assert cfg . added == { '<STR_LIT:g>' , '<STR_LIT:h>' , '<STR_LIT>' } \n def test_typechange ( conf_dict ) : \n cfg = conf_dict ( { '<STR_LIT:a>' : '<STR_LIT:bar>' , '<STR_LIT:b>' : '<STR_LIT:foo>' , '<STR_LIT:c>' : <NUM_LIT:1> } ) \n assert cfg . typechanged == { '<STR_LIT:a>' : ( int , type ( '<STR_LIT:bar>' ) ) , \n '<STR_LIT:b>' : ( float , type ( '<STR_LIT:foo>' ) ) , \n '<STR_LIT:c>' : ( bool , int ) } \n def test_nested_typechange ( conf_dict ) : \n cfg = conf_dict ( { '<STR_LIT:f>' : { '<STR_LIT:a>' : <NUM_LIT:10> } } ) \n assert cfg . typechanged == { '<STR_LIT>' : ( type ( '<STR_LIT:a>' ) , int ) } \n def is_dogmatic ( a ) : \n if isinstance ( a , ( DogmaticDict , DogmaticList ) ) : \n return True \n elif isinstance ( a , dict ) : \n return any ( is_dogmatic ( v ) for v in a . values ( ) ) \n elif isinstance ( a , ( list , tuple ) ) : \n return any ( is_dogmatic ( v ) for v in a ) \n def test_result_of_conf_dict_is_not_dogmatic ( conf_dict ) : \n cfg = conf_dict ( { '<STR_LIT:e>' : [ <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> ] } ) \n assert not is_dogmatic ( cfg ) \n @ pytest . mark . skipif ( not opt . has_numpy , reason = \"<STR_LIT>\" ) \n def test_conf_scope_handles_numpy_bools ( ) : \n cfg = ConfigDict ( { \n \"<STR_LIT:a>\" : opt . np . bool_ ( <NUM_LIT:1> ) \n } ) \n assert '<STR_LIT:a>' in cfg ( ) \n assert cfg ( ) [ '<STR_LIT:a>' ] \n def test_conf_scope_contains_presets ( ) : \n conf_dict = ConfigDict ( { \n \"<STR_LIT>\" : <NUM_LIT> \n } ) \n cfg = conf_dict ( preset = { '<STR_LIT:a>' : <NUM_LIT> , '<STR_LIT>' : True } ) \n assert set ( cfg . keys ( ) ) == { '<STR_LIT:a>' , '<STR_LIT>' , '<STR_LIT>' } \n assert cfg [ '<STR_LIT:a>' ] == <NUM_LIT> \n assert cfg [ '<STR_LIT>' ] == <NUM_LIT> \n assert cfg [ '<STR_LIT>' ] is True \n def test_conf_scope_does_not_contain_fallback ( ) : \n config_dict = ConfigDict ( { \n \"<STR_LIT>\" : <NUM_LIT> \n } ) \n cfg = config_dict ( fallback = { '<STR_LIT:a>' : <NUM_LIT> , '<STR_LIT:b>' : <NUM_LIT:10> } ) \n assert set ( cfg . keys ( ) ) == { '<STR_LIT>' } \n def test_fixed_subentry_of_preset ( ) : \n config_dict = ConfigDict ( { } ) \n cfg = config_dict ( preset = { '<STR_LIT:d>' : { '<STR_LIT:a>' : <NUM_LIT:1> , '<STR_LIT:b>' : <NUM_LIT:2> } } , fixed = { '<STR_LIT:d>' : { '<STR_LIT:a>' : <NUM_LIT:10> } } ) \n assert set ( cfg . keys ( ) ) == { '<STR_LIT:d>' } \n assert set ( cfg [ '<STR_LIT:d>' ] . keys ( ) ) == { '<STR_LIT:a>' , '<STR_LIT:b>' } \n assert cfg [ '<STR_LIT:d>' ] [ '<STR_LIT:a>' ] == <NUM_LIT:10> \n assert cfg [ '<STR_LIT:d>' ] [ <mask0> ] == <NUM_LIT:2> \n", "gt": "'<STR_LIT:b>'"}
{"input": "\n class PID ( object ) : \n def __init__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . kd = <NUM_LIT:0> \n self . ki = <NUM_LIT:0> \n self . kp = <NUM_LIT:1> \n self . previous_error = <NUM_LIT:0> \n self . integral_error = <NUM_LIT:0> \n def set_k_values ( self , kp , kd , ki ) : \n self . kp = kp \n self . ki = ki \n self . kd = kd \n def clear_error ( self ) : \n self . previous_error = <NUM_LIT:0> \n self . integeral_error = <NUM_LIT:0> \n def pid ( self , target , process_var , timestep ) : \n current_error = ( target - process_var ) \n p_error = self . kp * current_error \n d_error = self . kd * ( current_error - self . previous_error ) / timestep \n self . integral_error = ( \n current_error + self . previous_error ) / <NUM_LIT:2> + self . integral_error \n i_error = self . ki * self . integral_error \n total_error = p_error + d_error + i_error \n self . previous_error = current_error \n return <mask0> \n", "gt": "total_error"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import cmd \n import sys \n import os \n import bot . client . ctrl_client as ctrl_client_mod \n import bot . client . sub_client as sub_client_mod \n class CLI ( cmd . Cmd ) : \n \"\"\"<STR_LIT>\"\"\" \n prompt = \"<STR_LIT>\" \n def __init__ ( self , ctrl_addr , sub_addr ) : \n \"\"\"<STR_LIT>\"\"\" \n cmd . Cmd . __init__ ( self ) \n try : \n self . ctrl_client = ctrl_client_mod . CtrlClient ( ctrl_addr ) \n except Exception , e : \n print \"<STR_LIT>\" . format ( ctrl_addr , e ) \n sys . exit ( - <NUM_LIT:1> ) \n try : \n self . sub_client = sub_client_mod . SubClient ( sub_addr ) \n except Exception , e : \n print \"<STR_LIT>\" . format ( sub_addr , e ) \n sys . exit ( - <NUM_LIT:1> ) \n def default ( self , raw_args ) : \n \"\"\"<STR_LIT>\"\"\" \n obj_name , _ , rest = raw_args . partition ( \"<STR_LIT:U+0020>\" ) \n if obj_name in self . ctrl_client . objects : \n method_name , _ , params = rest . partition ( \"<STR_LIT:U+0020>\" ) \n if method_name in self . ctrl_client . objects [ obj_name ] : \n try : \n param_dict = { } \n for param in params . split ( ) : \n key , value = param . split ( \"<STR_LIT::>\" ) \n try : \n if \"<STR_LIT:.>\" in value : \n value = float ( value ) \n else : \n value = int ( value ) \n except ValueError : \n if value == \"<STR_LIT:True>\" : \n value = True \n elif value == \"<STR_LIT:False>\" : \n value = False \n elif value . startswith ( \"<STR_LIT:'>\" ) and value . endswith ( \"<STR_LIT:'>\" ) : \n value = value [ <NUM_LIT:1> : - <NUM_LIT:1> ] \n param_dict [ key ] = value \n except IndexError : \n print \"<STR_LIT>\" \n return \n except ValueError : \n print \"<STR_LIT>\" \n return \n result = self . ctrl_client . call ( \n obj_name , method_name , param_dict ) \n print \"<STR_LIT>\" , result \n else : \n print \"<STR_LIT>\" , method_name \n else : \n print \"<STR_LIT>\" , obj_name \n def completenames ( self , text , * ignored ) : \n \"\"\"<STR_LIT>\"\"\" \n cmd_match_names = cmd . Cmd . completenames ( self , text , * ignored ) \n obj_names = self . ctrl_client . objects . keys ( ) \n api_match_names = [ x for x in obj_names if x . startswith ( text ) ] \n return cmd_match_names + api_match_names \n def completedefault ( self , text , line , begidx , endidx ) : \n \"\"\"<STR_LIT>\"\"\" \n obj , _ , rest = line . partition ( \"<STR_LIT:U+0020>\" ) \n if obj in self . ctrl_client . objects : \n method , _ , params = rest . strip ( ) . partition ( \"<STR_LIT:U+0020>\" ) \n if method == text : \n method_names = self . ctrl_client . objects [ obj ] \n match_names = [ x for x in method_names if x . startswith ( text ) ] \n return match_names \n def do_list ( self , raw_args ) : \n \"\"\"<STR_LIT>\"\"\" \n print \n print \"<STR_LIT>\" \n print \n for obj_name , methods in sorted ( self . ctrl_client . objects . items ( ) ) : \n print \"<STR_LIT>\" . format ( obj_name ) \n for method in methods : \n print \"<STR_LIT>\" . format ( method ) \n print \n def help_list ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT:list>\" \n print \"<STR_LIT>\" \n def do_ping ( self , raw_args ) : \n \"\"\"<STR_LIT>\"\"\" \n reply_time = self . ctrl_client . ping ( ) \n print \"<STR_LIT>\" . format ( reply_time ) \n def help_ping ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n def do_sub_add ( self , raw_args ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n topic = raw_args . split ( ) [ <NUM_LIT:0> ] \n except ( ValueError , IndexError ) : \n print \"<STR_LIT>\" \n return \n self . sub_client . add_topic ( topic ) \n def help_sub_add ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n def do_sub_del ( self , raw_args ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n topic = raw_args . split ( ) [ <NUM_LIT:0> ] \n except ( ValueError , IndexError ) : \n print \"<STR_LIT>\" \n return \n self . sub_client . del_topic ( topic ) \n def help_sub_del ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n def do_sub ( self , raw_args ) : \n \"\"\"<STR_LIT>\"\"\" \n self . sub_client . print_msgs ( ) \n def help_sub ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n def do_stop ( self , raw_args ) : \n \"\"\"<STR_LIT>\"\"\" \n self . ctrl_client . stop_full ( ) \n def help_stop ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n def do_kill ( self , raw_args ) : \n \"\"\"<STR_LIT>\"\"\" \n self . ctrl_client . exit_server ( ) \n def help_kill ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n def do_die ( self , raw_args ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n self . ctrl_client . clean_up ( ) \n self . sub_client . clean_up ( ) \n print \"<STR_LIT>\" \n return True \n def help_die ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n def do_shell ( self , cmd ) : \n \"\"\"<STR_LIT>\"\"\" \n os . system ( cmd ) \n def help_shell ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n def do_EOF ( self , raw_args ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n self . ctrl_client . clean_up ( ) \n self . sub_client . clean_up ( ) \n print \"<STR_LIT>\" \n return True \n def help_EOF ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n def help_help ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n if __name__ == '<STR_LIT:__main__>' : \n if len ( sys . argv ) == <NUM_LIT:1> : \n print \"<STR_LIT>\" \n CLI ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . cmdloop ( ) \n elif len ( sys . argv ) == <NUM_LIT:3> : \n ctrl_addr = sys . argv [ <NUM_LIT:1> ] \n sub_addr = sys . argv [ <NUM_LIT:2> ] \n CLI ( ctrl_addr , sub_addr ) . cmdloop ( ) \n else : \n <mask0> \"<STR_LIT>\" \n", "gt": "print"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from random import randint \n from os import path \n import bot . lib . lib as lib \n import bot . hardware . servo as s_mod \n import tests . test_bot as test_bot \n class TestPosition ( test_bot . TestBot ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n super ( TestPosition , self ) . setUp ( ) \n config = path . dirname ( path . realpath ( __file__ ) ) + \"<STR_LIT>\" \n self . config = lib . get_config ( config ) \n self . pwm_num = self . config [ '<STR_LIT>' ] \n self . setup_pwm ( self . pwm_num , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n self . servo = s_mod . Servo ( self . pwm_num ) \n def tearDown ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n super ( TestPosition , self ) . tearDown ( ) \n def test_0 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . servo . position = <NUM_LIT:0> \n assert self . servo . position == <NUM_LIT:0> , self . servo . position \n def test_180 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . servo . position = <NUM_LIT> \n assert self . servo . position == <NUM_LIT> , self . servo . position \n def test_middle ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . servo . position = <NUM_LIT> \n assert self . servo . position == <NUM_LIT> , self . servo . position \n def test_series ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for position in range ( <NUM_LIT:0> , <NUM_LIT> , <NUM_LIT> ) : \n self . servo . position = position \n assert self . servo . position == position , self . servo . position \n def test_manually_confirm ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for i in range ( <NUM_LIT:10> ) : \n test_pos = randint ( <NUM_LIT:0> , <NUM_LIT> ) \n self . servo . position = test_pos \n cur_pwm = self . get_pwm ( self . pwm_num ) \n duty = int ( cur_pwm [ \"<STR_LIT>\" ] ) \n read_pos = int ( round ( ( ( duty - <NUM_LIT> ) / <NUM_LIT> ) * <NUM_LIT> ) ) \n assert read_pos == test_pos , \"<STR_LIT>\" . format ( read_pos , test_pos ) \n def test_over_max ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . servo . position = <NUM_LIT> \n assert self . servo . position == <NUM_LIT> , \"<STR_LIT>\" . format ( self . servo . position ) \n def test_under_min ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . servo . position = - <NUM_LIT:1> \n assert self . servo . position == <NUM_LIT:0> , \"<STR_LIT>\" . format ( self . servo . <mask0> ) \n", "gt": "position"}
{"input": "\n from django . contrib . syndication . views import Feed as SyndicationFeed \n from django . core . urlresolvers import reverse \n from django . conf import settings \n from lifestream . models import Lifestream , Item \n class RecentItemsFeed ( SyndicationFeed ) : \n title = \"<STR_LIT>\" \n description = \"<STR_LIT>\" \n def link ( self , obj ) : \n return reverse ( '<STR_LIT>' , kwargs = { \n '<STR_LIT>' : obj . slug , \n } ) \n def get_object ( self , bits ) : \n return Lifestream . objects . get ( slug = bits [ <NUM_LIT:0> ] ) \n def items ( self , obj ) : \n return Item . objects . published ( ) . filter ( feed__lifestream = obj ) [ : <NUM_LIT:10> ] \n def item_pubdate ( self , item ) : \n return item . date \n def item_categories ( self , item ) : \n def item_categories ( self , item ) : \n if '<STR_LIT>' in settings . INSTALLED_APPS : \n return [ tag . name for tag in item . tag_set ] \n else : \n <mask0> [ ] \n", "gt": "return"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from functools import update_wrapper \n from google . appengine . api import users \n from werkzeug import redirect \n from werkzeug . exceptions import Forbidden \n from kay . utils import ( \n create_login_url , create_logout_url \n ) \n from kay . utils . decorators import auto_adapt_to_methods \n def login_required ( func ) : \n def inner ( request , * args , ** kwargs ) : \n if request . user . is_anonymous ( ) : \n if request . is_xhr : \n return Forbidden ( ) \n else : \n return redirect ( create_login_url ( request . url ) ) \n return func ( request , * args , ** kwargs ) \n update_wrapper ( inner , func ) \n return inner \n login_required = auto_adapt_to_methods ( login_required ) \n def admin_required ( func ) : \n def inner ( request , * args , ** kwargs ) : \n if not request . user . is_admin : \n if request . user . is_anonymous ( ) : \n return redirect ( create_login_url ( request . url ) ) \n else : \n raise Forbidden ( \n description = \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' % \n create_logout_url ( request . url ) \n ) \n return func ( request , * args , ** kwargs ) \n update_wrapper ( inner , func ) \n return inner \n admin_required = auto_adapt_to_methods ( <mask0> ) \n", "gt": "admin_required"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n PARSE_ERROR = - <NUM_LIT> \n INVALID_REQUEST = - <NUM_LIT> \n METHOD_NOT_FOUND = - <NUM_LIT> \n INVALID_PARAMS = - <NUM_LIT> \n INTERNAL_ERROR = - <NUM_LIT> \n errors = { } \n errors [ PARSE_ERROR ] = \"<STR_LIT>\" \n errors [ INVALID_REQUEST ] = \"<STR_LIT>\" \n errors [ METHOD_NOT_FOUND ] = \"<STR_LIT>\" \n errors [ INVALID_PARAMS ] = \"<STR_LIT>\" \n errors [ INTERNAL_ERROR ] = \"<STR_LIT>\" \n try : \n import json \n except ImportError : \n try : \n import django . utils . simplejson as json \n except ImportError : \n import simplejson as json \n import sys \n import logging \n import itertools \n from werkzeug import Request , Response \n from werkzeug import exceptions \n class JsonRpcApplication ( object ) : \n def __init__ ( self , methods = None ) : \n if methods is not None : \n self . methods = methods \n else : \n self . methods = { } \n def add_module ( self , mod , namespace = None ) : \n if namespace is None : \n namespace = mod . __name__ \n for k , v in ( ( k , v ) for k , v in mod . __dict__ . iteritems ( ) \n if not k . startswith ( '<STR_LIT:_>' ) and callable ( v ) ) : \n self . add ( namespace + '<STR_LIT:.>' + k , v ) \n def add ( self , name , func ) : \n self . methods [ name ] = func \n def process ( self , data ) : \n if data . get ( '<STR_LIT>' ) != \"<STR_LIT>\" : \n return { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:id>' : data . get ( '<STR_LIT:id>' ) , \n '<STR_LIT:error>' : { '<STR_LIT:code>' : INVALID_REQUEST , \n '<STR_LIT:message>' : errors [ INVALID_REQUEST ] } } \n if '<STR_LIT>' not in data : \n return { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:id>' : data . get ( '<STR_LIT:id>' ) , \n '<STR_LIT:error>' : { '<STR_LIT:code>' : INVALID_REQUEST , \n '<STR_LIT:message>' : errors [ INVALID_REQUEST ] } } \n methodname = data [ '<STR_LIT>' ] \n if not isinstance ( methodname , basestring ) : \n return { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:id>' : data . get ( '<STR_LIT:id>' ) , \n '<STR_LIT:error>' : { '<STR_LIT:code>' : INVALID_REQUEST , \n '<STR_LIT:message>' : errors [ INVALID_REQUEST ] } } \n if methodname . startswith ( '<STR_LIT:_>' ) : \n return { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:id>' : data . get ( '<STR_LIT:id>' ) , \n '<STR_LIT:error>' : { '<STR_LIT:code>' : METHOD_NOT_FOUND , \n '<STR_LIT:message>' : errors [ METHOD_NOT_FOUND ] } } \n if methodname not in self . methods : \n return { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:id>' : data . get ( '<STR_LIT:id>' ) , \n '<STR_LIT:error>' : { '<STR_LIT:code>' : METHOD_NOT_FOUND , \n '<STR_LIT:message>' : errors [ METHOD_NOT_FOUND ] } } \n method = self . methods [ methodname ] \n try : \n params = data . get ( '<STR_LIT>' , [ ] ) \n if isinstance ( params , list ) : \n result = method ( * params ) \n elif isinstance ( params , dict ) : \n result = method ( ** dict ( [ ( str ( k ) , v ) for k , v in params . iteritems ( ) ] ) ) \n else : \n return { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:id>' : data . get ( '<STR_LIT:id>' ) , \n '<STR_LIT:error>' : { '<STR_LIT:code>' : INVALID_REQUEST , \n '<STR_LIT:message>' : errors [ INVALID_REQUEST ] } } \n resdata = None \n if data . get ( '<STR_LIT:id>' ) : \n resdata = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:id>' : data . get ( '<STR_LIT:id>' ) , \n '<STR_LIT:result>' : result , \n } \n return resdata \n except Exception , e : \n return { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:id>' : data . get ( '<STR_LIT:id>' ) , \n '<STR_LIT:error>' : { '<STR_LIT:code>' : INTERNAL_ERROR , \n '<STR_LIT:message>' : errors [ INTERNAL_ERROR ] , \n '<STR_LIT:data>' : str ( e ) } } \n def __call__ ( self , environ , start_response ) : \n request = Request ( environ ) \n if request . method != \"<STR_LIT:POST>\" : \n raise exceptions . MethodNotAllowed \n if not request . content_type . startswith ( '<STR_LIT:application/json>' ) : \n raise exceptions . BadRequest \n try : \n data = json . loads ( request . data ) \n except ValueError , e : \n resdata = { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:id>' : None , \n '<STR_LIT:error>' : { '<STR_LIT:code>' : PARSE_ERROR , \n '<STR_LIT:message>' : errors [ PARSE_ERROR ] } } \n else : \n if isinstance ( data , dict ) : \n resdata = self . process ( data ) \n elif isinstance ( data , list ) : \n if len ( [ x for x in data if not isinstance ( x , dict ) ] ) : \n resdata = { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:id>' : None , \n '<STR_LIT:error>' : { '<STR_LIT:code>' : INVALID_REQUEST , \n '<STR_LIT:message>' : errors [ INVALID_REQUEST ] } } \n else : \n resdata = [ d for d in ( self . process ( d ) for d in data ) \n if d is not None ] \n response = Response ( content_type = \"<STR_LIT:application/json>\" ) \n if resdata : \n response . headers [ \"<STR_LIT>\" ] = \"<STR_LIT>\" \n response . headers [ \"<STR_LIT>\" ] = \"<STR_LIT>\" \n response . headers [ \"<STR_LIT>\" ] = \"<STR_LIT>\" \n response . data = json . dumps ( resdata ) \n return response ( environ , start_response ) \n def getmod ( modname ) : \n try : \n __import__ ( modname ) \n except ImportError , e : \n logging . warn ( \"<STR_LIT>\" % e ) \n return None \n mod = sys . modules [ modname ] \n return mod \n def HTTPExceptionMiddleware ( app ) : \n def wrap ( environ , start_response ) : \n try : \n return app ( environ , start_response ) \n except exceptions . HTTPException , e : \n return e ( environ , start_response ) \n return wrap \n def make_application ( methods ) : \n app = JsonRpcApplication ( ) \n for name , value in methods . iteritems ( ) : \n if \"<STR_LIT::>\" in value : \n modname , funcname = value . split ( \"<STR_LIT::>\" , <NUM_LIT:1> ) \n mod = getmod ( modname ) \n if mod : \n app . add ( name , getattr ( mod , funcname ) ) \n else : \n modname = value \n mod = getmod ( modname ) \n if mod : \n app . add_module ( mod , name ) \n app = HTTPExceptionMiddleware ( app ) \n return <mask0> \n", "gt": "app"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import re \n from jinja2 . runtime import Undefined \n __test__ = False \n number_re = re . compile ( r'<STR_LIT>' ) \n regex_type = type ( number_re ) \n try : \n test_callable = callable \n except NameError : \n def test_callable ( x ) : \n return hasattr ( x , '<STR_LIT>' ) \n def test_odd ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n return value % <NUM_LIT:2> == <NUM_LIT:1> \n def test_even ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n return value % <NUM_LIT:2> == <NUM_LIT:0> \n def test_divisibleby ( value , num ) : \n \"\"\"<STR_LIT>\"\"\" \n return value % num == <NUM_LIT:0> \n def test_defined ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n return not isinstance ( value , Undefined ) \n def test_undefined ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n return isinstance ( value , Undefined ) \n def test_none ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n return value is None \n def test_lower ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n return unicode ( value ) . islower ( ) \n def test_upper ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n return unicode ( value ) . isupper ( ) \n def test_string ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n return isinstance ( value , basestring ) \n def test_number ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n return isinstance ( value , ( int , long , float , complex ) ) \n def test_sequence ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n len ( value ) \n value . __getitem__ \n except : \n return False \n return True \n def test_sameas ( value , other ) : \n \"\"\"<STR_LIT>\"\"\" \n return value is other \n def test_iterable ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n iter ( value ) \n except TypeError : \n return False \n return True \n def test_escaped ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n return hasattr ( value , '<STR_LIT>' ) \n TESTS = { \n '<STR_LIT>' : test_odd , \n '<STR_LIT>' : test_even , \n '<STR_LIT>' : test_divisibleby , \n '<STR_LIT>' : test_defined , \n '<STR_LIT>' : test_undefined , \n '<STR_LIT:none>' : test_none , \n '<STR_LIT>' : test_lower , \n '<STR_LIT>' : test_upper , \n '<STR_LIT:string>' : test_string , \n '<STR_LIT>' : test_number , \n '<STR_LIT>' : test_sequence , \n '<STR_LIT>' : test_iterable , \n '<STR_LIT>' : test_callable , \n '<STR_LIT>' : test_sameas , \n '<STR_LIT>' : <mask0> \n } \n", "gt": "test_escaped"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import re \n import codecs \n import mimetypes \n from werkzeug . _internal import _proxy_repr , _missing , _empty_stream \n _locale_delim_re = re . compile ( r'<STR_LIT>' ) \n def is_immutable ( self ) : \n raise TypeError ( '<STR_LIT>' % self . __class__ . __name__ ) \n def iter_multi_items ( mapping ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( mapping , MultiDict ) : \n for item in mapping . iteritems ( multi = True ) : \n yield item \n elif isinstance ( mapping , dict ) : \n for key , value in mapping . iteritems ( ) : \n if isinstance ( value , ( tuple , list ) ) : \n for value in value : \n yield key , value \n else : \n yield key , value \n else : \n for item in mapping : \n yield item \n class ImmutableListMixin ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __reduce_ex__ ( self , protocol ) : \n return type ( self ) , ( list ( self ) , ) \n def __delitem__ ( self , key ) : \n is_immutable ( self ) \n def __delslice__ ( self , i , j ) : \n is_immutable ( self ) \n def __iadd__ ( self , other ) : \n is_immutable ( self ) \n __imul__ = __iadd__ \n def __setitem__ ( self , key , value ) : \n is_immutable ( self ) \n def __setslice__ ( self , i , j , value ) : \n is_immutable ( self ) \n def append ( self , item ) : \n is_immutable ( self ) \n remove = append \n def extend ( self , iterable ) : \n is_immutable ( self ) \n def insert ( self , pos , value ) : \n is_immutable ( self ) \n def pop ( self , index = - <NUM_LIT:1> ) : \n is_immutable ( self ) \n def reverse ( self ) : \n is_immutable ( self ) \n def sort ( self , cmp = None , key = None , reverse = None ) : \n is_immutable ( self ) \n class ImmutableList ( ImmutableListMixin , list ) : \n \"\"\"<STR_LIT>\"\"\" \n __repr__ = _proxy_repr ( list ) \n class ImmutableDictMixin ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __reduce_ex__ ( self , protocol ) : \n return type ( self ) , ( dict ( self ) , ) \n def setdefault ( self , key , default = None ) : \n is_immutable ( self ) \n def update ( self , * args , ** kwargs ) : \n is_immutable ( self ) \n def pop ( self , key , default = None ) : \n is_immutable ( self ) \n def popitem ( self ) : \n is_immutable ( self ) \n def __setitem__ ( self , key , value ) : \n is_immutable ( self ) \n def __delitem__ ( self , key ) : \n is_immutable ( self ) \n def clear ( self ) : \n is_immutable ( self ) \n class ImmutableMultiDictMixin ( ImmutableDictMixin ) : \n \"\"\"<STR_LIT>\"\"\" \n def __reduce_ex__ ( self , protocol ) : \n return type ( self ) , ( self . items ( multi = True ) , ) \n def add ( self , key , value ) : \n is_immutable ( self ) \n def popitemlist ( self ) : \n is_immutable ( self ) \n def poplist ( self , key ) : \n is_immutable ( self ) \n def setlist ( self , key , new_list ) : \n is_immutable ( self ) \n def setlistdefault ( self , key , default_list = None ) : \n is_immutable ( self ) \n class UpdateDictMixin ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n on_update = None \n def calls_update ( name ) : \n def oncall ( self , * args , ** kw ) : \n rv = getattr ( super ( UpdateDictMixin , self ) , name ) ( * args , ** kw ) \n if self . on_update is not None : \n self . on_update ( self ) \n return rv \n oncall . __name__ = name \n return oncall \n __setitem__ = calls_update ( '<STR_LIT>' ) \n __delitem__ = calls_update ( '<STR_LIT>' ) \n clear = calls_update ( '<STR_LIT>' ) \n pop = calls_update ( '<STR_LIT>' ) \n popitem = calls_update ( '<STR_LIT>' ) \n setdefault = calls_update ( '<STR_LIT>' ) \n update = calls_update ( '<STR_LIT>' ) \n del calls_update \n class TypeConversionDict ( dict ) : \n \"\"\"<STR_LIT>\"\"\" \n def get ( self , key , default = None , type = None ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n rv = self [ key ] \n if type is not None : \n rv = type ( rv ) \n except ( KeyError , ValueError ) : \n rv = default \n return rv \n class ImmutableTypeConversionDict ( ImmutableDictMixin , TypeConversionDict ) : \n \"\"\"<STR_LIT>\"\"\" \n def copy ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return TypeConversionDict ( self ) \n def __copy__ ( self ) : \n return self \n class MultiDict ( TypeConversionDict ) : \n \"\"\"<STR_LIT>\"\"\" \n KeyError = None \n def __init__ ( self , mapping = None ) : \n if isinstance ( mapping , MultiDict ) : \n dict . __init__ ( self , ( ( k , l [ : ] ) for k , l in mapping . iterlists ( ) ) ) \n elif isinstance ( mapping , dict ) : \n tmp = { } \n for key , value in mapping . iteritems ( ) : \n if isinstance ( value , ( tuple , list ) ) : \n value = list ( value ) \n else : \n value = [ value ] \n tmp [ key ] = value \n dict . __init__ ( self , tmp ) \n else : \n tmp = { } \n for key , value in mapping or ( ) : \n tmp . setdefault ( key , [ ] ) . append ( value ) \n dict . __init__ ( self , tmp ) \n def __getstate__ ( self ) : \n return dict ( self . lists ( ) ) \n def __setstate__ ( self , value ) : \n dict . clear ( self ) \n dict . update ( self , value ) \n def __iter__ ( self ) : \n return self . iterkeys ( ) \n def __getitem__ ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n if key in self : \n return dict . __getitem__ ( self , key ) [ <NUM_LIT:0> ] \n raise self . KeyError ( key ) \n def __setitem__ ( self , key , value ) : \n \"\"\"<STR_LIT>\"\"\" \n dict . __setitem__ ( self , key , [ value ] ) \n def add ( self , key , value ) : \n \"\"\"<STR_LIT>\"\"\" \n dict . setdefault ( self , key , [ ] ) . append ( value ) \n def getlist ( self , key , type = None ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n rv = dict . __getitem__ ( self , key ) \n except KeyError : \n return [ ] \n if type is None : \n return list ( rv ) \n result = [ ] \n for item in rv : \n try : \n result . append ( type ( item ) ) \n except ValueError : \n pass \n return result \n def setlist ( self , key , new_list ) : \n \"\"\"<STR_LIT>\"\"\" \n dict . __setitem__ ( self , key , list ( new_list ) ) \n def setdefault ( self , key , default = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if key not in self : \n self [ key ] = default \n else : \n default = self [ key ] \n return default \n def setlistdefault ( self , key , default_list = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if key not in self : \n default_list = list ( default_list or ( ) ) \n dict . __setitem__ ( self , key , default_list ) \n else : \n default_list = dict . __getitem__ ( self , key ) \n return default_list \n def items ( self , multi = False ) : \n \"\"\"<STR_LIT>\"\"\" \n return list ( self . iteritems ( multi ) ) \n def lists ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return list ( self . iterlists ( ) ) \n def values ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return [ self [ key ] for key in self . iterkeys ( ) ] \n def listvalues ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return list ( self . iterlistvalues ( ) ) \n def iteritems ( self , multi = False ) : \n \"\"\"<STR_LIT>\"\"\" \n for key , values in dict . iteritems ( self ) : \n if multi : \n for value in values : \n yield key , value \n else : \n yield key , values [ <NUM_LIT:0> ] \n def iterlists ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for key , values in dict . iteritems ( self ) : \n yield key , list ( values ) \n def itervalues ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for values in dict . itervalues ( self ) : \n yield values [ <NUM_LIT:0> ] \n def iterlistvalues ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for values in dict . itervalues ( self ) : \n yield list ( values ) \n def copy ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . __class__ ( self ) \n def to_dict ( self , flat = True ) : \n \"\"\"<STR_LIT>\"\"\" \n if flat : \n return dict ( self . iteritems ( ) ) \n return dict ( self . lists ( ) ) \n def update ( self , other_dict ) : \n \"\"\"<STR_LIT>\"\"\" \n for key , value in iter_multi_items ( other_dict ) : \n MultiDict . add ( self , key , value ) \n def pop ( self , key , default = _missing ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n return dict . pop ( self , key ) [ <NUM_LIT:0> ] \n except KeyError , e : \n if default is not _missing : \n return default \n raise self . KeyError ( str ( e ) ) \n def popitem ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n item = dict . popitem ( self ) \n return ( item [ <NUM_LIT:0> ] , item [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] ) \n except KeyError , e : \n raise self . KeyError ( str ( e ) ) \n def poplist ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n return dict . pop ( self , key , [ ] ) \n def popitemlist ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n return dict . popitem ( self ) \n except KeyError , e : \n raise self . KeyError ( str ( e ) ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( self . __class__ . __name__ , self . items ( multi = True ) ) \n class _omd_bucket ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n __slots__ = ( '<STR_LIT>' , '<STR_LIT:key>' , '<STR_LIT:value>' , '<STR_LIT>' ) \n def __init__ ( self , omd , key , value ) : \n self . prev = omd . _last_bucket \n self . key = key \n self . value = value \n self . next = None \n if omd . _first_bucket is None : \n omd . _first_bucket = self \n if omd . _last_bucket is not None : \n omd . _last_bucket . next = self \n omd . _last_bucket = self \n def unlink ( self , omd ) : \n if self . prev : \n self . prev . next = self . next \n if self . next : \n self . next . prev = self . prev \n if omd . _first_bucket is self : \n omd . _first_bucket = self . next \n if omd . _last_bucket is self : \n omd . _last_bucket = self . prev \n class OrderedMultiDict ( MultiDict ) : \n \"\"\"<STR_LIT>\"\"\" \n KeyError = None \n def __init__ ( self , mapping = None ) : \n dict . __init__ ( self ) \n self . _first_bucket = self . _last_bucket = None \n if mapping is not None : \n OrderedMultiDict . update ( self , mapping ) \n def __eq__ ( self , other ) : \n if not isinstance ( other , MultiDict ) : \n return NotImplemented \n if isinstance ( other , OrderedMultiDict ) : \n iter1 = self . iteritems ( multi = True ) \n iter2 = other . iteritems ( multi = True ) \n try : \n for k1 , v1 in iter1 : \n k2 , v2 = iter2 . next ( ) \n if k1 != k2 or v1 != v2 : \n return False \n except StopIteration : \n return False \n try : \n iter2 . next ( ) \n except StopIteration : \n return True \n return False \n if len ( self ) != len ( other ) : \n return False \n for key , values in self . iterlists ( ) : \n if other . getlist ( key ) != values : \n return False \n return True \n def __ne__ ( self , other ) : \n return not self . __eq__ ( other ) \n def __reduce_ex__ ( self , protocol ) : \n return type ( self ) , ( self . items ( multi = True ) , ) \n def __getstate__ ( self ) : \n return self . items ( multi = True ) \n def __setstate__ ( self , values ) : \n dict . clear ( self ) \n for key , value in values : \n self . add ( key , value ) \n def __getitem__ ( self , key ) : \n if key in self : \n return dict . __getitem__ ( self , key ) [ <NUM_LIT:0> ] . value \n raise self . KeyError ( key ) \n def __setitem__ ( self , key , value ) : \n self . poplist ( key ) \n self . add ( key , value ) \n def __delitem__ ( self , key ) : \n self . pop ( key ) \n def iterkeys ( self ) : \n return ( key for key , value in self . iteritems ( ) ) \n def itervalues ( self ) : \n return ( value for key , value in self . iteritems ( ) ) \n def iteritems ( self , multi = False ) : \n ptr = self . _first_bucket \n if multi : \n while ptr is not None : \n yield ptr . key , ptr . value \n ptr = ptr . next \n else : \n returned_keys = set ( ) \n while ptr is not None : \n if ptr . key not in returned_keys : \n returned_keys . add ( ptr . key ) \n yield ptr . key , ptr . value \n ptr = ptr . next \n def iterlists ( self ) : \n returned_keys = set ( ) \n ptr = self . _first_bucket \n while ptr is not None : \n if ptr . key not in returned_keys : \n yield ptr . key , self . getlist ( ptr . key ) \n returned_keys . add ( ptr . key ) \n ptr = ptr . next \n def iterlistvalues ( self ) : \n for key , values in self . iterlists ( ) : \n yield values \n def add ( self , key , value ) : \n dict . setdefault ( self , key , [ ] ) . append ( _omd_bucket ( self , key , value ) ) \n def getlist ( self , key , type = None ) : \n try : \n rv = dict . __getitem__ ( self , key ) \n except KeyError : \n return [ ] \n if type is None : \n return [ x . value for x in rv ] \n result = [ ] \n for item in rv : \n try : \n result . append ( type ( item . value ) ) \n except ValueError : \n pass \n return result \n def setlist ( self , key , new_list ) : \n self . poplist ( key ) \n for value in new_list : \n self . add ( key , value ) \n def setlistdefault ( self , key , default_list = None ) : \n raise TypeError ( '<STR_LIT>' \n '<STR_LIT>' ) \n def update ( self , mapping ) : \n for key , value in iter_multi_items ( mapping ) : \n OrderedMultiDict . add ( self , key , value ) \n def poplist ( self , key ) : \n buckets = dict . pop ( self , key , ( ) ) \n for bucket in buckets : \n bucket . unlink ( self ) \n return [ x . value for x in buckets ] \n def pop ( self , key , default = _missing ) : \n try : \n buckets = dict . pop ( self , key ) \n except KeyError , e : \n if default is not _missing : \n return default \n raise self . KeyError ( str ( e ) ) \n for bucket in buckets : \n bucket . unlink ( self ) \n return buckets [ <NUM_LIT:0> ] . value \n def popitem ( self ) : \n try : \n key , buckets = dict . popitem ( self ) \n except KeyError , e : \n raise self . KeyError ( str ( e ) ) \n for bucket in buckets : \n bucket . unlink ( self ) \n return key , buckets [ <NUM_LIT:0> ] . value \n def popitemlist ( self ) : \n try : \n key , buckets = dict . popitem ( self ) \n except KeyError , e : \n raise self . KeyError ( str ( e ) ) \n for bucket in buckets : \n bucket . unlink ( self ) \n return key , [ x . value for x in buckets ] \n def _options_header_vkw ( value , kw ) : \n if not kw : \n return value \n return dump_options_header ( value , dict ( ( k . replace ( '<STR_LIT:_>' , '<STR_LIT:->' ) , v ) \n for k , v in kw . items ( ) ) ) \n class Headers ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n KeyError = None \n def __init__ ( self , defaults = None , _list = None ) : \n if _list is None : \n _list = [ ] \n self . _list = _list \n if defaults is not None : \n if isinstance ( defaults , ( list , Headers ) ) : \n self . _list . extend ( defaults ) \n else : \n self . extend ( defaults ) \n @ classmethod \n def linked ( cls , headerlist ) : \n \"\"\"<STR_LIT>\"\"\" \n return cls ( _list = headerlist ) \n def __getitem__ ( self , key , _index_operation = True ) : \n if _index_operation : \n if isinstance ( key , ( int , long ) ) : \n return self . _list [ key ] \n elif isinstance ( key , slice ) : \n return self . __class__ ( self . _list [ key ] ) \n ikey = key . lower ( ) \n for k , v in self . _list : \n if k . lower ( ) == ikey : \n return v \n raise self . KeyError ( key ) \n def __eq__ ( self , other ) : \n return other . __class__ is self . __class__ and set ( other . _list ) == set ( self . _list ) \n def __ne__ ( self , other ) : \n return not self . __eq__ ( other ) \n def get ( self , key , default = None , type = None ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n rv = self . __getitem__ ( key , _index_operation = False ) \n except KeyError : \n return default \n if type is None : \n return rv \n try : \n return type ( rv ) \n except ValueError : \n return default \n def getlist ( self , key , type = None ) : \n \"\"\"<STR_LIT>\"\"\" \n ikey = key . lower ( ) \n result = [ ] \n for k , v in self : \n if k . lower ( ) == ikey : \n if type is not None : \n try : \n v = type ( v ) \n except ValueError : \n continue \n result . append ( v ) \n return result \n def get_all ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . getlist ( name ) \n def iteritems ( self , lower = False ) : \n for key , value in self : \n if lower : \n key = key . lower ( ) \n yield key , value \n def iterkeys ( self , lower = False ) : \n for key , _ in self . iteritems ( lower ) : \n yield key \n def itervalues ( self ) : \n for _ , value in self . iteritems ( ) : \n yield value \n def keys ( self , lower = False ) : \n return list ( self . iterkeys ( lower ) ) \n def values ( self ) : \n return list ( self . itervalues ( ) ) \n def items ( self , lower = False ) : \n return list ( self . iteritems ( lower ) ) \n def extend ( self , iterable ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( iterable , dict ) : \n for key , value in iterable . iteritems ( ) : \n if isinstance ( value , ( tuple , list ) ) : \n for v in value : \n self . add ( key , v ) \n else : \n self . add ( key , value ) \n else : \n for key , value in iterable : \n self . add ( key , value ) \n def __delitem__ ( self , key , _index_operation = True ) : \n if _index_operation and isinstance ( key , ( int , long , slice ) ) : \n del self . _list [ key ] \n return \n key = key . lower ( ) \n new = [ ] \n for k , v in self . _list : \n if k . lower ( ) != key : \n new . append ( ( k , v ) ) \n self . _list [ : ] = new \n def remove ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . __delitem__ ( key , _index_operation = False ) \n def pop ( self , key = None , default = _missing ) : \n \"\"\"<STR_LIT>\"\"\" \n if key is None : \n return self . _list . pop ( ) \n if isinstance ( key , ( int , long ) ) : \n return self . _list . pop ( key ) \n try : \n rv = self [ key ] \n self . remove ( key ) \n except KeyError : \n if default is not _missing : \n return default \n raise \n return rv \n def popitem ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . pop ( ) \n def __contains__ ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n self . __getitem__ ( key , _index_operation = False ) \n except KeyError : \n return False \n return True \n has_key = __contains__ \n def __iter__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return iter ( self . _list ) \n def __len__ ( self ) : \n return len ( self . _list ) \n def add ( self , _key , _value , ** kw ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _list . append ( ( _key , _options_header_vkw ( _value , kw ) ) ) \n def add_header ( self , _key , _value , ** _kw ) : \n \"\"\"<STR_LIT>\"\"\" \n self . add ( _key , _value , ** _kw ) \n def clear ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n del self . _list [ : ] \n def set ( self , _key , _value , ** kw ) : \n \"\"\"<STR_LIT>\"\"\" \n lc_key = _key . lower ( ) \n _value = _options_header_vkw ( _value , kw ) \n for idx , ( old_key , old_value ) in enumerate ( self . _list ) : \n if old_key . lower ( ) == lc_key : \n self . _list [ idx ] = ( _key , _value ) \n break \n else : \n return self . add ( _key , _value ) \n self . _list [ idx + <NUM_LIT:1> : ] = [ ( k , v ) for k , v in self . _list [ idx + <NUM_LIT:1> : ] \n if k . lower ( ) != lc_key ] \n def setdefault ( self , key , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if key in self : \n return self [ key ] \n self . set ( key , value ) \n return value \n def __setitem__ ( self , key , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( key , ( slice , int , long ) ) : \n self . _list [ key ] = value \n else : \n self . set ( key , value ) \n def to_list ( self , charset = '<STR_LIT:utf-8>' ) : \n \"\"\"<STR_LIT>\"\"\" \n result = [ ] \n for k , v in self : \n if isinstance ( v , unicode ) : \n v = v . encode ( charset ) \n else : \n v = str ( v ) \n result . append ( ( k , v ) ) \n return result \n def copy ( self ) : \n return self . __class__ ( self . _list ) \n def __copy__ ( self ) : \n return self . copy ( ) \n def __str__ ( self , charset = '<STR_LIT:utf-8>' ) : \n \"\"\"<STR_LIT>\"\"\" \n strs = [ ] \n for key , value in self . to_list ( charset ) : \n strs . append ( '<STR_LIT>' % ( key , value ) ) \n strs . append ( '<STR_LIT:\\r\\n>' ) \n return '<STR_LIT:\\r\\n>' . join ( strs ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( \n self . __class__ . __name__ , \n list ( self ) \n ) \n class ImmutableHeadersMixin ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __delitem__ ( self , key ) : \n is_immutable ( self ) \n def __setitem__ ( self , key , value ) : \n is_immutable ( self ) \n set = __setitem__ \n def add ( self , item ) : \n is_immutable ( self ) \n remove = add_header = add \n def extend ( self , iterable ) : \n is_immutable ( self ) \n def insert ( self , pos , value ) : \n is_immutable ( self ) \n def pop ( self , index = - <NUM_LIT:1> ) : \n is_immutable ( self ) \n def popitem ( self ) : \n is_immutable ( self ) \n def setdefault ( self , key , default ) : \n is_immutable ( self ) \n class EnvironHeaders ( ImmutableHeadersMixin , Headers ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , environ ) : \n self . environ = environ \n @ classmethod \n def linked ( cls , environ ) : \n raise TypeError ( '<STR_LIT>' \n '<STR_LIT>' % cls . __name__ ) \n def __eq__ ( self , other ) : \n return self . environ is other . environ \n def __getitem__ ( self , key , _index_operation = False ) : \n key = key . upper ( ) . replace ( '<STR_LIT:->' , '<STR_LIT:_>' ) \n if key in ( '<STR_LIT>' , '<STR_LIT>' ) : \n return self . environ [ key ] \n return self . environ [ '<STR_LIT>' + key ] \n def __len__ ( self ) : \n return len ( list ( iter ( self ) ) ) \n def __iter__ ( self ) : \n for key , value in self . environ . iteritems ( ) : \n if key . startswith ( '<STR_LIT>' ) and key not in ( '<STR_LIT>' , '<STR_LIT>' ) : \n yield key [ <NUM_LIT:5> : ] . replace ( '<STR_LIT:_>' , '<STR_LIT:->' ) . title ( ) , value \n elif key in ( '<STR_LIT>' , '<STR_LIT>' ) : \n yield key . replace ( '<STR_LIT:_>' , '<STR_LIT:->' ) . title ( ) , value \n def copy ( self ) : \n raise TypeError ( '<STR_LIT>' % self . __class__ . __name__ ) \n class CombinedMultiDict ( ImmutableMultiDictMixin , MultiDict ) : \n \"\"\"<STR_LIT>\"\"\" \n def __reduce_ex__ ( self , protocol ) : \n return type ( self ) , ( self . dicts , ) \n def __init__ ( self , dicts = None ) : \n self . dicts = dicts or [ ] \n @ classmethod \n def fromkeys ( cls ) : \n raise TypeError ( '<STR_LIT>' % \n cls . __name__ ) \n def __getitem__ ( self , key ) : \n for d in self . dicts : \n if key in d : \n return d [ key ] \n raise self . KeyError ( key ) \n def get ( self , key , default = None , type = None ) : \n for d in self . dicts : \n if key in d : \n if type is not None : \n try : \n return type ( d [ key ] ) \n except ValueError : \n continue \n return d [ key ] \n return default \n def getlist ( self , key , type = None ) : \n rv = [ ] \n for d in self . dicts : \n rv . extend ( d . getlist ( key , type ) ) \n return rv \n def keys ( self ) : \n rv = set ( ) \n for d in self . dicts : \n rv . update ( d . keys ( ) ) \n return list ( rv ) \n def iteritems ( self , multi = False ) : \n found = set ( ) \n for d in self . dicts : \n for key , value in d . iteritems ( multi ) : \n if multi : \n yield key , value \n elif key not in found : \n found . add ( key ) \n yield key , value \n def itervalues ( self ) : \n for key , value in self . iteritems ( ) : \n yield value \n def values ( self ) : \n return list ( self . itervalues ( ) ) \n def items ( self , multi = False ) : \n return list ( self . iteritems ( multi ) ) \n def iterlists ( self ) : \n rv = { } \n for d in self . dicts : \n for key , values in d . iterlists ( ) : \n rv . setdefault ( key , [ ] ) . extend ( values ) \n return rv . iteritems ( ) \n def lists ( self ) : \n return list ( self . iterlists ( ) ) \n def iterlistvalues ( self ) : \n return ( x [ <NUM_LIT:0> ] for x in self . lists ( ) ) \n def listvalues ( self ) : \n return list ( self . iterlistvalues ( ) ) \n def iterkeys ( self ) : \n return iter ( self . keys ( ) ) \n __iter__ = iterkeys \n def copy ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . __class__ ( self . dicts [ : ] ) \n def to_dict ( self , flat = True ) : \n \"\"\"<STR_LIT>\"\"\" \n rv = { } \n for d in reversed ( self . dicts ) : \n rv . update ( d . to_dict ( flat ) ) \n return rv \n def __len__ ( self ) : \n return len ( self . keys ( ) ) \n def __contains__ ( self , key ) : \n for d in self . dicts : \n if key in d : \n return True \n return False \n has_key = __contains__ \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( self . __class__ . __name__ , self . dicts ) \n class FileMultiDict ( MultiDict ) : \n \"\"\"<STR_LIT>\"\"\" \n def add_file ( self , name , file , filename = None , content_type = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( file , FileStorage ) : \n self [ name ] = file \n return \n if isinstance ( file , basestring ) : \n if filename is None : \n filename = file \n file = open ( file , '<STR_LIT:rb>' ) \n if filename and content_type is None : \n content_type = mimetypes . guess_type ( filename ) [ <NUM_LIT:0> ] or '<STR_LIT>' \n self [ name ] = FileStorage ( file , filename , name , content_type ) \n class ImmutableDict ( ImmutableDictMixin , dict ) : \n \"\"\"<STR_LIT>\"\"\" \n __repr__ = _proxy_repr ( dict ) \n def copy ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return dict ( self ) \n def __copy__ ( self ) : \n return self \n class ImmutableMultiDict ( ImmutableMultiDictMixin , MultiDict ) : \n \"\"\"<STR_LIT>\"\"\" \n def copy ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return MultiDict ( self ) \n def __copy__ ( self ) : \n return self \n class ImmutableOrderedMultiDict ( ImmutableMultiDictMixin , OrderedMultiDict ) : \n \"\"\"<STR_LIT>\"\"\" \n def copy ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return OrderedMultiDict ( self ) \n def __copy__ ( self ) : \n return self \n class Accept ( ImmutableList ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , values = ( ) ) : \n if values is None : \n list . __init__ ( self ) \n self . provided = False \n elif isinstance ( values , Accept ) : \n self . provided = values . provided \n list . __init__ ( self , values ) \n else : \n self . provided = True \n values = [ ( a , b ) for b , a in values ] \n values . sort ( ) \n values . reverse ( ) \n list . __init__ ( self , [ ( a , b ) for b , a in values ] ) \n def _value_matches ( self , value , item ) : \n \"\"\"<STR_LIT>\"\"\" \n return item == '<STR_LIT:*>' or item . lower ( ) == value . lower ( ) \n def __getitem__ ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( key , basestring ) : \n return self . quality ( key ) \n return list . __getitem__ ( self , key ) \n def quality ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n for item , quality in self : \n if self . _value_matches ( key , item ) : \n return quality \n return <NUM_LIT:0> \n def __contains__ ( self , value ) : \n for item , quality in self : \n if self . _value_matches ( value , item ) : \n return True \n return False \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( \n self . __class__ . __name__ , \n '<STR_LIT:U+002CU+0020>' . join ( '<STR_LIT>' % ( x , y ) for x , y in self ) \n ) \n def index ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( key , basestring ) : \n for idx , ( item , quality ) in enumerate ( self ) : \n if self . _value_matches ( key , item ) : \n return idx \n raise ValueError ( key ) \n return list . index ( self , key ) \n def find ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n return self . index ( key ) \n except ValueError : \n return - <NUM_LIT:1> \n def values ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return list ( self . itervalues ( ) ) \n def itervalues ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for item in self : \n yield item [ <NUM_LIT:0> ] \n def to_header ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n result = [ ] \n for value , quality in self : \n if quality != <NUM_LIT:1> : \n value = '<STR_LIT>' % ( value , quality ) \n result . append ( value ) \n return '<STR_LIT:U+002C>' . join ( result ) \n def __str__ ( self ) : \n return self . to_header ( ) \n def best_match ( self , matches , default = None ) : \n \"\"\"<STR_LIT>\"\"\" \n best_quality = - <NUM_LIT:1> \n result = default \n for server_item in matches : \n for client_item , quality in self : \n if quality <= best_quality : \n break \n if self . _value_matches ( client_item , server_item ) : \n best_quality = quality \n result = server_item \n return result \n @ property \n def best ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self : \n return self [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] \n class MIMEAccept ( Accept ) : \n \"\"\"<STR_LIT>\"\"\" \n def _value_matches ( self , value , item ) : \n def _normalize ( x ) : \n x = x . lower ( ) \n return x == '<STR_LIT:*>' and ( '<STR_LIT:*>' , '<STR_LIT:*>' ) or x . split ( '<STR_LIT:/>' , <NUM_LIT:1> ) \n if '<STR_LIT:/>' not in value : \n raise ValueError ( '<STR_LIT>' % value ) \n value_type , value_subtype = _normalize ( value ) \n if value_type == '<STR_LIT:*>' and value_subtype != '<STR_LIT:*>' : \n raise ValueError ( '<STR_LIT>' % value ) \n if '<STR_LIT:/>' not in item : \n return False \n item_type , item_subtype = _normalize ( item ) \n if item_type == '<STR_LIT:*>' and item_subtype != '<STR_LIT:*>' : \n return False \n return ( \n ( item_type == item_subtype == '<STR_LIT:*>' or \n value_type == value_subtype == '<STR_LIT:*>' ) or \n ( item_type == value_type and ( item_subtype == '<STR_LIT:*>' or \n value_subtype == '<STR_LIT:*>' or \n item_subtype == value_subtype ) ) \n ) \n @ property \n def accept_html ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return ( \n '<STR_LIT>' in self or \n '<STR_LIT>' in self or \n self . accept_xhtml \n ) \n @ property \n def accept_xhtml ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return ( \n '<STR_LIT>' in self or \n '<STR_LIT>' in self \n ) \n class LanguageAccept ( Accept ) : \n \"\"\"<STR_LIT>\"\"\" \n def _value_matches ( self , value , item ) : \n def _normalize ( language ) : \n return _locale_delim_re . split ( language . lower ( ) ) \n return item == '<STR_LIT:*>' or _normalize ( value ) == _normalize ( item ) \n class CharsetAccept ( Accept ) : \n \"\"\"<STR_LIT>\"\"\" \n def _value_matches ( self , value , item ) : \n def _normalize ( name ) : \n try : \n return codecs . lookup ( name ) . name \n except LookupError : \n return name . lower ( ) \n return item == '<STR_LIT:*>' or _normalize ( value ) == _normalize ( item ) \n def cache_property ( key , empty , type ) : \n \"\"\"<STR_LIT>\"\"\" \n return property ( lambda x : x . _get_cache_value ( key , empty , type ) , \n lambda x , v : x . _set_cache_value ( key , v , type ) , \n lambda x : x . _del_cache_value ( key ) , \n '<STR_LIT>' % key ) \n class _CacheControl ( UpdateDictMixin , dict ) : \n \"\"\"<STR_LIT>\"\"\" \n no_cache = cache_property ( '<STR_LIT>' , '<STR_LIT:*>' , None ) \n no_store = cache_property ( '<STR_LIT>' , None , bool ) \n max_age = cache_property ( '<STR_LIT>' , - <NUM_LIT:1> , int ) \n no_transform = cache_property ( '<STR_LIT>' , None , None ) \n def __init__ ( self , values = ( ) , on_update = None ) : \n dict . __init__ ( self , values or ( ) ) \n self . on_update = on_update \n self . provided = values is not None \n def _get_cache_value ( self , key , empty , type ) : \n \"\"\"<STR_LIT>\"\"\" \n if type is bool : \n return key in self \n if key in self : \n value = self [ key ] \n if value is None : \n return empty \n elif type is not None : \n try : \n value = type ( value ) \n except ValueError : \n pass \n return value \n def _set_cache_value ( self , key , value , type ) : \n \"\"\"<STR_LIT>\"\"\" \n if type is bool : \n if value : \n self [ key ] = None \n else : \n self . pop ( key , None ) \n else : \n if value is None : \n self . pop ( key ) \n elif value is True : \n self [ key ] = None \n else : \n self [ key ] = value \n def _del_cache_value ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n if key in self : \n del self [ key ] \n def to_header ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return dump_header ( self ) \n def __str__ ( self ) : \n return self . to_header ( ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( \n self . __class__ . __name__ , \n self . to_header ( ) \n ) \n class RequestCacheControl ( ImmutableDictMixin , _CacheControl ) : \n \"\"\"<STR_LIT>\"\"\" \n max_stale = cache_property ( '<STR_LIT>' , '<STR_LIT:*>' , int ) \n min_fresh = cache_property ( '<STR_LIT>' , '<STR_LIT:*>' , int ) \n no_transform = cache_property ( '<STR_LIT>' , None , None ) \n only_if_cached = cache_property ( '<STR_LIT>' , None , bool ) \n class ResponseCacheControl ( _CacheControl ) : \n \"\"\"<STR_LIT>\"\"\" \n public = cache_property ( '<STR_LIT>' , None , bool ) \n private = cache_property ( '<STR_LIT>' , '<STR_LIT:*>' , None ) \n must_revalidate = cache_property ( '<STR_LIT>' , None , bool ) \n proxy_revalidate = cache_property ( '<STR_LIT>' , None , bool ) \n s_maxage = cache_property ( '<STR_LIT>' , None , None ) \n _CacheControl . cache_property = staticmethod ( cache_property ) \n class CallbackDict ( UpdateDictMixin , dict ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , initial = None , on_update = None ) : \n dict . __init__ ( self , initial or ( ) ) \n self . on_update = on_update \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( \n self . __class__ . __name__ , \n dict . __repr__ ( self ) \n ) \n class HeaderSet ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , headers = None , on_update = None ) : \n self . _headers = list ( headers or ( ) ) \n self . _set = set ( [ x . lower ( ) for x in self . _headers ] ) \n self . on_update = on_update \n def add ( self , header ) : \n \"\"\"<STR_LIT>\"\"\" \n self . update ( ( header , ) ) \n def remove ( self , header ) : \n \"\"\"<STR_LIT>\"\"\" \n key = header . lower ( ) \n if key not in self . _set : \n raise KeyError ( header ) \n self . _set . remove ( key ) \n for idx , key in enumerate ( self . _headers ) : \n if key . lower ( ) == header : \n del self . _headers [ idx ] \n break \n if self . on_update is not None : \n self . on_update ( self ) \n def update ( self , iterable ) : \n \"\"\"<STR_LIT>\"\"\" \n inserted_any = False \n for header in iterable : \n key = header . lower ( ) \n if key not in self . _set : \n self . _headers . append ( header ) \n self . _set . add ( key ) \n inserted_any = True \n if inserted_any and self . on_update is not None : \n self . on_update ( self ) \n def discard ( self , header ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n return self . remove ( header ) \n except KeyError : \n pass \n def find ( self , header ) : \n \"\"\"<STR_LIT>\"\"\" \n header = header . lower ( ) \n for idx , item in enumerate ( self . _headers ) : \n if item . lower ( ) == header : \n return idx \n return - <NUM_LIT:1> \n def index ( self , header ) : \n \"\"\"<STR_LIT>\"\"\" \n rv = self . find ( header ) \n if rv < <NUM_LIT:0> : \n raise IndexError ( header ) \n return rv \n def clear ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _set . clear ( ) \n del self . _headers [ : ] \n if self . on_update is not None : \n self . on_update ( self ) \n def as_set ( self , preserve_casing = False ) : \n \"\"\"<STR_LIT>\"\"\" \n if preserve_casing : \n return set ( self . _headers ) \n return set ( self . _set ) \n def to_header ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return '<STR_LIT:U+002CU+0020>' . join ( map ( quote_header_value , self . _headers ) ) \n def __getitem__ ( self , idx ) : \n return self . _headers [ idx ] \n def __delitem__ ( self , idx ) : \n rv = self . _headers . pop ( idx ) \n self . _set . remove ( rv . lower ( ) ) \n if self . on_update is not None : \n self . on_update ( self ) \n def __setitem__ ( self , idx , value ) : \n old = self . _headers [ idx ] \n self . _set . remove ( old . lower ( ) ) \n self . _headers [ idx ] = value \n self . _set . add ( value . lower ( ) ) \n if self . on_update is not None : \n self . on_update ( self ) \n def __contains__ ( self , header ) : \n return header . lower ( ) in self . _set \n def __len__ ( self ) : \n return len ( self . _set ) \n def __iter__ ( self ) : \n return iter ( self . _headers ) \n def __nonzero__ ( self ) : \n return bool ( self . _set ) \n def __str__ ( self ) : \n return self . to_header ( ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( \n self . __class__ . __name__ , \n self . _headers \n ) \n class ETags ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , strong_etags = None , weak_etags = None , star_tag = False ) : \n self . _strong = frozenset ( not star_tag and strong_etags or ( ) ) \n self . _weak = frozenset ( weak_etags or ( ) ) \n self . star_tag = star_tag \n def as_set ( self , include_weak = False ) : \n \"\"\"<STR_LIT>\"\"\" \n rv = set ( self . _strong ) \n if include_weak : \n rv . update ( self . _weak ) \n return rv \n def is_weak ( self , etag ) : \n \"\"\"<STR_LIT>\"\"\" \n return etag in self . _weak \n def contains_weak ( self , etag ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . is_weak ( etag ) or self . contains ( etag ) \n def contains ( self , etag ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . star_tag : \n return True \n return etag in self . _strong \n def contains_raw ( self , etag ) : \n \"\"\"<STR_LIT>\"\"\" \n etag , weak = unquote_etag ( etag ) \n if weak : \n return self . contains_weak ( etag ) \n return self . contains ( etag ) \n def to_header ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . star_tag : \n return '<STR_LIT:*>' \n return '<STR_LIT:U+002CU+0020>' . join ( \n [ '<STR_LIT>' % x for x in self . _strong ] + \n [ '<STR_LIT>' % x for x in self . _weak ] \n ) \n def __call__ ( self , etag = None , data = None , include_weak = False ) : \n if [ etag , data ] . count ( None ) != <NUM_LIT:1> : \n raise TypeError ( '<STR_LIT>' ) \n if etag is None : \n etag = generate_etag ( data ) \n if include_weak : \n if etag in self . _weak : \n return True \n return etag in self . _strong \n def __nonzero__ ( self ) : \n return bool ( self . star_tag or self . _strong ) \n def __str__ ( self ) : \n return self . to_header ( ) \n def __iter__ ( self ) : \n return iter ( self . _strong ) \n def __contains__ ( self , etag ) : \n return self . contains ( etag ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( self . __class__ . __name__ , str ( self ) ) \n class Authorization ( ImmutableDictMixin , dict ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , auth_type , data = None ) : \n dict . __init__ ( self , data or { } ) \n self . type = auth_type \n username = property ( lambda x : x . get ( '<STR_LIT:username>' ) , doc = '''<STR_LIT>''' ) \n password = property ( lambda x : x . get ( '<STR_LIT:password>' ) , doc = '''<STR_LIT>''' ) \n realm = property ( lambda x : x . get ( '<STR_LIT>' ) , doc = '''<STR_LIT>''' ) \n nonce = property ( lambda x : x . get ( '<STR_LIT>' ) , doc = '''<STR_LIT>''' ) \n uri = property ( lambda x : x . get ( '<STR_LIT>' ) , doc = '''<STR_LIT>''' ) \n nc = property ( lambda x : x . get ( '<STR_LIT>' ) , doc = '''<STR_LIT>''' ) \n cnonce = property ( lambda x : x . get ( '<STR_LIT>' ) , doc = '''<STR_LIT>''' ) \n response = property ( lambda x : x . get ( '<STR_LIT>' ) , doc = '''<STR_LIT>''' ) \n opaque = property ( lambda x : x . get ( '<STR_LIT>' ) , doc = '''<STR_LIT>''' ) \n @ property \n def qop ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n def on_update ( header_set ) : \n if not header_set and '<STR_LIT>' in self : \n del self [ '<STR_LIT>' ] \n elif header_set : \n self [ '<STR_LIT>' ] = header_set . to_header ( ) \n return parse_set_header ( self . get ( '<STR_LIT>' ) , on_update ) \n class WWWAuthenticate ( UpdateDictMixin , dict ) : \n \"\"\"<STR_LIT>\"\"\" \n _require_quoting = frozenset ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n def __init__ ( self , auth_type = None , values = None , on_update = None ) : \n dict . __init__ ( self , values or ( ) ) \n if auth_type : \n self [ '<STR_LIT>' ] = auth_type \n self . on_update = on_update \n def set_basic ( self , realm = '<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n dict . clear ( self ) \n dict . update ( self , { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : realm } ) \n if self . on_update : \n self . on_update ( self ) \n def set_digest ( self , realm , nonce , qop = ( '<STR_LIT>' , ) , opaque = None , \n algorithm = None , stale = False ) : \n \"\"\"<STR_LIT>\"\"\" \n d = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : realm , \n '<STR_LIT>' : nonce , \n '<STR_LIT>' : dump_header ( qop ) \n } \n if stale : \n d [ '<STR_LIT>' ] = '<STR_LIT>' \n if opaque is not None : \n d [ '<STR_LIT>' ] = opaque \n if algorithm is not None : \n d [ '<STR_LIT>' ] = algorithm \n dict . clear ( self ) \n dict . update ( self , d ) \n if self . on_update : \n self . on_update ( self ) \n def to_header ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n d = dict ( self ) \n auth_type = d . pop ( '<STR_LIT>' , None ) or '<STR_LIT>' \n return '<STR_LIT>' % ( auth_type . title ( ) , '<STR_LIT:U+002CU+0020>' . join ( [ \n '<STR_LIT>' % ( key , quote_header_value ( value , \n allow_token = key not in self . _require_quoting ) ) \n for key , value in d . iteritems ( ) \n ] ) ) \n def __str__ ( self ) : \n return self . to_header ( ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( \n self . __class__ . __name__ , \n self . to_header ( ) \n ) \n def auth_property ( name , doc = None ) : \n \"\"\"<STR_LIT>\"\"\" \n def _set_value ( self , value ) : \n if value is None : \n self . pop ( name , None ) \n else : \n self [ name ] = str ( value ) \n return property ( lambda x : x . get ( name ) , _set_value , doc = doc ) \n def _set_property ( name , doc = None ) : \n def fget ( self ) : \n def on_update ( header_set ) : \n if not header_set and name in self : \n del self [ name ] \n elif header_set : \n self [ name ] = header_set . to_header ( ) \n return parse_set_header ( self . get ( name ) , on_update ) \n return property ( fget , doc = doc ) \n type = auth_property ( '<STR_LIT>' , doc = '''<STR_LIT>''' ) \n realm = auth_property ( '<STR_LIT>' , doc = '''<STR_LIT>''' ) \n domain = _set_property ( '<STR_LIT>' , doc = '''<STR_LIT>''' ) \n nonce = auth_property ( '<STR_LIT>' , doc = '''<STR_LIT>''' ) \n opaque = auth_property ( '<STR_LIT>' , doc = '''<STR_LIT>''' ) \n algorithm = auth_property ( '<STR_LIT>' , doc = '''<STR_LIT>''' ) \n qop = _set_property ( '<STR_LIT>' , doc = '''<STR_LIT>''' ) \n def _get_stale ( self ) : \n val = self . get ( '<STR_LIT>' ) \n if val is not None : \n return val . lower ( ) == '<STR_LIT:true>' \n def _set_stale ( self , value ) : \n if value is None : \n self . pop ( '<STR_LIT>' , None ) \n else : \n self [ '<STR_LIT>' ] = value and '<STR_LIT>' or '<STR_LIT>' \n stale = property ( _get_stale , _set_stale , doc = '''<STR_LIT>''' ) \n del _get_stale , _set_stale \n auth_property = staticmethod ( auth_property ) \n del _set_property \n class FileStorage ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , stream = None , filename = None , name = None , \n content_type = '<STR_LIT>' , content_length = - <NUM_LIT:1> , \n headers = None ) : \n self . name = name \n self . stream = stream or _empty_stream \n self . filename = filename or getattr ( stream , '<STR_LIT:name>' , None ) \n self . content_type = content_type \n self . content_length = content_length \n if headers is None : \n headers = Headers ( ) \n self . headers = headers \n def save ( self , dst , buffer_size = <NUM_LIT> ) : \n \"\"\"<STR_LIT>\"\"\" \n from shutil import copyfileobj \n close_dst = False \n if isinstance ( dst , basestring ) : \n dst = file ( dst , '<STR_LIT:wb>' ) \n close_dst = True \n try : \n copyfileobj ( self . stream , dst , buffer_size ) \n finally : \n if close_dst : \n dst . close ( ) \n def close ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n self . stream . close ( ) \n except : \n pass \n def __nonzero__ ( self ) : \n return bool ( self . filename ) \n def __getattr__ ( self , name ) : \n return getattr ( self . stream , name ) \n def __iter__ ( self ) : \n return iter ( self . readline , '<STR_LIT>' ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( \n self . __class__ . __name__ , \n self . filename , \n self . content_type \n ) \n from werkzeug . http import dump_options_header , dump_header , generate_etag , quote_header_value , parse_set_header , unquote_etag \n from werkzeug . exceptions import BadRequest \n for _cls in MultiDict , OrderedMultiDict , CombinedMultiDict , Headers , EnvironHeaders : \n _cls . KeyError = BadRequest . wrap ( KeyError , _cls . __name__ + '<STR_LIT>' ) \n del <mask0> \n", "gt": "_cls"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n import os \n from werkzeug . utils import import_string \n from kay . management . shell import ( \n rshell , shell , clear_datastore , create_user , \n ) \n from kay . management . runserver import runserver_passthru_argv \n from kay . management . startapp import startapp \n from kay . management . startapp import startproject \n from kay . management . appcfg import do_appcfg_passthru_argv \n from kay . management . bulkloader import ( \n do_bulkloader_passthru_argv , dump_all , restore_all , \n ) \n from kay . management . test import do_runtest \n from kay . management . preparse import do_preparse_bundle \n from kay . management . preparse import do_preparse_apps \n from kay . management . extract_messages import do_extract_messages \n from kay . management . add_translations import do_add_translations \n from kay . management . update_translations import do_update_translations \n from kay . management . compile_translations import do_compile_translations \n from kay . management . wxadmin import do_wxadmin \n from kay . management . compile_media import do_compile_media \n from kay . conf import settings \n action_dump_all = dump_all \n action_restore_all = restore_all \n action_shell = shell \n action_rshell = rshell \n action_startapp = startapp \n action_startproject = startproject \n action_test = do_runtest \n action_preparse_bundle = do_preparse_bundle \n action_preparse_apps = do_preparse_apps \n action_extract_messages = do_extract_messages \n action_add_translations = do_add_translations \n action_update_translations = do_update_translations \n action_compile_translations = do_compile_translations \n action_appcfg = do_appcfg_passthru_argv \n action_runserver = runserver_passthru_argv \n action_bulkloader = do_bulkloader_passthru_argv \n action_clear_datastore = clear_datastore \n action_create_user = create_user \n action_wxadmin = do_wxadmin \n action_compile_media = do_compile_media \n additional_actions = [ ] \n for app in settings . INSTALLED_APPS : \n try : \n appmod = import_string ( app ) \n if not os . path . exists ( os . path . join ( os . path . dirname ( appmod . __file__ ) , \n '<STR_LIT>' ) ) : \n continue \n management_mod = import_string ( \"<STR_LIT>\" % app ) \n for name , val in vars ( management_mod ) . iteritems ( ) : \n if name . startswith ( \"<STR_LIT>\" ) : \n locals ( ) [ name ] = getattr ( management_mod , name ) \n additional_actions . append ( name ) \n except Exception , e : \n import traceback \n sys . stderr . write ( '<STR_LIT:\\n>' . join ( traceback . format_exception ( * ( sys . exc_info ( ) ) ) ) ) \n pass \n __all__ = [ \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n ] + additional_actions \n def print_status ( msg ) : \n print ( msg ) \n sys . stdout . <mask0> ( ) \n", "gt": "flush"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from kay . routing import ( \n ViewGroup , Rule \n ) \n view_groups = [ \n ViewGroup ( \n Rule ( '<STR_LIT>' , endpoint = '<STR_LIT>' , \n view = ( '<STR_LIT>' , ( ) , { } ) ) , \n Rule ( '<STR_LIT>' , endpoint = '<STR_LIT>' , \n view = ( '<STR_LIT>' , ( ) , { } ) ) , \n Rule ( '<STR_LIT>' , endpoint = '<STR_LIT>' , \n <mask0> = '<STR_LIT>' ) , \n ) \n ] \n", "gt": "view"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from google . appengine . ext import db \n from kay . utils . forms import ValidationError \n from kay . utils . forms . modelform import ModelForm \n class MaxLengthValidator ( object ) : \n def __init__ ( self , length ) : \n self . length = length \n def __call__ ( self , val ) : \n if len ( val ) > self . length : \n raise ValidationError ( \"<STR_LIT>\" ) \n return True \n class TestModel ( db . Model ) : \n number = db . IntegerProperty ( required = True ) \n data_field = db . StringProperty ( required = True , \n validator = MaxLengthValidator ( <NUM_LIT:20> ) ) \n is_active = db . BooleanProperty ( required = True ) \n string_list_field = db . StringListProperty ( required = True ) \n class TestModel2 ( db . Model ) : \n number = db . IntegerProperty ( required = True ) \n data_field = db . StringProperty ( required = True , \n validator = MaxLengthValidator ( <NUM_LIT:20> ) ) \n is_active = db . BooleanProperty ( required = True ) \n string_list_field = db . StringListProperty ( required = True ) \n class TestModelForm ( ModelForm ) : \n csrf_protected = False \n class Meta ( ) : \n model = TestModel \n def __init__ ( self , instance = None , initial = None ) : \n super ( TestModelForm , self ) . __init__ ( instance , initial ) \n self . string_list_field . min_size = <NUM_LIT:1> \n class JsonTestModel ( db . Model ) : \n s = db . StringProperty ( ) \n i = db . IntegerProperty ( ) \n b = db . BooleanProperty ( ) \n l = db . StringListProperty ( ) \n r = db . ReferenceProperty ( ) \n class ModelFormTestModel ( db . Model ) : \n s_name = db . StringProperty ( ) \n zip_code = db . StringProperty ( ) \n addr = db . StringProperty ( ) \n class ModelFormTestForm ( ModelForm ) : \n csrf_protected = False \n class Meta : \n model = ModelFormTestModel \n fields = ( '<STR_LIT>' ) \n class ValidationTestModel ( db . Model ) : \n slist = db . StringListProperty ( ) \n class ValidationTestForm ( ModelForm ) : \n csrf_protected = False \n class Meta : \n model = ValidationTestModel \n def context_validate ( self , data ) : \n raise <mask0> ( \"<STR_LIT>\" ) \n", "gt": "ValidationError"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import re \n import sys \n from os import path , listdir , mkdir \n def compile_file ( env , src_path , dst_path , encoding = '<STR_LIT:utf-8>' , base_dir = '<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n src_file = file ( src_path , '<STR_LIT:r>' ) \n try : \n source = src_file . read ( ) . decode ( encoding ) \n except Exception , e : \n sys . stderr . write ( \"<STR_LIT>\" \n \"<STR_LIT>\" % src_path ) \n raise \n src_file . close ( ) \n name = src_path . replace ( base_dir , '<STR_LIT>' ) \n raw = env . compile ( source , name = name , filename = name , raw = True ) \n dst_file = open ( dst_path , '<STR_LIT:wb>' ) \n dst_file . write ( raw ) \n dst_file . close ( ) \n def compile_dir ( env , src_path , dst_path , pattern = r'<STR_LIT>' , \n encoding = '<STR_LIT:utf-8>' , base_dir = None , \n negative_pattern = r'<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n if base_dir is None : \n base_dir = src_path \n for filename in listdir ( src_path ) : \n if filename . startswith ( \"<STR_LIT:.>\" ) : \n continue \n src_name = path . join ( src_path , filename ) \n dst_name = path . join ( dst_path , filename ) \n if path . isdir ( src_name ) : \n if not path . isdir ( dst_name ) : \n mkdir ( dst_name ) \n compile_dir ( env , src_name , dst_name , encoding = encoding , \n base_dir = base_dir ) \n elif path . isfile ( src_name ) and re . match ( pattern , filename ) and not re . match ( negative_pattern , filename ) : \n compile_file ( env , src_name , dst_name , encoding = encoding , \n base_dir = <mask0> ) \n", "gt": "base_dir"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import api \n import random \n import imp \n import shutil \n import os \n from os import path \n from functools import partial \n from bson import json_util \n from api . common import InternalException , SevereInternalException \n log = api . logger . use ( __name__ ) \n modifiable_problem_fields = [ \"<STR_LIT:description>\" ] \n seed = \"<STR_LIT>\" \n def is_autogen_problem ( pid ) : \n \"\"\"<STR_LIT>\"\"\" \n return api . problem . get_problem ( pid = pid ) . get ( \"<STR_LIT>\" , False ) \n def get_metadata_path ( pid , n ) : \n \"\"\"<STR_LIT>\"\"\" \n return path . join ( get_instance_path ( pid , n = n , public = False ) , \"<STR_LIT>\" ) \n def write_metadata ( pid , n , data ) : \n \"\"\"<STR_LIT>\"\"\" \n metadata_path = get_metadata_path ( pid , n ) \n with open ( metadata_path , \"<STR_LIT:w>\" ) as f : \n f . write ( json_util . dumps ( data ) ) \n @ api . cache . memoize ( timeout = <NUM_LIT> , fast = True ) \n def read_metadata ( pid , n ) : \n \"\"\"<STR_LIT>\"\"\" \n metadata_path = get_metadata_path ( pid , n ) \n with open ( metadata_path , \"<STR_LIT:r>\" ) as f : \n return json_util . loads ( f . read ( ) ) \n def build_problem_instances ( pid , instances ) : \n \"\"\"<STR_LIT>\"\"\" \n problem = api . problem . get_problem ( pid = pid ) \n if not is_autogen_problem ( pid ) : \n raise InternalException ( \"<STR_LIT>\" . format ( problem [ \"<STR_LIT:name>\" ] ) ) \n previous_state = seed_generator ( \"<STR_LIT>\" , pid ) \n instance_path , static_instance_path = get_instance_path ( pid ) , get_static_instance_path ( pid ) \n for autogen_path in [ instance_path , static_instance_path ] : \n log . debug ( \"<STR_LIT>\" , autogen_path ) \n if not path . isdir ( autogen_path ) : \n log . debug ( \"<STR_LIT>\" ) \n os . makedirs ( autogen_path ) \n for n in range ( instances ) : \n log . debug ( \"<STR_LIT>\" , problem [ \"<STR_LIT:name>\" ] , str ( n ) ) \n build = get_generator ( pid ) . generate ( random , pid , api . autogen_tools , n ) \n autogen_instance_path = get_instance_path ( pid , n = n ) \n file_type_paths = { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : get_instance_path ( pid , n = n , public = True ) , \n \"<STR_LIT>\" : get_instance_path ( pid , n = n , public = False ) \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : get_static_instance_path ( pid , public = True ) , \n \"<STR_LIT>\" : get_static_instance_path ( pid , public = False ) \n } \n } \n for _ , file_types in file_type_paths . items ( ) : \n for _ , autogen_path in file_types . items ( ) : \n if not path . isdir ( autogen_path ) : \n os . makedirs ( autogen_path ) \n problem_updates = build . get ( \"<STR_LIT>\" , None ) \n if problem_updates is None : \n raise InternalException ( \"<STR_LIT>\" . format ( problem [ \"<STR_LIT>\" ] ) ) \n write_metadata ( pid , n , problem_updates ) \n for file_type , listings in build . items ( ) : \n destination_type = file_type_paths . get ( file_type , None ) \n if destination_type is not None : \n for listing in listings : \n destination = destination_type . get ( listing , None ) \n if destination is not None : \n files = listings [ listing ] \n for f , name in files : \n if path . isfile ( f ) : \n shutil . copyfile ( f , path . join ( destination , name ) ) \n elif path . isdir ( f ) : \n shutil . copytree ( f , autogen_instance_path ) \n api . autogen_tools . clear_build_directories ( ) \n log . debug ( \"<STR_LIT>\" ) \n random . setstate ( previous_state ) \n def get_generator_path ( pid ) : \n \"\"\"<STR_LIT>\"\"\" \n problem = api . problem . get_problem ( pid = pid ) \n if not is_autogen_problem ( pid ) : \n raise InternalException ( \"<STR_LIT>\" ) \n if not problem . get ( \"<STR_LIT>\" , False ) : \n raise InternalException ( \"<STR_LIT>\" . format ( problem [ \"<STR_LIT:name>\" ] ) ) \n return path . join ( api . problem . grader_base_path , problem [ \"<STR_LIT>\" ] ) \n def get_generator ( pid ) : \n \"\"\"<STR_LIT>\"\"\" \n generator_path = get_generator_path ( pid ) \n if not path . isfile ( generator_path ) : \n raise InternalException ( \"<STR_LIT>\" . format ( generator_path ) ) \n return imp . load_source ( generator_path [ : - <NUM_LIT:3> ] , generator_path ) \n def get_seed ( pid , tid ) : \n \"\"\"<STR_LIT>\"\"\" \n return seed + tid + pid \n def seed_generator ( pid , tid ) : \n \"\"\"<STR_LIT>\"\"\" \n previous_state = random . getstate ( ) \n random . seed ( get_seed ( pid , tid ) ) \n return previous_state \n @ api . cache . memoize ( timeout = <NUM_LIT> , fast = True ) \n def get_instance_number ( pid , tid ) : \n \"\"\"<STR_LIT>\"\"\" \n previous_state = seed_generator ( tid , pid ) \n total_instances = get_number_of_instances ( pid ) \n if total_instances == <NUM_LIT:0> : \n raise InternalException ( \"<STR_LIT>\" . format ( pid ) ) \n instance_number = random . randint ( <NUM_LIT:0> , total_instances - <NUM_LIT:1> ) \n random . setstate ( previous_state ) \n return instance_number \n @ api . cache . memoize ( timeout = <NUM_LIT> , fast = True ) \n def get_number_of_instances ( pid ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n return [ dirname . isdigit ( ) for dirname in os . listdir ( get_instance_path ( pid , public = False ) ) ] . count ( True ) \n except FileNotFoundError : \n raise InternalException ( \"<STR_LIT>\" ) \n def get_static_instance_path ( pid , public = True ) : \n \"\"\"<STR_LIT>\"\"\" \n return path . abspath ( path . join ( get_instance_path ( pid , public = public ) , \"<STR_LIT>\" ) ) \n def get_instance_path ( pid , n = \"<STR_LIT>\" , public = True ) : \n \"\"\"<STR_LIT>\"\"\" \n generator_path = get_generator_path ( pid ) \n name = api . problem . get_problem ( pid ) [ \"<STR_LIT:name>\" ] \n instance_path = path . join ( path . dirname ( generator_path ) , \"<STR_LIT>\" , name , str ( n ) ) \n if public : \n instance_path = path . join ( instance_path , \"<STR_LIT>\" ) \n return path . abspath ( instance_path ) \n @ api . cache . memoize ( timeout = <NUM_LIT> , fast = True ) \n def get_problem_instance ( pid , tid ) : \n \"\"\"<STR_LIT>\"\"\" \n problem = api . problem . get_problem ( pid = pid ) \n n = get_instance_number ( pid , tid ) \n metadata = read_metadata ( pid , n ) \n if not set ( metadata ) . issubset ( modifiable_problem_fields ) : \n invalid_keys = set ( metadata ) . difference ( modifiable_problem_fields ) \n raise InternalException ( \"<STR_LIT>\" . format ( pid , invalid_keys ) ) \n problem . update ( metadata ) \n return problem \n def grade_problem_instance ( pid , tid , key ) : \n \"\"\"<STR_LIT>\"\"\" \n if not is_autogen_problem ( pid ) : \n raise InternalException ( \"<STR_LIT>\" . format ( pid ) ) \n problem = api . problem . get_problem ( pid ) \n n = get_instance_number ( pid , tid ) \n grader_problem_instance = GraderProblemInstance ( pid , tid , n ) \n grader = api . problem . get_grader ( pid ) \n try : \n correct , message = grader . grade ( grader_problem_instance , key ) \n except Exception as e : \n raise SevereInternalException ( \"<STR_LIT>\" . format ( pid , str ( e ) ) ) \n return { \n \"<STR_LIT>\" : correct , \n \"<STR_LIT>\" : problem [ \"<STR_LIT>\" ] , \n \"<STR_LIT:message>\" : message \n } \n class GraderProblemInstance ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , pid , tid , n ) : \n self . instance = n \n self . get_instance_path = partial ( get_instance_path , pid , n = n ) \n self . seed_generator = partial ( seed_generator , pid , tid ) \n self . write_metadata = partial ( write_metadata , pid , n ) \n self . read_metadata = partial ( read_metadata , <mask0> ) \n", "gt": "pid"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n def generate ( random , pid , tools , n ) : \n \"\"\"<STR_LIT>\"\"\" \n f = open ( \"<STR_LIT>\" , \"<STR_LIT:w>\" ) \n k = str ( random . randint ( <NUM_LIT:0> , <NUM_LIT:1000> ) ) \n f . write ( k ) \n f . close ( ) \n return { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ] , \n \"<STR_LIT>\" : [ ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ] \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ] , \n \"<STR_LIT>\" : [ ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ] \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT:description>\" : \"<STR_LIT>\" + <mask0> + \"<STR_LIT>\" \n } \n } \n", "gt": "k"}
{"input": "\n import IECore \n import GafferUI \n import GafferScene \n import GafferSceneUI \n import os \n scriptNode = script \n scriptWindow = GafferUI . ScriptWindow . acquire ( script ) \n layout = eval ( \"<STR_LIT>\" ) \n scriptWindow . setLayout ( layout ) \n scriptWindow . _Widget__qtWidget . resize ( <NUM_LIT> , <NUM_LIT> ) \n for nodeName in [ '<STR_LIT>' ] : \n script . selection ( ) . add ( script . descendant ( nodeName ) ) \n script . context ( ) [ \"<STR_LIT>\" ] = GafferScene . PathMatcherData ( GafferScene . PathMatcher ( [ '<STR_LIT:/>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) ) \n script . context ( ) [ \"<STR_LIT>\" ] = IECore . <mask0> ( [ \"<STR_LIT>\" ] ) \n", "gt": "StringVectorData"}
{"input": "\n import os \n import glob \n import IECore \n class convertAnimCache ( IECore . Op ) : \n def __init__ ( self ) : \n IECore . Op . __init__ ( self , \"<STR_LIT>\" , IECore . FileSequenceParameter ( \"<STR_LIT:result>\" , \"<STR_LIT>\" ) ) \n self . parameters ( ) . addParameters ( \n [ \n IECore . FileSequenceParameter ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n defaultValue = \"<STR_LIT>\" , \n allowEmptyString = False , \n check = IECore . FileSequenceParameter . CheckType . MustExist , \n extensions = \"<STR_LIT>\" , \n ) , \n IECore . FileSequenceParameter ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n defaultValue = \"<STR_LIT>\" , \n allowEmptyString = False , \n extensions = \"<STR_LIT>\" , \n ) , \n ] , \n ) \n def doOperation ( self , args ) : \n src = self . parameters ( ) [ \"<STR_LIT>\" ] . getFileSequenceValue ( ) \n dst = self . parameters ( ) [ \"<STR_LIT>\" ] . getFileSequenceValue ( ) \n if isinstance ( dst . frameList , IECore . EmptyFrameList ) : \n dst . frameList = src . frameList \n for ( sf , df ) in zip ( src . fileNames ( ) , dst . fileNames ( ) ) : \n sc = IECore . AttributeCache ( sf , IECore . IndexedIOOpenMode . Read ) \n dc = IECore . AttributeCache ( df , IECore . IndexedIOOpenMode . Write ) \n combinedBound = IECore . Box3f ( ) \n for objectName in sc . objects ( ) : \n p = b = None \n with IECore . IgnoredExceptions ( Exception ) : \n p = sc . read ( objectName , \"<STR_LIT>\" ) \n b = sc . read ( objectName , \"<STR_LIT>\" ) \n if p is not None and b is not None : \n combinedBound . extendBy ( b . value ) \n dc . write ( \"<STR_LIT:->\" + objectName , \"<STR_LIT>\" , p ) \n dc . write ( \"<STR_LIT:->\" + objectName , \"<STR_LIT>\" , b ) \n dc . write ( \"<STR_LIT:->\" , \"<STR_LIT>\" , IECore . Box3fData ( combinedBound ) ) \n return args [ \"<STR_LIT>\" ] . value \n IECore . registerRunTimeTyped ( <mask0> ) \n", "gt": "convertAnimCache"}
{"input": "\n import os \n import unittest \n import subprocess32 as subprocess \n import IECore \n import Gaffer \n import GafferTest \n import GafferScene \n import GafferAppleseed \n import GafferAppleseedTest \n class AppleseedRenderTest ( GafferTest . TestCase ) : \n def setUp ( self ) : \n GafferTest . TestCase . setUp ( self ) \n self . __scriptFileName = self . temporaryDirectory ( ) + \"<STR_LIT>\" \n def testExecute ( self ) : \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Plane ( ) \n s [ \"<STR_LIT>\" ] = GafferAppleseed . AppleseedRender ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] = Gaffer . Expression ( ) \n s [ \"<STR_LIT>\" ] . setExpression ( \"<STR_LIT>\" + self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] . setValue ( self . __scriptFileName ) \n s . save ( ) \n p = subprocess . Popen ( \n \"<STR_LIT>\" + self . __scriptFileName + \"<STR_LIT>\" , \n shell = True , \n stderr = subprocess . PIPE , \n ) \n p . wait ( ) \n self . failIf ( p . returncode ) \n for i in range ( <NUM_LIT:1> , <NUM_LIT:4> ) : \n self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" % i ) ) \n def testWaitForImage ( self ) : \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Plane ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Outputs ( ) \n s [ \"<STR_LIT>\" ] . addOutput ( \n \"<STR_LIT>\" , \n IECore . Display ( \n self . temporaryDirectory ( ) + \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n { } \n ) \n ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] = GafferAppleseed . AppleseedRender ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] . setValue ( self . __scriptFileName ) \n s . save ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . execute ( ) \n self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n def testExecuteWithStringSubstitutions ( self ) : \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Plane ( ) \n s [ \"<STR_LIT>\" ] = GafferAppleseed . AppleseedRender ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] . setValue ( self . __scriptFileName ) \n s . save ( ) \n p = subprocess . Popen ( \n \"<STR_LIT>\" + self . __scriptFileName + \"<STR_LIT>\" , \n shell = True , \n stderr = subprocess . PIPE , \n ) \n p . wait ( ) \n self . failIf ( p . returncode ) \n for i in range ( <NUM_LIT:1> , <NUM_LIT:4> ) : \n self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" % i ) ) \n def testImageOutput ( self ) : \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Plane ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Outputs ( ) \n s [ \"<STR_LIT>\" ] . addOutput ( \n \"<STR_LIT>\" , \n IECore . Display ( \n self . temporaryDirectory ( ) + \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n { } \n ) \n ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] = GafferAppleseed . AppleseedRender ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] . setValue ( self . __scriptFileName ) \n s . save ( ) \n c = Gaffer . Context ( ) \n for i in range ( <NUM_LIT:1> , <NUM_LIT:4> ) : \n c . setFrame ( i ) \n with c : \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . execute ( ) \n for i in range ( <NUM_LIT:1> , <NUM_LIT:4> ) : \n self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" % i ) ) \n def testTypeNamePrefixes ( self ) : \n self . assertTypeNamesArePrefixed ( GafferAppleseed ) \n self . assertTypeNamesArePrefixed ( GafferAppleseedTest ) \n def testDefaultNames ( self ) : \n self . assertDefaultNamesAreCorrect ( GafferAppleseed ) \n self . assertDefaultNamesAreCorrect ( GafferAppleseedTest ) \n def testNodesConstructWithDefaultValues ( self ) : \n self . assertNodesConstructWithDefaultValues ( GafferAppleseed ) \n self . assertNodesConstructWithDefaultValues ( GafferAppleseedTest ) \n def testDirectoryCreation ( self ) : \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT>\" ] . addMember ( \"<STR_LIT>\" , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] . addMember ( \"<STR_LIT>\" , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] = GafferScene . Plane ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Outputs ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] . addOutput ( \n \"<STR_LIT>\" , \n IECore . Display ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n { } \n ) \n ) \n s [ \"<STR_LIT>\" ] = GafferAppleseed . AppleseedRender ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n self . assertFalse ( os . path . exists ( self . __scriptFileName ) ) \n s [ \"<STR_LIT>\" ] . setValue ( self . __scriptFileName ) \n s . save ( ) \n with s . context ( ) : \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . execute ( ) \n self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n self . assertTrue ( os . path . exists ( self . __scriptFileName ) ) \n with s . context ( ) : \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . execute ( ) \n self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import GafferUITest \n import GafferScene \n import GafferSceneUI \n import GafferArnold \n import GafferArnoldUI \n class DocumentationTest ( GafferUITest . TestCase ) : \n def test ( self ) : \n self . maxDiff = None \n self . assertNodesAreDocumented ( \n GafferArnold , \n additionalTerminalPlugTypes = ( GafferScene . ScenePlug , ) \n ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import IECore \n class parameterChangedCallback ( IECore . Parameterised ) : \n def __init__ ( self ) : \n IECore . Parameterised . __init__ ( self , \"<STR_LIT>\" ) \n self . parameters ( ) . addParameters ( \n [ \n IECore . IntParameter ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n <NUM_LIT:0> \n ) , \n IECore . IntParameter ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n <NUM_LIT:0> \n ) , \n ] , \n ) \n self . changes = [ ] \n def parameterChanged ( self , parameter ) : \n self . changes . append ( ( parameter , str ( parameter . getValue ( ) ) ) ) \n if parameter . isSame ( self . parameters ( ) [ \"<STR_LIT>\" ] ) : \n self . parameters ( ) [ \"<STR_LIT>\" ] . setNumericValue ( self . parameters ( ) [ \"<STR_LIT>\" ] . getNumericValue ( ) * <NUM_LIT:5> ) \n IECore . registerRunTimeTyped ( <mask0> ) \n", "gt": "parameterChangedCallback"}
{"input": "\n import GafferUI \n import GafferCortexUI \n class ToolParameterValueWidget ( GafferCortexUI . ParameterValueWidget ) : \n def __init__ ( self , parameterHandler , parenting = None ) : \n GafferCortexUI . ParameterValueWidget . __init__ ( \n self , \n GafferUI . ToolPlugValueWidget ( parameterHandler . plug ( ) ) , \n parameterHandler , \n parenting = <mask0> \n ) \n", "gt": "parenting"}
{"input": "\n import os \n import IECore \n import Gaffer \n import GafferDispatch \n class TextWriter ( GafferDispatch . ExecutableNode ) : \n def __init__ ( self , name = \"<STR_LIT>\" , requiresSequenceExecution = False ) : \n GafferDispatch . ExecutableNode . __init__ ( self , name ) \n self . __requiresSequenceExecution = requiresSequenceExecution \n self . addChild ( Gaffer . StringPlug ( \"<STR_LIT>\" , Gaffer . Plug . Direction . In ) ) \n self . addChild ( Gaffer . StringPlug ( \"<STR_LIT>\" , defaultValue = \"<STR_LIT:w>\" , direction = Gaffer . Plug . Direction . In ) ) \n self . addChild ( Gaffer . StringPlug ( \"<STR_LIT:text>\" , Gaffer . Plug . Direction . In ) ) \n def execute ( self ) : \n context = Gaffer . Context . current ( ) \n fileName = self [ \"<STR_LIT>\" ] . getValue ( ) \n directory = os . path . dirname ( fileName ) \n if directory : \n try : \n os . makedirs ( directory ) \n except OSError : \n if not os . path . isdir ( directory ) : \n raise \n text = self . __processText ( context ) \n with file ( fileName , self [ \"<STR_LIT>\" ] . getValue ( ) ) as f : \n f . write ( text ) \n def executeSequence ( self , frames ) : \n if not self . __requiresSequenceExecution : \n GafferDispatch . ExecutableNode . executeSequence ( self , frames ) \n return \n context = Gaffer . Context ( Gaffer . Context . current ( ) ) \n fileName = self [ \"<STR_LIT>\" ] . getValue ( ) \n with file ( fileName , self [ \"<STR_LIT>\" ] . getValue ( ) ) as f : \n with context : \n for frame in frames : \n context . setFrame ( frame ) \n text = self . __processText ( context ) \n f . write ( text ) \n def hash ( self , context ) : \n h = GafferDispatch . ExecutableNode . hash ( self , context ) \n h . append ( context . getFrame ( ) ) \n h . append ( context . get ( \"<STR_LIT>\" , IECore . StringVectorData ( ) ) ) \n self [ \"<STR_LIT>\" ] . hash ( h ) \n self [ \"<STR_LIT>\" ] . hash ( h ) \n self [ \"<STR_LIT:text>\" ] . hash ( h ) \n return h \n def requiresSequenceExecution ( self ) : \n return self . __requiresSequenceExecution \n def __processText ( self , context ) : \n text = self [ \"<STR_LIT:text>\" ] . getValue ( ) \n replace = context . get ( \"<STR_LIT>\" , IECore . StringVectorData ( ) ) \n if replace and len ( replace ) == <NUM_LIT:2> : \n text = text . replace ( replace [ <NUM_LIT:0> ] , replace [ <NUM_LIT:1> ] ) \n return text \n IECore . registerRunTimeTyped ( TextWriter , <mask0> = \"<STR_LIT>\" ) \n", "gt": "typeName"}
{"input": "\n import os \n import IECore \n import Gaffer \n import GafferImage \n import GafferTest \n import GafferImageTest \n class CopyImageMetadataTest ( GafferImageTest . ImageTestCase ) : \n checkerFile = os . path . expandvars ( \"<STR_LIT>\" ) \n def test ( self ) : \n r = GafferImage . ImageReader ( ) \n r [ \"<STR_LIT>\" ] . setValue ( self . checkerFile ) \n inMetadata = r [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) \n d = GafferImage . DeleteImageMetadata ( ) \n d [ \"<STR_LIT>\" ] . setInput ( r [ \"<STR_LIT>\" ] ) \n d [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT:*>\" ) \n m = GafferImage . CopyImageMetadata ( ) \n m [ \"<STR_LIT>\" ] . setInput ( d [ \"<STR_LIT>\" ] ) \n m [ \"<STR_LIT>\" ] . setInput ( r [ \"<STR_LIT>\" ] ) \n m [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n metadata = m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) \n self . assertEqual ( m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , IECore . CompoundObject ( ) ) \n self . assertEqual ( m [ \"<STR_LIT>\" ] . image ( ) , d [ \"<STR_LIT>\" ] . image ( ) ) \n m [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n metadata = m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) \n expected = set ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n self . assertEqual ( set ( metadata . keys ( ) ) , expected ) \n for key in metadata . keys ( ) : \n self . assertEqual ( metadata [ key ] , inMetadata [ key ] ) \n m [ \"<STR_LIT>\" ] . setValue ( True ) \n metadata = m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) \n expected = set ( [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n self . assertEqual ( set ( metadata . keys ( ) ) , expected ) \n for key in metadata . keys ( ) : \n self . assertEqual ( metadata [ key ] , inMetadata [ key ] ) \n def testOverwrite ( self ) : \n r = GafferImage . ImageReader ( ) \n r [ \"<STR_LIT>\" ] . setValue ( self . checkerFile ) \n inMetadata = r [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) \n a = GafferImage . ImageMetadata ( ) \n a [ \"<STR_LIT>\" ] . addMember ( \"<STR_LIT>\" , IECore . StringData ( \"<STR_LIT>\" ) ) \n m = GafferImage . CopyImageMetadata ( ) \n m [ \"<STR_LIT>\" ] . setInput ( r [ \"<STR_LIT>\" ] ) \n m [ \"<STR_LIT>\" ] . setInput ( a [ \"<STR_LIT>\" ] ) \n m [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n metadata = m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) \n self . assertEqual ( metadata [ \"<STR_LIT>\" ] , IECore . StringData ( \"<STR_LIT>\" ) ) \n self . assertEqual ( m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , inMetadata ) \n self . assertEqual ( m [ \"<STR_LIT>\" ] . image ( ) , r [ \"<STR_LIT>\" ] . image ( ) ) \n m [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n metadata = m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) \n self . assertTrue ( \"<STR_LIT>\" in metadata . keys ( ) ) \n self . assertEqual ( metadata [ \"<STR_LIT>\" ] , IECore . StringData ( \"<STR_LIT>\" ) ) \n def testDirtyPropogation ( self ) : \n c = GafferImage . Constant ( ) \n r = GafferImage . ImageReader ( ) \n r [ \"<STR_LIT>\" ] . setValue ( self . checkerFile ) \n inMetadata = r [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) \n m = GafferImage . CopyImageMetadata ( ) \n m [ \"<STR_LIT>\" ] . setInput ( c [ \"<STR_LIT>\" ] ) \n m [ \"<STR_LIT>\" ] . setInput ( r [ \"<STR_LIT>\" ] ) \n cs = GafferTest . CapturingSlot ( m . plugDirtiedSignal ( ) ) \n m [ \"<STR_LIT>\" ] . setInput ( c [ \"<STR_LIT>\" ] ) \n self . assertTrue ( m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] in set ( e [ <NUM_LIT:0> ] for e in cs ) ) \n del cs [ : ] \n m [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT:test>\" ) \n self . assertTrue ( m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] in set ( e [ <NUM_LIT:0> ] for e in cs ) ) \n del cs [ : ] \n m [ \"<STR_LIT>\" ] . setValue ( True ) \n self . assertTrue ( m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] in set ( e [ <NUM_LIT:0> ] for e in cs ) ) \n def testPassThrough ( self ) : \n c = GafferImage . Constant ( ) \n i = GafferImage . ImageReader ( ) \n i [ \"<STR_LIT>\" ] . setValue ( self . checkerFile ) \n m = GafferImage . CopyImageMetadata ( ) \n m [ \"<STR_LIT>\" ] . setInput ( i [ \"<STR_LIT>\" ] ) \n m [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT:*>\" ) \n self . assertEqual ( i [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hash ( ) , m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hash ( ) ) \n self . assertEqual ( i [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hash ( ) , m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hash ( ) ) \n self . assertEqual ( i [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hash ( ) , m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hash ( ) ) \n self . assertEqual ( i [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) ) \n self . assertEqual ( i [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) ) \n self . assertEqual ( i [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) ) \n context = Gaffer . Context ( ) \n context [ \"<STR_LIT>\" ] = IECore . V2i ( <NUM_LIT:0> ) \n with context : \n for c in [ \"<STR_LIT>\" , \"<STR_LIT:B>\" , \"<STR_LIT:A>\" ] : \n context [ \"<STR_LIT>\" ] = c \n self . assertEqual ( i [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hash ( ) , m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hash ( ) ) \n self . assertEqual ( i [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , m [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import os \n import unittest \n import IECore \n import Gaffer \n import GafferImage \n import GafferImageTest \n class ObjectToImageTest ( GafferImageTest . ImageTestCase ) : \n fileName = os . path . expandvars ( \"<STR_LIT>\" ) \n negFileName = os . path . expandvars ( \"<STR_LIT>\" ) \n def test ( self ) : \n i = IECore . Reader . create ( self . fileName ) . read ( ) \n n = GafferImage . ObjectToImage ( ) \n n [ \"<STR_LIT:object>\" ] . setValue ( i ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] . image ( ) , i ) \n def testImageWithANegativeDataWindow ( self ) : \n i = IECore . Reader . create ( self . negFileName ) . read ( ) \n n = GafferImage . ObjectToImage ( ) \n n [ \"<STR_LIT:object>\" ] . setValue ( i ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] . image ( ) , i ) \n def testHashVariesPerTileAndChannel ( self ) : \n n = GafferImage . ObjectToImage ( ) \n n [ \"<STR_LIT:object>\" ] . setValue ( IECore . Reader . create ( self . fileName ) . read ( ) ) \n self . assertNotEqual ( \n n [ \"<STR_LIT>\" ] . channelDataHash ( \"<STR_LIT:R>\" , IECore . V2i ( <NUM_LIT:0> ) ) , \n n [ \"<STR_LIT>\" ] . channelDataHash ( \"<STR_LIT>\" , IECore . V2i ( <NUM_LIT:0> ) ) \n ) \n self . assertNotEqual ( \n n [ \"<STR_LIT>\" ] . channelDataHash ( \"<STR_LIT:R>\" , IECore . V2i ( <NUM_LIT:0> ) ) , \n n [ \"<STR_LIT>\" ] . channelDataHash ( \"<STR_LIT:R>\" , IECore . V2i ( GafferImage . ImagePlug . tileSize ( ) ) ) \n ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import threading \n import IECore \n import Gaffer \n import GafferUI \n import GafferImage \n __all__ = [ ] \n Gaffer . Metadata . registerNode ( \n GafferImage . Display , \n \"<STR_LIT:description>\" , \n \"\"\"<STR_LIT>\"\"\" , \n plugs = { \n \"<STR_LIT:port>\" : [ \n \"<STR_LIT:description>\" , \n \"\"\"<STR_LIT>\"\"\" , \n ] , \n } \n ) \n __plugsPendingUpdate = [ ] \n __plugsPendingUpdateLock = threading . Lock ( ) \n def __scheduleUpdate ( plug , force = False ) : \n if not force : \n global __plugsPendingUpdate \n global __plugsPendingUpdateLock \n with __plugsPendingUpdateLock : \n for p in __plugsPendingUpdate : \n if plug . isSame ( p ) : \n return \n __plugsPendingUpdate . append ( plug ) \n GafferUI . EventLoop . executeOnUIThread ( lambda : __update ( plug ) ) \n def __update ( plug ) : \n node = plug . node ( ) \n if node : \n updateCountPlug = node [ \"<STR_LIT>\" ] \n updateCountPlug . setValue ( updateCountPlug . getValue ( ) + <NUM_LIT:1> ) \n global __plugsPendingUpdate \n global __plugsPendingUpdateLock \n with __plugsPendingUpdateLock : \n __plugsPendingUpdate = [ p for p in __plugsPendingUpdate if not p . isSame ( plug ) ] \n __displayDataReceivedConnection = GafferImage . Display . dataReceivedSignal ( ) . connect ( __scheduleUpdate ) \n __displayImageReceivedConnection = GafferImage . Display . imageReceivedSignal ( ) . connect ( IECore . curry ( __scheduleUpdate , force = <mask0> ) ) \n", "gt": "True"}
{"input": "\n from _GafferImageUI import * \n import DisplayUI \n from FormatPlugValueWidget import FormatPlugValueWidget \n from ChannelMaskPlugValueWidget import ChannelMaskPlugValueWidget \n import OpenImageIOReaderUI \n import ImageReaderUI \n import ImageViewToolbar \n import ImageTransformUI \n import ConstantUI \n import ImageSwitchUI \n import ColorSpaceUI \n import ImageContextVariablesUI \n import ImageStatsUI \n import DeleteChannelsUI \n import ObjectToImageUI \n import ClampUI \n import ImageWriterUI \n import GradeUI \n import ImageTimeWarpUI \n import ImageSamplerUI \n import MergeUI \n import ImageNodeUI \n import ChannelDataProcessorUI \n import ImageProcessorUI \n import ImageMetadataUI \n import DeleteImageMetadataUI \n import CopyImageMetadataUI \n import ImageLoopUI \n import ShuffleUI \n import PremultiplyUI \n import UnpremultiplyUI \n import CropUI \n import ResizeUI \n import ResampleUI \n import LUTUI \n import CDLUI \n import DisplayTransformUI \n import OffsetUI \n import BlurUI \n import ShapeUI \n import TextUI \n import WarpUI \n import UVWarpUI \n __import__ ( \"<STR_LIT>\" ) . loadConfig ( \"<STR_LIT>\" , { } , <mask0> = \"<STR_LIT>\" ) \n", "gt": "subdirectory"}
{"input": "\n import os \n import unittest \n import IECore \n import Gaffer \n import GafferTest \n import GafferScene \n import GafferSceneTest \n import GafferRenderMan \n import GafferRenderManTest \n class RenderManShaderTest ( GafferRenderManTest . RenderManTestCase ) : \n def setUp ( self ) : \n GafferRenderManTest . RenderManTestCase . setUp ( self ) \n GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n def test ( self ) : \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( \"<STR_LIT>\" ) \n self . failUnless ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . FloatPlug ) ) \n self . failUnless ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . FloatPlug ) ) \n self . failUnless ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . FloatPlug ) ) \n self . failUnless ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . FloatPlug ) ) \n self . failUnless ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . Color3fPlug ) ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:0.5> ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:0.5> ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:1> ) \n self . assertAlmostEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:0.1> ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , IECore . Color3f ( <NUM_LIT:1> ) ) \n def testSerialisation ( self ) : \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT:n>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:n>\" ] . loadShader ( \"<STR_LIT>\" ) \n ss = s . serialise ( ) \n s = Gaffer . ScriptNode ( ) \n s . execute ( ss ) \n st = s [ \"<STR_LIT:n>\" ] . state ( ) \n self . assertEqual ( len ( st ) , <NUM_LIT:1> ) \n self . assertEqual ( st [ <NUM_LIT:0> ] . type , \"<STR_LIT>\" ) \n self . assertEqual ( st [ <NUM_LIT:0> ] . name , \"<STR_LIT>\" ) \n self . failUnless ( isinstance ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . FloatPlug ) ) \n self . failUnless ( isinstance ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . FloatPlug ) ) \n self . failUnless ( isinstance ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . FloatPlug ) ) \n self . failUnless ( isinstance ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . FloatPlug ) ) \n self . failUnless ( isinstance ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . Color3fPlug ) ) \n self . assertTrue ( \"<STR_LIT>\" not in s [ \"<STR_LIT:n>\" ] ) \n def testShader ( self ) : \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( \"<STR_LIT>\" ) \n s = n . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:1> ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . type , \"<STR_LIT>\" ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , \"<STR_LIT>\" ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] , IECore . FloatData ( <NUM_LIT> ) ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] , IECore . FloatData ( <NUM_LIT> ) ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] , IECore . FloatData ( <NUM_LIT:1> ) ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] , IECore . FloatData ( <NUM_LIT> ) ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] , IECore . Color3fData ( IECore . Color3f ( <NUM_LIT:1> ) ) ) \n def testShaderHash ( self ) : \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( \"<STR_LIT>\" ) \n h1 = n . stateHash ( ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT> ) \n self . assertNotEqual ( n . stateHash ( ) , h1 ) \n def testCoshaderHash ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n self . assertTrue ( \"<STR_LIT>\" in shaderNode [ \"<STR_LIT>\" ] ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . typeId ( ) , Gaffer . Plug . staticTypeId ( ) ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n h1 = shaderNode . stateHash ( ) \n coshaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT> ) \n self . assertNotEqual ( shaderNode . stateHash ( ) , h1 ) \n def testParameterOrdering ( self ) : \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( \"<STR_LIT>\" ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getName ( ) , \"<STR_LIT>\" ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . getName ( ) , \"<STR_LIT>\" ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ <NUM_LIT:2> ] . getName ( ) , \"<STR_LIT>\" ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ <NUM_LIT:3> ] . getName ( ) , \"<STR_LIT>\" ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ <NUM_LIT:4> ] . getName ( ) , \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( \"<STR_LIT>\" ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getName ( ) , \"<STR_LIT>\" ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . getName ( ) , \"<STR_LIT>\" ) \n def testCoshader ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n self . assertTrue ( \"<STR_LIT>\" in shaderNode [ \"<STR_LIT>\" ] ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . typeId ( ) , Gaffer . Plug . staticTypeId ( ) ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n s = shaderNode . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:2> ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . name , shader ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] ) \n def testInputAcceptance ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n random = Gaffer . Random ( ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderNode [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( random [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( random [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderNode [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( coshaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( random [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( coshaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderNode [ \"<STR_LIT>\" ] ) ) \n def testParameterDefaultValue ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . defaultValue ( ) , <NUM_LIT:1> ) \n def testParameterMinMax ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . minValue ( ) , - <NUM_LIT:1> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . maxValue ( ) , <NUM_LIT:10> ) \n def testReload ( self ) : \n shader1 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader1 ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT:0.1> ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT:test>\" ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( IECore . Color3f ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) ) \n self . assertAlmostEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:0.1> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , \"<STR_LIT:test>\" ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , IECore . Color3f ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) ) \n shader2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode . loadShader ( shader2 , keepExistingValues = True ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] . keys ( ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n self . assertAlmostEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:0.1> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , \"<STR_LIT:test>\" ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , IECore . Color3f ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) ) \n shaderNode . loadShader ( shader1 , keepExistingValues = True ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] . keys ( ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n self . assertAlmostEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:0.1> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , \"<STR_LIT:test>\" ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , IECore . Color3f ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) ) \n shaderNode . loadShader ( shader1 , keepExistingValues = False ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] . keys ( ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:1> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , \"<STR_LIT>\" ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , IECore . Color3f ( <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> ) ) \n def testReloadRemovesOldParameters ( self ) : \n shader2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader2 ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] . keys ( ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n shader3 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode . loadShader ( shader3 ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] . keys ( ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def testAutomaticReloadOnScriptLoad ( self ) : \n shader1 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" , shaderName = \"<STR_LIT>\" ) \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT>\" ] . loadShader ( shader1 ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT:0.1> ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT:test>\" ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( IECore . Color3f ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) ) \n ss = s . serialise ( ) \n self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" , shaderName = \"<STR_LIT>\" ) \n GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n s = Gaffer . ScriptNode ( ) \n s . execute ( ss ) \n self . assertEqual ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . keys ( ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n self . assertAlmostEqual ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:0.1> ) \n self . assertEqual ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , \"<STR_LIT:test>\" ) \n self . assertEqual ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , IECore . Color3f ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) ) \n def testReloadPreservesConnections ( self ) : \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( \"<STR_LIT>\" ) \n random = Gaffer . Random ( ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( random [ \"<STR_LIT>\" ] ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( random [ \"<STR_LIT>\" ] ) \n n . loadShader ( \"<STR_LIT>\" , keepExistingValues = True ) \n self . assertTrue ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getInput ( ) . isSame ( random [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getInput ( ) . isSame ( random [ \"<STR_LIT>\" ] ) ) \n def testReloadPreservesConnectionsWhenMinMaxOrDefaultChanges ( self ) : \n shader1 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" , shaderName = \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( shader1 ) \n self . assertFalse ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hasMinValue ( ) ) \n self . assertFalse ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hasMaxValue ( ) ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . defaultValue ( ) , \"<STR_LIT>\" ) \n nn = Gaffer . Node ( ) \n nn [ \"<STR_LIT>\" ] = Gaffer . FloatPlug ( direction = Gaffer . Plug . Direction . Out ) \n nn [ \"<STR_LIT>\" ] = Gaffer . StringPlug ( direction = Gaffer . Plug . Direction . Out ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( nn [ \"<STR_LIT>\" ] ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( nn [ \"<STR_LIT>\" ] ) \n shader2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" , shaderName = \"<STR_LIT>\" ) \n GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n n . loadShader ( shader1 , keepExistingValues = True ) \n self . assertTrue ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hasMinValue ( ) ) \n self . assertTrue ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . hasMaxValue ( ) ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . minValue ( ) , - <NUM_LIT:1> ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . maxValue ( ) , <NUM_LIT:2> ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . defaultValue ( ) , \"<STR_LIT>\" ) \n self . assertTrue ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getInput ( ) . isSame ( nn [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getInput ( ) . isSame ( nn [ \"<STR_LIT>\" ] ) ) \n def testReloadPreservesPartialConnectionsWhenMinMaxOrDefaultChanges ( self ) : \n shader1 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" , shaderName = \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( shader1 ) \n nn = Gaffer . Node ( ) \n nn [ \"<STR_LIT>\" ] = Gaffer . FloatPlug ( direction = Gaffer . Plug . Direction . Out ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . setInput ( nn [ \"<STR_LIT>\" ] ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . setInput ( nn [ \"<STR_LIT>\" ] ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:2> ] . setValue ( <NUM_LIT> ) \n shader2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" , shaderName = \"<STR_LIT>\" ) \n GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n n . loadShader ( shader1 , keepExistingValues = True ) \n self . assertTrue ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) . isSame ( nn [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . getInput ( ) . isSame ( nn [ \"<STR_LIT>\" ] ) ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:2> ] . getValue ( ) , <NUM_LIT> ) \n def testReloadPreservesValuesWhenMinMaxOrDefaultChanges ( self ) : \n shader1 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" , shaderName = \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( shader1 ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT> ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( IECore . Color3f ( <NUM_LIT:0.1> , <NUM_LIT> , <NUM_LIT:0.5> ) ) \n shader2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" , shaderName = \"<STR_LIT>\" ) \n GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n n . loadShader ( shader1 , keepExistingValues = True ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT> ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , \"<STR_LIT>\" ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , IECore . Color3f ( <NUM_LIT:0.1> , <NUM_LIT> , <NUM_LIT:0.5> ) ) \n def testOutputParameters ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( shader ) \n self . failIf ( \"<STR_LIT>\" in n [ \"<STR_LIT>\" ] . keys ( ) ) \n def testAssignmentDirtyPropagation ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n plane = GafferScene . Plane ( ) \n assignment = GafferScene . ShaderAssignment ( ) \n assignment [ \"<STR_LIT>\" ] . setInput ( plane [ \"<STR_LIT>\" ] ) \n assignment [ \"<STR_LIT>\" ] . setInput ( shaderNode [ \"<STR_LIT>\" ] ) \n cs = GafferTest . CapturingSlot ( assignment . plugDirtiedSignal ( ) ) \n coshaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT:12> ) \n dirtiedNames = [ x [ <NUM_LIT:0> ] . fullName ( ) for x in cs ] \n self . assertEqual ( len ( dirtiedNames ) , <NUM_LIT:3> ) \n self . assertEqual ( dirtiedNames [ <NUM_LIT:0> ] , \"<STR_LIT>\" ) \n self . assertEqual ( dirtiedNames [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n self . assertEqual ( dirtiedNames [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n def testArrayParameters ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( shader ) \n expected = { \n \"<STR_LIT>\" : IECore . FloatVectorData ( [ ] ) , \n \"<STR_LIT>\" : IECore . FloatVectorData ( [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:4> ] ) , \n \"<STR_LIT>\" : IECore . StringVectorData ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) , \n \"<STR_LIT>\" : IECore . StringVectorData ( [ \"<STR_LIT:hello>\" , \"<STR_LIT>\" ] ) , \n \"<STR_LIT>\" : IECore . Color3fVectorData ( [ IECore . Color3f ( <NUM_LIT:1> ) , IECore . Color3f ( <NUM_LIT:2> ) ] ) , \n \"<STR_LIT>\" : IECore . Color3fVectorData ( [ IECore . Color3f ( <NUM_LIT:1> ) , IECore . Color3f ( <NUM_LIT:2> ) ] ) , \n \"<STR_LIT>\" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Vector ) , \n \"<STR_LIT>\" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( <NUM_LIT:1> , <NUM_LIT:6> ) ] , IECore . GeometricData . Interpretation . Vector ) , \n \"<STR_LIT>\" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Point ) , \n \"<STR_LIT>\" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( <NUM_LIT:1> , <NUM_LIT:6> ) ] , IECore . GeometricData . Interpretation . Point ) , \n \"<STR_LIT>\" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Normal ) , \n \"<STR_LIT>\" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( <NUM_LIT:1> , <NUM_LIT:6> ) ] , IECore . GeometricData . Interpretation . Normal ) , \n } \n self . assertEqual ( set ( n [ \"<STR_LIT>\" ] . keys ( ) ) , set ( expected . keys ( ) ) ) \n for name , value in expected . items ( ) : \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ name ] . defaultValue ( ) , value ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] [ name ] . getValue ( ) , value ) \n s = n . state ( ) [ <NUM_LIT:0> ] \n for name , value in expected . items ( ) : \n self . assertEqual ( s . parameters [ name ] , value ) \n def testFixedCoshaderArrayParameters ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( shader ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] . keys ( ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n self . assertTrue ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . ArrayPlug ) ) \n self . assertEqual ( len ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:4> ) \n self . assertTrue ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . Plug ) ) \n self . assertTrue ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . Plug ) ) \n self . assertTrue ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . Plug ) ) \n self . assertTrue ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . Plug ) ) \n state = n . state ( ) \n self . assertEqual ( state [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] , IECore . StringVectorData ( [ \"<STR_LIT>\" ] * <NUM_LIT:4> ) ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n state = n . state ( ) \n self . assertEqual ( state [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , IECore . StringVectorData ( [ state [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) ) \n def testCoshaderType ( self ) : \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n self . assertEqual ( coshaderNode . state ( ) [ <NUM_LIT:0> ] . type , \"<STR_LIT>\" ) \n def testCantConnectSurfaceShaderIntoCoshaderInput ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n n1 = GafferRenderMan . RenderManShader ( ) \n n1 . loadShader ( shader ) \n n2 = GafferRenderMan . RenderManShader ( ) \n n2 . loadShader ( \"<STR_LIT>\" ) \n self . assertFalse ( n1 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( n2 [ \"<STR_LIT>\" ] ) ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n n3 = GafferRenderMan . RenderManShader ( ) \n n3 . loadShader ( coshader ) \n self . assertTrue ( n1 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( n3 [ \"<STR_LIT>\" ] ) ) \n arrayShader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n n4 = GafferRenderMan . RenderManShader ( ) \n n4 . loadShader ( arrayShader ) \n self . assertFalse ( n4 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( n2 [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( n4 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( n3 [ \"<STR_LIT>\" ] ) ) \n def testConnectionsBetweenParameters ( self ) : \n s = GafferRenderMan . RenderManShader ( ) \n s . loadShader ( \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT> ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n shader = s . state ( ) [ <NUM_LIT:0> ] \n self . assertEqual ( shader . parameters [ \"<STR_LIT>\" ] . value , <NUM_LIT> ) \n self . assertEqual ( shader . parameters [ \"<STR_LIT>\" ] . value , <NUM_LIT> ) \n def testFixedCoshaderArrayParameterHash ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( shader ) \n h1 = n . stateHash ( ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n h2 = n . stateHash ( ) \n self . assertNotEqual ( h2 , h1 ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n h3 = n . stateHash ( ) \n self . assertNotEqual ( h3 , h2 ) \n self . assertNotEqual ( h3 , h1 ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( None ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n h4 = n . stateHash ( ) \n self . assertNotEqual ( h4 , h3 ) \n self . assertNotEqual ( h4 , h2 ) \n self . assertNotEqual ( h4 , h1 ) \n def testDisabling ( self ) : \n s = GafferRenderMan . RenderManShader ( ) \n s . loadShader ( \"<STR_LIT>\" ) \n stateHash = s . stateHash ( ) \n state = s . state ( ) \n self . assertEqual ( len ( state ) , <NUM_LIT:1> ) \n self . assertEqual ( state [ <NUM_LIT:0> ] . name , \"<STR_LIT>\" ) \n self . assertTrue ( s [ \"<STR_LIT>\" ] . isSame ( s . enabledPlug ( ) ) ) \n s [ \"<STR_LIT>\" ] . setValue ( False ) \n stateHash2 = s . stateHash ( ) \n self . assertNotEqual ( stateHash2 , stateHash ) \n state2 = s . state ( ) \n self . assertEqual ( len ( state2 ) , <NUM_LIT:0> ) \n def testDisablingCoshaders ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n s = shaderNode . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:2> ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . name , shader ) \n h = shaderNode . stateHash ( ) \n coshaderNode [ \"<STR_LIT>\" ] . setValue ( False ) \n s2 = shaderNode . state ( ) \n self . assertEqual ( len ( s2 ) , <NUM_LIT:1> ) \n self . assertEqual ( s2 [ <NUM_LIT:0> ] . name , shader ) \n self . assertTrue ( \"<STR_LIT>\" not in s2 [ <NUM_LIT:0> ] . parameters ) \n self . assertNotEqual ( shaderNode . stateHash ( ) , h ) \n def testDisablingCoshaderArrayInputs ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( shader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode1 = GafferRenderMan . RenderManShader ( ) \n coshaderNode1 . loadShader ( coshader ) \n coshaderNode2 = GafferRenderMan . RenderManShader ( ) \n coshaderNode2 . loadShader ( coshader ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . setInput ( coshaderNode1 [ \"<STR_LIT>\" ] ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:2> ] . setInput ( coshaderNode2 [ \"<STR_LIT>\" ] ) \n state = n . state ( ) \n h1 = n . stateHash ( ) \n self . assertEqual ( \n state [ <NUM_LIT:2> ] . parameters [ \"<STR_LIT>\" ] , \n IECore . StringVectorData ( [ \n state [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , \n \"<STR_LIT>\" , \n state [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] . value , \n \"<STR_LIT>\" \n ] ) \n ) \n coshaderNode1 [ \"<STR_LIT>\" ] . setValue ( False ) \n state = n . state ( ) \n self . assertEqual ( \n state [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , \n IECore . StringVectorData ( [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n state [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , \n \"<STR_LIT>\" \n ] ) \n ) \n h2 = n . stateHash ( ) \n self . assertNotEqual ( h2 , h1 ) \n coshaderNode2 [ \"<STR_LIT>\" ] . setValue ( False ) \n state = n . state ( ) \n self . assertEqual ( \n state [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] , \n IECore . StringVectorData ( [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" \n ] ) \n ) \n self . assertNotEqual ( n . stateHash ( ) , h1 ) \n self . assertNotEqual ( n . stateHash ( ) , h2 ) \n def testCorrespondingInput ( self ) : \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n self . assertEqual ( coshaderNode . correspondingInput ( coshaderNode [ \"<STR_LIT>\" ] ) , None ) \n coshader2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode2 = GafferRenderMan . RenderManShader ( ) \n coshaderNode2 . loadShader ( coshader2 ) \n self . assertTrue ( coshaderNode2 . correspondingInput ( coshaderNode2 [ \"<STR_LIT>\" ] ) . isSame ( coshaderNode2 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) ) \n def testCoshaderPassThrough ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n passThroughCoshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n passThroughCoshaderNode = GafferRenderMan . RenderManShader ( ) \n passThroughCoshaderNode . loadShader ( passThroughCoshader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( passThroughCoshaderNode [ \"<STR_LIT>\" ] ) \n passThroughCoshaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n h = shaderNode . stateHash ( ) \n s = shaderNode . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:3> ) \n self . assertEqual ( s [ <NUM_LIT:2> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . name , passThroughCoshader ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n passThroughCoshaderNode [ \"<STR_LIT>\" ] . setValue ( False ) \n s = shaderNode . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:2> ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n def testSplineParameters ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( shader ) \n self . assertEqual ( n [ \"<STR_LIT>\" ] . keys ( ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n self . assertTrue ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . SplineffPlug ) ) \n self . assertTrue ( isinstance ( n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . SplinefColor3fPlug ) ) \n self . assertEqual ( \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . defaultValue ( ) , \n IECore . Splineff ( \n IECore . CubicBasisf . catmullRom ( ) , \n [ \n ( <NUM_LIT:0> , <NUM_LIT:0> ) , \n ( <NUM_LIT:0> , <NUM_LIT:0> ) , \n ( <NUM_LIT:1> , <NUM_LIT:1> ) , \n ( <NUM_LIT:1> , <NUM_LIT:1> ) , \n ] \n ) \n ) \n self . assertEqual ( \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . defaultValue ( ) , \n IECore . SplinefColor3f ( \n IECore . CubicBasisf . catmullRom ( ) , \n [ \n ( <NUM_LIT:0> , IECore . Color3f ( <NUM_LIT:0> ) ) , \n ( <NUM_LIT:0> , IECore . Color3f ( <NUM_LIT:0> ) ) , \n ( <NUM_LIT:1> , IECore . Color3f ( <NUM_LIT:1> ) ) , \n ( <NUM_LIT:1> , IECore . Color3f ( <NUM_LIT:1> ) ) , \n ] \n ) \n ) \n floatValue = IECore . Splineff ( \n IECore . CubicBasisf . catmullRom ( ) , \n [ \n ( <NUM_LIT:0> , <NUM_LIT:0> ) , \n ( <NUM_LIT:0> , <NUM_LIT:0> ) , \n ( <NUM_LIT:1> , <NUM_LIT:2> ) , \n ( <NUM_LIT:1> , <NUM_LIT:2> ) , \n ] \n ) \n colorValue = IECore . SplinefColor3f ( \n IECore . CubicBasisf . catmullRom ( ) , \n [ \n ( <NUM_LIT:0> , IECore . Color3f ( <NUM_LIT:0> ) ) , \n ( <NUM_LIT:0> , IECore . Color3f ( <NUM_LIT:0> ) ) , \n ( <NUM_LIT:1> , IECore . Color3f ( <NUM_LIT> ) ) , \n ( <NUM_LIT:1> , IECore . Color3f ( <NUM_LIT> ) ) , \n ] \n ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( floatValue ) \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( colorValue ) \n s = n . state ( ) [ <NUM_LIT:0> ] \n self . assertEqual ( s . parameters [ \"<STR_LIT>\" ] . value , floatValue ) \n self . assertEqual ( s . parameters [ \"<STR_LIT>\" ] . value , colorValue ) \n def testSplineParameterSerialisationKeepsExistingValues ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT:n>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:n>\" ] . loadShader ( shader ) \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \n IECore . Splineff ( \n IECore . CubicBasisf . catmullRom ( ) , \n [ \n ( <NUM_LIT:0> , <NUM_LIT:0> ) , \n ( <NUM_LIT:0> , <NUM_LIT:0> ) , \n ( <NUM_LIT:1> , <NUM_LIT:2> ) , \n ( <NUM_LIT:1> , <NUM_LIT:2> ) , \n ] \n ) \n ) \n self . assertEqual ( \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , \n IECore . Splineff ( \n IECore . CubicBasisf . catmullRom ( ) , \n [ \n ( <NUM_LIT:0> , <NUM_LIT:0> ) , \n ( <NUM_LIT:0> , <NUM_LIT:0> ) , \n ( <NUM_LIT:1> , <NUM_LIT:2> ) , \n ( <NUM_LIT:1> , <NUM_LIT:2> ) , \n ] \n ) , \n ) \n ss = s . serialise ( ) \n s2 = Gaffer . ScriptNode ( ) \n s2 . execute ( ss ) \n self . assertEqual ( \n s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , \n IECore . Splineff ( \n IECore . CubicBasisf . catmullRom ( ) , \n [ \n ( <NUM_LIT:0> , <NUM_LIT:0> ) , \n ( <NUM_LIT:0> , <NUM_LIT:0> ) , \n ( <NUM_LIT:1> , <NUM_LIT:2> ) , \n ( <NUM_LIT:1> , <NUM_LIT:2> ) , \n ] \n ) , \n ) \n def testSplineParameterDefaultValueAnnotation ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n n = GafferRenderMan . RenderManShader ( ) \n n . loadShader ( shader ) \n self . assertEqual ( \n n [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , \n IECore . SplinefColor3f ( \n IECore . CubicBasisf . catmullRom ( ) , \n [ \n ( <NUM_LIT:0> , IECore . Color3f ( <NUM_LIT:1> ) ) , \n ( <NUM_LIT:0> , IECore . Color3f ( <NUM_LIT:1> ) ) , \n ( <NUM_LIT:0.5> , IECore . Color3f ( <NUM_LIT:1> , <NUM_LIT:0.5> , <NUM_LIT> ) ) , \n ( <NUM_LIT:1> , IECore . Color3f ( <NUM_LIT:0> ) ) , \n ( <NUM_LIT:1> , IECore . Color3f ( <NUM_LIT:0> ) ) , \n ] \n ) , \n ) \n def testCoshadersInBox ( self ) : \n s = Gaffer . ScriptNode ( ) \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT>\" ] . loadShader ( shader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT>\" ] . loadShader ( coshader ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n b = Gaffer . Box . create ( s , Gaffer . StandardSet ( [ s [ \"<STR_LIT>\" ] ] ) ) \n self . assertTrue ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getInput ( ) . parent ( ) . isSame ( b ) ) \n s = s [ \"<STR_LIT>\" ] . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:2> ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n def testShaderInBoxWithExternalCoshader ( self ) : \n s = Gaffer . ScriptNode ( ) \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT>\" ] . loadShader ( shader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT>\" ] . loadShader ( coshader ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n b = Gaffer . Box . create ( s , Gaffer . StandardSet ( [ s [ \"<STR_LIT>\" ] ] ) ) \n self . assertTrue ( b [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getInput ( ) . parent ( ) . isSame ( b ) ) \n s = b [ \"<STR_LIT>\" ] . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:2> ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n def testNumericTypeAnnotations ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n self . assertTrue ( isinstance ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . FloatPlug ) ) \n self . assertTrue ( isinstance ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . FloatPlug ) ) \n self . assertTrue ( isinstance ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . IntPlug ) ) \n self . assertTrue ( isinstance ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , Gaffer . BoolPlug ) ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . defaultValue ( ) , <NUM_LIT> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . defaultValue ( ) , <NUM_LIT> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . defaultValue ( ) , <NUM_LIT:10> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . defaultValue ( ) , True ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:10> ) \n self . assertEqual ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , True ) \n def testCoshaderTypeAnnotations ( self ) : \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n coshaderType1 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderType1Node = GafferRenderMan . RenderManShader ( ) \n coshaderType1Node . loadShader ( coshaderType1 ) \n coshaderType2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderType2Node = GafferRenderMan . RenderManShader ( ) \n coshaderType2Node . loadShader ( coshaderType2 ) \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderNode [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType1Node [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType2Node [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderNode [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType1Node [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType2Node [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderNode [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType1Node [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType2Node [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderNode [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType1Node [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType2Node [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderNode [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType1Node [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType2Node [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . acceptsInput ( coshaderNode [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . acceptsInput ( coshaderType1Node [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . acceptsInput ( coshaderType2Node [ \"<STR_LIT>\" ] ) ) \n def testMultipleCoshaderTypeAnnotations ( self ) : \n coshaderType1And2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderType1And2Node = GafferRenderMan . RenderManShader ( ) \n coshaderType1And2Node . loadShader ( coshaderType1And2 ) \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType1And2Node [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType1And2Node [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType1And2Node [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType1And2Node [ \"<STR_LIT>\" ] ) ) \n def testSplitCoshaderPassThrough ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n S = GafferRenderMan . RenderManShader ( ) \n S . loadShader ( shader ) \n passThroughCoshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n D = GafferRenderMan . RenderManShader ( ) \n D . loadShader ( passThroughCoshader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n C = GafferRenderMan . RenderManShader ( ) \n C . loadShader ( coshader ) \n S [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . setInput ( C [ \"<STR_LIT>\" ] ) \n S [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . setInput ( D [ \"<STR_LIT>\" ] ) \n D [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( C [ \"<STR_LIT>\" ] ) \n h = S . stateHash ( ) \n s = S . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:3> ) \n self . assertEqual ( s [ <NUM_LIT:2> ] . parameters [ \"<STR_LIT>\" ] , IECore . StringVectorData ( [ s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] . value , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . name , passThroughCoshader ) \n D [ \"<STR_LIT>\" ] . setValue ( False ) \n self . assertNotEqual ( S . stateHash ( ) , h ) \n s = S . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:2> ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , IECore . StringVectorData ( [ s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n def testSerialDisabledShaders ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n S = GafferRenderMan . RenderManShader ( ) \n S . loadShader ( shader ) \n passThroughCoshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n D1 = GafferRenderMan . RenderManShader ( ) \n D1 . loadShader ( passThroughCoshader ) \n D2 = GafferRenderMan . RenderManShader ( ) \n D2 . loadShader ( passThroughCoshader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n C = GafferRenderMan . RenderManShader ( ) \n C . loadShader ( coshader ) \n S [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( D2 [ \"<STR_LIT>\" ] ) \n D2 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( D1 [ \"<STR_LIT>\" ] ) \n D1 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( C [ \"<STR_LIT>\" ] ) \n h1 = S . stateHash ( ) \n s = S . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:4> ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . name , passThroughCoshader ) \n self . assertEqual ( s [ <NUM_LIT:2> ] . name , passThroughCoshader ) \n self . assertEqual ( s [ <NUM_LIT:3> ] . name , shader ) \n self . assertEqual ( s [ <NUM_LIT:3> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:2> ] . parameters [ \"<STR_LIT>\" ] ) \n self . assertEqual ( s [ <NUM_LIT:2> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] ) \n D2 [ \"<STR_LIT>\" ] . setValue ( False ) \n h2 = S . stateHash ( ) \n self . assertNotEqual ( h1 , h2 ) \n s = S . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:3> ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . name , passThroughCoshader ) \n self . assertEqual ( s [ <NUM_LIT:2> ] . name , shader ) \n self . assertEqual ( s [ <NUM_LIT:2> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] ) \n D1 [ \"<STR_LIT>\" ] . setValue ( False ) \n h3 = S . stateHash ( ) \n self . assertNotEqual ( h3 , h2 ) \n self . assertNotEqual ( h3 , h1 ) \n s = S . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:2> ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , coshader ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . name , shader ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] ) \n def testDynamicCoshaderArrayParameters ( self ) : \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n self . assertEqual ( len ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:1> ) \n self . assertTrue ( isinstance ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] , Gaffer . Plug ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) is None ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n self . assertEqual ( len ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:2> ) \n self . assertTrue ( isinstance ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] , Gaffer . Plug ) ) \n self . assertTrue ( isinstance ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] , Gaffer . Plug ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) . isSame ( coshaderNode [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . getInput ( ) is None ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . setInput ( None ) \n self . assertEqual ( len ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:1> ) \n self . assertTrue ( isinstance ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] , Gaffer . Plug ) ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) is None ) \n def testSerialiseDynamicCoshaderArrayParameters ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT:n>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:n>\" ] . loadShader ( shader ) \n s [ \"<STR_LIT:c>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:c>\" ] . loadShader ( coshader ) \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . setInput ( s [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . setInput ( s [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:2> ] . setInput ( s [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . setInput ( None ) \n self . assertEqual ( len ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:4> ) \n s2 = Gaffer . ScriptNode ( ) \n s2 . execute ( s . serialise ( ) ) \n self . assertEqual ( len ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:4> ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) . isSame ( s2 [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . getInput ( ) is None ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:2> ] . getInput ( ) . isSame ( s2 [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:3> ] . getInput ( ) is None ) \n s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:3> ] . setInput ( s2 [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) \n self . assertEqual ( len ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:5> ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) . isSame ( s2 [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . getInput ( ) is None ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:2> ] . getInput ( ) . isSame ( s2 [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:3> ] . getInput ( ) . isSame ( s2 [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:4> ] . getInput ( ) is None ) \n def testConvertFixedCoshaderArrayToDynamic ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderV2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT:n>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:n>\" ] . loadShader ( shader ) \n s [ \"<STR_LIT:c>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:c>\" ] . loadShader ( coshader ) \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . setInput ( s [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) \n self . assertTrue ( len ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:4> ) \n s [ \"<STR_LIT:n>\" ] . loadShader ( shaderV2 , keepExistingValues = True ) \n self . assertTrue ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) . isSame ( s [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . getInput ( ) is None ) \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . setInput ( None ) \n self . assertEqual ( len ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:1> ) \n self . assertTrue ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) is None ) \n def testConvertFixedCoshaderArrayToDynamicWithFirstPlugUnconnected ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderV2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT:n>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:n>\" ] . loadShader ( shader ) \n s [ \"<STR_LIT:c>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:c>\" ] . loadShader ( coshader ) \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . setInput ( s [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) \n self . assertTrue ( len ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:4> ) \n s [ \"<STR_LIT:n>\" ] . loadShader ( shaderV2 , keepExistingValues = True ) \n self . assertTrue ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . getInput ( ) . isSame ( s [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) is None ) \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . setInput ( None ) \n self . assertEqual ( len ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:1> ) \n self . assertTrue ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) is None ) \n def testConvertFixedCoshaderArrayToDynamicDuringLoading ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT:n>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:n>\" ] . loadShader ( shader ) \n s [ \"<STR_LIT:c>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:c>\" ] . loadShader ( coshader ) \n s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . setInput ( s [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) \n self . assertTrue ( len ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:4> ) \n GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" , shaderName = \"<STR_LIT>\" ) \n s2 = Gaffer . ScriptNode ( ) \n s2 . execute ( s . serialise ( ) ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . getInput ( ) . isSame ( s2 [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) is None ) \n s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . setInput ( None ) \n self . assertEqual ( len ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) , <NUM_LIT:1> ) \n self . assertTrue ( s2 [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . getInput ( ) is None ) \n def testHashThroughBox ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n b = Gaffer . Box ( ) \n b . addChild ( Gaffer . Plug ( \"<STR_LIT>\" ) ) \n b . addChild ( Gaffer . Plug ( \"<STR_LIT>\" , direction = Gaffer . Plug . Direction . Out ) ) \n intermediateCoshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n intermediateCoshaderNode = GafferRenderMan . RenderManShader ( ) \n intermediateCoshaderNode . loadShader ( intermediateCoshader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n b [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n intermediateCoshaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( b [ \"<STR_LIT>\" ] ) \n b [ \"<STR_LIT>\" ] . setInput ( intermediateCoshaderNode [ \"<STR_LIT>\" ] ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( b [ \"<STR_LIT>\" ] ) \n h1 = shaderNode . stateHash ( ) \n coshaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT> ) \n self . assertNotEqual ( shaderNode . stateHash ( ) , h1 ) \n def testDanglingBoxConnection ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode1 = GafferRenderMan . RenderManShader ( ) \n shaderNode1 . loadShader ( shader ) \n shaderNode2 = GafferRenderMan . RenderManShader ( ) \n shaderNode2 . loadShader ( shader ) \n b = Gaffer . Box ( ) \n b . addChild ( Gaffer . Plug ( \"<STR_LIT>\" ) ) \n b . addChild ( Gaffer . Plug ( \"<STR_LIT>\" , direction = Gaffer . Plug . Direction . Out ) ) \n b [ \"<STR_LIT>\" ] = shaderNode1 \n shaderNode1 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( b [ \"<STR_LIT>\" ] ) \n shaderNode2 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( b [ \"<STR_LIT>\" ] ) \n def testUnconnectedCustomBoxInput ( self ) : \n class CustomBox ( Gaffer . Box ) : \n def __init__ ( self , name = \"<STR_LIT>\" ) : \n Gaffer . Box . __init__ ( self , name ) \n IECore . registerRunTimeTyped ( CustomBox ) \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n b = CustomBox ( ) \n b [ \"<STR_LIT:s>\" ] = GafferRenderMan . RenderManShader ( ) \n b [ \"<STR_LIT:s>\" ] . loadShader ( shader ) \n b [ \"<STR_LIT>\" ] = b [ \"<STR_LIT:s>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . createCounterpart ( \"<STR_LIT>\" , Gaffer . Plug . Direction . In ) \n b [ \"<STR_LIT:s>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( b [ \"<STR_LIT>\" ] ) \n s = b [ \"<STR_LIT:s>\" ] . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:1> ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . name , shader ) \n self . assertTrue ( b [ \"<STR_LIT:s>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getInput ( ) . isSame ( b [ \"<STR_LIT>\" ] ) ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n c = GafferRenderMan . RenderManShader ( ) \n c . loadShader ( coshader ) \n self . assertTrue ( b [ \"<STR_LIT>\" ] . acceptsInput ( c [ \"<STR_LIT>\" ] ) ) \n b [ \"<STR_LIT>\" ] . setInput ( c [ \"<STR_LIT>\" ] ) \n s = b [ \"<STR_LIT:s>\" ] . state ( ) \n self . assertEqual ( len ( s ) , <NUM_LIT:2> ) \n self . assertEqual ( s [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] , s [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] ) \n n = Gaffer . Node ( ) \n n [ \"<STR_LIT>\" ] = b [ \"<STR_LIT>\" ] . createCounterpart ( \"<STR_LIT>\" , Gaffer . Plug . Direction . Out ) \n self . assertFalse ( b [ \"<STR_LIT>\" ] . acceptsInput ( n [ \"<STR_LIT>\" ] ) ) \n self . assertRaises ( RuntimeError , b [ \"<STR_LIT>\" ] . setInput , n [ \"<STR_LIT>\" ] ) \n b [ \"<STR_LIT:s>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( None ) \n self . assertTrue ( b [ \"<STR_LIT>\" ] . acceptsInput ( n [ \"<STR_LIT>\" ] ) ) \n b [ \"<STR_LIT>\" ] . setInput ( n [ \"<STR_LIT>\" ] ) \n self . assertTrue ( b [ \"<STR_LIT>\" ] . getInput ( ) . isSame ( n [ \"<STR_LIT>\" ] ) ) \n self . assertFalse ( b [ \"<STR_LIT:s>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( b [ \"<STR_LIT>\" ] ) ) \n self . assertRaises ( RuntimeError , b [ \"<STR_LIT:s>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput , b [ \"<STR_LIT>\" ] ) \n def testCoshaderSwitching ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode0 = GafferRenderMan . RenderManShader ( ) \n coshaderNode0 . loadShader ( coshader ) \n coshaderNode1 = GafferRenderMan . RenderManShader ( ) \n coshaderNode1 . loadShader ( coshader ) \n coshaderNode0 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT:0> ) \n coshaderNode1 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT:1> ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n switch = GafferScene . ShaderSwitch ( ) \n switch [ \"<STR_LIT>\" ] . setInput ( coshaderNode0 [ \"<STR_LIT>\" ] ) \n switch [ \"<STR_LIT>\" ] . setInput ( coshaderNode1 [ \"<STR_LIT>\" ] ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( switch [ \"<STR_LIT>\" ] ) \n self . assertEqual ( shaderNode . state ( ) [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , <NUM_LIT:0> ) \n switch [ \"<STR_LIT:index>\" ] . setValue ( <NUM_LIT:1> ) \n self . assertEqual ( shaderNode . state ( ) [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , <NUM_LIT:1> ) \n switch [ \"<STR_LIT>\" ] . setValue ( False ) \n self . assertEqual ( shaderNode . state ( ) [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , <NUM_LIT:0> ) \n def testCoshaderTypingPreventsNewInvalidSwitchInputs ( self ) : \n coshaderType1 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderType1Node = GafferRenderMan . RenderManShader ( ) \n coshaderType1Node . loadShader ( coshaderType1 ) \n coshaderType2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderType2Node = GafferRenderMan . RenderManShader ( ) \n coshaderType2Node . loadShader ( coshaderType2 ) \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n switch = GafferScene . ShaderSwitch ( ) \n switch [ \"<STR_LIT>\" ] . setInput ( coshaderType1Node [ \"<STR_LIT>\" ] ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( switch [ \"<STR_LIT>\" ] ) \n self . assertFalse ( switch [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType2Node [ \"<STR_LIT>\" ] ) ) \n self . assertTrue ( switch [ \"<STR_LIT>\" ] . acceptsInput ( coshaderType1Node [ \"<STR_LIT>\" ] ) ) \n def testAcceptInputFromEmptySwitch ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n switch = GafferScene . ShaderSwitch ( ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( switch [ \"<STR_LIT>\" ] ) ) \n def testCoshaderSwitchingInBox ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n script = Gaffer . ScriptNode ( ) \n script [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n script [ \"<STR_LIT>\" ] . loadShader ( coshader ) \n script [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n script [ \"<STR_LIT>\" ] . loadShader ( coshader ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT:0> ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT:1> ) \n script [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n script [ \"<STR_LIT>\" ] . loadShader ( shader ) \n script [ \"<STR_LIT>\" ] = GafferScene . ShaderSwitch ( ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n self . assertEqual ( script [ \"<STR_LIT>\" ] . state ( ) [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , <NUM_LIT:0> ) \n box = Gaffer . Box . create ( script , Gaffer . StandardSet ( script . children ( Gaffer . Node ) ) ) \n self . assertEqual ( box [ \"<STR_LIT>\" ] . state ( ) [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , <NUM_LIT:0> ) \n promotedIndex = box . promotePlug ( box [ \"<STR_LIT>\" ] [ \"<STR_LIT:index>\" ] ) \n self . assertEqual ( box [ \"<STR_LIT>\" ] . state ( ) [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , <NUM_LIT:0> ) \n promotedIndex . setValue ( <NUM_LIT:1> ) \n self . assertEqual ( box [ \"<STR_LIT>\" ] . state ( ) [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value , <NUM_LIT:1> ) \n def testRepeatability ( self ) : \n s1 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n sn1 = GafferRenderMan . RenderManShader ( ) \n sn2 = GafferRenderMan . RenderManShader ( ) \n sn1 . loadShader ( s1 ) \n sn2 . loadShader ( s2 ) \n sn2 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( sn1 [ \"<STR_LIT>\" ] ) \n self . assertEqual ( sn2 . stateHash ( ) , sn2 . stateHash ( ) ) \n self . assertEqual ( sn2 . state ( ) , sn2 . state ( ) ) \n def testHandlesAreHumanReadable ( self ) : \n s1 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n sn1 = GafferRenderMan . RenderManShader ( \"<STR_LIT>\" ) \n sn2 = GafferRenderMan . RenderManShader ( \"<STR_LIT>\" ) \n sn1 . loadShader ( s1 ) \n sn2 . loadShader ( s2 ) \n sn2 [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( sn1 [ \"<STR_LIT>\" ] ) \n state = sn2 . state ( ) \n self . assertTrue ( \"<STR_LIT>\" in state [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] . value ) \n def testHandlesAreUniqueEvenIfNodeNamesArent ( self ) : \n s1 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s2 = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n script = Gaffer . ScriptNode ( ) \n script [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n script [ \"<STR_LIT>\" ] . loadShader ( s1 ) \n script [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n script [ \"<STR_LIT>\" ] . loadShader ( s1 ) \n script [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n script [ \"<STR_LIT>\" ] . loadShader ( s2 ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] . setInput ( script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] . setInput ( script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n box = Gaffer . Box . create ( script , Gaffer . StandardSet ( [ script [ \"<STR_LIT>\" ] ] ) ) \n box [ \"<STR_LIT>\" ] . setName ( \"<STR_LIT>\" ) \n script [ \"<STR_LIT>\" ] . setName ( \"<STR_LIT>\" ) \n state = script [ \"<STR_LIT>\" ] . state ( ) \n self . assertNotEqual ( state [ <NUM_LIT:0> ] . parameters [ \"<STR_LIT>\" ] , state [ <NUM_LIT:1> ] . parameters [ \"<STR_LIT>\" ] ) \n def testShaderTypesInState ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( coshaderNode [ \"<STR_LIT>\" ] ) \n state = shaderNode . state ( ) \n self . assertEqual ( state [ <NUM_LIT:0> ] . type , \"<STR_LIT>\" ) \n self . assertEqual ( state [ <NUM_LIT:1> ] . type , \"<STR_LIT>\" ) \n def testAssignmentAttributeName ( self ) : \n p = GafferScene . Plane ( ) \n s = GafferRenderMan . RenderManShader ( ) \n s . loadShader ( \"<STR_LIT>\" ) \n a = GafferScene . ShaderAssignment ( ) \n a [ \"<STR_LIT>\" ] . setInput ( p [ \"<STR_LIT>\" ] ) \n a [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] ) \n self . assertEqual ( a [ \"<STR_LIT>\" ] . attributes ( \"<STR_LIT>\" ) . keys ( ) , [ \"<STR_LIT>\" ] ) \n def testVolumeShader ( self ) : \n s = GafferRenderMan . RenderManShader ( ) \n s . loadShader ( \"<STR_LIT>\" ) \n self . assertEqual ( s [ \"<STR_LIT:type>\" ] . getValue ( ) , \"<STR_LIT>\" ) \n s [ \"<STR_LIT:type>\" ] . setValue ( \"<STR_LIT>\" ) \n s . loadShader ( \"<STR_LIT>\" , keepExistingValues = True ) \n self . assertEqual ( s [ \"<STR_LIT:type>\" ] . getValue ( ) , \"<STR_LIT>\" ) \n s . loadShader ( \"<STR_LIT>\" , keepExistingValues = False ) \n self . assertEqual ( s [ \"<STR_LIT:type>\" ] . getValue ( ) , \"<STR_LIT>\" ) \n def testInputAcceptanceFromDots ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshaderNode = GafferRenderMan . RenderManShader ( ) \n coshaderNode . loadShader ( coshader ) \n dot = Gaffer . Dot ( ) \n dot . setup ( coshaderNode [ \"<STR_LIT>\" ] ) \n self . assertTrue ( shaderNode [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . acceptsInput ( dot [ \"<STR_LIT>\" ] ) ) \n def testShaderTypeOverride ( self ) : \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n shaderNode = GafferRenderMan . RenderManShader ( ) \n shaderNode . loadShader ( shader ) \n self . assertEqual ( shaderNode [ '<STR_LIT:type>' ] . getValue ( ) , \"<STR_LIT>\" ) \n def testReferencePromotedCoshader ( self ) : \n s = Gaffer . ScriptNode ( ) \n shader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n coshader = self . compileShader ( os . path . dirname ( __file__ ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT:b>\" ] = Gaffer . Box ( ) \n s [ \"<STR_LIT:b>\" ] [ \"<STR_LIT:s>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:b>\" ] [ \"<STR_LIT:s>\" ] . loadShader ( shader ) \n p = s [ \"<STR_LIT:b>\" ] . promotePlug ( s [ \"<STR_LIT:b>\" ] [ \"<STR_LIT:s>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n p . setName ( \"<STR_LIT:p>\" ) \n s [ \"<STR_LIT:c>\" ] = GafferRenderMan . RenderManShader ( ) \n s [ \"<STR_LIT:c>\" ] . loadShader ( coshader ) \n self . assertTrue ( s [ \"<STR_LIT:b>\" ] [ \"<STR_LIT:p>\" ] . acceptsInput ( s [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) ) \n s [ \"<STR_LIT:b>\" ] . exportForReference ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT:r>\" ] = Gaffer . Reference ( ) \n s [ \"<STR_LIT:r>\" ] . load ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertTrue ( s [ \"<STR_LIT:r>\" ] [ \"<STR_LIT:p>\" ] . acceptsInput ( s [ \"<STR_LIT:c>\" ] [ \"<STR_LIT>\" ] ) ) \n def testLoadAndGIL ( self ) : \n script = Gaffer . ScriptNode ( ) \n script [ \"<STR_LIT>\" ] = GafferScene . Plane ( ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( IECore . V2i ( <NUM_LIT:20> ) ) \n script [ \"<STR_LIT>\" ] = GafferScene . Sphere ( ) \n script [ \"<STR_LIT>\" ] = Gaffer . Expression ( ) \n script [ \"<STR_LIT>\" ] . setExpression ( \"<STR_LIT>\" ) \n script [ \"<STR_LIT>\" ] = GafferScene . Instancer ( ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n script [ \"<STR_LIT>\" ] = GafferRenderMan . RenderManShader ( ) \n script [ \"<STR_LIT>\" ] = GafferScene . ShaderAssignment ( ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n traverseConnection = Gaffer . ScopedConnection ( GafferSceneTest . connectTraverseSceneToPlugDirtiedSignal ( script [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) ) \n script [ \"<STR_LIT>\" ] . loadShader ( \"<STR_LIT>\" ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import os \n import unittest \n import IECore \n import Gaffer \n import GafferImage \n import GafferScene \n import GafferSceneTest \n @ unittest . skipIf ( \"<STR_LIT>\" in os . environ , \"<STR_LIT>\" ) \n class OpenGLRenderTest ( GafferSceneTest . SceneTestCase ) : \n def test ( self ) : \n self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Plane ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( IECore . V3f ( <NUM_LIT:0> , <NUM_LIT:0> , - <NUM_LIT:5> ) ) \n s [ \"<STR_LIT:image>\" ] = GafferImage . ImageReader ( ) \n s [ \"<STR_LIT:image>\" ] [ \"<STR_LIT>\" ] . setValue ( os . path . expandvars ( \"<STR_LIT>\" ) ) \n s [ \"<STR_LIT>\" ] = GafferScene . OpenGLShader ( ) \n s [ \"<STR_LIT>\" ] . loadShader ( \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT:image>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT:1> ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue ( IECore . Color4f ( <NUM_LIT:1> ) ) \n s [ \"<STR_LIT>\" ] = GafferScene . ShaderAssignment ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] = GafferScene . Outputs ( ) \n s [ \"<STR_LIT>\" ] . addOutput ( \n \"<STR_LIT>\" , \n IECore . Display ( \n self . temporaryDirectory ( ) + \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n { } \n ) \n ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] = GafferScene . OpenGLRender ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] . setValue ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n s . save ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . execute ( ) \n self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n i = IECore . EXRImageReader ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) . read ( ) \n e = IECore . ImagePrimitiveEvaluator ( i ) \n r = e . createResult ( ) \n e . pointAtUV ( IECore . V2f ( <NUM_LIT:0.5> ) , r ) \n self . assertAlmostEqual ( r . floatPrimVar ( e . R ( ) ) , <NUM_LIT> , <NUM_LIT:5> ) \n self . assertAlmostEqual ( r . floatPrimVar ( e . G ( ) ) , <NUM_LIT> , <NUM_LIT:5> ) \n self . assertEqual ( r . floatPrimVar ( e . B ( ) ) , <NUM_LIT:0> ) \n def testOutputDirectoryCreation ( self ) : \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT>\" ] . addMember ( \"<STR_LIT>\" , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n s [ \"<STR_LIT>\" ] = GafferScene . Plane ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Outputs ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] . addOutput ( \n \"<STR_LIT>\" , \n IECore . Display ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n { } \n ) \n ) \n s [ \"<STR_LIT>\" ] = GafferScene . OpenGLRender ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n s [ \"<STR_LIT>\" ] . setValue ( \"<STR_LIT>\" ) \n with s . context ( ) : \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . execute ( ) \n self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) ) \n def testHash ( self ) : \n c = Gaffer . Context ( ) \n c . setFrame ( <NUM_LIT:1> ) \n c2 = Gaffer . Context ( ) \n c2 . setFrame ( <NUM_LIT:2> ) \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Plane ( ) \n s [ \"<STR_LIT>\" ] = GafferScene . Outputs ( ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n s [ \"<STR_LIT>\" ] . addOutput ( \"<STR_LIT>\" , IECore . Display ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , { } ) ) \n s [ \"<STR_LIT>\" ] = GafferScene . OpenGLRender ( ) \n self . assertEqual ( s [ \"<STR_LIT>\" ] . hash ( c ) , IECore . MurmurHash ( ) ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n self . assertNotEqual ( s [ \"<STR_LIT>\" ] . hash ( c ) , IECore . MurmurHash ( ) ) \n self . assertNotEqual ( s [ \"<STR_LIT>\" ] . hash ( c ) , s [ \"<STR_LIT>\" ] . hash ( c2 ) ) \n current = s [ \"<STR_LIT>\" ] . hash ( c ) \n c [ \"<STR_LIT>\" ] = self . temporaryDirectory ( ) + \"<STR_LIT>\" \n self . assertNotEqual ( s [ \"<STR_LIT>\" ] . hash ( c ) , current ) \n current = s [ \"<STR_LIT>\" ] . hash ( c ) \n c [ \"<STR_LIT>\" ] = self . temporaryDirectory ( ) + \"<STR_LIT>\" \n self . assertNotEqual ( s [ \"<STR_LIT>\" ] . hash ( c ) , current ) \n current = s [ \"<STR_LIT>\" ] . hash ( c ) \n c [ \"<STR_LIT>\" ] = \"<STR_LIT>\" \n self . assertEqual ( s [ \"<STR_LIT>\" ] . hash ( c ) , current ) \n current = s [ \"<STR_LIT>\" ] . hash ( c ) \n s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setInput ( s [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] ) \n self . assertNotEqual ( s [ \"<STR_LIT>\" ] . hash ( c ) , current ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import unittest \n import IECore \n import Gaffer \n import GafferTest \n import GafferScene \n import GafferSceneTest \n class SceneTimeWarpTest ( GafferSceneTest . SceneTestCase ) : \n def testConstruct ( self ) : \n s = Gaffer . ScriptNode ( ) \n s [ \"<STR_LIT:n>\" ] = GafferScene . SceneTimeWarp ( ) \n self . assertEqual ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:1> ) \n self . assertEqual ( s [ \"<STR_LIT:n>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , <NUM_LIT:0> ) \n def testRunTimeTyped ( self ) : \n n = GafferScene . SceneTimeWarp ( ) \n self . failUnless ( n . isInstanceOf ( GafferScene . SceneTimeWarp . staticTypeId ( ) ) ) \n self . failUnless ( n . isInstanceOf ( GafferScene . SceneContextProcessor . staticTypeId ( ) ) ) \n self . failUnless ( n . isInstanceOf ( GafferScene . SceneProcessor . staticTypeId ( ) ) ) \n self . failUnless ( n . isInstanceOf ( GafferScene . SceneNode . staticTypeId ( ) ) ) \n self . failUnless ( n . isInstanceOf ( Gaffer . Node . staticTypeId ( ) ) ) \n baseTypeIds = IECore . RunTimeTyped . baseTypeIds ( n . typeId ( ) ) \n self . failUnless ( GafferScene . SceneContextProcessor . staticTypeId ( ) in baseTypeIds ) \n self . failUnless ( GafferScene . SceneProcessor . staticTypeId ( ) in baseTypeIds ) \n self . failUnless ( GafferScene . SceneNode . staticTypeId ( ) in baseTypeIds ) \n self . failUnless ( Gaffer . Node . staticTypeId ( ) in baseTypeIds ) \n def testAffects ( self ) : \n n = GafferScene . SceneTimeWarp ( ) \n c = GafferTest . CapturingSlot ( n . plugDirtiedSignal ( ) ) \n n [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT:2> ) \n found = False \n for cc in c : \n if cc [ <NUM_LIT:0> ] . isSame ( n [ \"<STR_LIT>\" ] ) : \n found = True \n self . failUnless ( found ) \n del c [ : ] \n n [ \"<STR_LIT>\" ] . setValue ( <NUM_LIT:2> ) \n found = False \n for cc in c : \n if cc [ <NUM_LIT:0> ] . isSame ( n [ \"<STR_LIT>\" ] ) : \n found = True \n self . failUnless ( found ) \n def testNoExtraInputs ( self ) : \n p = GafferScene . Plane ( ) \n n = GafferScene . SceneTimeWarp ( ) \n n [ \"<STR_LIT>\" ] . setInput ( p [ \"<STR_LIT>\" ] ) \n self . assertTrue ( \"<STR_LIT>\" not in n ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import Gaffer \n import GafferScene \n Gaffer . Metadata . registerNode ( \n GafferScene . Cube , \n \"<STR_LIT:description>\" , \n \"\"\"<STR_LIT>\"\"\" , \n plugs = { \n \"<STR_LIT>\" : [ \n <mask0> , \n \"\"\"<STR_LIT>\"\"\" , \n ] , \n } \n ) \n", "gt": "\"<STR_LIT:description>\""}
{"input": "\n import Gaffer \n import GafferScene \n import GafferUI \n Gaffer . Metadata . registerNode ( \n GafferScene . ObjectSource , \n \"<STR_LIT:description>\" , \n \"\"\"<STR_LIT>\"\"\" , \n plugs = { \n \"<STR_LIT:name>\" : [ \n \"<STR_LIT:description>\" , \n \"\"\"<STR_LIT>\"\"\" , \n ] , \n \"<STR_LIT>\" : [ \n \"<STR_LIT:description>\" , \n \"\"\"<STR_LIT>\"\"\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \n ] , \n \"<STR_LIT>\" : [ \n <mask0> , \n \"\"\"<STR_LIT>\"\"\" , \n ] , \n } \n ) \n", "gt": "\"<STR_LIT:description>\""}
{"input": "\n import functools \n import IECore \n import Gaffer \n import GafferUI \n import GafferScene \n import GafferSceneUI \n Gaffer . Metadata . registerNode ( \n GafferSceneUI . SceneView , \n plugs = { \n \"<STR_LIT>\" : [ \n \"<STR_LIT>\" , <NUM_LIT:2> , \n \"<STR_LIT>\" , True , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \n ] , \n \"<STR_LIT>\" : [ \n \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , True , \n ] , \n \"<STR_LIT>\" : [ \n \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , True , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \n ] , \n \"<STR_LIT>\" : [ \n \"<STR_LIT:description>\" , \n \"\"\"<STR_LIT>\"\"\" , \n ] , \n \"<STR_LIT>\" : [ \n \"<STR_LIT:description>\" , \n \"\"\"<STR_LIT>\"\"\" , \n ] , \n \"<STR_LIT>\" : [ \n \"<STR_LIT>\" , \"<STR_LIT>\" , \n ] , \n \"<STR_LIT>\" : [ \n \"<STR_LIT>\" , \"<STR_LIT>\" , \n ] , \n } \n ) \n class _ShadingModePlugValueWidget ( GafferUI . PlugValueWidget ) : \n def __init__ ( self , plug , parenting = None ) : \n menuButton = GafferUI . MenuButton ( \n image = \"<STR_LIT>\" , \n menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) , \n hasFrame = False , \n ) \n GafferUI . PlugValueWidget . __init__ ( self , menuButton , plug , parenting = parenting ) \n def hasLabel ( self ) : \n return True \n def _updateFromPlug ( self ) : \n pass \n def __menuDefinition ( self ) : \n m = IECore . MenuDefinition ( ) \n currentName = self . getPlug ( ) . getValue ( ) \n for name in [ \"<STR_LIT>\" ] + GafferSceneUI . SceneView . registeredShadingModes ( ) : \n m . append ( \n \"<STR_LIT:/>\" + name if name else \"<STR_LIT>\" , \n { \n \"<STR_LIT>\" : name == currentName , \n \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __setValue ) , name if name != currentName else \"<STR_LIT>\" ) , \n } \n ) \n if not name : \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n return m \n def __setValue ( self , value , * unused ) : \n self . getPlug ( ) . setValue ( value ) \n class _ExpansionPlugValueWidget ( GafferUI . PlugValueWidget ) : \n def __init__ ( self , plug , parenting = None ) : \n menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) \n menuButton = GafferUI . MenuButton ( menu = menu , image = \"<STR_LIT>\" , hasFrame = False ) \n GafferUI . PlugValueWidget . __init__ ( self , menuButton , plug , parenting = parenting ) \n def hasLabel ( self ) : \n return True \n def _updateFromPlug ( self ) : \n pass \n def __menuDefinition ( self ) : \n expandAll = bool ( self . getPlug ( ) . getValue ( ) ) \n m = IECore . MenuDefinition ( ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : self . getPlug ( ) . node ( ) . expandSelection , \"<STR_LIT>\" : not expandAll , \"<STR_LIT>\" : \"<STR_LIT>\" } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( self . getPlug ( ) . node ( ) . expandSelection , depth = <NUM_LIT> ) , \"<STR_LIT>\" : not expandAll , \"<STR_LIT>\" : \"<STR_LIT>\" } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : self . getPlug ( ) . node ( ) . collapseSelection , \"<STR_LIT>\" : not expandAll , \"<STR_LIT>\" : \"<STR_LIT>\" } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : expandAll , \"<STR_LIT>\" : Gaffer . WeakMethod ( self . __toggleMinimumExpansionDepth ) } ) \n return m \n def __toggleMinimumExpansionDepth ( self , * unused ) : \n self . getPlug ( ) . setValue ( <NUM_LIT:0> if self . getPlug ( ) . getValue ( ) else <NUM_LIT> ) \n class _LookThroughPlugValueWidget ( GafferUI . PlugValueWidget ) : \n def __init__ ( self , plug , parenting = None ) : \n row = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal ) \n GafferUI . PlugValueWidget . __init__ ( self , row , plug , parenting = parenting ) \n with row : \n self . __enabledWidget = GafferUI . BoolPlugValueWidget ( plug [ \"<STR_LIT>\" ] , displayMode = GafferUI . BoolWidget . DisplayMode . Switch ) \n self . __cameraWidget = GafferSceneUI . ScenePathPlugValueWidget ( \n plug [ \"<STR_LIT>\" ] , \n path = GafferScene . ScenePath ( \n plug . node ( ) [ \"<STR_LIT>\" ] , \n plug . node ( ) . getContext ( ) , \n \"<STR_LIT:/>\" , \n filter = GafferScene . ScenePath . createStandardFilter ( [ \"<STR_LIT>\" ] , \"<STR_LIT>\" ) \n ) , \n ) \n self . __cameraWidget . pathWidget ( ) . setFixedCharacterWidth ( <NUM_LIT> ) \n if hasattr ( self . __cameraWidget . pathWidget ( ) . _qtWidget ( ) , \"<STR_LIT>\" ) : \n self . __cameraWidget . pathWidget ( ) . _qtWidget ( ) . setPlaceholderText ( \"<STR_LIT>\" ) \n self . _updateFromPlug ( ) \n def _updateFromPlug ( self ) : \n with self . getContext ( ) : \n self . __cameraWidget . setEnabled ( self . getPlug ( ) [ \"<STR_LIT>\" ] . getValue ( ) ) \n class _GridPlugValueWidget ( GafferUI . PlugValueWidget ) : \n def __init__ ( self , plug , parenting = None ) : \n menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) \n menuButton = GafferUI . MenuButton ( menu = menu , image = \"<STR_LIT>\" , hasFrame = False ) \n GafferUI . PlugValueWidget . __init__ ( self , menuButton , plug , parenting = parenting ) \n def hasLabel ( self ) : \n return True \n def _updateFromPlug ( self ) : \n pass \n def __menuDefinition ( self ) : \n m = IECore . MenuDefinition ( ) \n m . append ( \n \"<STR_LIT>\" , \n { \n \"<STR_LIT>\" : self . getPlug ( ) [ \"<STR_LIT>\" ] . getValue ( ) , \n \"<STR_LIT>\" : self . getPlug ( ) [ \"<STR_LIT>\" ] . setValue , \n } \n ) \n m . append ( \n \"<STR_LIT>\" , \n { \n \"<STR_LIT>\" : self . getPlug ( ) . node ( ) [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . getValue ( ) , \n \"<STR_LIT>\" : self . getPlug ( ) . node ( ) [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . setValue , \n } \n ) \n return <mask0> \n", "gt": "m"}
{"input": "\n import IECore \n import Gaffer \n class AddNode ( Gaffer . ComputeNode ) : \n def __init__ ( self , name = \"<STR_LIT>\" ) : \n Gaffer . ComputeNode . __init__ ( self , name ) \n p1 = Gaffer . IntPlug ( \"<STR_LIT>\" , Gaffer . Plug . Direction . In ) \n p2 = Gaffer . IntPlug ( \"<STR_LIT>\" , Gaffer . Plug . Direction . In ) \n self . addChild ( Gaffer . BoolPlug ( \"<STR_LIT>\" , defaultValue = True ) ) \n self . addChild ( p1 ) \n self . addChild ( p2 ) \n p3 = Gaffer . IntPlug ( \"<STR_LIT>\" , Gaffer . Plug . Direction . Out ) \n self . addChild ( p3 ) \n self . numHashCalls = <NUM_LIT:0> \n self . numComputeCalls = <NUM_LIT:0> \n def enabledPlug ( self ) : \n return self [ \"<STR_LIT>\" ] \n def correspondingInput ( self , output ) : \n if output . isSame ( self [ \"<STR_LIT>\" ] ) : \n return self [ \"<STR_LIT>\" ] \n return Gaffer . ComputeNode . correspondingInput ( self , output ) \n def affects ( self , input ) : \n outputs = Gaffer . ComputeNode . affects ( self , input ) \n if input . getName ( ) in ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) : \n outputs . append ( self . getChild ( \"<STR_LIT>\" ) ) \n return outputs \n def hash ( self , output , context , h ) : \n assert ( output . isSame ( self . getChild ( \"<STR_LIT>\" ) ) or plug . getFlags ( ) & plug . Flags . Dynamic ) \n self . getChild ( \"<STR_LIT>\" ) . hash ( h ) \n self . getChild ( \"<STR_LIT>\" ) . hash ( h ) \n self . getChild ( \"<STR_LIT>\" ) . hash ( h ) \n self . numHashCalls += <NUM_LIT:1> \n def compute ( self , plug , context ) : \n assert ( plug . isSame ( self . getChild ( \"<STR_LIT>\" ) ) or plug . getFlags ( ) & plug . Flags . Dynamic ) \n assert ( isinstance ( context , Gaffer . Context ) ) \n assert ( plug . settable ( ) ) \n assert ( not self [ \"<STR_LIT>\" ] . settable ( ) ) \n assert ( not self [ \"<STR_LIT>\" ] . settable ( ) ) \n if self [ \"<STR_LIT>\" ] . getValue ( ) : \n plug . setValue ( self . getChild ( \"<STR_LIT>\" ) . getValue ( ) + self . getChild ( \"<STR_LIT>\" ) . getValue ( ) ) \n else : \n plug . setValue ( self . getChild ( \"<STR_LIT>\" ) . getValue ( ) ) \n self . numComputeCalls += <NUM_LIT:1> \n IECore . registerRunTimeTyped ( AddNode , <mask0> = \"<STR_LIT>\" ) \n", "gt": "typeName"}
{"input": "\n from __future__ import with_statement \n import unittest \n import time \n import datetime \n import pwd \n import grp \n import os \n import IECore \n import Gaffer \n import GafferTest \n class FileSystemPathTest ( GafferTest . TestCase ) : \n def test ( self ) : \n p = Gaffer . FileSystemPath ( __file__ ) \n self . assert_ ( p . isValid ( ) ) \n self . assert_ ( p . isLeaf ( ) ) \n while len ( p ) : \n del p [ - <NUM_LIT:1> ] \n self . assert_ ( p . isValid ( ) ) \n self . assert_ ( not p . isLeaf ( ) ) \n def testIsLeaf ( self ) : \n path = Gaffer . FileSystemPath ( \"<STR_LIT>\" ) \n self . assert_ ( not path . isLeaf ( ) ) \n def testConstructWithFilter ( self ) : \n p = Gaffer . FileSystemPath ( __file__ ) \n self . failUnless ( p . getFilter ( ) is None ) \n f = Gaffer . FileNamePathFilter ( [ \"<STR_LIT>\" ] ) \n p = Gaffer . FileSystemPath ( __file__ , filter = f ) \n self . failUnless ( p . getFilter ( ) . isSame ( f ) ) \n def testBrokenSymbolicLinks ( self ) : \n os . symlink ( self . temporaryDirectory ( ) + \"<STR_LIT>\" , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n d = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n c = d . children ( ) \n self . assertEqual ( len ( c ) , <NUM_LIT:1> ) \n l = c [ <NUM_LIT:0> ] \n self . assertEqual ( str ( l ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( l . isValid ( ) , True ) \n info = l . info ( ) \n self . failUnless ( info is not None ) \n def testSymLinkInfo ( self ) : \n with open ( self . temporaryDirectory ( ) + \"<STR_LIT>\" , \"<STR_LIT:w>\" ) as f : \n f . write ( \"<STR_LIT>\" ) \n os . symlink ( self . temporaryDirectory ( ) + \"<STR_LIT>\" , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n a = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n l = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n aInfo = a . info ( ) \n self . assertEqual ( aInfo [ \"<STR_LIT>\" ] , l . info ( ) [ \"<STR_LIT>\" ] ) \n os . remove ( str ( a ) ) \n self . assertNotEqual ( aInfo [ \"<STR_LIT>\" ] , l . info ( ) [ \"<STR_LIT>\" ] ) \n def testCopy ( self ) : \n p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n p2 = p . copy ( ) \n self . assertEqual ( p , p2 ) \n self . assertEqual ( str ( p ) , str ( p2 ) ) \n def testEmptyPath ( self ) : \n p = Gaffer . FileSystemPath ( ) \n self . assertEqual ( str ( p ) , \"<STR_LIT>\" ) \n self . assertTrue ( p . isEmpty ( ) ) \n self . assertFalse ( p . isValid ( ) ) \n def testRelativePath ( self ) : \n os . chdir ( self . temporaryDirectory ( ) ) \n with open ( self . temporaryDirectory ( ) + \"<STR_LIT>\" , \"<STR_LIT:w>\" ) as f : \n f . write ( \"<STR_LIT>\" ) \n p = Gaffer . FileSystemPath ( \"<STR_LIT:a>\" ) \n self . assertEqual ( str ( p ) , \"<STR_LIT:a>\" ) \n self . assertFalse ( p . isEmpty ( ) ) \n self . assertTrue ( p . isValid ( ) ) \n p2 = Gaffer . FileSystemPath ( \"<STR_LIT>\" ) \n self . assertEqual ( str ( p2 ) , \"<STR_LIT>\" ) \n self . assertFalse ( p2 . isEmpty ( ) ) \n self . assertFalse ( p2 . isValid ( ) ) \n def testRelativePathChildren ( self ) : \n os . chdir ( self . temporaryDirectory ( ) ) \n os . mkdir ( \"<STR_LIT>\" ) \n with open ( self . temporaryDirectory ( ) + \"<STR_LIT>\" , \"<STR_LIT:w>\" ) as f : \n f . write ( \"<STR_LIT>\" ) \n p = Gaffer . FileSystemPath ( \"<STR_LIT>\" ) \n c = p . children ( ) \n self . assertEqual ( len ( c ) , <NUM_LIT:1> ) \n self . assertEqual ( str ( c [ <NUM_LIT:0> ] ) , \"<STR_LIT>\" ) \n self . assertTrue ( c [ <NUM_LIT:0> ] . isValid ( ) ) \n def testChildrenOfFile ( self ) : \n p = Gaffer . FileSystemPath ( __file__ ) \n self . assertEqual ( p . children ( ) , [ ] ) \n def testModificationTimes ( self ) : \n p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n p . append ( \"<STR_LIT:t>\" ) \n with open ( str ( p ) , \"<STR_LIT:w>\" ) as f : \n f . write ( \"<STR_LIT>\" ) \n mt = p . property ( \"<STR_LIT>\" ) \n self . assertTrue ( isinstance ( mt , datetime . datetime ) ) \n self . assertLess ( ( datetime . datetime . utcnow ( ) - mt ) . total_seconds ( ) , <NUM_LIT:2> ) \n time . sleep ( <NUM_LIT:1> ) \n with open ( str ( p ) , \"<STR_LIT:w>\" ) as f : \n f . write ( \"<STR_LIT>\" ) \n mt = p . property ( \"<STR_LIT>\" ) \n self . assertTrue ( isinstance ( mt , datetime . datetime ) ) \n self . assertLess ( ( datetime . datetime . utcnow ( ) - mt ) . total_seconds ( ) , <NUM_LIT:2> ) \n def testOwner ( self ) : \n p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n p . append ( \"<STR_LIT:t>\" ) \n with open ( str ( p ) , \"<STR_LIT:w>\" ) as f : \n f . write ( \"<STR_LIT>\" ) \n o = p . property ( \"<STR_LIT>\" ) \n self . assertTrue ( isinstance ( o , str ) ) \n self . assertEqual ( o , pwd . getpwuid ( os . stat ( str ( p ) ) . st_uid ) . pw_name ) \n def testGroup ( self ) : \n p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n p . append ( \"<STR_LIT:t>\" ) \n with open ( str ( p ) , \"<STR_LIT:w>\" ) as f : \n f . write ( \"<STR_LIT>\" ) \n g = p . property ( \"<STR_LIT>\" ) \n self . assertTrue ( isinstance ( g , str ) ) \n self . assertEqual ( g , grp . getgrgid ( os . stat ( str ( p ) ) . st_gid ) . gr_name ) \n def testPropertyNames ( self ) : \n p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n a = p . propertyNames ( ) \n self . assertTrue ( isinstance ( a , list ) ) \n self . assertTrue ( \"<STR_LIT>\" in a ) \n self . assertTrue ( \"<STR_LIT>\" in a ) \n self . assertTrue ( \"<STR_LIT>\" in a ) \n self . assertTrue ( \"<STR_LIT>\" in a ) \n self . assertTrue ( \"<STR_LIT>\" not in a ) \n p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) , includeSequences = True ) \n self . assertTrue ( \"<STR_LIT>\" in p . propertyNames ( ) ) \n def testSequences ( self ) : \n os . mkdir ( self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n for n in [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] : \n with open ( self . temporaryDirectory ( ) + \"<STR_LIT:/>\" + n , \"<STR_LIT:w>\" ) as f : \n f . write ( \"<STR_LIT>\" ) \n p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) , includeSequences = True ) \n self . assertTrue ( p . getIncludeSequences ( ) ) \n c = p . children ( ) \n self . assertEqual ( len ( c ) , <NUM_LIT:8> ) \n s = sorted ( c , key = str ) \n self . assertEqual ( str ( s [ <NUM_LIT:0> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:1> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:2> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:3> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:4> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:5> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:6> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:7> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n for x in s : \n self . assertTrue ( x . isValid ( ) ) \n if not os . path . isdir ( str ( x ) ) : \n self . assertTrue ( x . isLeaf ( ) ) \n self . assertEqual ( x . property ( \"<STR_LIT>\" ) , pwd . getpwuid ( os . stat ( str ( p ) ) . st_uid ) . pw_name ) \n self . assertEqual ( x . property ( \"<STR_LIT>\" ) , grp . getgrgid ( os . stat ( str ( p ) ) . st_gid ) . gr_name ) \n self . assertLess ( ( datetime . datetime . utcnow ( ) - x . property ( \"<STR_LIT>\" ) ) . total_seconds ( ) , <NUM_LIT:2> ) \n if \"<STR_LIT>\" not in str ( x ) : \n self . assertFalse ( x . isFileSequence ( ) ) \n self . assertEqual ( x . fileSequence ( ) , None ) \n self . assertEqual ( x . property ( \"<STR_LIT>\" ) , \"<STR_LIT>\" ) \n if os . path . isdir ( str ( x ) ) : \n self . assertEqual ( x . property ( \"<STR_LIT>\" ) , <NUM_LIT:0> ) \n else : \n self . assertEqual ( x . property ( \"<STR_LIT>\" ) , <NUM_LIT:4> ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . property ( \"<STR_LIT>\" ) , \"<STR_LIT>\" ) \n self . assertTrue ( s [ <NUM_LIT:0> ] . isFileSequence ( ) ) \n self . assertTrue ( isinstance ( s [ <NUM_LIT:0> ] . fileSequence ( ) , IECore . FileSequence ) ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . fileSequence ( ) , IECore . FileSequence ( str ( s [ <NUM_LIT:0> ] ) , IECore . frameListFromList ( [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:4> ] ) ) ) \n self . assertEqual ( s [ <NUM_LIT:0> ] . property ( \"<STR_LIT>\" ) , <NUM_LIT:4> * <NUM_LIT:3> ) \n self . assertEqual ( s [ <NUM_LIT:4> ] . property ( \"<STR_LIT>\" ) , \"<STR_LIT:3>\" ) \n self . assertTrue ( s [ <NUM_LIT:4> ] . isFileSequence ( ) ) \n self . assertTrue ( isinstance ( s [ <NUM_LIT:4> ] . fileSequence ( ) , IECore . FileSequence ) ) \n self . assertEqual ( s [ <NUM_LIT:4> ] . fileSequence ( ) , IECore . FileSequence ( str ( s [ <NUM_LIT:4> ] ) , IECore . frameListFromList ( [ <NUM_LIT:3> ] ) ) ) \n self . assertEqual ( s [ <NUM_LIT:4> ] . property ( \"<STR_LIT>\" ) , <NUM_LIT:4> ) \n p2 = p . copy ( ) \n self . assertTrue ( p2 . getIncludeSequences ( ) ) \n self . assertEqual ( len ( p2 . children ( ) ) , <NUM_LIT:8> ) \n p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) , includeSequences = False ) \n self . assertFalse ( p . getIncludeSequences ( ) ) \n c = p . children ( ) \n self . assertEqual ( len ( c ) , <NUM_LIT:6> ) \n s = sorted ( c , key = str ) \n self . assertEqual ( str ( s [ <NUM_LIT:0> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:1> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:2> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:3> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:4> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n self . assertEqual ( str ( s [ <NUM_LIT:5> ] ) , self . temporaryDirectory ( ) + \"<STR_LIT>\" ) \n p . setIncludeSequences ( True ) \n self . assertTrue ( p . getIncludeSequences ( ) ) \n c = p . children ( ) \n self . assertEqual ( len ( c ) , <NUM_LIT:8> ) \n def setUp ( self ) : \n GafferTest . TestCase . setUp ( self ) \n self . __originalCWD = os . getcwd ( ) \n def tearDown ( self ) : \n GafferTest . TestCase . tearDown ( self ) \n os . chdir ( self . __originalCWD ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import unittest \n import IECore \n import Gaffer \n import GafferTest \n class SequencePathTest ( GafferTest . TestCase ) : \n def __dictPath ( self ) : \n dict = { } \n dict [ \"<STR_LIT>\" ] = { } \n for f in IECore . FileSequence ( \"<STR_LIT>\" ) . fileNames ( ) : \n dict [ \"<STR_LIT>\" ] [ f ] = <NUM_LIT:1> \n for f in IECore . FileSequence ( \"<STR_LIT>\" ) . fileNames ( ) : \n dict [ \"<STR_LIT>\" ] [ f ] = <NUM_LIT:1> \n return Gaffer . DictPath ( dict , \"<STR_LIT:/>\" ) \n def test ( self ) : \n path = Gaffer . SequencePath ( self . __dictPath ( ) ) \n self . failUnless ( path . isValid ( ) ) \n self . failUnless ( not path . isLeaf ( ) ) \n path . append ( \"<STR_LIT>\" ) \n self . failUnless ( path . isValid ( ) ) \n self . failUnless ( not path . isLeaf ( ) ) \n path [ <NUM_LIT:0> ] = \"<STR_LIT>\" \n self . failIf ( path . isValid ( ) ) \n self . failIf ( path . isLeaf ( ) ) \n path [ : ] = [ \"<STR_LIT>\" ] \n children = path . children ( ) \n for child in children : \n self . failUnless ( isinstance ( child , Gaffer . SequencePath ) ) \n self . assertEqual ( len ( children ) , <NUM_LIT:2> ) \n childrenStrings = [ str ( c ) for c in children ] \n self . failUnless ( \"<STR_LIT>\" in childrenStrings ) \n self . failUnless ( \"<STR_LIT>\" in childrenStrings ) \n def testNonLeafChildren ( self ) : \n path = Gaffer . SequencePath ( self . __dictPath ( ) ) \n children = path . children ( ) \n for child in children : \n self . failUnless ( isinstance ( child , Gaffer . SequencePath ) ) \n self . assertEqual ( len ( children ) , <NUM_LIT:1> ) \n self . assertEqual ( str ( children [ <NUM_LIT:0> ] ) , \"<STR_LIT>\" ) \n def testCopy ( self ) : \n path = Gaffer . SequencePath ( self . __dictPath ( ) ) \n path . append ( \"<STR_LIT>\" ) \n path2 = path . copy ( ) \n self . failUnless ( isinstance ( path2 , Gaffer . SequencePath ) ) \n self . assertEqual ( path [ : ] , path2 [ : ] ) \n self . failUnless ( path . getFilter ( ) is path2 . getFilter ( ) ) \n c = [ str ( p ) for p in path . children ( ) ] \n c2 = [ str ( p ) for p in path2 . children ( ) ] \n self . assertEqual ( c , c2 ) \n def testInfo ( self ) : \n dictPath = self . __dictPath ( ) \n path = Gaffer . SequencePath ( dictPath ) \n self . assertEqual ( dictPath . info ( ) , path . info ( ) ) \n def testInfoOfInvalidPath ( self ) : \n fp = Gaffer . FileSystemPath ( \"<STR_LIT>\" ) \n self . assertEqual ( fp . isValid ( ) , False ) \n self . assertEqual ( fp . info ( ) , None ) \n sp = Gaffer . SequencePath ( fp ) \n self . assertEqual ( sp . isValid ( ) , False ) \n self . assertEqual ( sp . info ( ) , None ) \n def testFilter ( self ) : \n dictPath = self . __dictPath ( ) \n path = Gaffer . SequencePath ( dictPath ) \n def testIsEmpty ( self ) : \n dictPath = self . __dictPath ( ) \n path = Gaffer . SequencePath ( dictPath ) \n path . setFromString ( \"<STR_LIT>\" ) \n self . assertTrue ( path . isEmpty ( ) ) \n path2 = path . copy ( ) \n self . assertTrue ( path2 . isEmpty ( ) ) \n def testProperties ( self ) : \n dictPath = self . __dictPath ( ) \n path = Gaffer . SequencePath ( dictPath ) \n self . assertEqual ( dictPath . propertyNames ( ) , path . propertyNames ( ) ) \n self . assertEqual ( dictPath . property ( \"<STR_LIT>\" ) , path . property ( \"<STR_LIT>\" ) ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n assert ( __name__ == <mask0> ) \n", "gt": "\"<STR_LIT:__main__>\""}
{"input": "\n from __future__ import with_statement \n import IECore \n import Gaffer \n import GafferUI \n class CompoundDataPlugValueWidget ( GafferUI . PlugValueWidget ) : \n def __init__ ( self , plug , parenting = None ) : \n self . __column = GafferUI . ListContainer ( spacing = <NUM_LIT:6> ) \n GafferUI . PlugValueWidget . __init__ ( self , self . __column , plug , parenting = parenting ) \n with self . __column : \n self . __layout = GafferUI . PlugLayout ( plug ) \n with GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal ) as self . __editRow : \n GafferUI . Spacer ( IECore . V2i ( GafferUI . PlugWidget . labelWidth ( ) , <NUM_LIT:1> ) ) \n GafferUI . MenuButton ( \n image = \"<STR_LIT>\" , \n hasFrame = False , \n menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __addMenuDefinition ) ) \n ) \n GafferUI . Spacer ( IECore . V2i ( <NUM_LIT:1> ) , IECore . V2i ( <NUM_LIT> , <NUM_LIT:1> ) , parenting = { \"<STR_LIT>\" : True } ) \n self . _updateFromPlug ( ) \n def hasLabel ( self ) : \n return True \n def setPlug ( self , plug ) : \n GafferUI . PlugValueWidget . setPlug ( self , plug ) \n self . __layout = GafferUI . PlugLayout ( plug ) \n self . __column [ <NUM_LIT:0> ] = self . __layout \n def setReadOnly ( self , readOnly ) : \n if readOnly == self . getReadOnly ( ) : \n return \n GafferUI . PlugValueWidget . setReadOnly ( self , readOnly ) \n self . __layout . setReadOnly ( readOnly ) \n def childPlugValueWidget ( self , childPlug , lazy = True ) : \n return self . __layout . plugValueWidget ( childPlug , lazy ) \n def _updateFromPlug ( self ) : \n editable = True \n if self . getPlug ( ) is not None : \n editable = Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) \n editable = editable if editable is not None else True \n self . __editRow . setVisible ( editable ) \n def __addMenuDefinition ( self ) : \n result = IECore . MenuDefinition ( ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , \"<STR_LIT>\" , IECore . BoolData ( False ) ) } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , \"<STR_LIT>\" , IECore . FloatData ( <NUM_LIT:0> ) ) } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , \"<STR_LIT>\" , IECore . IntData ( <NUM_LIT:0> ) ) } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , \"<STR_LIT>\" , IECore . StringData ( \"<STR_LIT>\" ) ) } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , \"<STR_LIT>\" , IECore . V2iData ( IECore . V2i ( <NUM_LIT:0> ) ) ) } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , \"<STR_LIT>\" , IECore . V3iData ( IECore . V3i ( <NUM_LIT:0> ) ) ) } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , \"<STR_LIT>\" , IECore . V2fData ( IECore . V2f ( <NUM_LIT:0> ) ) ) } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , \"<STR_LIT>\" , IECore . V3fData ( IECore . V3f ( <NUM_LIT:0> ) ) ) } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , \"<STR_LIT>\" , IECore . Color3fData ( IECore . Color3f ( <NUM_LIT:0> ) ) ) } ) \n result . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , \"<STR_LIT>\" , IECore . Color4fData ( IECore . Color4f ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1> ) ) ) } ) \n return result \n def __addItem ( self , name , value ) : \n with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode . staticTypeId ( ) ) ) : \n self . getPlug ( ) . addOptionalMember ( name , value , enabled = True ) \n class _MemberPlugValueWidget ( GafferUI . PlugValueWidget ) : \n def __init__ ( self , childPlug ) : \n self . __row = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = <NUM_LIT:4> ) \n GafferUI . PlugValueWidget . __init__ ( self , self . __row , childPlug ) \n if not childPlug . getFlags ( Gaffer . Plug . Flags . Dynamic ) : \n nameWidget = GafferUI . LabelPlugValueWidget ( \n childPlug , \n horizontalAlignment = GafferUI . Label . HorizontalAlignment . Right , \n verticalAlignment = GafferUI . Label . VerticalAlignment . Center , \n ) \n nameWidget . label ( ) . _qtWidget ( ) . setFixedWidth ( GafferUI . PlugWidget . labelWidth ( ) ) \n nameWidget . label ( ) . _qtWidget ( ) . setFixedHeight ( <NUM_LIT:20> ) \n else : \n nameWidget = GafferUI . StringPlugValueWidget ( childPlug [ \"<STR_LIT:name>\" ] ) \n nameWidget . textWidget ( ) . _qtWidget ( ) . setFixedWidth ( GafferUI . PlugWidget . labelWidth ( ) ) \n self . __row . append ( nameWidget , \n verticalAlignment = GafferUI . Label . VerticalAlignment . Top \n ) \n if \"<STR_LIT>\" in childPlug : \n self . __row . append ( \n GafferUI . BoolPlugValueWidget ( \n childPlug [ \"<STR_LIT>\" ] , \n displayMode = GafferUI . BoolWidget . DisplayMode . Switch \n ) , \n verticalAlignment = GafferUI . Label . VerticalAlignment . Top , \n ) \n self . __row . append ( GafferUI . PlugValueWidget . create ( childPlug [ \"<STR_LIT:value>\" ] ) , expand = True ) \n self . _updateFromPlug ( ) \n def setPlug ( self , plug ) : \n GafferUI . PlugValueWidget . setPlug ( self , plug ) \n if isinstance ( self . __row [ <NUM_LIT:0> ] , GafferUI . LabelPlugValueWidget ) : \n self . __row [ <NUM_LIT:0> ] . setPlug ( plug ) \n else : \n self . __row [ <NUM_LIT:0> ] . setPlug ( plug [ \"<STR_LIT:name>\" ] ) \n if \"<STR_LIT>\" in plug : \n self . __row [ <NUM_LIT:1> ] . setPlug ( plug [ \"<STR_LIT>\" ] ) \n self . __row [ - <NUM_LIT:1> ] . setPlug ( plug [ \"<STR_LIT:value>\" ] ) \n def hasLabel ( self ) : \n return True \n def childPlugValueWidget ( self , childPlug , lazy = True ) : \n for w in self . __row : \n if w . getPlug ( ) . isSame ( childPlug ) : \n return w \n return None \n def setReadOnly ( self , readOnly ) : \n if readOnly == self . getReadOnly ( ) : \n return \n GafferUI . PlugValueWidget . setReadOnly ( self , readOnly ) \n for w in self . __row : \n w . setReadOnly ( readOnly ) \n def _updateFromPlug ( self ) : \n if \"<STR_LIT>\" in self . getPlug ( ) : \n with self . getContext ( ) : \n enabled = self . getPlug ( ) [ \"<STR_LIT>\" ] . getValue ( ) \n if isinstance ( self . __row [ <NUM_LIT:0> ] , GafferUI . StringPlugValueWidget ) : \n self . __row [ <NUM_LIT:0> ] . setEnabled ( enabled ) \n self . __row [ - <NUM_LIT:1> ] . setEnabled ( enabled ) \n GafferUI . PlugValueWidget . registerType ( Gaffer . CompoundDataPlug , CompoundDataPlugValueWidget ) \n GafferUI . PlugValueWidget . registerType ( Gaffer . CompoundDataPlug . MemberPlug , _MemberPlugValueWidget ) \n def __deletePlug ( plug ) : \n with Gaffer . UndoContext ( plug . ancestor ( Gaffer . ScriptNode ) ) : \n plug . parent ( ) . removeChild ( plug ) \n def __plugPopupMenu ( menuDefinition , plugValueWidget ) : \n plug = plugValueWidget . getPlug ( ) \n memberPlug = plug if isinstance ( plug , Gaffer . CompoundDataPlug . MemberPlug ) else None \n memberPlug = memberPlug if memberPlug is not None else plug . ancestor ( Gaffer . CompoundDataPlug . MemberPlug ) \n if memberPlug is None : \n return \n if not memberPlug . getFlags ( Gaffer . Plug . Flags . Dynamic ) : \n return \n menuDefinition . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n menuDefinition . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( __deletePlug , memberPlug ) , \"<STR_LIT>\" : not plugValueWidget . getReadOnly ( ) } ) \n __plugPopupMenuConnection = GafferUI . PlugValueWidget . popupMenuSignal ( ) . connect ( <mask0> ) \n", "gt": "__plugPopupMenu"}
{"input": "\n import Gaffer \n import GafferUI \n class FileSystemPathPlugValueWidget ( GafferUI . PathPlugValueWidget ) : \n def __init__ ( self , plug , path = None , parenting = None ) : \n GafferUI . PathPlugValueWidget . __init__ ( \n self , \n plug , \n path , \n parenting = parenting \n ) \n self . _updateFromPlug ( ) \n self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __plugMetadataChanged ) ) \n def getToolTip ( self ) : \n result = GafferUI . PathPlugValueWidget . getToolTip ( self ) \n extensions = self . __extensions ( ) \n if extensions : \n result += \"<STR_LIT>\" + \"<STR_LIT:U+002CU+0020>\" . join ( extensions ) \n return result \n def _pathChooserDialogue ( self ) : \n dialogue = GafferUI . PathPlugValueWidget . _pathChooserDialogue ( self ) \n if Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) : \n columns = dialogue . pathChooserWidget ( ) . pathListingWidget ( ) . getColumns ( ) \n columns . append ( GafferUI . PathListingWidget . StandardColumn ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n dialogue . pathChooserWidget ( ) . pathListingWidget ( ) . setColumns ( columns ) \n return dialogue \n def _updateFromPlug ( self ) : \n GafferUI . PathPlugValueWidget . _updateFromPlug ( self ) \n includeSequences = Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) or False \n self . path ( ) . setFilter ( \n Gaffer . FileSystemPath . createStandardFilter ( \n self . __extensions ( ) , \n Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) or \"<STR_LIT>\" , \n includeSequenceFilter = includeSequences , \n ) \n ) \n self . path ( ) . setIncludeSequences ( includeSequences ) \n def _setPlugFromPath ( self , path ) : \n if Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) : \n sequence = path . fileSequence ( ) \n if sequence : \n self . getPlug ( ) . setValue ( str ( sequence ) ) \n return \n GafferUI . PathPlugValueWidget . _setPlugFromPath ( self , path ) \n def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n if self . getPlug ( ) is None : \n return \n if plug is not None and not plug . isSame ( self . getPlug ( ) ) : \n return \n if not self . getPlug ( ) . node ( ) . isInstanceOf ( nodeTypeId ) : \n return \n if key . startswith ( \"<STR_LIT>\" ) : \n self . _updateFromPlug ( ) \n def __extensions ( self ) : \n if self . getPlug ( ) is None : \n return [ ] \n extensions = Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) or [ ] \n if isinstance ( extensions , str ) : \n extensions = extensions . split ( ) \n else : \n extensions = list ( extensions ) \n return <mask0> \n", "gt": "extensions"}
{"input": "\n import IECore \n import Gaffer \n import GafferUI \n class NameLabel ( GafferUI . Label ) : \n def __init__ ( self , graphComponent , horizontalAlignment = GafferUI . Label . HorizontalAlignment . Left , verticalAlignment = GafferUI . Label . VerticalAlignment . Center , numComponents = <NUM_LIT:1> , formatter = None , parenting = None ) : \n GafferUI . Label . __init__ ( self , \"<STR_LIT>\" , horizontalAlignment , verticalAlignment , parenting = parenting ) \n self . __formatter = formatter if formatter is not None else self . defaultFormatter \n self . __numComponents = numComponents \n self . __connections = [ ] \n self . __graphComponent = False \n self . setGraphComponent ( graphComponent ) \n self . __buttonPressConnection = self . buttonPressSignal ( ) . connect ( Gaffer . WeakMethod ( self . __buttonPress ) ) \n self . __dragBeginConnection = self . dragBeginSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragBegin ) ) \n self . __dragEndConnection = self . dragEndSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnd ) ) \n def setText ( self , text ) : \n GafferUI . Label . setText ( self , text ) \n self . __connections = [ ] \n def setGraphComponent ( self , graphComponent ) : \n if graphComponent is not None and self . __graphComponent is not False : \n if graphComponent . isSame ( self . __graphComponent ) : \n return \n elif self . __graphComponent is None : \n return \n self . __graphComponent = graphComponent \n self . __setupConnections ( ) \n self . __setText ( ) \n def getGraphComponent ( self ) : \n return self . __graphComponent \n def setNumComponents ( self , numComponents ) : \n assert ( numComponents > <NUM_LIT:0> ) \n if numComponents == self . __numComponents : \n return \n self . __numComponents = numComponents \n self . __setupConnections ( ) \n self . __setText ( ) \n def getNumComponents ( self ) : \n return self . __numComponents \n def setFormatter ( self , formatter ) : \n self . __formatter = formatter \n self . __setText ( ) \n def getFormatter ( self ) : \n return self . __formatter \n @ staticmethod \n def defaultFormatter ( graphComponents ) : \n return \"<STR_LIT:.>\" . join ( IECore . CamelCase . toSpaced ( g . getName ( ) ) for g in graphComponents ) \n def __setupConnections ( self , reuseUntil = None ) : \n if self . __graphComponent is None : \n self . __connections = [ ] \n return \n updatedConnections = [ ] \n n = <NUM_LIT:0> \n g = self . __graphComponent \n reuse = reuseUntil is not None \n while g is not None and n < self . __numComponents : \n if reuse : \n updatedConnections . extend ( self . __connections [ n * <NUM_LIT:2> : n * <NUM_LIT:2> + <NUM_LIT:2> ] ) \n else : \n updatedConnections . append ( g . nameChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __setText ) ) ) \n if n < self . __numComponents - <NUM_LIT:1> : \n updatedConnections . append ( g . parentChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __parentChanged ) ) ) \n if g . isSame ( reuseUntil ) : \n reuse = False \n g = g . parent ( ) \n n += <NUM_LIT:1> \n self . __connections = updatedConnections \n def __parentChanged ( self , child , oldParent ) : \n self . __setText ( ) \n self . __setupConnections ( reuseUntil = child ) \n def __setText ( self , * unwantedArgs ) : \n graphComponents = [ ] \n n = <NUM_LIT:0> \n g = self . __graphComponent \n while g is not None and n < self . __numComponents : \n graphComponents . append ( g ) \n g = g . parent ( ) \n n += <NUM_LIT:1> \n graphComponents . reverse ( ) \n GafferUI . Label . setText ( self , self . __formatter ( graphComponents ) ) \n def __buttonPress ( self , widget , event ) : \n return self . getGraphComponent ( ) is not None and event . buttons & ( event . Buttons . Left | event . Buttons . Middle ) \n def __dragBegin ( self , widget , event ) : \n if event . buttons & ( event . Buttons . Left | event . Buttons . Middle ) : \n GafferUI . Pointer . setCurrent ( \"<STR_LIT>\" ) \n return self . getGraphComponent ( ) \n return None \n def __dragEnd ( self , widget , event ) : \n GafferUI . Pointer . setCurrent ( <mask0> ) \n", "gt": "None"}
{"input": "\n import functools \n import IECore \n import Gaffer \n import GafferUI \n class PresetsPlugValueWidget ( GafferUI . PlugValueWidget ) : \n def __init__ ( self , plug , parenting = None ) : \n self . __menuButton = GafferUI . MenuButton ( \"<STR_LIT>\" , menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) ) \n GafferUI . PlugValueWidget . __init__ ( self , self . __menuButton , plug , parenting = parenting ) \n self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __plugMetadataChanged ) ) \n self . _addPopupMenu ( self . __menuButton ) \n self . _updateFromPlug ( ) \n def _updateFromPlug ( self ) : \n self . __menuButton . setEnabled ( self . _editable ( ) ) \n text = \"<STR_LIT>\" \n if self . getPlug ( ) is not None : \n with self . getContext ( ) : \n text = Gaffer . NodeAlgo . currentPreset ( self . getPlug ( ) ) or \"<STR_LIT>\" \n self . __menuButton . setText ( text ) \n def __menuDefinition ( self ) : \n result = IECore . MenuDefinition ( ) \n if self . getPlug ( ) is None : \n return result \n currentPreset = Gaffer . NodeAlgo . currentPreset ( self . getPlug ( ) ) \n for n in Gaffer . NodeAlgo . presets ( self . getPlug ( ) ) : \n result . append ( \n \"<STR_LIT:/>\" + n , \n { \n \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __applyPreset ) , preset = n ) , \n \"<STR_LIT>\" : n == currentPreset , \n } \n ) \n return result \n def __applyPreset ( self , unused , preset ) : \n with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n Gaffer . NodeAlgo . applyPreset ( self . getPlug ( ) , preset ) \n def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n if self . getPlug ( ) is None : \n return \n if plug is not None and not plug . isSame ( self . getPlug ( ) ) : \n return \n if not self . getPlug ( ) . node ( ) . isInstanceOf ( nodeTypeId ) : \n return \n if key . startswith ( \"<STR_LIT>\" ) : \n self . <mask0> ( ) \n", "gt": "_updateFromPlug"}
{"input": "\n import weakref \n import functools \n import types \n import re \n import collections \n import IECore \n import Gaffer \n import GafferUI \n class UIEditor ( GafferUI . NodeSetEditor ) : \n def __init__ ( self , scriptNode , parenting = None ) : \n self . __frame = GafferUI . Frame ( borderWidth = <NUM_LIT:4> , borderStyle = GafferUI . Frame . BorderStyle . None ) \n GafferUI . NodeSetEditor . __init__ ( self , self . __frame , scriptNode , parenting = parenting ) \n self . __nodeMetadataWidgets = [ ] \n self . __plugMetadataWidgets = [ ] \n with self . __frame : \n self . __tabbedContainer = GafferUI . TabbedContainer ( ) \n with self . __tabbedContainer : \n with GafferUI . ListContainer ( spacing = <NUM_LIT:4> , borderWidth = <NUM_LIT:8> , parenting = { \"<STR_LIT:label>\" : \"<STR_LIT>\" } ) as self . __nodeTab : \n with _Row ( ) : \n _Label ( \"<STR_LIT:Name>\" ) \n self . __nodeNameWidget = GafferUI . NameWidget ( None ) \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" , parenting = { \"<STR_LIT>\" : GafferUI . ListContainer . VerticalAlignment . Top } ) \n self . __nodeMetadataWidgets . append ( \n _MultiLineStringMetadataWidget ( key = \"<STR_LIT:description>\" ) \n ) \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" ) \n self . __nodeMetadataWidgets . append ( \n _ColorSwatchMetadataWidget ( key = \"<STR_LIT>\" ) \n ) \n with GafferUI . SplitContainer ( orientation = GafferUI . SplitContainer . Orientation . Horizontal , borderWidth = <NUM_LIT:8> , parenting = { \"<STR_LIT:label>\" : \"<STR_LIT>\" } ) as self . __plugTab : \n self . __plugListing = _PlugListing ( ) \n self . __plugListingSelectionChangedConnection = self . __plugListing . selectionChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __plugListingSelectionChanged ) ) \n with GafferUI . TabbedContainer ( ) as self . __plugAndSectionEditorsContainer : \n self . __plugEditor = _PlugEditor ( ) \n self . __sectionEditor = _SectionEditor ( ) \n self . __sectionEditorNameChangedConnection = self . __sectionEditor . nameChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __sectionEditorNameChanged ) ) \n self . __plugAndSectionEditorsContainer . setTabsVisible ( False ) \n self . __plugTab . setSizes ( [ <NUM_LIT> , <NUM_LIT> ] ) \n self . __node = None \n self . __selectedPlug = None \n self . __updateFromSetInternal ( lazy = False ) \n def setSelection ( self , selection ) : \n self . __plugListing . setSelection ( selection ) \n def getSelection ( self ) : \n return self . __plugListing . getSelection ( ) \n def nodeEditor ( self ) : \n return self . __nodeTab \n def plugEditor ( self ) : \n return self . __plugTab \n @ classmethod \n def appendNodeContextMenuDefinitions ( cls , nodeGraph , node , menuDefinition ) : \n menuDefinition . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n menuDefinition . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( cls . __setColor , node = node ) } ) \n @ classmethod \n def appendNodeEditorToolMenuDefinitions ( cls , nodeEditor , node , menuDefinition ) : \n menuDefinition . append ( \n \"<STR_LIT>\" , \n { \n \"<STR_LIT>\" : functools . partial ( GafferUI . UIEditor . acquire , node ) , \n \"<STR_LIT>\" : isinstance ( node , Gaffer . Box ) or nodeEditor . nodeUI ( ) . plugValueWidget ( node [ \"<STR_LIT:user>\" ] ) is not None \n } \n ) \n def _updateFromSet ( self ) : \n GafferUI . NodeSetEditor . _updateFromSet ( self ) \n self . __updateFromSetInternal ( ) \n def __updateFromSetInternal ( self , lazy = True ) : \n node = self . _lastAddedNode ( ) \n if lazy and node == self . __node : \n return \n self . __node = node \n self . __nodeNameWidget . setGraphComponent ( self . __node ) \n self . __nodeTab . setEnabled ( self . __node is not None ) \n if self . __node is None : \n self . __plugListing . setPlugParent ( None ) \n self . __sectionEditor . setPlugParent ( None ) \n else : \n plugParent = self . __node [ \"<STR_LIT:user>\" ] \n if isinstance ( self . __node , Gaffer . Box ) : \n plugParent = self . __node \n self . __plugListing . setPlugParent ( plugParent ) \n self . __sectionEditor . setPlugParent ( plugParent ) \n for widget in self . __nodeMetadataWidgets : \n widget . setTarget ( self . __node ) \n self . setSelection ( None ) \n def __plugListingSelectionChanged ( self , listing ) : \n selection = listing . getSelection ( ) \n if selection is None or isinstance ( selection , Gaffer . Plug ) : \n self . __plugEditor . setPlug ( selection ) \n self . __plugAndSectionEditorsContainer . setCurrent ( self . __plugEditor ) \n elif isinstance ( selection , basestring ) : \n self . __plugEditor . setPlug ( None ) \n self . __sectionEditor . setSection ( selection ) \n self . __plugAndSectionEditorsContainer . setCurrent ( self . __sectionEditor ) \n def __sectionEditorNameChanged ( self , sectionEditor , oldName , newName ) : \n self . __plugListing . setSelection ( newName ) \n def __repr__ ( self ) : \n return \"<STR_LIT>\" \n @ classmethod \n def __setColor ( cls , menu , node ) : \n color = Gaffer . Metadata . nodeValue ( node , \"<STR_LIT>\" ) or IECore . Color3f ( <NUM_LIT:1> ) \n dialogue = GafferUI . ColorChooserDialogue ( color = color , useDisplayTransform = False ) \n color = dialogue . waitForColor ( parentWindow = menu . ancestor ( GafferUI . Window ) ) \n if color is not None : \n with Gaffer . UndoContext ( node . ancestor ( Gaffer . ScriptNode ) ) : \n Gaffer . Metadata . registerNodeValue ( node , \"<STR_LIT>\" , color ) \n GafferUI . EditorWidget . registerType ( \"<STR_LIT>\" , UIEditor ) \n def __editPlugUI ( node , plug ) : \n editor = GafferUI . UIEditor . acquire ( node ) \n editor . setSelection ( plug ) \n editor . plugEditor ( ) . reveal ( ) \n def __plugPopupMenu ( menuDefinition , plugValueWidget ) : \n plug = plugValueWidget . getPlug ( ) \n node = plug . node ( ) \n if node is None : \n return \n if isinstance ( node , Gaffer . Box ) : \n if not plug . parent ( ) . isSame ( node ) : \n return \n else : \n if not plug . parent ( ) . isSame ( node [ \"<STR_LIT:user>\" ] ) : \n return \n menuDefinition . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n menuDefinition . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : IECore . curry ( __editPlugUI , node , plug ) , \"<STR_LIT>\" : not plugValueWidget . getReadOnly ( ) } ) \n __plugPopupMenuConnection = GafferUI . PlugValueWidget . popupMenuSignal ( ) . connect ( __plugPopupMenu ) \n class _Label ( GafferUI . Label ) : \n def __init__ ( self , * args , ** kw ) : \n GafferUI . Label . __init__ ( \n self , \n horizontalAlignment = GafferUI . Label . HorizontalAlignment . Right , \n * args , ** kw \n ) \n self . _qtWidget ( ) . setFixedWidth ( <NUM_LIT> ) \n class _Row ( GafferUI . ListContainer ) : \n def __init__ ( self , * args , ** kw ) : \n GafferUI . ListContainer . __init__ ( self , GafferUI . ListContainer . Orientation . Horizontal , spacing = <NUM_LIT:4> , * args , ** kw ) \n class _MetadataWidget ( GafferUI . Widget ) : \n def __init__ ( self , topLevelWidget , key , target = None , parenting = None ) : \n GafferUI . Widget . __init__ ( self , topLevelWidget , parenting = parenting ) \n self . __key = key \n self . __target = None \n self . setTarget ( target ) \n def setTarget ( self , target ) : \n assert ( isinstance ( target , ( Gaffer . Node , Gaffer . Plug , type ( None ) ) ) ) \n self . __target = target \n self . setEnabled ( self . __target is not None ) \n if isinstance ( self . __target , Gaffer . Node ) : \n self . __metadataChangedConnection = Gaffer . Metadata . nodeValueChangedSignal ( ) . connect ( \n Gaffer . WeakMethod ( self . __nodeMetadataChanged ) \n ) \n elif isinstance ( self . __target , Gaffer . Plug ) : \n self . __metadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( \n Gaffer . WeakMethod ( self . __plugMetadataChanged ) \n ) \n else : \n self . __metadataChangedConnection = None \n self . __update ( ) \n def getTarget ( self ) : \n return self . __target \n def setKey ( self , key ) : \n if key == self . __key : \n return \n self . __key = key \n self . __update ( ) \n def getKey ( self , key ) : \n return self . __key \n def _updateFromValue ( self , value ) : \n raise NotImplementedError \n def _updateFromWidget ( self , value ) : \n if self . __target is None : \n return \n with Gaffer . UndoContext ( self . __target . ancestor ( Gaffer . ScriptNode ) ) : \n _registerMetadata ( self . __target , self . __key , value ) \n def _deregisterValue ( self ) : \n if self . __target is None : \n return \n with Gaffer . UndoContext ( self . __target . ancestor ( Gaffer . ScriptNode ) ) : \n _deregisterMetadata ( self . __target , self . __key ) \n def __update ( self ) : \n if isinstance ( self . __target , Gaffer . Node ) : \n self . _updateFromValue ( Gaffer . Metadata . nodeValue ( self . __target , self . __key ) ) \n elif isinstance ( self . __target , Gaffer . Plug ) : \n self . _updateFromValue ( Gaffer . Metadata . plugValue ( self . __target , self . __key ) ) \n else : \n self . _updateFromValue ( None ) \n def __nodeMetadataChanged ( self , nodeTypeId , key , node ) : \n if self . __key != key : \n return \n if node is not None and not node . isSame ( self . __target ) : \n return \n if not self . __target . isInstanceOf ( nodeTypeId ) : \n return \n self . __update ( ) \n def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n if self . __key != key : \n return \n if plug is not None and not plug . isSame ( self . __target ) : \n return \n if not self . __target . node ( ) . isInstanceOf ( nodeTypeId ) : \n return \n if not Gaffer . match ( self . __target . relativeName ( self . __target . node ( ) ) , plugPath ) : \n return \n self . __update ( ) \n class _BoolMetadataWidget ( _MetadataWidget ) : \n def __init__ ( self , key , target = None , parenting = None ) : \n self . __boolWidget = GafferUI . BoolWidget ( ) \n _MetadataWidget . __init__ ( self , self . __boolWidget , key , target , parenting = parenting ) \n self . __stateChangedConnection = self . __boolWidget . stateChangedSignal ( ) . connect ( \n Gaffer . WeakMethod ( self . __stateChanged ) \n ) \n def _updateFromValue ( self , value ) : \n self . __boolWidget . setState ( value if value is not None else False ) \n def __stateChanged ( self , * unused ) : \n self . _updateFromWidget ( self . __boolWidget . getState ( ) ) \n class _StringMetadataWidget ( _MetadataWidget ) : \n def __init__ ( self , key , target = None , acceptEmptyString = True , parenting = None ) : \n self . __textWidget = GafferUI . TextWidget ( ) \n _MetadataWidget . __init__ ( self , self . __textWidget , key , target , parenting = None ) \n self . __acceptEmptyString = acceptEmptyString \n self . __editingFinishedConnection = self . __textWidget . editingFinishedSignal ( ) . connect ( \n Gaffer . WeakMethod ( self . __editingFinished ) \n ) \n def textWidget ( self ) : \n return self . __textWidget \n def _updateFromValue ( self , value ) : \n self . __textWidget . setText ( value if value is not None else \"<STR_LIT>\" ) \n def __editingFinished ( self , * unused ) : \n text = self . __textWidget . getText ( ) \n if text or self . __acceptEmptyString : \n self . _updateFromWidget ( text ) \n else : \n self . _deregisterValue ( ) \n class _MultiLineStringMetadataWidget ( _MetadataWidget ) : \n def __init__ ( self , key , target = None , parenting = None ) : \n self . __textWidget = GafferUI . MultiLineTextWidget ( ) \n _MetadataWidget . __init__ ( self , self . __textWidget , key , target , parenting = None ) \n self . __editingFinishedConnection = self . __textWidget . editingFinishedSignal ( ) . connect ( \n Gaffer . WeakMethod ( self . __editingFinished ) \n ) \n def textWidget ( self ) : \n return self . __textWidget \n def _updateFromValue ( self , value ) : \n self . __textWidget . setText ( value if value is not None else \"<STR_LIT>\" ) \n def __editingFinished ( self , * unused ) : \n self . _updateFromWidget ( self . __textWidget . getText ( ) ) \n class _ColorSwatchMetadataWidget ( _MetadataWidget ) : \n def __init__ ( self , key , target = None , parenting = None ) : \n self . __swatch = GafferUI . ColorSwatch ( useDisplayTransform = False ) \n _MetadataWidget . __init__ ( self , self . __swatch , key , target , parenting = parenting ) \n self . __swatch . _qtWidget ( ) . setFixedHeight ( <NUM_LIT> ) \n self . __swatch . _qtWidget ( ) . setMaximumWidth ( <NUM_LIT> ) \n self . __value = None \n self . __buttonReleaseConnection = self . __swatch . buttonReleaseSignal ( ) . connect ( Gaffer . WeakMethod ( self . __buttonRelease ) ) \n def _updateFromValue ( self , value ) : \n if value is not None : \n self . __swatch . setColor ( value ) \n else : \n self . __swatch . setColor ( IECore . Color4f ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> ) ) \n self . __value = value \n def __buttonRelease ( self , swatch , event ) : \n if event . button != event . Buttons . Left : \n return False \n color = self . __value if self . __value is not None else IECore . Color3f ( <NUM_LIT:1> ) \n dialogue = GafferUI . ColorChooserDialogue ( color = color , useDisplayTransform = False ) \n color = dialogue . waitForColor ( parentWindow = self . ancestor ( GafferUI . Window ) ) \n if color is not None : \n self . _updateFromWidget ( color ) \n class _MenuMetadataWidget ( _MetadataWidget ) : \n def __init__ ( self , key , labelsAndValues , target = None , parenting = None ) : \n self . __menuButton = GafferUI . MenuButton ( \n menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) \n ) \n self . __labelsAndValues = labelsAndValues \n self . __currentValue = None \n _MetadataWidget . __init__ ( self , self . __menuButton , key , target , parenting = parenting ) \n def _updateFromValue ( self , value ) : \n self . __currentValue = value \n buttonText = str ( value ) \n for label , value in self . __labelsAndValues : \n if value == self . __currentValue : \n buttonText = label \n break \n self . __menuButton . setText ( buttonText ) \n def __menuDefinition ( self ) : \n result = IECore . MenuDefinition ( ) \n for label , value in self . __labelsAndValues : \n result . append ( \n \"<STR_LIT:/>\" + label , \n { \n \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __setValue ) , value = value ) , \n \"<STR_LIT>\" : value == self . __currentValue \n } \n ) \n return result \n def __setValue ( self , unused , value ) : \n self . _updateFromWidget ( value ) \n class _LayoutItem ( object ) : \n def __init__ ( self ) : \n self . __parent = None \n self . __children = [ ] \n def parent ( self ) : \n if self . __parent is None : \n return None \n else : \n return self . __parent ( ) \n def child ( self , name ) : \n for c in self . __children : \n if c . name ( ) == name : \n return c \n return None \n def isAncestorOf ( self , item ) : \n while item is not None : \n parent = item . parent ( ) \n if parent is self : \n return True \n item = parent \n return False \n def append ( self , child ) : \n self . insert ( len ( self ) , child ) \n def insert ( self , index , child ) : \n assert ( child . parent ( ) is None ) \n self . __children . insert ( index , child ) \n child . __parent = weakref . ref ( self ) \n def remove ( self , child ) : \n assert ( child . parent ( ) is self ) \n self . __children . remove ( child ) \n child . __parent = None \n def index ( self , child ) : \n return self . __children . index ( child ) \n def name ( self ) : \n raise NotImplementedError \n def fullName ( self ) : \n result = \"<STR_LIT>\" \n item = self \n while item . parent ( ) is not None : \n if result : \n result = item . name ( ) + \"<STR_LIT:.>\" + result \n else : \n result = item . name ( ) \n item = item . parent ( ) \n return result \n def __len__ ( self ) : \n return len ( self . __children ) \n def __getitem__ ( self , index ) : \n return self . __children [ index ] \n class _SectionLayoutItem ( _LayoutItem ) : \n def __init__ ( self , sectionName ) : \n _LayoutItem . __init__ ( self ) \n self . __sectionName = sectionName \n def name ( self ) : \n return self . __sectionName \n class _PlugLayoutItem ( _LayoutItem ) : \n def __init__ ( self , plug ) : \n _LayoutItem . __init__ ( self ) \n self . plug = plug \n self . __name = plug . getName ( ) \n def name ( self ) : \n return self . __name \n class _PlugListing ( GafferUI . Widget ) : \n class __LayoutPath ( Gaffer . Path ) : \n def __init__ ( self , rootItem , path , root = \"<STR_LIT:/>\" , filter = None ) : \n Gaffer . Path . __init__ ( self , path , root , filter ) \n self . __rootItem = rootItem \n def rootItem ( self ) : \n return self . __rootItem \n def item ( self ) : \n result = self . __rootItem \n for name in self : \n result = result . child ( name ) \n if result is None : \n return None \n return result \n def copy ( self ) : \n return self . __class__ ( self . __rootItem , self [ : ] , self . root ( ) , self . getFilter ( ) ) \n def isLeaf ( self ) : \n return not isinstance ( self . item ( ) , _SectionLayoutItem ) \n def isValid ( self ) : \n return self . item ( ) is not None \n def _children ( self ) : \n item = self . item ( ) \n if item is None : \n return [ ] \n result = [ \n self . __class__ ( self . __rootItem , self [ : ] + [ c . name ( ) ] , self . root ( ) , self . getFilter ( ) ) \n for c in item \n ] \n if len ( result ) == <NUM_LIT:0> and isinstance ( item , _SectionLayoutItem ) : \n result . append ( self . __class__ ( self . __rootItem , self [ : ] + [ \"<STR_LIT:U+0020>\" ] , self . root ( ) , self . getFilter ( ) ) ) \n return result \n def __init__ ( self , parenting = None ) : \n column = GafferUI . ListContainer ( spacing = <NUM_LIT:4> ) \n GafferUI . Widget . __init__ ( self , column , parenting = parenting ) \n with column : \n self . __pathListing = GafferUI . PathListingWidget ( \n self . __LayoutPath ( _SectionLayoutItem ( \"<STR_LIT>\" ) , \"<STR_LIT:/>\" ) , \n columns = ( GafferUI . PathListingWidget . defaultNameColumn , ) , \n displayMode = GafferUI . PathListingWidget . DisplayMode . Tree , \n ) \n self . __pathListing . setDragPointer ( \"<STR_LIT>\" ) \n self . __pathListing . setSortable ( False ) \n self . __pathListing . setHeaderVisible ( False ) \n with GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = <NUM_LIT:4> ) : \n GafferUI . MenuButton ( \n image = \"<STR_LIT>\" , \n hasFrame = False , \n menu = GafferUI . Menu ( \n definition = Gaffer . WeakMethod ( self . __addMenuDefinition ) \n ) \n ) \n self . __deleteButton = GafferUI . Button ( image = \"<STR_LIT>\" , hasFrame = False ) \n self . __deleteButtonClickedConnection = self . __deleteButton . clickedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __deleteButtonClicked ) ) \n self . __parent = None \n self . __dragItem = None \n self . __selectionChangedSignal = Gaffer . Signal1 ( ) \n self . __dragEnterConnection = self . __pathListing . dragEnterSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnter ) ) \n self . __dragMoveConnection = self . __pathListing . dragMoveSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragMove ) ) \n self . __dragEndConnection = self . __pathListing . dragEndSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnd ) ) \n self . __selectionChangedConnection = self . __pathListing . selectionChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __selectionChanged ) ) \n self . __keyPressConnection = self . keyPressSignal ( ) . connect ( Gaffer . WeakMethod ( self . __keyPress ) ) \n self . __nodeMetadataChangedConnection = Gaffer . Metadata . nodeValueChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __nodeMetadataChanged ) ) \n self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __plugMetadataChanged ) ) \n def setPlugParent ( self , parent ) : \n assert ( isinstance ( parent , ( Gaffer . Plug , Gaffer . Node , types . NoneType ) ) ) \n self . __parent = parent \n self . __childAddedConnection = None \n self . __childRemovedConnection = None \n self . __childNameChangedConnections = { } \n if self . __parent is not None : \n self . __childAddedConnection = self . __parent . childAddedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __childAddedOrRemoved ) ) \n self . __childRemovedConnection = self . __parent . childRemovedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __childAddedOrRemoved ) ) \n for child in self . __parent . children ( ) : \n self . __updateChildNameChangedConnection ( child ) \n self . __updatePath ( ) \n def getPlugParent ( self ) : \n return self . __parent \n def setSelection ( self , selection ) : \n self . __updatePathLazily . flush ( self ) \n def findPlugPath ( path , plug ) : \n item = path . item ( ) \n if isinstance ( item , _PlugLayoutItem ) and item . plug . isSame ( plug ) : \n return path \n else : \n for child in path . children ( ) : \n r = findPlugPath ( child , plug ) \n if r is not None : \n return r \n return None \n if isinstance ( selection , Gaffer . Plug ) : \n path = findPlugPath ( self . __pathListing . getPath ( ) , selection ) \n if path is None : \n self . __pathListing . setSelectedPaths ( [ ] ) \n else : \n self . __pathListing . setSelectedPaths ( [ path ] ) \n elif isinstance ( selection , basestring ) : \n path = self . __pathListing . getPath ( ) . copy ( ) \n path [ : ] = selection . split ( \"<STR_LIT:.>\" ) \n self . __pathListing . setSelectedPaths ( [ path ] ) \n else : \n assert ( selection is None ) \n self . __pathListing . setSelectedPaths ( [ ] ) \n def getSelection ( self ) : \n item = self . __selectedItem ( ) \n if item is None : \n return None \n elif isinstance ( item , _PlugLayoutItem ) : \n return item . plug \n elif isinstance ( item , _SectionLayoutItem ) : \n return item . fullName ( ) \n else : \n return None \n def selectionChangedSignal ( self ) : \n return self . __selectionChangedSignal \n def __updatePath ( self ) : \n if self . __parent is None : \n self . __pathListing . setPath ( self . __LayoutPath ( _SectionLayoutItem ( \"<STR_LIT>\" ) , \"<STR_LIT:/>\" ) ) \n return \n def section ( rootLayoutItem , sectionPath ) : \n sectionItem = rootLayoutItem \n if sectionPath != \"<STR_LIT>\" : \n for sectionName in sectionPath . split ( \"<STR_LIT:.>\" ) : \n childSectionItem = sectionItem . child ( sectionName ) \n if childSectionItem is None : \n childSectionItem = _SectionLayoutItem ( sectionName ) \n sectionItem . append ( childSectionItem ) \n sectionItem = childSectionItem \n return sectionItem \n layout = _SectionLayoutItem ( \"<STR_LIT>\" ) \n for sectionPath in GafferUI . PlugLayout . layoutSections ( self . __parent ) : \n if sectionPath == \"<STR_LIT>\" and isinstance ( self . __parent , Gaffer . Node ) : \n continue \n sectionItem = section ( layout , sectionPath ) \n for plug in GafferUI . PlugLayout . layoutOrder ( self . __parent , section = sectionPath ) : \n sectionItem . append ( _PlugLayoutItem ( plug ) ) \n emptySections = _metadata ( self . getPlugParent ( ) , \"<STR_LIT>\" ) \n emptySectionIndices = _metadata ( self . getPlugParent ( ) , \"<STR_LIT>\" ) \n if emptySections and emptySectionIndices : \n for sectionPath , sectionIndex in zip ( emptySections , emptySectionIndices ) : \n parentPath , unused , sectionName = sectionPath . rpartition ( \"<STR_LIT:.>\" ) \n parentSection = section ( layout , parentPath ) \n if parentSection . child ( sectionName ) is None : \n parentSection . insert ( sectionIndex , _SectionLayoutItem ( sectionName ) ) \n if len ( layout ) == <NUM_LIT:0> and isinstance ( self . __parent , Gaffer . Node ) : \n layout . append ( _SectionLayoutItem ( \"<STR_LIT>\" ) ) \n expandedPaths = self . __pathListing . getExpandedPaths ( ) \n self . __pathListing . setPath ( self . __LayoutPath ( layout , \"<STR_LIT:/>\" ) ) \n self . __pathListing . setExpandedPaths ( expandedPaths ) \n @ GafferUI . LazyMethod ( ) \n def __updatePathLazily ( self ) : \n self . __updatePath ( ) \n def __updateMetadata ( self ) : \n emptySections = IECore . StringVectorData ( ) \n emptySectionIndices = IECore . IntVectorData ( ) \n def walk ( layoutItem , path = \"<STR_LIT>\" , index = <NUM_LIT:0> ) : \n for childItem in layoutItem : \n if isinstance ( childItem , _PlugLayoutItem ) : \n Gaffer . Metadata . registerPlugValue ( childItem . plug , \"<STR_LIT>\" , path ) \n Gaffer . Metadata . registerPlugValue ( childItem . plug , \"<STR_LIT>\" , index ) \n index += <NUM_LIT:1> \n elif isinstance ( childItem , _SectionLayoutItem ) : \n childPath = path + \"<STR_LIT:.>\" + childItem . name ( ) if path else childItem . name ( ) \n if len ( childItem ) : \n index = walk ( childItem , childPath , index ) \n else : \n emptySections . append ( childPath ) \n emptySectionIndices . append ( layoutItem . index ( childItem ) ) \n return index \n with Gaffer . BlockedConnection ( self . __plugMetadataChangedConnection ) : \n walk ( self . __pathListing . getPath ( ) . copy ( ) . setFromString ( \"<STR_LIT:/>\" ) . item ( ) ) \n _registerMetadata ( self . getPlugParent ( ) , \"<STR_LIT>\" , emptySections ) \n _registerMetadata ( self . getPlugParent ( ) , \"<STR_LIT>\" , emptySectionIndices ) \n def __childAddedOrRemoved ( self , parent , child ) : \n assert ( parent . isSame ( self . __parent ) ) \n self . __updateChildNameChangedConnection ( child ) \n self . __updatePathLazily ( ) \n def __childNameChanged ( self , child ) : \n selection = self . getSelection ( ) \n self . __updatePath ( ) \n if isinstance ( selection , Gaffer . Plug ) and child . isSame ( selection ) : \n self . setSelection ( selection ) \n def __updateChildNameChangedConnection ( self , child ) : \n if self . __parent . isSame ( child . parent ( ) ) : \n if child not in self . __childNameChangedConnections : \n self . __childNameChangedConnections [ child ] = child . nameChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __childNameChanged ) ) \n else : \n if child in self . __childNameChangedConnections : \n del self . __childNameChangedConnections [ child ] \n def __dragEnter ( self , listing , event ) : \n if event . sourceWidget is not self . __pathListing : \n return False \n if not isinstance ( event . data , IECore . StringVectorData ) : \n return False \n dragPath = self . __pathListing . getPath ( ) . copy ( ) . setFromString ( event . data [ <NUM_LIT:0> ] ) \n self . __dragItem = dragPath . item ( ) \n self . __pathListing . setPathExpanded ( dragPath , False ) \n return True \n def __dragMove ( self , listing , event ) : \n if self . __dragItem is None : \n return False \n targetPath = self . __pathListing . pathAt ( event . line . p0 ) \n if targetPath is not None : \n targetItem = targetPath . item ( ) \n if targetItem is not None : \n if isinstance ( targetItem , _SectionLayoutItem ) and self . __pathListing . getPathExpanded ( targetPath ) and targetItem . parent ( ) is self . __dragItem . parent ( ) : \n newParent = targetItem \n newIndex = <NUM_LIT:0> \n else : \n newParent = targetItem . parent ( ) \n newIndex = newParent . index ( targetItem ) \n else : \n newParent = targetPath . copy ( ) . truncateUntilValid ( ) . item ( ) \n newIndex = <NUM_LIT:0> \n else : \n newParent = self . __pathListing . getPath ( ) . rootItem ( ) \n newIndex = <NUM_LIT:0> if event . line . p0 . y < <NUM_LIT:1> else len ( newParent ) \n if newParent is self . __dragItem or self . __dragItem . isAncestorOf ( newParent ) : \n return True \n firstNonPlugIndex = next ( \n ( x [ <NUM_LIT:0> ] for x in enumerate ( newParent ) if not isinstance ( x [ <NUM_LIT:1> ] , _PlugLayoutItem ) ) , \n len ( newParent ) \n ) \n if self . __dragItem . parent ( ) is newParent and newParent . index ( self . __dragItem ) < firstNonPlugIndex : \n firstNonPlugIndex -= <NUM_LIT:1> \n if isinstance ( self . __dragItem , _PlugLayoutItem ) : \n if newIndex > firstNonPlugIndex : \n return True \n else : \n if newIndex < firstNonPlugIndex : \n newIndex = max ( newIndex , firstNonPlugIndex ) \n self . __dragItem . parent ( ) . remove ( self . __dragItem ) \n newParent . insert ( newIndex , self . __dragItem ) \n self . __pathListing . getPath ( ) . pathChangedSignal ( ) ( self . __pathListing . getPath ( ) ) \n selection = self . __pathListing . getPath ( ) . copy ( ) \n selection [ : ] = self . __dragItem . fullName ( ) . split ( \"<STR_LIT:.>\" ) \n self . __pathListing . setSelectedPaths ( [ selection ] , scrollToFirst = False , expandNonLeaf = False ) \n return True \n def __dragEnd ( self , listing , event ) : \n if self . __dragItem is None : \n return False \n with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n self . __updateMetadata ( ) \n self . __dragItem = None \n return True \n def __selectionChanged ( self , pathListing ) : \n self . __deleteButton . setEnabled ( bool ( pathListing . getSelectedPaths ( ) ) ) \n self . __selectionChangedSignal ( self ) \n def __deleteButtonClicked ( self , button ) : \n self . __deleteSelected ( ) \n def __nodeMetadataChanged ( self , nodeTypeId , key , node ) : \n if self . __parent is None : \n return \n if node is not None and not self . __parent . isSame ( node ) : \n return \n if not self . __parent . isInstanceOf ( nodeTypeId ) : \n return \n if key in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : \n self . __updatePathLazily ( ) \n def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n if self . __parent is None : \n return \n if plug is not None and not self . __parent . isSame ( plug ) and not self . __parent . isSame ( plug . parent ( ) ) : \n return \n node = self . __parent . node ( ) if isinstance ( self . __parent , Gaffer . Plug ) else self . __parent \n if not node . isInstanceOf ( nodeTypeId ) : \n return \n if key in ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) : \n self . __updatePathLazily ( ) \n def __keyPress ( self , widget , event ) : \n assert ( widget is self ) \n if event . key == \"<STR_LIT>\" or event . key == \"<STR_LIT>\" : \n self . __deleteSelected ( ) \n return True \n return False \n def __addMenuDefinition ( self ) : \n m = IECore . MenuDefinition ( ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer . BoolPlug ) } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer . FloatPlug ) } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer . IntPlug ) } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer . StringPlug ) } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer . V2iPlug ) } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer . V3iPlug ) } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer . V2fPlug ) } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer . V3fPlug ) } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer . Color3fPlug ) } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer . Color4fPlug ) } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : True } ) \n m . append ( \"<STR_LIT>\" , { \"<STR_LIT>\" : Gaffer . WeakMethod ( self . __addSection ) } ) \n return m \n def __addPlug ( self , plugType ) : \n plug = plugType ( flags = Gaffer . Plug . Flags . Default | Gaffer . Plug . Flags . Dynamic ) \n _registerMetadata ( plug , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n parentItem = self . __selectedItem ( ) \n if parentItem is not None : \n while not isinstance ( parentItem , _SectionLayoutItem ) : \n parentItem = parentItem . parent ( ) \n else : \n parentItem = self . __pathListing . getPath ( ) . rootItem ( ) \n parentItem = next ( \n ( c for c in parentItem if isinstance ( c , _SectionLayoutItem ) ) , \n parentItem \n ) \n _registerMetadata ( plug , \"<STR_LIT>\" , parentItem . fullName ( ) ) \n with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n self . getPlugParent ( ) . addChild ( plug ) \n self . __updatePathLazily . flush ( self ) \n self . setSelection ( plug ) \n def __addSection ( self ) : \n rootItem = self . __pathListing . getPath ( ) . rootItem ( ) \n existingSectionNames = set ( c . name ( ) for c in rootItem if isinstance ( c , _SectionLayoutItem ) ) \n name = \"<STR_LIT>\" \n index = <NUM_LIT:1> \n while name in existingSectionNames : \n name = \"<STR_LIT>\" % index \n index += <NUM_LIT:1> \n rootItem . append ( _SectionLayoutItem ( name ) ) \n self . __pathListing . getPath ( ) . pathChangedSignal ( ) ( self . __pathListing . getPath ( ) ) \n with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n self . __updateMetadata ( ) \n self . __pathListing . setSelectedPaths ( \n self . __pathListing . getPath ( ) . copy ( ) . setFromString ( \"<STR_LIT:/>\" + name ) \n ) \n def __selectedItem ( self ) : \n selectedPaths = self . __pathListing . getSelectedPaths ( ) \n if not len ( selectedPaths ) : \n return None \n assert ( len ( selectedPaths ) == <NUM_LIT:1> ) \n return selectedPaths [ <NUM_LIT:0> ] . item ( ) \n def __deleteSelected ( self ) : \n selectedItem = self . __selectedItem ( ) \n if selectedItem is None : \n return \n selectedItem . parent ( ) . remove ( selectedItem ) \n def deletePlugsWalk ( item ) : \n if isinstance ( item , _PlugLayoutItem ) : \n item . plug . parent ( ) . removeChild ( item . plug ) \n else : \n for childItem in item : \n deletePlugsWalk ( childItem ) \n with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n deletePlugsWalk ( selectedItem ) \n self . __updateMetadata ( ) \n class _PresetsEditor ( GafferUI . Widget ) : \n def __init__ ( self , parenting = None ) : \n row = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = <NUM_LIT:8> ) \n GafferUI . Widget . __init__ ( self , row , parenting = parenting ) \n with row : \n with GafferUI . ListContainer ( spacing = <NUM_LIT:4> ) : \n self . __pathListing = GafferUI . PathListingWidget ( \n Gaffer . DictPath ( collections . OrderedDict ( ) , \"<STR_LIT:/>\" ) , \n columns = ( GafferUI . PathListingWidget . defaultNameColumn , ) , \n ) \n self . __pathListing . setDragPointer ( \"<STR_LIT>\" ) \n self . __pathListing . setSortable ( False ) \n self . __pathListing . setHeaderVisible ( False ) \n self . __pathListing . _qtWidget ( ) . setFixedWidth ( <NUM_LIT:200> ) \n self . __pathListing . _qtWidget ( ) . setFixedHeight ( <NUM_LIT:200> ) \n self . __pathListingSelectionChangedConnection = self . __pathListing . selectionChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __selectionChanged ) ) \n self . __dragEnterConnection = self . __pathListing . dragEnterSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnter ) ) \n self . __dragMoveConnection = self . __pathListing . dragMoveSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragMove ) ) \n self . __dragEndConnection = self . __pathListing . dragEndSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnd ) ) \n with GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = <NUM_LIT:4> ) : \n self . __addButton = GafferUI . Button ( image = \"<STR_LIT>\" , hasFrame = False ) \n self . __addButtonClickedConnection = self . __addButton . clickedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __addButtonClicked ) ) \n self . __deleteButton = GafferUI . Button ( image = \"<STR_LIT>\" , hasFrame = False ) \n self . __deleteButtonClickedConnection = self . __deleteButton . clickedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __deleteButtonClicked ) ) \n with GafferUI . ListContainer ( spacing = <NUM_LIT:4> ) as self . __editingColumn : \n GafferUI . Label ( \"<STR_LIT:Name>\" ) \n self . __nameWidget = GafferUI . TextWidget ( ) \n self . __nameEditingFinishedConnection = self . __nameWidget . editingFinishedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __nameEditingFinished ) ) \n GafferUI . Spacer ( IECore . V2i ( <NUM_LIT:4> ) , maximumSize = IECore . V2i ( <NUM_LIT:4> ) ) \n GafferUI . Label ( \"<STR_LIT>\" ) \n self . __valueNode = Gaffer . Node ( \"<STR_LIT>\" ) \n self . __valuePlugSetConnection = self . __valueNode . plugSetSignal ( ) . connect ( Gaffer . WeakMethod ( self . __valuePlugSet ) ) \n def setPlug ( self , plug ) : \n self . __plug = plug \n self . __plugMetadataChangedConnection = None \n del self . __editingColumn [ <NUM_LIT:4> : ] \n plugValueWidget = None \n if self . __plug is not None : \n self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __plugMetadataChanged ) ) \n self . __valueNode [ \"<STR_LIT>\" ] = plug . createCounterpart ( \"<STR_LIT>\" , plug . Direction . In ) \n if hasattr ( self . __plug , \"<STR_LIT>\" ) : \n plugValueWidget = GafferUI . PlugValueWidget . create ( self . __valueNode [ \"<STR_LIT>\" ] , useTypeOnly = True ) \n self . __editingColumn . append ( plugValueWidget if plugValueWidget is not None else GafferUI . TextWidget ( ) ) \n self . __editingColumn . append ( GafferUI . Spacer ( IECore . V2i ( <NUM_LIT:0> ) , parenting = { \"<STR_LIT>\" : True } ) ) \n self . __updatePath ( ) \n self . __addButton . setEnabled ( hasattr ( self . __plug , \"<STR_LIT>\" ) ) \n def getPlug ( self ) : \n return self . __plug \n def __updatePath ( self ) : \n d = self . __pathListing . getPath ( ) . dict ( ) \n d . clear ( ) \n if self . __plug is not None : \n for name in _registeredMetadata ( self . __plug , instanceOnly = True , persistentOnly = True ) : \n if name . startswith ( \"<STR_LIT>\" ) : \n d [ name [ <NUM_LIT:7> : ] ] = _metadata ( self . __plug , name ) \n self . __pathListing . getPath ( ) . pathChangedSignal ( ) ( self . __pathListing . getPath ( ) ) \n def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n if plug is None or not plug . isSame ( self . __plug ) : \n return \n if key . startswith ( \"<STR_LIT>\" ) : \n self . __updatePath ( ) \n def __selectionChanged ( self , listing ) : \n selectedPaths = listing . getSelectedPaths ( ) \n self . __nameWidget . setText ( selectedPaths [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] if selectedPaths else \"<STR_LIT>\" ) \n if selectedPaths : \n with Gaffer . BlockedConnection ( self . __valuePlugSetConnection ) : \n self . __valueNode [ \"<STR_LIT>\" ] . setValue ( \n Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" + selectedPaths [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] ) \n ) \n self . __editingColumn . setEnabled ( bool ( selectedPaths ) ) \n self . __deleteButton . setEnabled ( bool ( selectedPaths ) ) \n def __dragEnter ( self , listing , event ) : \n if event . sourceWidget is not self . __pathListing : \n return False \n if not isinstance ( event . data , IECore . StringVectorData ) : \n return False \n return True \n def __dragMove ( self , listing , event ) : \n d = self . __pathListing . getPath ( ) . dict ( ) \n srcPath = self . __pathListing . getPath ( ) . copy ( ) . setFromString ( event . data [ <NUM_LIT:0> ] ) \n srcIndex = d . keys ( ) . index ( srcPath [ <NUM_LIT:0> ] ) \n targetPath = self . __pathListing . pathAt ( event . line . p0 ) \n if targetPath is not None : \n targetIndex = d . keys ( ) . index ( targetPath [ <NUM_LIT:0> ] ) \n else : \n targetIndex = <NUM_LIT:0> if event . line . p0 . y < <NUM_LIT:1> else len ( d ) \n if srcIndex == targetIndex : \n return True \n items = d . items ( ) \n item = items [ srcIndex ] \n del items [ srcIndex ] \n items . insert ( targetIndex , item ) \n d . clear ( ) \n d . update ( items ) \n self . __pathListing . getPath ( ) . pathChangedSignal ( ) ( self . __pathListing . getPath ( ) ) \n return True \n def __dragEnd ( self , listing , event ) : \n d = self . __pathListing . getPath ( ) . dict ( ) \n with Gaffer . BlockedConnection ( self . __plugMetadataChangedConnection ) : \n with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n for item in d . items ( ) : \n Gaffer . Metadata . deregisterPlugValue ( self . getPlug ( ) , \"<STR_LIT>\" + item [ <NUM_LIT:0> ] ) \n for item in d . items ( ) : \n Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , \"<STR_LIT>\" + item [ <NUM_LIT:0> ] , item [ <NUM_LIT:1> ] ) \n self . __updatePath ( ) \n return True \n def __addButtonClicked ( self , button ) : \n existingNames = [ p [ <NUM_LIT:0> ] for p in self . __pathListing . getPath ( ) . children ( ) ] \n name = \"<STR_LIT>\" \n index = <NUM_LIT:1> \n while name in existingNames : \n name = \"<STR_LIT>\" % index \n index += <NUM_LIT:1> \n with Gaffer . UndoContext ( self . __plug . ancestor ( Gaffer . ScriptNode ) ) : \n Gaffer . Metadata . registerPlugValue ( self . __plug , \"<STR_LIT>\" + name , self . __plug . getValue ( ) ) \n self . __pathListing . setSelectedPaths ( \n self . __pathListing . getPath ( ) . copy ( ) . setFromString ( \"<STR_LIT:/>\" + name ) \n ) \n self . __nameWidget . grabFocus ( ) \n self . __nameWidget . setSelection ( <NUM_LIT:0> , len ( name ) ) \n return True \n def __deleteButtonClicked ( self , button ) : \n paths = self . __pathListing . getPath ( ) . children ( ) \n selectedPreset = self . __pathListing . getSelectedPaths ( ) [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] \n selectedIndex = [ p [ <NUM_LIT:0> ] for p in paths ] . index ( selectedPreset ) \n with Gaffer . UndoContext ( self . __plug . ancestor ( Gaffer . ScriptNode ) ) : \n Gaffer . Metadata . deregisterPlugValue ( self . __plug , \"<STR_LIT>\" + selectedPreset ) \n del paths [ selectedIndex ] \n if len ( paths ) : \n self . __pathListing . setSelectedPaths ( [ paths [ min ( selectedIndex , len ( paths ) - <NUM_LIT:1> ) ] ] ) \n return True \n def __nameEditingFinished ( self , nameWidget ) : \n selectedPaths = self . __pathListing . getSelectedPaths ( ) \n if not len ( selectedPaths ) : \n return True \n oldName = selectedPaths [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] \n newName = nameWidget . getText ( ) \n items = self . __pathListing . getPath ( ) . dict ( ) . items ( ) \n with Gaffer . BlockedConnection ( self . __plugMetadataChangedConnection ) : \n with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n for item in items : \n Gaffer . Metadata . deregisterPlugValue ( self . getPlug ( ) , \"<STR_LIT>\" + item [ <NUM_LIT:0> ] ) \n for item in items : \n Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , \"<STR_LIT>\" + ( item [ <NUM_LIT:0> ] if item [ <NUM_LIT:0> ] != oldName else newName ) , item [ <NUM_LIT:1> ] ) \n self . __updatePath ( ) \n self . __pathListing . setSelectedPaths ( [ self . __pathListing . getPath ( ) . copy ( ) . setFromString ( \"<STR_LIT:/>\" + newName ) ] ) \n return True \n def __valuePlugSet ( self , plug ) : \n if not plug . isSame ( self . __valueNode [ \"<STR_LIT>\" ] ) : \n return \n selectedPaths = self . __pathListing . getSelectedPaths ( ) \n preset = selectedPaths [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] \n with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , \"<STR_LIT>\" + preset , plug . getValue ( ) ) \n class _PlugEditor ( GafferUI . Widget ) : \n def __init__ ( self , parenting = None ) : \n scrolledContainer = GafferUI . ScrolledContainer ( horizontalMode = GafferUI . ScrolledContainer . ScrollMode . Never , borderWidth = <NUM_LIT:8> ) \n GafferUI . Widget . __init__ ( self , scrolledContainer , parenting = parenting ) \n self . __metadataWidgets = { } \n scrolledContainer . setChild ( GafferUI . ListContainer ( spacing = <NUM_LIT:4> ) ) \n with scrolledContainer . getChild ( ) : \n with _Row ( ) : \n _Label ( \"<STR_LIT:Name>\" ) \n self . __nameWidget = GafferUI . NameWidget ( None ) \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" ) \n self . __metadataWidgets [ \"<STR_LIT:label>\" ] = _StringMetadataWidget ( key = \"<STR_LIT:label>\" , acceptEmptyString = False ) \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" , parenting = { \"<STR_LIT>\" : GafferUI . ListContainer . VerticalAlignment . Top } ) \n self . __metadataWidgets [ \"<STR_LIT:description>\" ] = _MultiLineStringMetadataWidget ( key = \"<STR_LIT:description>\" ) \n self . __metadataWidgets [ \"<STR_LIT:description>\" ] . textWidget ( ) . setFixedLineHeight ( <NUM_LIT:10> ) \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" ) \n self . __widgetMenu = GafferUI . MenuButton ( \n menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __widgetMenuDefinition ) ) \n ) \n with GafferUI . Collapsible ( \"<STR_LIT>\" , collapsed = True ) : \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" ) \n self . __presetsEditor = _PresetsEditor ( ) \n with GafferUI . Collapsible ( \"<STR_LIT>\" , collapsed = True ) : \n with GafferUI . ListContainer ( spacing = <NUM_LIT:4> ) : \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" ) \n self . __metadataWidgets [ \"<STR_LIT>\" ] = _BoolMetadataWidget ( key = \"<STR_LIT>\" ) \n for m in self . __metadataDefinitions : \n with _Row ( ) : \n _Label ( m . label ) \n self . __metadataWidgets [ m . key ] = m . metadataWidgetType ( key = m . key ) \n with GafferUI . Collapsible ( \"<STR_LIT>\" , collapsed = True ) : \n with GafferUI . ListContainer ( spacing = <NUM_LIT:4> ) as self . __nodeGraphSection : \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" ) \n self . __gadgetMenu = GafferUI . MenuButton ( \n menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __gadgetMenuDefinition ) ) \n ) \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" ) \n self . __metadataWidgets [ \"<STR_LIT>\" ] = _MenuMetadataWidget ( \n key = \"<STR_LIT>\" , \n labelsAndValues = [ \n ( \"<STR_LIT>\" , None ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT:left>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT:right>\" ) , \n ] \n ) \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" ) \n self . __metadataWidgets [ \"<STR_LIT>\" ] = _ColorSwatchMetadataWidget ( key = \"<STR_LIT>\" ) \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" ) \n self . __metadataWidgets [ \"<STR_LIT>\" ] = _ColorSwatchMetadataWidget ( key = \"<STR_LIT>\" ) \n GafferUI . Spacer ( IECore . V2i ( <NUM_LIT:0> ) , parenting = { \"<STR_LIT>\" : True } ) \n self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __plugMetadataChanged ) ) \n self . __plug = None \n def setPlug ( self , plug ) : \n self . __plug = plug \n self . __nameWidget . setGraphComponent ( self . __plug ) \n for widget in self . __metadataWidgets . values ( ) : \n widget . setTarget ( self . __plug ) \n self . __updateWidgetMenuText ( ) \n self . __updateWidgetSettings ( ) \n self . __updateGadgetMenuText ( ) \n self . __presetsEditor . setPlug ( plug ) \n self . __nodeGraphSection . setEnabled ( self . __plug is not None and self . __plug . parent ( ) . isSame ( self . __plug . node ( ) ) ) \n self . setEnabled ( self . __plug is not None ) \n def getPlug ( self ) : \n return self . __plug \n def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n if self . getPlug ( ) is None : \n return \n if plug is not None and not plug . isSame ( self . getPlug ( ) ) : \n return \n if not self . getPlug ( ) . node ( ) . isInstanceOf ( nodeTypeId ) : \n return \n if key == \"<STR_LIT>\" : \n self . __updateWidgetMenuText ( ) \n self . __updateWidgetSettings ( ) \n elif key == \"<STR_LIT>\" : \n self . __updateGadgetMenuText ( ) \n def __updateWidgetMenuText ( self ) : \n if self . getPlug ( ) is None : \n self . __widgetMenu . setText ( \"<STR_LIT>\" ) \n return \n metadata = Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) \n for w in self . __widgetDefinitions : \n if w . metadata == metadata : \n self . __widgetMenu . setText ( w . label ) \n return \n self . __widgetMenu . setText ( metadata ) \n def __updateWidgetSettings ( self ) : \n widgetType = None \n if self . getPlug ( ) is not None : \n widgetType = Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) \n for m in self . __metadataDefinitions : \n widget = self . __metadataWidgets [ m . key ] \n widget . parent ( ) . setEnabled ( m . plugValueWidgetType == widgetType ) \n self . __metadataWidgets [ \"<STR_LIT>\" ] . parent ( ) . setEnabled ( \n self . getPlug ( ) is not None and self . getPlug ( ) . direction ( ) == Gaffer . Plug . Direction . In \n ) \n def __widgetMenuDefinition ( self ) : \n result = IECore . MenuDefinition ( ) \n if self . getPlug ( ) is None : \n return result \n metadata = Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) \n for w in self . __widgetDefinitions : \n if not isinstance ( self . getPlug ( ) , w . plugType ) : \n continue \n result . append ( \n \"<STR_LIT:/>\" + w . label , \n { \n \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __registerOrDeregisterMetadata ) , key = \"<STR_LIT>\" , value = w . metadata ) , \n \"<STR_LIT>\" : metadata == w . metadata , \n } \n ) \n return result \n def __updateGadgetMenuText ( self ) : \n if self . getPlug ( ) is None : \n self . __gadgetMenu . setText ( \"<STR_LIT>\" ) \n return \n metadata = Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) \n metadata = None if metadata == \"<STR_LIT>\" else metadata \n for g in self . __gadgetDefinitions : \n if g . metadata == metadata : \n self . __gadgetMenu . setText ( g . label ) \n return \n self . __gadgetMenu . setText ( metadata ) \n def __gadgetMenuDefinition ( self ) : \n result = IECore . MenuDefinition ( ) \n if self . getPlug ( ) is None : \n return result \n metadata = Gaffer . Metadata . plugValue ( self . getPlug ( ) , \"<STR_LIT>\" ) \n for g in self . __gadgetDefinitions : \n if not isinstance ( self . getPlug ( ) , g . plugType ) : \n continue \n result . append ( \n \"<STR_LIT:/>\" + g . label , \n { \n \"<STR_LIT>\" : functools . partial ( Gaffer . WeakMethod ( self . __registerOrDeregisterMetadata ) , key = \"<STR_LIT>\" , value = g . metadata ) , \n \"<STR_LIT>\" : metadata == g . metadata , \n } \n ) \n return result \n def __registerOrDeregisterMetadata ( self , unused , key , value ) : \n with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n if value is not None : \n Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , key , value ) \n else : \n Gaffer . Metadata . deregisterPlugValue ( self . getPlug ( ) , key ) \n __WidgetDefinition = collections . namedtuple ( \"<STR_LIT>\" , ( \"<STR_LIT:label>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n __widgetDefinitions = ( \n __WidgetDefinition ( \"<STR_LIT>\" , Gaffer . Plug , None ) , \n __WidgetDefinition ( \"<STR_LIT>\" , Gaffer . IntPlug , \"<STR_LIT>\" ) , \n __WidgetDefinition ( \"<STR_LIT>\" , Gaffer . StringPlug , \"<STR_LIT>\" ) , \n __WidgetDefinition ( \"<STR_LIT>\" , Gaffer . StringPlug , \"<STR_LIT>\" ) , \n __WidgetDefinition ( \"<STR_LIT>\" , Gaffer . ValuePlug , \"<STR_LIT>\" ) , \n __WidgetDefinition ( \"<STR_LIT>\" , Gaffer . Plug , \"<STR_LIT>\" ) , \n __WidgetDefinition ( \"<STR_LIT:None>\" , Gaffer . Plug , \"<STR_LIT>\" ) , \n ) \n __MetadataDefinition = collections . namedtuple ( \"<STR_LIT>\" , ( \"<STR_LIT:key>\" , \"<STR_LIT:label>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n __metadataDefinitions = ( \n __MetadataDefinition ( \"<STR_LIT>\" , \"<STR_LIT>\" , _StringMetadataWidget , \"<STR_LIT>\" ) , \n __MetadataDefinition ( \"<STR_LIT>\" , \"<STR_LIT>\" , _StringMetadataWidget , \"<STR_LIT>\" ) , \n __MetadataDefinition ( \"<STR_LIT>\" , \"<STR_LIT>\" , _BoolMetadataWidget , \"<STR_LIT>\" ) , \n __MetadataDefinition ( \"<STR_LIT>\" , \"<STR_LIT>\" , _BoolMetadataWidget , \"<STR_LIT>\" ) , \n __MetadataDefinition ( \"<STR_LIT>\" , \"<STR_LIT>\" , _BoolMetadataWidget , \"<STR_LIT>\" ) , \n __MetadataDefinition ( \"<STR_LIT>\" , \"<STR_LIT>\" , _BoolMetadataWidget , \"<STR_LIT>\" ) , \n ) \n __GadgetDefinition = collections . namedtuple ( \"<STR_LIT>\" , ( \"<STR_LIT:label>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n __gadgetDefinitions = ( \n __GadgetDefinition ( \"<STR_LIT>\" , Gaffer . Plug , None ) , \n __GadgetDefinition ( \"<STR_LIT>\" , Gaffer . ArrayPlug , \"<STR_LIT>\" ) , \n __GadgetDefinition ( \"<STR_LIT:None>\" , Gaffer . Plug , \"<STR_LIT>\" ) , \n ) \n class _SectionEditor ( GafferUI . Widget ) : \n def __init__ ( self , parenting = None ) : \n column = GafferUI . ListContainer ( spacing = <NUM_LIT:4> , borderWidth = <NUM_LIT:8> ) \n GafferUI . Widget . __init__ ( self , column , parenting = parenting ) \n with column : \n with _Row ( ) : \n _Label ( \"<STR_LIT:Name>\" ) \n self . __nameWidget = GafferUI . TextWidget ( ) \n self . __nameWidgetEditingFinishedConnection = self . __nameWidget . editingFinishedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __nameWidgetEditingFinished ) ) \n with _Row ( ) : \n _Label ( \"<STR_LIT>\" , parenting = { \"<STR_LIT>\" : GafferUI . ListContainer . VerticalAlignment . Top } ) \n self . __summaryMetadataWidget = _MultiLineStringMetadataWidget ( key = \"<STR_LIT>\" ) \n self . __section = \"<STR_LIT>\" \n self . __plugParent = None \n self . __nameChangedSignal = Gaffer . Signal3 ( ) \n def setPlugParent ( self , plugParent ) : \n self . __plugParent = plugParent \n self . __summaryMetadataWidget . setTarget ( self . __plugParent ) \n def getPlugParent ( self ) : \n return self . __plugParent \n def setSection ( self , section ) : \n assert ( isinstance ( section , basestring ) ) \n self . __section = section \n self . __nameWidget . setText ( section . rpartition ( \"<STR_LIT:.>\" ) [ - <NUM_LIT:1> ] ) \n self . __summaryMetadataWidget . setKey ( \"<STR_LIT>\" + self . __section + \"<STR_LIT>\" ) \n def getSection ( self ) : \n return self . __section \n def nameChangedSignal ( self ) : \n return self . __nameChangedSignal \n def __nameWidgetEditingFinished ( self , nameWidget ) : \n if nameWidget . getText ( ) == \"<STR_LIT>\" : \n self . setSection ( self . __section ) \n return \n oldSectionPath = self . __section . split ( \"<STR_LIT:.>\" ) \n newSectionPath = oldSectionPath [ : ] \n newSectionPath [ - <NUM_LIT:1> ] = nameWidget . getText ( ) . replace ( \"<STR_LIT:.>\" , \"<STR_LIT>\" ) \n if oldSectionPath == newSectionPath : \n return \n def newSection ( oldSection ) : \n s = oldSection . split ( \"<STR_LIT:.>\" ) \n if s [ : len ( oldSectionPath ) ] == oldSectionPath : \n s [ : len ( oldSectionPath ) ] = newSectionPath \n return \"<STR_LIT:.>\" . join ( s ) \n else : \n return oldSection \n with Gaffer . UndoContext ( self . __plugParent . ancestor ( Gaffer . ScriptNode ) ) : \n for plug in self . __plugParent . children ( Gaffer . Plug ) : \n s = _metadata ( plug , \"<STR_LIT>\" ) \n if s is not None : \n _registerMetadata ( plug , \"<STR_LIT>\" , newSection ( s ) ) \n emptySections = _metadata ( self . getPlugParent ( ) , \"<STR_LIT>\" ) \n if emptySections : \n for i in range ( <NUM_LIT:0> , len ( emptySections ) ) : \n emptySections [ i ] = newSection ( emptySections [ i ] ) \n _registerMetadata ( self . getPlugParent ( ) , \"<STR_LIT>\" , emptySections ) \n for name in _registeredMetadata ( self . getPlugParent ( ) , instanceOnly = True , persistentOnly = True ) : \n m = re . match ( \"<STR_LIT>\" , name ) \n if m : \n if newSection ( m . group ( <NUM_LIT:2> ) ) != m . group ( <NUM_LIT:2> ) : \n _registerMetadata ( \n self . getPlugParent ( ) , \n m . group ( <NUM_LIT:1> ) + newSection ( m . group ( <NUM_LIT:2> ) ) + m . group ( <NUM_LIT:3> ) , \n _metadata ( self . getPlugParent ( ) , name ) \n ) \n _deregisterMetadata ( self . getPlugParent ( ) , name ) \n self . setSection ( \"<STR_LIT:.>\" . join ( newSectionPath ) ) \n self . nameChangedSignal ( ) ( self , \"<STR_LIT:.>\" . join ( oldSectionPath ) , \"<STR_LIT:.>\" . join ( newSectionPath ) ) \n def _registerMetadata ( target , name , value ) : \n if isinstance ( target , Gaffer . Node ) : \n Gaffer . Metadata . registerNodeValue ( target , name , value ) \n else : \n Gaffer . Metadata . registerPlugValue ( target , name , value ) \n def _registeredMetadata ( target , inherit = True , instanceOnly = False , persistentOnly = False ) : \n if isinstance ( target , Gaffer . Node ) : \n return Gaffer . Metadata . registeredNodeValues ( target , inherit , instanceOnly , persistentOnly ) \n else : \n return Gaffer . Metadata . registeredPlugValues ( target , inherit , instanceOnly , persistentOnly ) \n def _metadata ( target , name ) : \n if isinstance ( target , Gaffer . Node ) : \n return Gaffer . Metadata . nodeValue ( target , name ) \n else : \n return Gaffer . Metadata . plugValue ( target , name ) \n def _deregisterMetadata ( target , name ) : \n if isinstance ( target , Gaffer . Node ) : \n return Gaffer . Metadata . deregisterNodeValue ( target , name ) \n else : \n return Gaffer . Metadata . deregisterPlugValue ( target , <mask0> ) \n", "gt": "name"}
{"input": "\n import unittest \n import GafferTest \n import GafferUI \n class NumericSliderTest ( unittest . TestCase ) : \n def testConstruction ( self ) : \n s = GafferUI . NumericSlider ( value = <NUM_LIT:0> , min = <NUM_LIT:0> , max = <NUM_LIT:1> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:0> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:0> ) \n self . assertEqual ( s . getRange ( ) , ( <NUM_LIT:0> , <NUM_LIT:1> , <NUM_LIT:0> , <NUM_LIT:1> ) ) \n def testSetValue ( self ) : \n s = GafferUI . NumericSlider ( value = <NUM_LIT:0> , min = <NUM_LIT:0> , max = <NUM_LIT:2> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:0> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:0> ) \n s . setValue ( <NUM_LIT:0.5> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:0.5> ) \n def testSetRange ( self ) : \n s = GafferUI . NumericSlider ( value = <NUM_LIT:1> , min = <NUM_LIT:0> , max = <NUM_LIT:2> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:0.5> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:1> ) \n s . setRange ( <NUM_LIT:0> , <NUM_LIT:1> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:1> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:1> ) \n def testSetZeroRange ( self ) : \n s = GafferUI . NumericSlider ( value = <NUM_LIT:1> , min = <NUM_LIT:1> , max = <NUM_LIT:2> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:0> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:1> ) \n s . setRange ( <NUM_LIT:1> , <NUM_LIT:1> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:1> ) \n def testSetPosition ( self ) : \n s = GafferUI . NumericSlider ( value = <NUM_LIT:0> , min = <NUM_LIT:0> , max = <NUM_LIT:2> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:0> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:0> ) \n s . setPosition ( <NUM_LIT:0.5> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:0.5> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:1> ) \n def testValuesOutsideRangeAreClamped ( self ) : \n s = GafferUI . NumericSlider ( value = <NUM_LIT:0.1> , min = <NUM_LIT:0> , max = <NUM_LIT:2> ) \n cs = GafferTest . CapturingSlot ( s . valueChangedSignal ( ) , s . positionChangedSignal ( ) ) \n s . setValue ( <NUM_LIT:3> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:2> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:2> ) \n s . setValue ( <NUM_LIT:3> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:2> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:2> ) \n def testPositionsOutsideRangeAreClamped ( self ) : \n s = GafferUI . NumericSlider ( value = <NUM_LIT:0.1> , min = <NUM_LIT:0> , max = <NUM_LIT:2> ) \n cs = GafferTest . CapturingSlot ( s . valueChangedSignal ( ) , s . positionChangedSignal ( ) ) \n s . setPosition ( <NUM_LIT:2> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:2> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:2> ) \n s . setPosition ( <NUM_LIT:2> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:2> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:2> ) \n def testHardRange ( self ) : \n s = GafferUI . NumericSlider ( value = <NUM_LIT:0.1> , min = <NUM_LIT:0> , max = <NUM_LIT:2> , hardMin = - <NUM_LIT:1> , hardMax = <NUM_LIT:3> ) \n self . assertEqual ( s . getRange ( ) , ( <NUM_LIT:0> , <NUM_LIT:2> , - <NUM_LIT:1> , <NUM_LIT:3> ) ) \n cs = GafferTest . CapturingSlot ( s . valueChangedSignal ( ) , s . positionChangedSignal ( ) ) \n s . setValue ( <NUM_LIT:3> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:3> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT> ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:2> ) \n s . setValue ( <NUM_LIT> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:3> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT> ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:2> ) \n s . setValue ( - <NUM_LIT:1> ) \n self . assertEqual ( s . getValue ( ) , - <NUM_LIT:1> ) \n self . assertEqual ( s . getPosition ( ) , - <NUM_LIT:0.5> ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:4> ) \n s . setValue ( - <NUM_LIT:2> ) \n self . assertEqual ( s . getValue ( ) , - <NUM_LIT:1> ) \n self . assertEqual ( s . getPosition ( ) , - <NUM_LIT:0.5> ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:4> ) \n def testSetRangeClampsValue ( self ) : \n s = GafferUI . NumericSlider ( value = <NUM_LIT:0.5> , min = <NUM_LIT:0> , max = <NUM_LIT:2> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:0.5> ) \n s . setRange ( <NUM_LIT:1> , <NUM_LIT:2> ) \n self . assertEqual ( s . getPosition ( ) , <NUM_LIT:0> ) \n self . assertEqual ( s . getValue ( ) , <NUM_LIT:1> ) \n def testMultipleValues ( self ) : \n self . assertRaises ( Exception , GafferUI . NumericSlider , value = <NUM_LIT:0> , values = [ <NUM_LIT:1> , <NUM_LIT:2> ] ) \n s = GafferUI . NumericSlider ( values = [ <NUM_LIT:1> , <NUM_LIT> ] , min = <NUM_LIT:0> , max = <NUM_LIT:2> ) \n self . assertEqual ( s . getValues ( ) , [ <NUM_LIT:1> , <NUM_LIT> ] ) \n self . assertEqual ( s . getPositions ( ) , [ <NUM_LIT:0.5> , <NUM_LIT> ] ) \n self . assertRaises ( ValueError , s . getValue ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import unittest \n import weakref \n import sys \n import IECore \n import Gaffer \n import GafferTest \n import GafferUI \n import GafferUITest \n QtCore = GafferUI . _qtImport ( \"<STR_LIT>\" ) \n QtGui = GafferUI . _qtImport ( \"<STR_LIT>\" ) \n class TestWidget ( GafferUI . Widget ) : \n def __init__ ( self , ** kw ) : \n GafferUI . Widget . __init__ ( self , QtGui . QLabel ( \"<STR_LIT:hello>\" ) , ** kw ) \n class TestWidget2 ( GafferUI . Widget ) : \n def __init__ ( self ) : \n self . topLevelGafferWidget = TestWidget ( ) \n GafferUI . Widget . __init__ ( self , self . topLevelGafferWidget ) \n class WidgetTest ( GafferUITest . TestCase ) : \n def testOwner ( self ) : \n w = TestWidget ( ) \n self . assert_ ( GafferUI . Widget . _owner ( w . _qtWidget ( ) ) is w ) \n def testParent ( self ) : \n w = TestWidget ( ) \n self . assert_ ( w . parent ( ) is None ) \n def testCanDie ( self ) : \n w = TestWidget ( ) \n wr1 = weakref . ref ( w ) \n wr2 = weakref . ref ( w . _qtWidget ( ) ) \n del w \n self . assert_ ( wr1 ( ) is None ) \n self . assert_ ( wr2 ( ) is None ) \n def testAncestor ( self ) : \n w = GafferUI . Window ( \"<STR_LIT:test>\" ) \n l = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Vertical ) \n p = GafferUI . SplitContainer ( ) \n l . append ( p ) \n w . setChild ( l ) \n self . assert_ ( p . ancestor ( GafferUI . ListContainer ) is l ) \n self . assert_ ( p . ancestor ( GafferUI . Window ) is w ) \n self . assert_ ( p . ancestor ( GafferUI . Menu ) is None ) \n def testIsAncestorOf ( self ) : \n with GafferUI . Window ( \"<STR_LIT:test>\" ) as w : \n with GafferUI . SplitContainer ( ) as p : \n with GafferUI . ListContainer ( ) as l1 : \n b1 = GafferUI . Button ( ) \n with GafferUI . ListContainer ( ) as l2 : \n b2 = GafferUI . Button ( ) \n self . assertTrue ( l2 . isAncestorOf ( b2 ) ) \n self . assertFalse ( l1 . isAncestorOf ( b2 ) ) \n self . assertTrue ( p . isAncestorOf ( b2 ) ) \n self . assertTrue ( w . isAncestorOf ( b2 ) ) \n self . assertFalse ( b2 . isAncestorOf ( b1 ) ) \n self . assertFalse ( b2 . isAncestorOf ( l1 ) ) \n self . assertFalse ( b2 . isAncestorOf ( l2 ) ) \n self . assertFalse ( b2 . isAncestorOf ( p ) ) \n self . assertFalse ( b2 . isAncestorOf ( w ) ) \n self . assertTrue ( l1 . isAncestorOf ( b1 ) ) \n self . assertFalse ( l2 . isAncestorOf ( b1 ) ) \n self . assertTrue ( p . isAncestorOf ( b1 ) ) \n self . assertTrue ( w . isAncestorOf ( b1 ) ) \n def testGafferWidgetAsTopLevel ( self ) : \n w = TestWidget2 ( ) \n self . assert_ ( GafferUI . Widget . _owner ( w . _qtWidget ( ) ) is w ) \n self . assert_ ( w . topLevelGafferWidget . parent ( ) is w ) \n self . assert_ ( GafferUI . Widget . _owner ( w . topLevelGafferWidget . _qtWidget ( ) ) is not w ) \n def testToolTip ( self ) : \n w = TestWidget ( ) \n self . assertEqual ( w . getToolTip ( ) , \"<STR_LIT>\" ) \n w = TestWidget ( toolTip = \"<STR_LIT>\" ) \n self . assertEqual ( w . getToolTip ( ) , \"<STR_LIT>\" ) \n w . setToolTip ( \"<STR_LIT:a>\" ) \n self . assertEqual ( w . getToolTip ( ) , \"<STR_LIT:a>\" ) \n def testEnabledState ( self ) : \n w = TestWidget ( ) \n self . assertEqual ( w . getEnabled ( ) , True ) \n self . assertEqual ( w . enabled ( ) , True ) \n w . setEnabled ( False ) \n self . assertEqual ( w . getEnabled ( ) , False ) \n self . assertEqual ( w . enabled ( ) , False ) \n w . setEnabled ( True ) \n self . assertEqual ( w . getEnabled ( ) , True ) \n self . assertEqual ( w . enabled ( ) , True ) \n def testDisabledWidgetsDontGetSignals ( self ) : \n w = TestWidget ( ) \n def f ( w , event ) : \n WidgetTest . signalsEmitted += <NUM_LIT:1> \n c = w . buttonPressSignal ( ) . connect ( f ) \n WidgetTest . signalsEmitted = <NUM_LIT:0> \n event = QtGui . QMouseEvent ( QtCore . QEvent . MouseButtonPress , QtCore . QPoint ( <NUM_LIT:0> , <NUM_LIT:0> ) , QtCore . Qt . LeftButton , QtCore . Qt . LeftButton , QtCore . Qt . NoModifier ) \n QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n self . assertEqual ( WidgetTest . signalsEmitted , <NUM_LIT:1> ) \n w . setEnabled ( False ) \n QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n self . assertEqual ( WidgetTest . signalsEmitted , <NUM_LIT:1> ) \n w . setEnabled ( True ) \n QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n self . assertEqual ( WidgetTest . signalsEmitted , <NUM_LIT:2> ) \n def testCanDieAfterUsingSignals ( self ) : \n w = TestWidget ( ) \n wr1 = weakref . ref ( w ) \n wr2 = weakref . ref ( w . _qtWidget ( ) ) \n w . buttonPressSignal ( ) \n w . buttonReleaseSignal ( ) \n w . mouseMoveSignal ( ) \n w . wheelSignal ( ) \n del w \n self . assert_ ( wr1 ( ) is None ) \n self . assert_ ( wr2 ( ) is None ) \n def testVisibility ( self ) : \n with GafferUI . Window ( ) as w : \n with GafferUI . ListContainer ( ) as l : \n t = TestWidget ( ) \n self . assertEqual ( w . getVisible ( ) , False ) \n self . assertEqual ( l . getVisible ( ) , True ) \n self . assertEqual ( t . getVisible ( ) , True ) \n self . assertEqual ( w . visible ( ) , False ) \n self . assertEqual ( l . visible ( ) , False ) \n self . assertEqual ( t . visible ( ) , False ) \n w . setVisible ( True ) \n self . assertEqual ( w . getVisible ( ) , True ) \n self . assertEqual ( l . getVisible ( ) , True ) \n self . assertEqual ( t . getVisible ( ) , True ) \n self . assertEqual ( w . visible ( ) , True ) \n self . assertEqual ( l . visible ( ) , True ) \n self . assertEqual ( t . visible ( ) , True ) \n w . setVisible ( False ) \n self . assertEqual ( w . getVisible ( ) , False ) \n self . assertEqual ( l . getVisible ( ) , True ) \n self . assertEqual ( t . getVisible ( ) , True ) \n self . assertEqual ( w . visible ( ) , False ) \n self . assertEqual ( l . visible ( ) , False ) \n self . assertEqual ( t . visible ( ) , False ) \n self . assertEqual ( t . visible ( relativeTo = l ) , True ) \n self . assertEqual ( t . visible ( relativeTo = w ) , True ) \n w . setVisible ( True ) \n t . setVisible ( False ) \n self . assertEqual ( t . getVisible ( ) , False ) \n self . assertEqual ( t . visible ( ) , False ) \n self . assertEqual ( t . visible ( relativeTo = l ) , False ) \n def testGetVisibleForNewWidgets ( self ) : \n w = TestWidget ( ) \n self . assertEqual ( w . getVisible ( ) , True ) \n def testVisibilityOfParentlessWidgets ( self ) : \n w = GafferUI . Window ( ) \n t = TestWidget ( ) \n self . assertEqual ( w . getVisible ( ) , False ) \n self . assertEqual ( w . visible ( ) , False ) \n self . assertEqual ( t . getVisible ( ) , True ) \n self . assertEqual ( t . visible ( ) , False ) \n w . setVisible ( True ) \n self . assertEqual ( w . getVisible ( ) , True ) \n self . assertEqual ( w . visible ( ) , True ) \n w . setChild ( t ) \n self . assertEqual ( t . getVisible ( ) , True ) \n self . assertEqual ( t . visible ( ) , True ) \n w . removeChild ( t ) \n self . assertEqual ( t . parent ( ) , None ) \n self . assertEqual ( t . getVisible ( ) , True ) \n self . assertEqual ( t . visible ( ) , False ) \n def testVisibilityWhenTransferringWidgets ( self ) : \n w1 = GafferUI . Window ( ) \n w1 . setVisible ( True ) \n w2 = GafferUI . Window ( ) \n w2 . setVisible ( True ) \n v = TestWidget ( ) \n self . assertEqual ( v . getVisible ( ) , True ) \n self . assertEqual ( v . visible ( ) , False ) \n h = TestWidget ( ) \n self . assertEqual ( h . getVisible ( ) , True ) \n h . setVisible ( False ) \n self . assertEqual ( h . getVisible ( ) , False ) \n self . assertEqual ( h . visible ( ) , False ) \n w1 . setChild ( v ) \n self . assertEqual ( v . getVisible ( ) , True ) \n self . assertEqual ( v . visible ( ) , True ) \n self . assertEqual ( h . getVisible ( ) , False ) \n self . assertEqual ( h . visible ( ) , False ) \n w2 . setChild ( v ) \n self . assertEqual ( v . getVisible ( ) , True ) \n self . assertEqual ( v . visible ( ) , True ) \n self . assertEqual ( h . getVisible ( ) , False ) \n self . assertEqual ( h . visible ( ) , False ) \n w1 . setChild ( h ) \n self . assertEqual ( v . getVisible ( ) , True ) \n self . assertEqual ( v . visible ( ) , True ) \n self . assertEqual ( h . getVisible ( ) , False ) \n self . assertEqual ( h . visible ( ) , False ) \n w2 . setChild ( h ) \n self . assertEqual ( v . getVisible ( ) , True ) \n self . assertEqual ( v . visible ( ) , False ) \n self . assertEqual ( h . getVisible ( ) , False ) \n self . assertEqual ( h . visible ( ) , False ) \n def testSignals ( self ) : \n w = TestWidget ( ) \n for s in [ \n ( \"<STR_LIT>\" , GafferUI . WidgetEventSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetEventSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetEventSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetEventSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetEventSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetEventSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetEventSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetSignal ) , \n ( \"<STR_LIT>\" , GafferUI . WidgetSignal ) , \n ] : \n self . failUnless ( isinstance ( getattr ( w , s [ <NUM_LIT:0> ] ) ( ) , s [ <NUM_LIT:1> ] ) ) \n self . failUnless ( getattr ( w , s [ <NUM_LIT:0> ] ) ( ) is getattr ( w , s [ <NUM_LIT:0> ] ) ( ) ) \n def testBound ( self ) : \n w = GafferUI . Window ( borderWidth = <NUM_LIT:8> ) \n b = GafferUI . Button ( ) \n w . setChild ( b ) \n w . setVisible ( True ) \n w . setPosition ( IECore . V2i ( <NUM_LIT:100> ) ) \n self . waitForIdle ( <NUM_LIT:1000> ) \n wb = w . bound ( ) \n bb = b . bound ( ) \n bbw = b . bound ( relativeTo = w ) \n self . failUnless ( isinstance ( wb , IECore . Box2i ) ) \n self . failUnless ( isinstance ( bb , IECore . Box2i ) ) \n self . failUnless ( isinstance ( bbw , IECore . Box2i ) ) \n self . assertEqual ( bb . size ( ) , bbw . size ( ) ) \n self . assertEqual ( bbw . min , bb . min - wb . min ) \n self . assertEqual ( b . size ( ) , bb . size ( ) ) \n def testParentChangedSignal ( self ) : \n w = TestWidget ( ) \n window = GafferUI . Window ( ) \n cs = GafferTest . CapturingSlot ( w . parentChangedSignal ( ) ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:0> ) \n window . setChild ( w ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:1> ) \n self . assertEqual ( cs [ <NUM_LIT:0> ] , ( w , ) ) \n window . setChild ( None ) \n self . assertEqual ( len ( cs ) , <NUM_LIT:2> ) \n self . assertEqual ( cs [ <NUM_LIT:1> ] , ( w , ) ) \n def testHighlighting ( self ) : \n w = TestWidget ( ) \n self . assertEqual ( w . getHighlighted ( ) , False ) \n w . setHighlighted ( True ) \n self . assertEqual ( w . getHighlighted ( ) , True ) \n w . setHighlighted ( False ) \n self . assertEqual ( w . getHighlighted ( ) , False ) \n def testWidgetAt ( self ) : \n with GafferUI . Window ( ) as w1 : \n t1 = GafferUI . TextWidget ( \"<STR_LIT:hello>\" ) \n with GafferUI . Window ( ) as w2 : \n t2 = GafferUI . TextWidget ( \"<STR_LIT:hello>\" ) \n w1 . setVisible ( True ) \n w2 . setVisible ( True ) \n w1 . setPosition ( IECore . V2i ( <NUM_LIT:100> ) ) \n w2 . setPosition ( IECore . V2i ( <NUM_LIT> ) ) \n self . waitForIdle ( <NUM_LIT:1000> ) \n self . assertTrue ( GafferUI . Widget . widgetAt ( w1 . bound ( ) . center ( ) ) is t1 ) \n self . assertTrue ( GafferUI . Widget . widgetAt ( w2 . bound ( ) . center ( ) ) is t2 ) \n self . assertTrue ( GafferUI . Widget . widgetAt ( w1 . bound ( ) . center ( ) , widgetType = GafferUI . Window ) is w1 ) \n self . assertTrue ( GafferUI . Widget . widgetAt ( w2 . bound ( ) . center ( ) , widgetType = GafferUI . Window ) is w2 ) \n def testMousePosition ( self ) : \n w = GafferUI . Window ( borderWidth = <NUM_LIT:8> ) \n b = GafferUI . Button ( ) \n w . setChild ( b ) \n w . setVisible ( True ) \n w . setPosition ( IECore . V2i ( <NUM_LIT:100> ) ) \n self . waitForIdle ( <NUM_LIT:1000> ) \n mouseGlobal = GafferUI . Widget . mousePosition ( ) \n mouseLocal = GafferUI . Widget . mousePosition ( relativeTo = b ) \n self . assertEqual ( mouseGlobal , mouseLocal + b . bound ( ) . min ) \n def testAddressAndObject ( self ) : \n button = GafferUI . Button ( ) \n address = GafferUI . _qtAddress ( button . _qtWidget ( ) ) \n self . assertTrue ( isinstance ( address , int ) ) \n widget = GafferUI . _qtObject ( address , QtGui . QPushButton ) \n self . assertTrue ( isinstance ( widget , QtGui . QPushButton ) ) \n def testSetVisibleWithNonBool ( self ) : \n w = TestWidget ( ) \n self . assertTrue ( w . getVisible ( ) is True ) \n w . setVisible ( <NUM_LIT:0> ) \n self . assertTrue ( w . getVisible ( ) is False ) \n w . setVisible ( <NUM_LIT:1> ) \n self . assertTrue ( w . getVisible ( ) is True ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import GafferUI \n import GafferSceneUI \n def __toolMenu ( nodeEditor , node , menuDefinition ) : \n GafferUI . UIEditor . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n GafferUI . BoxUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n GafferSceneUI . FilteredSceneProcessorUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n __nodeEditorToolMenuConnection = GafferUI . NodeEditor . toolMenuSignal ( ) . connect ( <mask0> ) \n", "gt": "__toolMenu"}
{"input": "\n VERSION = ( <NUM_LIT:0> , <NUM_LIT:1> , <NUM_LIT:0> , '<STR_LIT>' , <NUM_LIT:1> ) \n __version__ = '<STR_LIT:.>' . join ( map ( str , VERSION ) ) \n def get_version ( ) : \n version = '<STR_LIT>' % ( VERSION [ <NUM_LIT:0> ] , VERSION [ <NUM_LIT:1> ] ) \n if VERSION [ <NUM_LIT:2> ] : \n version = '<STR_LIT>' % ( version , VERSION [ <NUM_LIT:2> ] ) \n if VERSION [ <NUM_LIT:3> : ] == ( '<STR_LIT>' , <NUM_LIT:0> ) : \n version = '<STR_LIT>' % version \n else : \n if VERSION [ <NUM_LIT:3> ] != '<STR_LIT>' : \n version = '<STR_LIT>' % ( version , VERSION [ <NUM_LIT:3> ] , VERSION [ <NUM_LIT:4> ] ) \n return <mask0> \n", "gt": "version"}
{"input": "\n import yappi \n import os \n from totalimpact import backend \n rootdir = \"<STR_LIT:.>\" \n logfile = '<STR_LIT>' \n yappi . clear_stats ( ) \n yappi . start ( ) \n backend . main ( logfile ) \n yappi . stop ( ) \n yappi . print_stats ( sort_type = yappi . SORTTYPE_TTOT , limit = <NUM_LIT:30> , thread_stats_on = <mask0> ) \n", "gt": "False"}
{"input": "\n import os , collections , simplejson \n from totalimpact import db , app \n from totalimpact . providers import pmc \n from test . unit_tests . providers import common \n from test . unit_tests . providers . common import ProviderTestCase \n from totalimpact . providers . provider import Provider , ProviderContentMalformedError , ProviderFactory \n from totalimpact import provider_batch_data \n from test . utils import http \n from test . utils import setup_postgres_for_unittests , teardown_postgres_for_unittests \n from nose . tools import assert_equals , raises , nottest , assert_items_equal \n datadir = os . path . join ( os . path . split ( __file__ ) [ <NUM_LIT:0> ] , \"<STR_LIT>\" ) \n SAMPLE_EXTRACT_METRICS_PAGE = os . path . join ( datadir , \"<STR_LIT>\" ) \n SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH = os . path . join ( datadir , \"<STR_LIT>\" ) \n TEST_PMID = \"<STR_LIT>\" \n class TestPmc ( ProviderTestCase ) : \n provider_name = \"<STR_LIT>\" \n testitem_aliases = ( \"<STR_LIT>\" , TEST_PMID ) \n testitem_metrics = ( \"<STR_LIT>\" , TEST_PMID ) \n def setUp ( self ) : \n ProviderTestCase . setUp ( self ) \n self . db = setup_postgres_for_unittests ( db , app ) \n sample_data_dump = open ( SAMPLE_EXTRACT_METRICS_PAGE , \"<STR_LIT:r>\" ) . read ( ) \n sample_data_dump_different_month = open ( SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH , \"<STR_LIT:r>\" ) . read ( ) \n test_monthly_data = [ \n { \"<STR_LIT>\" : \"<STR_LIT:abc>\" , \n \"<STR_LIT:type>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : sample_data_dump , \n \"<STR_LIT>\" : <NUM_LIT:1.0> , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" ] } , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT:type>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : sample_data_dump_different_month , \n \"<STR_LIT>\" : <NUM_LIT:1.0> , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : [ \"<STR_LIT>\" ] } , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" \n ] \n } , \n \"<STR_LIT>\" : <NUM_LIT:1> , \n \"<STR_LIT:type>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n ] \n for doc in test_monthly_data : \n new_object = provider_batch_data . create_objects_from_doc ( doc ) \n print new_object \n self . provider = pmc . Pmc ( ) \n print \"<STR_LIT>\" \n def tearDown ( self ) : \n teardown_postgres_for_unittests ( self . db ) \n def test_has_applicable_batch_data_true ( self ) : \n response = self . provider . has_applicable_batch_data ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n assert_equals ( response , True ) \n def test_has_applicable_batch_data_false ( self ) : \n response = self . provider . has_applicable_batch_data ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n assert_equals ( response , False ) \n def test_build_batch_data_dict ( self ) : \n response = self . provider . build_batch_data_dict ( ) \n print response . keys ( ) \n expected = [ ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) ] \n assert_items_equal ( response . keys ( ) , expected ) \n def test_is_relevant_alias ( self ) : \n assert_equals ( self . provider . is_relevant_alias ( self . testitem_aliases ) , True ) \n def test_extract_metrics_success ( self ) : \n f = open ( SAMPLE_EXTRACT_METRICS_PAGE , \"<STR_LIT:r>\" ) \n good_page = f . read ( ) \n metrics_dict = self . provider . _extract_metrics ( good_page , id = \"<STR_LIT>\" ) \n print metrics_dict \n expected = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT:9> } \n assert_equals ( metrics_dict , expected ) \n def test_provider_metrics_500 ( self ) : \n pass \n def test_provider_metrics_400 ( self ) : \n pass \n def test_provider_metrics_nonsense_xml ( self ) : \n pass \n def test_provider_metrics_nonsense_txt ( self ) : \n pass \n def test_provider_metrics_empty ( self ) : \n pass \n @ http \n def test_metrics ( self ) : \n metrics_dict = self . provider . metrics ( [ ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ] ) \n expected = { '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT:9> , '<STR_LIT>' ) } \n print metrics_dict \n for key in expected : \n assert metrics_dict [ key ] [ <NUM_LIT:0> ] >= expected [ key ] [ <NUM_LIT:0> ] , [ key , metrics_dict [ key ] , expected [ key ] ] \n assert metrics_dict [ key ] [ <NUM_LIT:1> ] == expected [ key ] [ <NUM_LIT:1> ] , [ key , metrics_dict [ key ] , expected [ key ] ] \n @ http \n def test_metrics_multiple_months ( self ) : \n metrics_dict = self . provider . metrics ( [ ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ] ) \n expected = { '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) } \n print metrics_dict \n for key in expected : \n assert metrics_dict [ key ] [ <NUM_LIT:0> ] >= expected [ key ] [ <NUM_LIT:0> ] , [ key , metrics_dict [ key ] , expected [ key ] ] \n assert metrics_dict [ key ] [ <NUM_LIT:1> ] == expected [ key ] [ <NUM_LIT:1> ] , [ key , metrics_dict [ key ] , expected [ key ] ] \n @ http \n def test_metrics_real ( self ) : \n metrics_dict = self . provider . metrics ( [ ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ] ) \n expected = { '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) , '<STR_LIT>' : ( <NUM_LIT> , '<STR_LIT>' ) } \n print metrics_dict \n for key in expected : \n assert metrics_dict [ key ] [ <NUM_LIT:0> ] >= expected [ key ] [ <NUM_LIT:0> ] , [ key , metrics_dict [ key ] , expected [ key ] ] \n assert metrics_dict [ key ] [ <NUM_LIT:1> ] == expected [ key ] [ <NUM_LIT:1> ] , [ key , metrics_dict [ key ] , expected [ <mask0> ] ] \n", "gt": "key"}
{"input": "\n import os \n import sys \n import hashlib \n import logging \n import json \n from cPickle import PicklingError \n import redis \n from totalimpact import REDIS_CACHE_DATABASE_NUMBER \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n cache_client = redis . from_url ( os . getenv ( \"<STR_LIT>\" ) , REDIS_CACHE_DATABASE_NUMBER ) \n MAX_PAYLOAD_SIZE_BYTES = <NUM_LIT:1000> * <NUM_LIT:1000> \n MAX_CACHE_SIZE_BYTES = <NUM_LIT:100> * <NUM_LIT:1000> * <NUM_LIT:1000> \n class CacheException ( Exception ) : \n pass \n class Cache ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def _build_hash_key ( self , key ) : \n json_key = json . dumps ( key ) \n hash_key = hashlib . md5 ( json_key . encode ( \"<STR_LIT:utf-8>\" ) ) . hexdigest ( ) \n return hash_key \n def _get_client ( self ) : \n return cache_client \n def __init__ ( self , max_cache_age = <NUM_LIT> * <NUM_LIT> ) : \n self . max_cache_age = max_cache_age \n self . flush_cache ( ) \n def flush_cache ( self ) : \n mc = self . _get_client ( ) \n def get_cache_entry ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n mc = self . _get_client ( ) \n hash_key = self . _build_hash_key ( key ) \n response = mc . get ( hash_key ) \n if response : \n response = json . loads ( response ) \n return response \n def set_cache_entry ( self , key , data ) : \n \"\"\"<STR_LIT>\"\"\" \n if sys . getsizeof ( data [ \"<STR_LIT:text>\" ] ) > MAX_PAYLOAD_SIZE_BYTES : \n logger . debug ( u\"<STR_LIT>\" ) \n return None \n mc = self . _get_client ( ) \n if mc . info ( ) [ \"<STR_LIT>\" ] >= MAX_CACHE_SIZE_BYTES : \n logger . debug ( u\"<STR_LIT>\" ) \n return None \n hash_key = self . _build_hash_key ( key ) \n set_response = mc . set ( hash_key , json . dumps ( data ) ) \n mc . expire ( hash_key , self . max_cache_age ) \n if not set_response : \n logger . warning ( \"<STR_LIT>\" ) \n raise CacheException ( \"<STR_LIT>\" ) \n return <mask0> \n", "gt": "set_response"}
{"input": "\n from totalimpact . providers import provider \n from totalimpact . providers . provider import Provider , ProviderContentMalformedError \n import simplejson , os , re , urllib \n import logging \n logger = logging . getLogger ( '<STR_LIT>' ) \n class Plosalm ( Provider ) : \n example_id = ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" \n descr = \"<STR_LIT>\" \n metrics_url_template = \"<STR_LIT>\" + os . environ [ \"<STR_LIT>\" ] \n provenance_url_template = \"<STR_LIT>\" \n PLOS_ICON = \"<STR_LIT>\" \n static_meta_dict = { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT:description>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : PLOS_ICON , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT:description>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : PLOS_ICON , \n } \n } \n def __init__ ( self ) : \n super ( Plosalm , self ) . __init__ ( ) \n def is_relevant_alias ( self , alias ) : \n ( namespace , nid ) = alias \n relevant = ( ( \"<STR_LIT>\" == namespace ) and ( \"<STR_LIT>\" in nid ) ) \n return ( relevant ) \n def _extract_metrics ( self , page , status_code = <NUM_LIT:200> , id = None ) : \n if status_code != <NUM_LIT:200> : \n if status_code == <NUM_LIT> : \n return { } \n else : \n raise ( self . _get_error ( status_code ) ) \n if not \"<STR_LIT>\" in page : \n raise ProviderContentMalformedError \n json_response = provider . _load_json ( page ) \n this_article = json_response [ <NUM_LIT:0> ] [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] [ \"<STR_LIT>\" ] \n dict_of_keylists = { \n '<STR_LIT>' : [ '<STR_LIT:html>' ] , \n '<STR_LIT>' : [ '<STR_LIT>' ] \n } \n metrics_dict = provider . _extract_from_data_dict ( this_article , dict_of_keylists ) \n return <mask0> \n", "gt": "metrics_dict"}
{"input": "\n import os \n import sys \n import urlparse \n from kombu import Exchange , Queue \n sys . path . append ( '<STR_LIT:.>' ) \n redis_url = os . environ . get ( '<STR_LIT>' , \"<STR_LIT>\" ) \n if not redis_url . endswith ( \"<STR_LIT:/>\" ) : \n redis_url += \"<STR_LIT:/>\" \n BROKER_URL = redis_url + \"<STR_LIT:1>\" \n CELERY_RESULT_BACKEND = redis_url + \"<STR_LIT:2>\" \n REDIS_CONNECT_RETRY = True \n BROKER_TRANSPORT_OPTIONS = { '<STR_LIT>' : True , \n '<STR_LIT>' : True , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT:100> \n } \n CELERY_DEFAULT_QUEUE = '<STR_LIT>' \n CELERY_QUEUES = [ \n Queue ( '<STR_LIT>' , routing_key = '<STR_LIT>' ) , \n Queue ( '<STR_LIT>' , routing_key = '<STR_LIT>' ) \n ] \n BROKER_POOL_LIMIT = None \n CELERY_CREATE_MISSING_QUEUES = True \n CELERY_ACCEPT_CONTENT = [ '<STR_LIT>' , '<STR_LIT>' ] \n CELERY_ENABLE_UTC = True \n CELERY_TASK_RESULT_EXPIRES = <NUM_LIT> * <NUM_LIT> * <NUM_LIT:1> \n CELERY_ACKS_LATE = True \n CELERYD_FORCE_EXECV = True \n CELERY_TRACK_STARTED = <mask0> \n CELERYD_PREFETCH_MULTIPLIER = <NUM_LIT:1> \n CELERY_IMPORTS = ( \"<STR_LIT>\" , ) \n CELERY_ANNOTATIONS = { \n '<STR_LIT>' : { '<STR_LIT>' : <NUM_LIT> * <NUM_LIT:2> } \n } \n", "gt": "True"}
{"input": "\n from totalimpact . providers import provider \n from totalimpact . providers . provider import Provider , ProviderFactory \n from totalimpactwebapp import app , db \n from nose . tools import assert_equals , nottest \n from xml . dom import minidom \n from test . utils import setup_postgres_for_unittests , teardown_postgres_for_unittests \n import simplejson , BeautifulSoup \n import os \n from sqlalchemy . sql import text \n sampledir = os . path . join ( os . path . split ( __file__ ) [ <NUM_LIT:0> ] , \"<STR_LIT>\" ) \n class Test_Provider ( ) : \n TEST_PROVIDER_CONFIG = [ \n ( \"<STR_LIT>\" , { \"<STR_LIT>\" : <NUM_LIT:1> } ) , \n ( \"<STR_LIT>\" , { \"<STR_LIT>\" : <NUM_LIT:3> } ) , \n ( \"<STR_LIT>\" , { \"<STR_LIT>\" : <NUM_LIT:3> } ) , \n ] \n TEST_JSON = \"\"\"<STR_LIT>\"\"\" \n TEST_XML = open ( os . path . join ( sampledir , \"<STR_LIT>\" , \"<STR_LIT>\" ) ) . read ( ) \n def setUp ( self ) : \n self . db = setup_postgres_for_unittests ( db , app ) \n def tearDown ( self ) : \n teardown_postgres_for_unittests ( self . db ) \n def test_get_provider ( self ) : \n provider = ProviderFactory . get_provider ( \"<STR_LIT>\" ) \n assert_equals ( provider . __class__ . __name__ , \"<STR_LIT>\" ) \n def test_get_providers ( self ) : \n providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG ) \n provider_names = [ provider . __class__ . __name__ for provider in providers ] \n assert_equals ( set ( provider_names ) , set ( [ '<STR_LIT>' , '<STR_LIT>' , \"<STR_LIT>\" ] ) ) \n def test_get_providers_filters_by_metrics ( self ) : \n providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG , \"<STR_LIT>\" ) \n provider_names = [ provider . __class__ . __name__ for provider in providers ] \n assert_equals ( set ( provider_names ) , set ( [ '<STR_LIT>' , '<STR_LIT>' , \"<STR_LIT>\" ] ) ) \n def test_get_providers_filters_by_biblio ( self ) : \n providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG , \"<STR_LIT>\" ) \n provider_names = [ provider . __class__ . __name__ for provider in providers ] \n assert_equals ( set ( provider_names ) , set ( [ '<STR_LIT>' , '<STR_LIT>' ] ) ) \n def test_get_providers_filters_by_aliases ( self ) : \n providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG , \"<STR_LIT>\" ) \n provider_names = [ provider . __class__ . __name__ for provider in providers ] \n assert_equals ( set ( provider_names ) , set ( [ '<STR_LIT>' , '<STR_LIT>' ] ) ) \n def test_lookup_json ( self ) : \n page = self . TEST_JSON \n data = simplejson . loads ( page ) \n response = provider . _lookup_json ( data , [ '<STR_LIT>' , '<STR_LIT:name>' ] ) \n assert_equals ( response , u'<STR_LIT>' ) \n def test_extract_json ( self ) : \n page = self . TEST_JSON \n dict_of_keylists = { \n '<STR_LIT:title>' : [ '<STR_LIT>' , '<STR_LIT:name>' ] , \n '<STR_LIT:description>' : [ '<STR_LIT>' , '<STR_LIT:description>' ] } \n response = provider . _extract_from_json ( page , dict_of_keylists ) \n assert_equals ( response , { '<STR_LIT:description>' : u'<STR_LIT>' , '<STR_LIT:title>' : u'<STR_LIT>' } ) \n def test_lookup_xml_from_dom ( self ) : \n page = self . TEST_XML \n doc = minidom . parseString ( page . strip ( ) ) \n response = provider . _lookup_xml_from_dom ( doc , [ '<STR_LIT>' ] ) \n assert_equals ( response , <NUM_LIT> ) \n def test_lookup_xml_from_soup ( self ) : \n page = self . TEST_XML \n doc = BeautifulSoup . BeautifulStoneSoup ( page ) \n response = provider . _lookup_xml_from_soup ( doc , [ '<STR_LIT>' ] ) \n assert_equals ( response , <NUM_LIT> ) \n def test_extract_xml ( self ) : \n page = self . TEST_XML \n dict_of_keylists = { \n '<STR_LIT:count>' : [ '<STR_LIT>' ] } \n response = provider . _extract_from_xml ( page , dict_of_keylists ) \n assert_equals ( response , { '<STR_LIT:count>' : <NUM_LIT> } ) \n def test_doi_from_url_string ( self ) : \n test_url = \"<STR_LIT>\" \n expected = \"<STR_LIT>\" \n response = provider . doi_from_url_string ( test_url ) \n assert_equals ( response , expected ) \n def test_is_issn_in_doaj_false ( self ) : \n response = provider . is_issn_in_doaj ( \"<STR_LIT>\" ) \n assert_equals ( response , False ) \n def test_is_issn_in_doaj_true ( self ) : \n zookeys_issn = \"<STR_LIT>\" \n response = provider . is_issn_in_doaj ( zookeys_issn ) \n assert_equals ( response , True ) \n def test_import_products ( self ) : \n response = provider . import_products ( \"<STR_LIT>\" , \n { \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] } ) \n expected = [ ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT:url>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) ] \n assert_equals ( response , expected ) \n def test_import_products_bad_providername ( self ) : \n response = provider . import_products ( \"<STR_LIT>\" , { } ) \n expected = [ ] \n assert_equals ( response , expected ) \n class TestProviderFactory ( ) : \n TEST_PROVIDER_CONFIG = [ \n ( \"<STR_LIT>\" , { \"<STR_LIT>\" : <NUM_LIT:1> } ) , \n ( \"<STR_LIT>\" , { \"<STR_LIT>\" : <NUM_LIT:3> } ) , \n ( \"<STR_LIT>\" , { \"<STR_LIT>\" : <NUM_LIT:3> } ) , \n ] \n def test_get_all_static_meta ( self ) : \n sm = ProviderFactory . get_all_static_meta ( self . TEST_PROVIDER_CONFIG ) \n expected = '<STR_LIT>' \n assert_equals ( sm [ \"<STR_LIT>\" ] [ \"<STR_LIT:description>\" ] , expected ) \n def test_get_all_metric_names ( self ) : \n response = ProviderFactory . get_all_metric_names ( self . TEST_PROVIDER_CONFIG ) \n expected = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n assert_equals ( response , expected ) \n def test_get_all_metadata ( self ) : \n md = ProviderFactory . get_all_metadata ( self . TEST_PROVIDER_CONFIG ) \n print md [ \"<STR_LIT>\" ] \n assert_equals ( md [ \"<STR_LIT>\" ] [ <mask0> ] , '<STR_LIT>' ) \n", "gt": "'<STR_LIT:url>'"}
{"input": "\n import datetime \n import copy \n import unicode_helpers \n import json \n import logging \n from util import cached_property \n from util import dict_from_dir \n from totalimpactwebapp import db \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n def clean_id ( nid ) : \n try : \n nid = nid . strip ( '<STR_LIT>' ) . strip ( ) \n nid = unicode_helpers . remove_nonprinting_characters ( nid ) \n except ( TypeError , AttributeError ) : \n pass \n return ( nid ) \n def normalize_alias_tuple ( ns , nid ) : \n ns = clean_id ( ns ) \n ns = ns . lower ( ) \n if ns == \"<STR_LIT>\" : \n return ( ns , nid ) \n nid = clean_id ( nid ) \n from totalimpact . providers import crossref \n from totalimpact . providers import pubmed \n from totalimpact . providers import arxiv \n from totalimpact . providers import webpage \n from totalimpact import importer \n clean_nid = None \n if ns == \"<STR_LIT>\" or importer . is_doi ( nid ) : \n ns = \"<STR_LIT>\" \n clean_nid = crossref . clean_doi ( nid ) \n elif ns == \"<STR_LIT>\" or importer . is_pmid ( nid ) : \n ns = \"<STR_LIT>\" \n clean_nid = pubmed . clean_pmid ( nid ) \n elif ns == \"<STR_LIT>\" or importer . is_arxiv ( nid ) : \n ns = \"<STR_LIT>\" \n clean_nid = arxiv . clean_arxiv_id ( nid ) \n elif ns == \"<STR_LIT:url>\" or importer . is_url ( nid ) : \n ns = \"<STR_LIT:url>\" \n clean_nid = webpage . clean_url ( nid ) \n elif ns not in [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT:url>\" ] : \n clean_nid = nid \n if not clean_nid : \n return None \n return ( ns , clean_nid ) \n def clean_alias_tuple_for_comparing ( ns , nid ) : \n alias_tuple = normalize_alias_tuple ( ns , nid ) \n if not alias_tuple : \n return None \n try : \n ( ns , nid ) = alias_tuple \n cleaned_alias = ( ns . lower ( ) , nid . lower ( ) ) \n except AttributeError : \n logger . debug ( u\"<STR_LIT>\" . format ( \n ns = ns , nid = nid ) ) \n cleaned_alias = ( ns , nid ) \n return cleaned_alias \n def alias_tuples_from_dict ( aliases_dict ) : \n \"\"\"<STR_LIT>\"\"\" \n alias_tuples = [ ] \n for ns , ids in aliases_dict . iteritems ( ) : \n if isinstance ( ids , basestring ) : \n alias_tuples . append ( ( ns , ids ) ) \n else : \n for id in ids : \n alias_tuples . append ( ( ns , id ) ) \n return alias_tuples \n def alias_dict_from_tuples ( aliases_tuples ) : \n alias_dict = { } \n for ( ns , ids ) in aliases_tuples : \n if ns in alias_dict : \n alias_dict [ ns ] += [ ids ] \n else : \n alias_dict [ ns ] = [ ids ] \n return alias_dict \n def canonical_aliases ( orig_aliases_dict ) : \n lowercase_aliases_dict = { } \n for orig_namespace in orig_aliases_dict : \n lowercase_namespace = clean_id ( orig_namespace . lower ( ) ) \n if lowercase_namespace == \"<STR_LIT>\" : \n lowercase_aliases_dict [ lowercase_namespace ] = [ clean_id ( doi . lower ( ) ) for doi in orig_aliases_dict [ orig_namespace ] ] \n else : \n lowercase_aliases_dict [ lowercase_namespace ] = [ clean_id ( nid ) for nid in orig_aliases_dict [ orig_namespace ] ] \n return lowercase_aliases_dict \n def merge_alias_dicts ( aliases1 , aliases2 ) : \n merged_aliases = copy . deepcopy ( aliases1 ) \n for ns , nid_list in aliases2 . iteritems ( ) : \n for nid in nid_list : \n try : \n if not nid in merged_aliases [ ns ] : \n merged_aliases [ ns ] . append ( nid ) \n except KeyError : \n merged_aliases [ ns ] = [ nid ] \n return merged_aliases \n def matches_alias ( product1 , product2 , exclude = [ ] ) : \n alias_tuple_list1 = [ alias_row . my_alias_tuple_for_comparing for alias_row in product1 . alias_rows ] \n alias_tuple_list2 = [ alias_row . my_alias_tuple_for_comparing for alias_row in product2 . alias_rows ] \n has_matches = False \n for alias_tuple1 in alias_tuple_list1 : \n if alias_tuple1 : \n ( ns , nid ) = alias_tuple1 \n if alias_tuple1 in alias_tuple_list2 and ns not in exclude : \n has_matches = True \n return has_matches \n class AliasRow ( db . Model ) : \n __tablename__ = '<STR_LIT>' \n tiid = db . Column ( db . Text , db . ForeignKey ( '<STR_LIT>' ) , primary_key = True ) \n namespace = db . Column ( db . Text , primary_key = True ) \n nid = db . Column ( db . Text , primary_key = True ) \n collected_date = db . Column ( db . DateTime ( ) ) \n def __init__ ( self , ** kwargs ) : \n if \"<STR_LIT>\" not in kwargs : \n self . collected_date = datetime . datetime . utcnow ( ) \n super ( AliasRow , self ) . __init__ ( ** kwargs ) \n @ cached_property \n def alias_tuple ( self ) : \n return ( self . namespace , self . nid ) \n @ cached_property \n def my_alias_tuple_for_comparing ( self ) : \n return clean_alias_tuple_for_comparing ( self . namespace , self . nid ) \n def is_equivalent_alias ( self , given_namespace , given_nid ) : \n if not given_nid : \n return False \n given_clean_alias = clean_alias_tuple_for_comparing ( given_namespace , given_nid ) \n if not given_clean_alias : \n return False \n return given_clean_alias == self . my_alias_tuple_for_comparing \n class Aliases ( object ) : \n def __init__ ( self , alias_rows ) : \n ignore_namepaces = [ \"<STR_LIT>\" ] \n self . tiid = None \n for alias_row in alias_rows : \n if alias_row . namespace not in ignore_namepaces : \n self . tiid = alias_row . tiid \n try : \n getattr ( self , alias_row . namespace ) . append ( alias_row . nid ) \n except AttributeError : \n setattr ( self , alias_row . namespace , [ alias_row . nid ] ) \n @ cached_property \n def best_url ( self ) : \n if self . display_doi : \n return u\"<STR_LIT>\" + self . display_doi \n if self . display_pmid : \n return u\"<STR_LIT>\" + self . display_pmid \n if self . display_pmc : \n return u\"<STR_LIT>\" + self . display_pmc \n if self . resolved_url : \n return self . resolved_url \n try : \n return self . url [ <NUM_LIT:0> ] \n except AttributeError : \n return None \n @ cached_property \n def display_best_url ( self ) : \n return self . best_url \n @ cached_property \n def display_pmid ( self ) : \n try : \n return self . pmid [ <NUM_LIT:0> ] \n except AttributeError : \n return None \n @ cached_property \n def display_pmc ( self ) : \n try : \n return self . pmc [ <NUM_LIT:0> ] \n except AttributeError : \n return None \n @ cached_property \n def display_doi ( self ) : \n try : \n return self . doi [ <NUM_LIT:0> ] \n except AttributeError : \n return None \n @ cached_property \n def display_arxiv ( self ) : \n try : \n return self . arxiv [ <NUM_LIT:0> ] \n except AttributeError : \n return None \n @ cached_property \n def has_formal_alias ( self ) : \n if self . display_arxiv or self . display_doi or self . display_pmid or self . display_pmc : \n return True \n else : \n return False \n @ cached_property \n def resolved_url ( self ) : \n try : \n for url in self . url : \n if \"<STR_LIT>\" in url : \n continue \n elif \"<STR_LIT>\" in url : \n continue \n elif \"<STR_LIT>\" in url : \n continue \n elif \"<STR_LIT>\" in url : \n continue \n elif \"<STR_LIT>\" in url : \n continue \n else : \n return url \n return self . url [ <NUM_LIT:0> ] \n except AttributeError : \n return None \n def get_genre ( self ) : \n return self . _guess_genre_and_host_from_aliases ( ) [ <NUM_LIT:0> ] \n def get_host ( self ) : \n return self . _guess_genre_and_host_from_aliases ( ) [ <NUM_LIT:1> ] \n def _guess_genre_and_host_from_aliases ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n genre = \"<STR_LIT>\" \n host = \"<STR_LIT>\" \n if hasattr ( self , \"<STR_LIT>\" ) : \n joined_doi_string = \"<STR_LIT>\" . join ( self . doi ) . lower ( ) \n if \"<STR_LIT>\" in joined_doi_string : \n genre = \"<STR_LIT>\" \n host = \"<STR_LIT>\" \n elif \"<STR_LIT>\" in joined_doi_string : \n host = \"<STR_LIT>\" \n genre = \"<STR_LIT>\" \n else : \n genre = \"<STR_LIT>\" \n elif hasattr ( self , \"<STR_LIT>\" ) : \n genre = \"<STR_LIT>\" \n elif hasattr ( self , \"<STR_LIT>\" ) : \n genre = \"<STR_LIT>\" \n host = \"<STR_LIT>\" \n elif hasattr ( self , \"<STR_LIT>\" ) : \n genre = \"<STR_LIT>\" \n host = \"<STR_LIT>\" \n elif hasattr ( self , \"<STR_LIT>\" ) : \n genre = \"<STR_LIT>\" \n host = \"<STR_LIT>\" \n elif hasattr ( self , \"<STR_LIT:url>\" ) : \n joined_url_string = \"<STR_LIT>\" . join ( self . url ) . lower ( ) \n if \"<STR_LIT>\" in joined_url_string : \n genre = \"<STR_LIT>\" \n host = \"<STR_LIT>\" \n elif \"<STR_LIT>\" in joined_url_string : \n genre = \"<STR_LIT>\" \n host = \"<STR_LIT>\" \n elif ( \"<STR_LIT>\" in joined_url_string ) or ( \"<STR_LIT>\" in joined_url_string ) : \n genre = \"<STR_LIT>\" \n host = \"<STR_LIT>\" \n elif \"<STR_LIT>\" in joined_url_string : \n genre = \"<STR_LIT>\" \n host = \"<STR_LIT>\" \n else : \n genre = \"<STR_LIT>\" \n return genre , host \n def to_dict ( self ) : \n ret = dict_from_dir ( self ) \n return <mask0> \n", "gt": "ret"}
{"input": "\n from totalimpactwebapp import json_sqlalchemy \n from util import commit \n from util import cached_property \n from util import dict_from_dir \n from util import as_int_or_float_if_possible \n from totalimpactwebapp import db \n from totalimpactwebapp . tweeter import Tweeter \n from birdy . twitter import AppClient , TwitterApiError , TwitterRateLimitError , TwitterClientError \n from collections import defaultdict \n from sqlalchemy import case \n import os \n import re \n import datetime \n import logging \n logger = logging . getLogger ( '<STR_LIT>' ) \n def tweets_from_tiids ( tiids ) : \n if not tiids : \n return [ ] \n tweets = db . session . query ( Tweet ) . filter ( Tweet . tiid . in_ ( tiids ) ) . all ( ) \n return tweets \n def get_product_tweets_for_profile ( profile_id ) : \n tweets = db . session . query ( Tweet ) . filter ( Tweet . profile_id == profile_id ) . all ( ) \n response = defaultdict ( list ) \n for tweet in tweets : \n if tweet . tiid and tweet . tweet_text : \n response [ tweet . tiid ] . append ( tweet ) \n return response \n def store_tweet_payload_and_tweeter_from_twitter ( payload_dicts_from_twitter , tweets ) : \n tweets_by_tweet_id = defaultdict ( list ) \n for tweet in tweets : \n tweets_by_tweet_id [ tweet . tweet_id ] . append ( tweet ) \n for payload_dict in payload_dicts_from_twitter : \n tweet_id = payload_dict [ \"<STR_LIT>\" ] \n logger . debug ( \"<STR_LIT>\" . format ( \n tweet_id = tweet_id ) ) \n for tweet in tweets_by_tweet_id [ tweet_id ] : \n if not tweet . payload : \n tweet . payload = payload_dict \n logger . info ( u\"<STR_LIT>\" . format ( \n tweet_id = tweet_id , tiid = tweet . tiid ) ) \n if \"<STR_LIT:user>\" in payload_dict : \n try : \n tweet . tweeter . set_attributes_from_twitter_data ( payload_dict [ \"<STR_LIT:user>\" ] ) \n except AttributeError : \n tweeter = Tweeter . query . get ( tweet . screen_name ) \n if not tweeter : \n tweeter = Tweeter ( screen_name = tweet . screen_name ) \n db . session . add ( tweeter ) \n tweeter . set_attributes_from_twitter_data ( payload_dict [ \"<STR_LIT:user>\" ] ) \n tweet . tweeter = tweeter \n commit ( db ) \n if tweet . tweeter : \n logger . info ( u\"<STR_LIT>\" . format ( \n screen_name = tweet . tweeter . screen_name ) ) \n def flag_deleted_tweets ( tweet_ids ) : \n if not tweet_ids : \n return None \n for tweet in Tweet . query . filter ( Tweet . tweet_id . in_ ( tweet_ids ) ) . all ( ) : \n tweet . is_deleted = True \n db . session . merge ( tweet ) \n def handle_all_tweets ( data , tweets ) : \n store_tweet_payload_and_tweeter_from_twitter ( data , tweets ) \n tweet_ids = [ tweet . tweet_id for tweet in tweets ] \n tweet_ids_with_response = [ tweet [ \"<STR_LIT>\" ] for tweet in data ] \n tweet_ids_without_response = [ tweet for tweet in tweet_ids if tweet not in tweet_ids_with_response ] \n flag_deleted_tweets ( tweet_ids_without_response ) \n return True \n class AppDictClient ( AppClient ) : \n @ staticmethod \n def get_json_object_hook ( data ) : \n return data \n def get_and_save_tweet_text_and_tweeter_followers ( tweets ) : \n client = AppDictClient ( \n os . getenv ( \"<STR_LIT>\" ) , \n os . getenv ( \"<STR_LIT>\" ) , \n access_token = os . getenv ( \"<STR_LIT>\" ) \n ) \n logger . info ( u\"<STR_LIT>\" . format ( \n num = len ( tweets ) ) ) \n group_size = <NUM_LIT:100> \n list_of_groups = [ tweets [ i : i + group_size ] for i in range ( <NUM_LIT:0> , len ( tweets ) , group_size ) ] \n for tweet_subset in list_of_groups : \n tweet_id_string = \"<STR_LIT:U+002C>\" . join ( [ tweet . tweet_id for tweet in tweet_subset ] ) \n try : \n response = client . api . statuses . lookup . post ( id = tweet_id_string , trim_user = False ) \n handle_all_tweets ( response . data , tweet_subset ) \n except TwitterApiError , e : \n logger . exception ( \"<STR_LIT>\" ) \n except TwitterClientError , e : \n logger . exception ( \"<STR_LIT>\" ) \n except TwitterRateLimitError , e : \n logger . exception ( \"<STR_LIT>\" ) \n return \n def hydrate_twitter_text_and_followers ( profile_id , altmetric_twitter_posts ) : \n logger . info ( u\"<STR_LIT>\" . format ( \n profile_id = profile_id ) ) \n tweets_to_hydrate_from_twitter = [ ] \n tweets = Tweet . query . filter ( Tweet . profile_id == profile_id ) \n tweet_dict = dict ( [ ( ( tweet . tweet_id , tweet . tiid ) , tweet ) for tweet in tweets ] ) \n for tiid , post_list in altmetric_twitter_posts . iteritems ( ) : \n for post in post_list : \n tweet_id = post [ \"<STR_LIT>\" ] \n screen_name = post [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n if ( tweet_id , tiid ) in tweet_dict . keys ( ) : \n tweet = tweet_dict [ ( tweet_id , tiid ) ] \n if not tweet . tweet_text and not tweet . is_deleted : \n tweets_to_hydrate_from_twitter . append ( tweet ) \n else : \n if not Tweet . query . get ( ( tweet_id , tiid ) ) : \n tweet = Tweet ( tweet_id = tweet_id , tiid = tiid ) \n tweet . set_attributes_from_altmetric_post ( post ) \n tweet . profile_id = profile_id \n tweets_to_hydrate_from_twitter . append ( tweet ) \n db . session . add ( tweet ) \n if not tweet . tweeter : \n tweeter = Tweeter . query . get ( screen_name ) \n if not tweeter : \n tweeter = Tweeter ( screen_name = screen_name ) \n db . session . add ( tweeter ) \n tweeter . set_attributes_from_altmetric_post ( post ) \n commit ( db ) \n logger . info ( u\"<STR_LIT>\" . format ( \n profile_id = profile_id ) ) \n if tweets_to_hydrate_from_twitter : \n commit ( db ) \n tweet_ids = [ tweet . tweet_id for tweet in tweets_to_hydrate_from_twitter ] \n logger . info ( u\"<STR_LIT>\" . format ( \n profile_id = profile_id ) ) \n get_and_save_tweet_text_and_tweeter_followers ( tweets_to_hydrate_from_twitter ) \n commit ( db ) \n else : \n logger . info ( u\"<STR_LIT>\" . format ( \n profile_id = profile_id ) ) \n return \n handle_workaround_join_string = \"<STR_LIT>\" \n class Tweet ( db . Model ) : \n tweet_id = db . Column ( db . Text , primary_key = True ) \n tiid = db . Column ( db . Text , primary_key = True ) \n profile_id = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' ) ) \n screen_name = db . Column ( db . Text , db . ForeignKey ( '<STR_LIT>' ) ) \n tweet_timestamp = db . Column ( db . DateTime ( ) ) \n payload = db . Column ( json_sqlalchemy . JSONAlchemy ( db . Text ) ) \n is_deleted = db . Column ( db . Boolean ) \n tweet_url = db . Column ( db . Text ) \n country = db . Column ( db . Text ) \n followers_at_time_of_tweet = db . Column ( db . Integer ) \n tweeter = db . relationship ( \n '<STR_LIT>' , \n lazy = '<STR_LIT>' , \n cascade = '<STR_LIT:all>' , \n backref = db . backref ( \"<STR_LIT>\" ) , \n uselist = False , \n primaryjoin = handle_workaround_join_string \n ) \n def __init__ ( self , ** kwargs ) : \n if \"<STR_LIT>\" in kwargs : \n payload_dict = kwargs [ \"<STR_LIT>\" ] \n kwargs [ \"<STR_LIT>\" ] = payload_dict [ \"<STR_LIT>\" ] \n kwargs [ \"<STR_LIT>\" ] = payload_dict [ \"<STR_LIT:user>\" ] [ \"<STR_LIT>\" ] \n kwargs [ \"<STR_LIT>\" ] = payload_dict \n kwargs [ \"<STR_LIT>\" ] = datetime . datetime . strptime ( payload_dict [ \"<STR_LIT>\" ] , r\"<STR_LIT>\" ) \n if not \"<STR_LIT>\" in kwargs : \n try : \n kwargs [ \"<STR_LIT>\" ] = payload_dict [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n except ( AttributeError , TypeError ) : \n pass \n super ( Tweet , self ) . __init__ ( ** kwargs ) \n @ classmethod \n def most_recent_tweet_id ( cls , screen_name ) : \n screen_name = screen_name . replace ( \"<STR_LIT:@>\" , \"<STR_LIT>\" ) \n q = db . session . query ( Tweet ) . filter ( Tweet . screen_name == screen_name ) . order_by ( Tweet . tweet_timestamp . desc ( ) ) \n tweet = q . first ( ) \n try : \n tweet_id = tweet . tweet_id \n except AttributeError : \n tweet_id = None \n return tweet_id \n @ cached_property \n def tweet_text ( self ) : \n try : \n return self . payload [ \"<STR_LIT:text>\" ] \n except TypeError : \n return None \n @ cached_property \n def tweet_text_with_links ( self ) : \n if self . tweet_text is None : \n return None \n ret = self . tweet_text \n ret = re . sub ( r\"<STR_LIT>\" , r\"<STR_LIT>\" , ret ) \n for url_info in self . urls : \n my_link = u\"<STR_LIT>\" . format ( \n url = url_info [ \"<STR_LIT>\" ] , \n display_url = url_info [ \"<STR_LIT>\" ] \n ) \n ret = re . sub ( r\"<STR_LIT>\" , my_link , ret , <NUM_LIT:1> ) \n ret = re . sub ( r\"<STR_LIT>\" , r\"<STR_LIT>\" , ret ) \n ret = re . sub ( r\"<STR_LIT>\" , r\"<STR_LIT>\" , ret ) \n return ret \n @ cached_property \n def urls ( self ) : \n try : \n return self . payload [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n except TypeError : \n return None \n except KeyError : \n return [ ] \n @ cached_property \n def has_country ( self ) : \n return self . country != None \n def set_attributes_from_altmetric_post ( self , post ) : \n self . tweet_id = post [ \"<STR_LIT>\" ] \n self . screen_name = post [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n self . tweet_timestamp = post [ \"<STR_LIT>\" ] \n if \"<STR_LIT>\" in post [ \"<STR_LIT>\" ] : \n self . country = post [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . get ( \"<STR_LIT>\" , None ) \n return self \n def __repr__ ( self ) : \n return u'<STR_LIT>' . format ( \n tweet_id = self . tweet_id , \n profile_id = self . profile_id , \n screen_name = self . screen_name , \n timestamp = self . tweet_timestamp ) \n def to_dict ( self ) : \n attributes_to_ignore = [ \n \"<STR_LIT>\" \n ] \n ret = dict_from_dir ( self , attributes_to_ignore ) \n return ret \n <mask0> = \"\"\"<STR_LIT>\"\"\" \n", "gt": "twitter_example_contents"}
{"input": "\n import os \n import numpy as np \n def load_gender_data ( ntrain = <NUM_LIT> , ntest = <NUM_LIT> ) : \n import pandas as pd \n file_loc = os . path . dirname ( os . path . realpath ( __file__ ) ) \n relative_path = \"<STR_LIT>\" \n fullpath = os . path . join ( file_loc , relative_path ) \n data = pd . read_csv ( fullpath , nrows = ntrain + ntest ) \n X = data [ '<STR_LIT:text>' ] . values \n X = [ str ( x ) for x in X ] \n Y = data [ '<STR_LIT>' ] . values \n trX = X [ : - ntest ] \n teX = X [ - ntest : ] \n trY = Y [ : - ntest ] \n teY = Y [ - ntest : ] \n return trX , teX , trY , teY \n def load_mnist ( data_dir = None ) : \n if data_dir is None : \n import urllib \n import gzip \n url = '<STR_LIT>' \n fnames = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] \n for fname in fnames : \n if not os . path . isfile ( fname ) : \n print '<STR_LIT>' , fname \n urllib . urlretrieve ( url + fname , fname ) \n data_dir = '<STR_LIT>' \n fd = gzip . open ( os . path . join ( data_dir , '<STR_LIT>' ) ) \n loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n trX = loaded [ <NUM_LIT:16> : ] . reshape ( ( <NUM_LIT> , - <NUM_LIT:1> ) ) \n fd = gzip . open ( os . path . join ( data_dir , '<STR_LIT>' ) ) \n loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n trY = loaded [ <NUM_LIT:8> : ] . reshape ( ( <NUM_LIT> ) ) \n fd = gzip . open ( os . path . join ( data_dir , '<STR_LIT>' ) ) \n loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n teX = loaded [ <NUM_LIT:16> : ] . reshape ( ( <NUM_LIT> , - <NUM_LIT:1> ) ) \n fd = gzip . open ( os . path . join ( data_dir , '<STR_LIT>' ) ) \n loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n teY = loaded [ <NUM_LIT:8> : ] . reshape ( ( <NUM_LIT> ) ) \n trX = trX / <NUM_LIT> \n teX = teX / <NUM_LIT> \n trX = trX . reshape ( - <NUM_LIT:1> , <NUM_LIT> , <NUM_LIT> ) \n teX = teX . reshape ( - <NUM_LIT:1> , <NUM_LIT> , <NUM_LIT> ) \n return trX , teX , trY , <mask0> \n", "gt": "teY"}
{"input": "\n import unittest \n import os \n import commands \n from utils import get_temporary_location \n from utils import delete_repository \n from gitpy import LocalRepository \n from gitpy import find_repository \n from gitpy . exceptions import GitException \n class EmptyRepositoryTest ( unittest . TestCase ) : \n def setUp ( self ) : \n self . dirname = get_temporary_location ( ) \n self . repo = LocalRepository ( self . dirname ) \n self . assertFalse ( os . path . exists ( self . dirname ) ) \n self . assertFalse ( self . repo . isValid ( ) ) \n def tearDown ( self ) : \n if os . path . exists ( self . dirname ) : \n delete_repository ( self . repo ) \n class BasicRepositories ( EmptyRepositoryTest ) : \n def testRepositoryInit ( self ) : \n self . repo . init ( ) \n self . assertTrue ( self . repo . isValid ( ) ) \n self . failUnless ( os . path . isdir ( self . dirname ) ) \n self . failUnless ( os . path . isdir ( os . path . join ( self . dirname , \"<STR_LIT>\" ) ) ) \n def testConfiguration ( self ) : \n self . repo . init ( ) \n self . repo . config . setParameter ( '<STR_LIT>' , <NUM_LIT:2> ) \n self . assertEquals ( self . repo . config . getParameter ( '<STR_LIT>' ) , '<STR_LIT:2>' ) \n def testRepositoryInitWhenExists ( self ) : \n os . mkdir ( self . dirname ) \n self . repo . init ( ) \n self . failUnless ( os . path . isdir ( self . dirname ) ) \n self . failUnless ( os . path . isdir ( os . path . join ( self . dirname , \"<STR_LIT>\" ) ) ) \n class ModifiedRepositoryTest ( EmptyRepositoryTest ) : \n FILENAME = \"<STR_LIT>\" \n def setUp ( self ) : \n super ( ModifiedRepositoryTest , self ) . setUp ( ) \n self . repo . init ( ) \n with open ( os . path . join ( self . repo . path , self . FILENAME ) , \"<STR_LIT:wb>\" ) as f : \n print >> f , \"<STR_LIT>\" \n self . assertFalse ( self . repo . isWorkingDirectoryClean ( ) ) \n class ModifiedRepositories ( ModifiedRepositoryTest ) : \n def testStatus ( self ) : \n untracked = self . repo . getUntrackedFiles ( ) \n self . assertEquals ( untracked , [ self . FILENAME ] ) \n def testAdding ( self ) : \n untracked_files = self . repo . getUntrackedFiles ( ) \n for u in untracked_files : \n self . repo . add ( u ) \n self . assertEquals ( self . repo . getStagedFiles ( ) , untracked_files ) \n self . assertFalse ( self . repo . isWorkingDirectoryClean ( ) ) \n def testCommitting ( self ) : \n self . repo . addAll ( ) \n self . assertNotEquals ( self . repo . getStagedFiles ( ) , [ ] ) \n c = self . repo . commit ( message = \"<STR_LIT>\" ) \n self . assertTrue ( self . repo . isWorkingDirectoryClean ( ) ) \n self . assertEquals ( self . repo . getStagedFiles ( ) , [ ] ) \n class CleaningUntrackedFiles ( ModifiedRepositoryTest ) : \n def _clean ( self ) : \n self . repo . cleanUntrackedFiles ( ) \n self . failIf ( self . repo . getUntrackedFiles ( ) ) \n def testCleaningUpUntrackedFiles ( self ) : \n with open ( os . path . join ( self . repo . path , \"<STR_LIT>\" ) , \"<STR_LIT:wb>\" ) as f : \n print >> f , \"<STR_LIT:data>\" \n self . failUnless ( self . repo . getUntrackedFiles ( ) ) \n self . _clean ( ) \n dirpath = os . path . join ( self . repo . path , \"<STR_LIT>\" ) \n os . mkdir ( dirpath ) \n self . _clean ( ) \n self . failIf ( os . path . exists ( dirpath ) ) \n class TestAPI ( ModifiedRepositoryTest ) : \n def test_find_repository ( self ) : \n prev_path = os . path . realpath ( \"<STR_LIT:.>\" ) \n subpath = os . path . join ( self . repo . path , \"<STR_LIT:a>\" , \"<STR_LIT:b>\" , \"<STR_LIT:c>\" ) \n os . makedirs ( subpath ) \n os . chdir ( subpath ) \n try : \n repo = find_repository ( ) \n finally : \n os . chdir ( prev_path ) \n self . failUnless ( repo . path == self . repo . path ) \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import logging \n from okcupyd . db import model , txn , with_txn \n log = logging . getLogger ( __name__ ) \n class UserAdapter ( object ) : \n def __init__ ( self , profile ) : \n self . profile = profile \n def build ( self , session ) : \n found = model . User . query_no_txn ( session , model . User . handle == \n self . profile . username ) \n if found : \n return found [ <NUM_LIT:0> ] \n else : \n return model . User ( okc_id = self . profile . id , \n handle = self . profile . username , \n age = self . profile . age , \n location = self . profile . location ) \n def get_no_txn ( self , session ) : \n return model . User . upsert_one_no_txn ( session , self . build ( session ) , \n id_key = '<STR_LIT>' ) \n get = with_txn ( get_no_txn ) \n class ThreadAdapter ( object ) : \n def __init__ ( self , thread ) : \n self . thread = thread \n def _get_thread ( self , session ) : \n initiator = UserAdapter ( self . thread . initiator ) . get_no_txn ( session ) \n respondent = UserAdapter ( self . thread . respondent ) . get_no_txn ( session ) \n message_thread = model . MessageThread ( okc_id = self . thread . id , \n initiator = initiator , \n respondent = respondent ) \n return model . MessageThread . upsert_one_no_txn ( session , message_thread , \n id_key = '<STR_LIT>' ) \n def _add_messages ( self , thread_model ) : \n existing_message_ids = set ( [ m . okc_id for m in thread_model . messages ] ) \n new_messages = [ message for message in self . thread . messages \n if message . id not in existing_message_ids ] \n new_message_models = [ ] \n for new_message in new_messages : \n from_initiator = thread_model . initiator . handle . lower ( ) == new_message . sender . username . lower ( ) \n sender , recipient = ( thread_model . initiator , \n thread_model . respondent ) if from_initiator else ( thread_model . respondent , \n thread_model . initiator ) \n new_message_model = model . Message ( okc_id = new_message . id , \n text = new_message . content , \n sender = sender , \n recipient = recipient , \n time_sent = new_message . time_sent ) \n new_message_models . append ( new_message_model ) \n thread_model . messages . append ( new_message_model ) \n return new_message_models \n def add_messages ( self ) : \n with txn ( ) as session : \n thread_model = model . MessageThread . find_no_txn ( session , \n self . thread . id , \n id_key = '<STR_LIT>' ) \n return self . _add_messages ( thread_model ) \n def get_thread ( self ) : \n with txn ( ) as session : \n thread_model = self . _get_thread ( session ) \n return thread_model , self . _add_messages ( <mask0> ) \n", "gt": "thread_model"}
{"input": "\n import logging \n from invoke import task \n import IPython \n from okcupyd import db \n from okcupyd import util \n from okcupyd . db import mailbox , model \n from okcupyd . user import User \n log = logging . getLogger ( __name__ ) \n @ task ( default = True ) \n def session ( ) : \n with db . txn ( ) as session : \n IPython . embed ( ) \n @ task \n def reset ( ) : \n util . enable_logger ( __name__ ) \n log . info ( db . Base . metadata . bind ) \n db . Base . metadata . drop_all ( ) \n db . Base . metadata . create_all ( ) \n @ task \n def sync ( ) : \n user = User ( ) \n mailbox . Sync ( user ) . all ( ) \n log . info ( model . Message . query ( model . User . okc_id == user . profile . id ) ) \n @ task \n def make ( ) : \n user = User ( ) \n user_model = model . User . from_profile ( user . profile ) \n user_model . upsert_model ( id_key = '<STR_LIT>' ) \n okcupyd_user = model . OKCupydUser ( user_id = user_model . id ) \n okcupyd_user . upsert_model ( id_key = '<STR_LIT>' ) \n return <mask0> \n", "gt": "okcupyd_user"}
{"input": "\n from . import util \n from okcupyd import User , photo \n @ util . use_cassette ( path = '<STR_LIT>' , \n match_on = util . match_on_no_body ) \n def test_photo_upload ( ) : \n uploader = photo . PhotoUploader ( ) \n upload_response_dict = uploader . upload_and_confirm ( '<STR_LIT>' ) \n assert int ( upload_response_dict [ '<STR_LIT:id>' ] ) > <NUM_LIT:0> \n @ util . use_cassette ( path = '<STR_LIT>' , match_on = util . match_on_no_body ) \n def test_photo_delete ( ) : \n user = User ( ) \n response_dict = user . photo . upload_and_confirm ( user . quickmatch ( ) . photo_infos [ <NUM_LIT:0> ] ) \n before_delete_photos = user . profile . photo_infos \n user . photo . delete ( response_dict [ '<STR_LIT:id>' ] ) \n user . profile . refresh ( ) \n assert len ( before_delete_photos ) - <NUM_LIT:1> == len ( user . profile . photo_infos ) \n def test_make_photo_uri_from_https_link ( ) : \n photo_info = photo . Info . from_cdn_uri ( \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n ) \n assert photo_info . id == <NUM_LIT> \n assert photo_info . thumb_nail_top == <NUM_LIT> \n @ util . use_cassette \n def test_photo_info_upload ( vcr_live_sleep ) : \n user = User ( ) \n response = user . photo . upload_and_confirm ( user . quickmatch ( ) . photo_infos [ <NUM_LIT:0> ] ) \n vcr_live_sleep ( <NUM_LIT:2> ) \n assert int ( response [ '<STR_LIT:id>' ] ) in [ pi . id for pi in user . profile . <mask0> ] \n", "gt": "photo_infos"}
{"input": "\n import theano \n import theano . tensor as T \n from theano . sandbox . rng_mrg import MRG_RandomStreams \n from theano . tensor . nnet . conv import conv2d \n from theano . tensor . signal . downsample import max_pool_2d \n from theano . tensor . shared_randomstreams import RandomStreams \n import numpy as np \n from toolbox import * \n from modelbase import * \n class LM_gru ( ModelLMBase ) : \n def __init__ ( self , data , hp ) : \n super ( LM_gru , self ) . __init__ ( self . __class__ . __name__ , data , hp ) \n self . n_h = <NUM_LIT> \n self . dropout = <NUM_LIT:0.5> \n self . params = Parameters ( ) \n self . hiddenstates = Parameters ( ) \n n_tokens = self . data [ '<STR_LIT>' ] \n n_h = self . n_h \n scale = hp . init_scale \n gates = <NUM_LIT:3> \n with self . hiddenstates : \n b1_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n b2_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n if hp . load_model and os . path . isfile ( self . filename ) : \n self . params . load ( self . filename ) \n else : \n with self . params : \n W_emb = shared_normal ( ( n_tokens , n_h ) , scale = scale ) \n W1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * <NUM_LIT> ) \n V1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * <NUM_LIT> ) \n b1 = shared_zeros ( ( n_h * gates ) ) \n W2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * <NUM_LIT> ) \n V2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * <NUM_LIT> ) \n b2 = shared_zeros ( ( n_h * gates , ) ) \n def lstm ( X , h , c , W , U , b ) : \n g_on = T . dot ( X , W ) + T . dot ( h , U ) + b \n i_on = T . nnet . sigmoid ( g_on [ : , : n_h ] ) \n f_on = T . nnet . sigmoid ( g_on [ : , n_h : <NUM_LIT:2> * n_h ] ) \n o_on = T . nnet . sigmoid ( g_on [ : , <NUM_LIT:2> * n_h : <NUM_LIT:3> * n_h ] ) \n c = f_on * c + i_on * T . tanh ( g_on [ : , <NUM_LIT:3> * n_h : ] ) \n h = o_on * T . tanh ( c ) \n return h , c \n def gru ( X , h , W , U , b ) : \n z_t = T . nnet . sigmoid ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n r_t = T . nnet . sigmoid ( T . dot ( X , W [ : , n_h : <NUM_LIT:2> * n_h ] ) + T . dot ( h , U [ : , n_h : <NUM_LIT:2> * n_h ] ) + b [ n_h : <NUM_LIT:2> * n_h ] ) \n h_t = T . tanh ( T . dot ( X , W [ : , <NUM_LIT:2> * n_h : <NUM_LIT:3> * n_h ] ) + r_t * T . dot ( h , U [ : , <NUM_LIT:2> * n_h : <NUM_LIT:3> * n_h ] ) + b [ <NUM_LIT:2> * n_h : <NUM_LIT:3> * n_h ] ) \n return ( <NUM_LIT:1> - z_t ) * h + z_t * h_t \n def sgru ( X , h , W , U , b ) : \n z_t = T . tanh ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n h_t = T . tanh ( T . dot ( X , W [ : , <NUM_LIT:1> * n_h : <NUM_LIT:2> * n_h ] ) + T . dot ( h , U [ : , <NUM_LIT:1> * n_h : <NUM_LIT:2> * n_h ] ) + b [ <NUM_LIT:1> * n_h : <NUM_LIT:2> * n_h ] ) \n return z_t * h_t \n def model ( x , p , p_dropout ) : \n input_size = x . shape [ <NUM_LIT:1> ] \n h0 = p . W_emb [ x ] \n h0 = dropout ( h0 , p_dropout ) \n cost , h1 , h2 = [ <NUM_LIT:0.> , b1_h , b2_h ] \n for t in xrange ( <NUM_LIT:0> , self . hp . seq_size ) : \n if t >= self . hp . warmup_size : \n pyx = softmax ( T . dot ( dropout ( h2 , p_dropout ) , T . transpose ( p . W_emb ) ) ) \n cost += T . sum ( T . nnet . categorical_crossentropy ( pyx , theano_one_hot ( x [ t ] , n_tokens ) ) ) \n h1 = gru ( h0 [ t ] , h1 , p . W1 , p . V1 , p . b1 ) \n h2 = gru ( dropout ( h1 , p_dropout ) , h2 , p . W2 , p . V2 , p . b2 ) \n h_updates = [ ( b1_h , h1 ) , ( b2_h , h2 ) ] \n return cost , h_updates \n cost , h_updates = model ( self . X , self . params , self . dropout ) \n te_cost , te_h_updates = model ( self . X , self . params , <NUM_LIT:0.> ) \n self . compile ( cost , te_cost , h_updates , <mask0> ) \n", "gt": "te_h_updates"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import csv \n import sys \n def csvOutput ( queryResult , separator = '<STR_LIT:U+002C>' , quote = '<STR_LIT:\">' ) : \n \"\"\"<STR_LIT>\"\"\" \n csvWriter = csv . writer ( sys . stdout , delimiter = separator , quotechar = quote , \n quoting = csv . QUOTE_MINIMAL ) \n for line in queryResult : \n csvWriter . writerow ( <mask0> ) \n", "gt": "line"}
{"input": "\n import sys , os , stat \n import pythoncom \n from win32com . shell import shell , shellcon \n import commctrl \n import winerror \n from win32com . server . util import wrap \n from pywintypes import IID \n IPersist_Methods = [ \"<STR_LIT>\" ] \n IColumnProvider_Methods = IPersist_Methods + [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n class ColumnProvider : \n _reg_progid_ = \"<STR_LIT>\" \n _reg_desc_ = \"<STR_LIT>\" \n _reg_clsid_ = IID ( \"<STR_LIT>\" ) \n _com_interfaces_ = [ pythoncom . IID_IPersist , \n shell . IID_IColumnProvider , \n ] \n _public_methods_ = IColumnProvider_Methods \n def GetClassID ( self ) : \n return self . _reg_clsid_ \n def Initialize ( self , colInit ) : \n flags , reserved , name = colInit \n print \"<STR_LIT>\" , name \n def GetColumnInfo ( self , index ) : \n if index in [ <NUM_LIT:0> , <NUM_LIT:1> ] : \n if index == <NUM_LIT:0> : \n ext = \"<STR_LIT>\" \n else : \n ext = \"<STR_LIT>\" \n title = ext + \"<STR_LIT>\" \n description = \"<STR_LIT>\" % ext \n col_id = ( self . _reg_clsid_ , \n index ) \n col_info = ( \n col_id , \n pythoncom . VT_I4 , \n commctrl . LVCFMT_RIGHT , \n <NUM_LIT:20> , \n shellcon . SHCOLSTATE_TYPE_INT | shellcon . SHCOLSTATE_SECONDARYUI , \n title , \n description ) \n return col_info \n return None \n def GetItemData ( self , colid , colData ) : \n fmt_id , pid = colid \n fmt_id == self . _reg_clsid_ \n flags , attr , reserved , ext , name = colData \n if ext . lower ( ) not in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] : \n return None \n if pid == <NUM_LIT:0> : \n ext = \"<STR_LIT>\" \n else : \n ext = \"<STR_LIT>\" \n check_file = os . path . splitext ( name ) [ <NUM_LIT:0> ] + ext \n try : \n st = os . stat ( check_file ) \n return st [ stat . ST_SIZE ] \n except OSError : \n return None \n def DllRegisterServer ( ) : \n import _winreg \n key = _winreg . CreateKey ( _winreg . HKEY_CLASSES_ROOT , \n \"<STR_LIT>\" + str ( ColumnProvider . _reg_clsid_ ) ) \n _winreg . SetValueEx ( key , None , <NUM_LIT:0> , _winreg . REG_SZ , ColumnProvider . _reg_desc_ ) \n print ColumnProvider . _reg_desc_ , \"<STR_LIT>\" \n def DllUnregisterServer ( ) : \n import _winreg \n try : \n key = _winreg . DeleteKey ( _winreg . HKEY_CLASSES_ROOT , \n \"<STR_LIT>\" + str ( ColumnProvider . _reg_clsid_ ) ) \n except WindowsError , details : \n import errno \n if details . errno != errno . ENOENT : \n raise \n print ColumnProvider . _reg_desc_ , \"<STR_LIT>\" \n if __name__ == '<STR_LIT:__main__>' : \n from win32com . server import register \n register . UseCommandLine ( ColumnProvider , \n finalize_register = DllRegisterServer , \n finalize_unregister = <mask0> ) \n", "gt": "DllUnregisterServer"}
{"input": "\n def __load ( ) : \n import imp , os , sys \n try : \n dirname = os . path . dirname ( __loader__ . archive ) \n except NameError : \n dirname = sys . prefix \n path = os . path . join ( dirname , '<STR_LIT>' ) \n mod = imp . load_dynamic ( __name__ , path ) \n __load ( ) \n del <mask0> \n", "gt": "__load"}
{"input": "\n import logging \n class LoggerFactory ( object ) : \n _isSetup = False \n def __init__ ( self , level = logging . DEBUG ) : \n if LoggerFactory . _isSetup is False : \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n logger . setLevel ( level ) \n formatter = logging . Formatter ( '<STR_LIT>' ) \n ch = logging . StreamHandler ( ) \n ch . setLevel ( level ) \n ch . setFormatter ( formatter ) \n logger . addHandler ( ch ) \n LoggerFactory . _isSetup = True \n def getLogger ( self , name , level = logging . DEBUG ) : \n logger = logging . getLogger ( \"<STR_LIT>\" % name ) \n logger . setLevel ( level ) \n return <mask0> \n", "gt": "logger"}
{"input": "\n from . functions <mask0> * \n", "gt": "import"}
{"input": "\n from __future__ import division \n import numpy as np \n from pysd import functions \n def time ( ) : \n return _t \n def flowa ( ) : \n \"\"\"<STR_LIT>\"\"\" \n return <NUM_LIT:0.1> \n def stocka ( ) : \n return _state [ '<STR_LIT>' ] \n def _stocka_init ( ) : \n return - <NUM_LIT:5> \n def _dstocka_dt ( ) : \n return flowa ( ) \n def test_exp ( ) : \n \"\"\"<STR_LIT>\"\"\" \n return np . exp ( stocka ( ) ) \n def final_time ( ) : \n \"\"\"<STR_LIT>\"\"\" \n return <NUM_LIT:100> \n def initial_time ( ) : \n \"\"\"<STR_LIT>\"\"\" \n return <NUM_LIT:0> \n def saveper ( ) : \n \"\"\"<STR_LIT>\"\"\" \n return time_step ( ) \n def time_step ( ) : \n \"\"\"<STR_LIT>\"\"\" \n <mask0> <NUM_LIT:1> \n", "gt": "return"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n __all__ = [ \n '<STR_LIT>' \n ] \n <mask0> = '<STR_LIT>' \n", "gt": "__version__"}
{"input": "\n from myapp import utils \n module_name = utils . getFinalName ( __name__ ) \n module = utils . getModule ( __name__ , subdomain = module_name ) \n import views \n import views . <mask0> \n", "gt": "morepages"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n import os \n import subprocess \n def perform_testing ( config ) : \n requirements = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n print \"<STR_LIT>\" \n print canwrite ( config [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) , \"<STR_LIT>\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n for req in requirements : \n print checkcommand ( requirements [ req ] ) , req \n sys . exit ( <NUM_LIT:0> ) \n def canwrite ( path ) : \n try : \n ret = booltostatus ( os . access ( path , os . W_OK | os . X_OK ) ) \n except : \n ret = False \n finally : \n return ret \n def booltostatus ( inbool ) : \n if inbool : \n return \"<STR_LIT>\" \n else : \n return \"<STR_LIT>\" \n def checkcommand ( com ) : \n proc = subprocess . Popen ( \n [ \n '<STR_LIT>' , \n str ( com ) \n ] , \n stderr = subprocess . PIPE , \n stdout = subprocess . PIPE \n ) \n return booltostatus ( len ( proc . stdout . <mask0> ( ) ) > <NUM_LIT:0> ) \n", "gt": "read"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from collections import namedtuple \n from uuid import uuid4 \n from django . http import HttpResponse \n from django . contrib . gis . db . models . query import GeoQuerySet \n from django . contrib . gis . db . models import GeometryField \n from django import forms as f \n import json \n from django . shortcuts import render_to_response \n from ga_ows . views import common \n from ga_ows . utils import MultipleValueField , BBoxField , CaseInsensitiveDict \n from lxml import etree \n from ga_ows . views . common import RequestForm , CommonParameters , GetCapabilitiesMixin \n from osgeo import ogr \n from django . conf import settings \n from tempfile import gettempdir \n from django . db import connections \n import re \n from lxml import etree \n import os \n class InputParameters ( RequestForm ) : \n \"\"\"<STR_LIT:U+0020>\"\"\" \n srs_name = f . CharField ( ) \n input_format = f . CharField ( ) \n srs_format = f . CharField ( required = False ) \n @ classmethod \n def from_request ( cls , request ) : \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' , '<STR_LIT>' ) \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' , \"<STR_LIT>\" ) \n class PresentationParameters ( RequestForm ) : \n count = f . IntegerField ( ) \n start_index = f . IntegerField ( ) \n max_features = f . IntegerField ( ) \n output_format = f . CharField ( ) \n @ classmethod \n def from_request ( cls , request ) : \n request [ '<STR_LIT:count>' ] = int ( request . get ( '<STR_LIT:count>' , '<STR_LIT:1>' ) ) \n request [ '<STR_LIT>' ] = int ( request . get ( '<STR_LIT>' , '<STR_LIT:1>' ) ) \n request [ '<STR_LIT>' ] = int ( request . get ( '<STR_LIT>' , '<STR_LIT:1>' ) ) \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' , \"<STR_LIT>\" ) \n class AdHocQueryParameters ( RequestForm ) : \n type_names = MultipleValueField ( ) \n aliases = MultipleValueField ( required = False ) \n filter = f . CharField ( required = False ) \n filter_language = f . CharField ( required = False ) \n resource_id = f . CharField ( required = False ) \n bbox = BBoxField ( ) \n sort_by = f . CharField ( required = False ) \n @ classmethod \n def from_request ( cls , request ) : \n request [ '<STR_LIT>' ] = request . getlist ( '<STR_LIT>' ) \n request [ '<STR_LIT>' ] = request . getlist ( '<STR_LIT>' ) \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' ) \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' ) \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' ) \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' ) \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' ) \n class StoredQueryParameters ( RequestForm ) : \n stored_query_id = f . CharField ( required = False ) \n @ classmethod \n def from_request ( cls , request ) : \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' ) \n class GetFeatureByIdParameters ( RequestForm ) : \n feature_id = f . CharField ( ) \n @ classmethod \n def from_request ( cls , request ) : \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT:id>' ) \n class ResolveParameters ( RequestForm ) : \n resolve = f . CharField ( required = False ) \n resolve_depth = f . IntegerField ( ) \n resolve_timeout = f . FloatField ( ) \n @ classmethod \n def from_request ( cls , request ) : \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' ) \n request [ '<STR_LIT>' ] = int ( request . get ( '<STR_LIT>' , '<STR_LIT:0>' ) ) \n request [ '<STR_LIT>' ] = float ( request . get ( '<STR_LIT>' , '<STR_LIT:0>' ) ) \n class CannotLockAllFeatures ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n class DuplicateStoredQueryIdValue ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n class DuplicateStoredQueryParameterName ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n class FeaturesNotLocked ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n class InvalidLockId ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n class InvalidValue ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n class LockHasExpired ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n class OperationParsingFailed ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n class OperationProcessingFailed ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n class ResponseCacheExpired ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n class OperationNotSupported ( common . OWSException ) : \n \"\"\"<STR_LIT>\"\"\" \n FeatureDescription = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT:name>' , '<STR_LIT:title>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ) \n StoredQueryParameter = namedtuple ( \"<STR_LIT>\" , ( '<STR_LIT:type>' , '<STR_LIT:name>' , '<STR_LIT:title>' , '<STR_LIT>' , '<STR_LIT>' ) ) \n StoredQueryExpression = namedtuple ( \"<STR_LIT>\" , ( '<STR_LIT:text>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ) \n StoredQueryDescription = namedtuple ( \"<STR_LIT>\" , ( '<STR_LIT:name>' , '<STR_LIT>' , '<STR_LIT:title>' , '<STR_LIT>' ) ) \n class WFSAdapter ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def get_feature_descriptions ( self , request , * types ) : \n raise OperationNotSupported . at ( '<STR_LIT>' , '<STR_LIT>' ) \n def list_stored_queries ( self , request ) : \n \"\"\"<STR_LIT>\"\"\" \n queries = dict ( [ ( q [ <NUM_LIT:3> : ] , [ ] ) for q in filter ( lambda x : x . startswith ( \"<STR_LIT>\" ) , \n reduce ( \n list . __add__ , \n [ c . __dict__ . keys ( ) for c in self . __class__ . mro ( ) ] \n ) \n ) ] ) \n return queries \n def get_features ( self , request , parms ) : \n raise OperationNotSupported . at ( '<STR_LIT>' , \"<STR_LIT>\" ) \n def supports_feature_versioning ( self ) : \n return False \n class GeoDjangoWFSAdapter ( WFSAdapter ) : \n def __init__ ( self , models ) : \n self . models = { } \n self . srids = { } \n self . geometries = { } \n for model in models : \n self . models [ model . _meta . app_label + \"<STR_LIT::>\" + model . _meta . object_name ] = model \n for field in model . _meta . fields : \n if isinstance ( field , GeometryField ) : \n self . geometries [ model . _meta . app_label + \"<STR_LIT::>\" + model . _meta . object_name ] = field \n self . srids [ model . _meta . app_label + \"<STR_LIT::>\" + model . _meta . object_name ] = field . srid \n def list_stored_queries ( self , request ) : \n sq = super ( GeoDjangoWFSAdapter , self ) . list_stored_queries ( request ) \n fts = list ( self . models . keys ( ) ) \n for k in sq . keys ( ) : \n sq [ k ] = StoredQueryDescription ( name = k , feature_types = fts , title = k , parameters = [ ] ) \n return sq \n def get_feature_descriptions ( self , request , * types ) : \n namespace = request . build_absolute_uri ( ) . split ( '<STR_LIT:?>' ) [ <NUM_LIT:0> ] + \"<STR_LIT>\" \n for model in self . models . values ( ) : \n if model . objects . count ( ) > <NUM_LIT:0> : \n extent = model . objects . extent ( ) \n else : \n extent = ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> ) \n yield FeatureDescription ( \n ns = namespace , \n ns_name = model . _meta . app_label , \n name = model . _meta . object_name , \n abstract = model . __doc__ , \n title = model . _meta . verbose_name , \n keywords = [ ] , \n srs = self . srids [ model . _meta . app_label + \"<STR_LIT::>\" + model . _meta . object_name ] , \n bbox = extent , \n schema = namespace \n ) \n def get_features ( self , request , parms ) : \n if parms . cleaned_data [ '<STR_LIT>' ] : \n squid = \"<STR_LIT>\" + parms . cleaned_data [ '<STR_LIT>' ] \n try : \n return self . __getattribute__ ( squid ) ( request , parms ) \n except AttributeError : \n raise OperationNotSupported . at ( '<STR_LIT>' , '<STR_LIT>' . format ( squid = squid ) ) \n else : \n return self . AdHocQuery ( request , parms ) \n def AdHocQuery ( self , request , parms ) : \n type_names = parms . cleaned_data [ '<STR_LIT>' ] \n flt = parms . cleaned_data [ '<STR_LIT>' ] \n flt_lang = parms . cleaned_data [ '<STR_LIT>' ] \n bbox = parms . cleaned_data [ '<STR_LIT>' ] \n sort_by = parms . cleaned_data [ '<STR_LIT>' ] \n count = parms . cleaned_data [ '<STR_LIT:count>' ] \n if not count : \n count = parms . cleaned_data [ '<STR_LIT>' ] \n start_index = parms . cleaned_data [ '<STR_LIT>' ] \n srs_name = parms . cleaned_data [ '<STR_LIT>' ] \n srs_format = parms . cleaned_data [ '<STR_LIT>' ] \n model = self . models [ type_names [ <NUM_LIT:0> ] ] \n geometry_field = self . geometries [ type_names [ <NUM_LIT:0> ] ] \n query_set = model . objects . all ( ) \n if bbox : \n mnx , mny , mxx , mxy = bbox \n query_set . filter ( ** { geometry_field . name + \"<STR_LIT>\" : \n \"<STR_LIT>\" . format ( \n mnx = mnx , \n mny = mny , \n mxx = mxx , \n mxy = mxy ) \n } ) \n if flt : \n flt = json . loads ( flt ) \n query_set = query_set . filter ( ** flt ) \n if sort_by and '<STR_LIT:U+002C>' in sort_by : \n sort_by = sort_by . split ( '<STR_LIT:U+002C>' ) \n query_set = query_set . order_by ( * sort_by ) \n elif sort_by : \n query_set = query_set . order_by ( sort_by ) \n if start_index and count : \n query_set = query_set [ start_index : start_index + count ] \n elif start_index : \n query_set = query_set [ start_index : ] \n elif count : \n query_set = query_set [ : count ] \n if srs_name : \n if ( not srs_format or srs_format == '<STR_LIT>' ) and srs_name != geometry_field . srid : \n if srs_name . lower ( ) . startswith ( '<STR_LIT>' ) : \n srs_name = srs_name [ <NUM_LIT:5> : ] \n query_set . transform ( int ( srs_name ) ) \n return query_set \n def SQ_GetFeatureById ( self , request , parms ) : \n my_parms = GetFeatureByIdParameters . create ( request . REQUEST ) \n typename , pk = my_parms . cleaned_data [ '<STR_LIT>' ] . split ( '<STR_LIT:.>' ) \n return self . models [ typename ] . objects . filter ( pk = int ( pk ) ) \n class WFSBase ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n adapter = None \n class DescribeFeatureTypeMixin ( WFSBase ) : \n \"\"\"<STR_LIT>\"\"\" \n class Parameters ( \n CommonParameters \n ) : \n type_names = MultipleValueField ( ) \n output_format = f . CharField ( ) \n @ classmethod \n def from_request ( cls , request ) : \n request [ '<STR_LIT>' ] = request . getlist ( '<STR_LIT>' ) + request . getlist ( '<STR_LIT>' ) \n request [ '<STR_LIT>' ] = request . get ( '<STR_LIT>' , \"<STR_LIT>\" ) \n def _parse_xml_DescribeFeatureType ( self , request ) : \n \"\"\"<STR_LIT>\"\"\" \n def add_ns ( it , ns ) : \n x = it . split ( '<STR_LIT::>' ) \n if len ( x ) > <NUM_LIT:1> : \n return ns [ x [ <NUM_LIT:0> ] ] , x [ <NUM_LIT:1> ] \n else : \n return '<STR_LIT>' , x \n root = etree . fromstring ( request ) \n xmlns = root . get ( '<STR_LIT>' ) \n output_format = root . get ( '<STR_LIT>' , '<STR_LIT>' ) \n if xmlns is not None : \n xmlns = \"<STR_LIT:{>\" + xmlns + \"<STR_LIT:}>\" \n else : \n xmlns = \"<STR_LIT>\" \n namespaces = { } \n for name , value in root . attrib . items ( ) : \n if name . startswith ( xmlns ) : \n namespaces [ value ] = name [ len ( xmlns ) : ] \n type_names = root . get ( '<STR_LIT>' ) \n if type_names is not None : \n type_names = [ add_ns ( n , namespaces ) for n in type_names . split ( '<STR_LIT:U+002C>' ) ] \n else : \n type_names = [ ] \n for elt in root : \n if elt . tag . endswith ( \"<STR_LIT>\" ) : \n namespace , name = elt . text . split ( \"<STR_LIT::>\" ) \n namespace = namespaces [ namespace ] \n type_names . append ( ( namespace , name ) ) \n if not len ( type_names ) : \n type_names = '<STR_LIT:all>' \n return DescribeFeatureTypeMixin . Parameters . create ( CaseInsensitiveDict ( { \"<STR_LIT>\" : type_names , \"<STR_LIT>\" : output_format } ) ) \n def _response_xml_DescribeFeatureType ( self , response ) : \n return render_to_response ( \"<STR_LIT>\" , { \"<STR_LIT>\" : list ( response ) } ) \n def _response_json_DescribeFeatureType ( self , response , callback = None ) : \n rsp = [ ] \n for feature_type in response : \n rsp . append ( { \n \"<STR_LIT>\" : feature_type . schema , \n \"<STR_LIT:name>\" : feature_type . name , \n \"<STR_LIT>\" : feature_type . abstract , \n \"<STR_LIT:title>\" : feature_type . title , \n \"<STR_LIT>\" : feature_type . ns_name \n } ) \n if callback is not None : \n return HttpResponse ( callback + \"<STR_LIT:(>\" + json . dumps ( rsp ) + \"<STR_LIT:)>\" , mimetype = '<STR_LIT>' ) \n else : \n return HttpResponse ( json . dumps ( rsp ) , mimetype = '<STR_LIT:application/json>' ) \n def DescribeFeatureType ( self , request , kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n if '<STR_LIT>' in kwargs : \n parms = self . _parse_xml_DescribeFeatureType ( kwargs [ '<STR_LIT>' ] ) \n else : \n parms = DescribeFeatureTypeMixin . Parameters . create ( kwargs ) \n response = self . adapter . get_feature_descriptions ( request , * parms . cleaned_data [ '<STR_LIT>' ] ) \n if parms . cleaned_data [ '<STR_LIT>' ] . endswith ( '<STR_LIT>' ) : \n if '<STR_LIT>' in kwargs : \n return self . _response_json_DescribeFeatureType ( response , callback = kwargs [ '<STR_LIT>' ] ) \n elif '<STR_LIT>' in kwargs : \n return self . _response_json_DescribeFeatureType ( response , callback = kwargs [ '<STR_LIT>' ] ) \n else : \n return self . _response_json_DescribeFeatureType ( response ) \n else : \n return self . _response_xml_DescribeFeatureType ( response ) \n class GetFeatureMixin ( WFSBase ) : \n \"\"\"<STR_LIT>\"\"\" \n class Parameters ( \n CommonParameters , \n InputParameters , \n PresentationParameters , \n AdHocQueryParameters , \n StoredQueryParameters \n ) : \n pass \n def _parse_xml_GetFeature ( self , request ) : \n \"\"\"<STR_LIT:U+0020>\"\"\" \n raise OperationNotSupported . at ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def GetFeature ( self , request , kwargs ) : \n \"\"\"<STR_LIT:U+0020>\"\"\" \n mimetypes = { \n '<STR_LIT>' : '<STR_LIT:application/json>' \n } \n if '<STR_LIT>' in kwargs : \n parms = self . _parse_xml_GetFeature ( kwargs [ '<STR_LIT>' ] ) \n else : \n parms = GetFeatureMixin . Parameters . create ( kwargs ) \n response = self . adapter . get_features ( request , parms ) \n if isinstance ( response , GeoQuerySet ) : \n layer = None \n db_params = settings . DATABASES [ response . db ] \n if db_params [ '<STR_LIT>' ] . endswith ( '<STR_LIT>' ) : \n from psycopg2 . extensions import adapt \n query , parameters = response . query . get_compiler ( response . db ) . as_sql ( ) \n parameters = tuple ( [ adapt ( p ) for p in parameters ] ) \n query = query % parameters \n drv = ogr . GetDriverByName ( \"<STR_LIT>\" ) \n connection_string = \"<STR_LIT>\" . format ( db = db_params [ '<STR_LIT>' ] ) \n if '<STR_LIT>' in db_params and db_params [ '<STR_LIT>' ] : \n connection_string += \"<STR_LIT>\" . format ( host = db_params [ '<STR_LIT>' ] ) \n if '<STR_LIT>' in db_params and db_params [ '<STR_LIT>' ] : \n connection_string += \"<STR_LIT>\" . format ( port = db_params [ '<STR_LIT>' ] ) \n if '<STR_LIT>' in db_params and db_params [ '<STR_LIT>' ] : \n connection_string += \"<STR_LIT>\" . format ( user = db_params [ '<STR_LIT>' ] ) \n if '<STR_LIT>' in db_params and db_params [ '<STR_LIT>' ] : \n connection_string += \"<STR_LIT>\" . format ( password = db_params [ '<STR_LIT>' ] ) \n conn = drv . Open ( connection_string ) \n layer = conn . ExecuteSQL ( query . encode ( '<STR_LIT:ascii>' ) ) \n elif db_params [ '<STR_LIT>' ] . endswith ( '<STR_LIT>' ) : \n from psycopg2 . extensions import adapt \n query , parameters = response . query . get_compiler ( response . db ) . as_sql ( ) \n parameters = tuple ( [ adapt ( p ) for p in parameters ] ) \n query = query % parameters \n drv = ogr . GetDriverByName ( \"<STR_LIT>\" ) \n conn = drv . Open ( db_params [ '<STR_LIT>' ] ) \n layer = conn . ExecuteSQL ( query ) \n else : \n layer = response . GetLayerByIndex ( <NUM_LIT:0> ) \n drivers = dict ( [ ( ogr . GetDriver ( drv ) . GetName ( ) , ogr . GetDriver ( drv ) ) for drv in range ( ogr . GetDriverCount ( ) ) if ogr . GetDriver ( drv ) . TestCapability ( ogr . ODrCCreateDataSource ) ] ) \n output_format = parms . cleaned_data [ '<STR_LIT>' ] . decode ( '<STR_LIT:ascii>' ) \n if '<STR_LIT>' in output_format or '<STR_LIT>' in output_format : \n tmpname = \"<STR_LIT>\" . format ( tmpdir = gettempdir ( ) , uuid = uuid4 ( ) , output_format = '<STR_LIT>' , sep = os . path . sep ) \n drv = ogr . GetDriverByName ( \"<STR_LIT>\" ) \n ds = drv . CreateDataSource ( tmpname ) \n l2 = ds . CopyLayer ( layer , '<STR_LIT>' ) \n l2 . SyncToDisk ( ) \n del ds \n responsef = open ( tmpname ) \n rdata = responsef . read ( ) \n responsef . close ( ) \n os . unlink ( tmpname ) \n return HttpResponse ( rdata , mimetype = output_format ) \n elif output_format in drivers : \n tmpname = \"<STR_LIT>\" . format ( tmpdir = gettempdir ( ) , uuid = uuid4 ( ) , output_format = output_format , sep = os . path . sep ) \n drv = drivers [ output_format ] \n ds = drv . CreateDataSource ( tmpname ) \n l2 = ds . CopyLayer ( layer , '<STR_LIT>' ) \n l2 . SyncToDisk ( ) \n del ds \n responsef = open ( tmpname ) \n rdata = responsef . read ( ) \n responsef . close ( ) \n os . unlink ( tmpname ) \n return HttpResponse ( rdata , mimetype = mimetypes . get ( output_format , '<STR_LIT>' ) ) \n else : \n raise OperationProcessingFailed . at ( '<STR_LIT>' , '<STR_LIT>' . format ( of = output_format , formats = drivers . keys ( ) ) ) \n class ListStoredQueriesMixin ( WFSBase ) : \n \"\"\"<STR_LIT>\"\"\" \n def ListStoredQueries ( self , request , kwargs ) : \n \"\"\"<STR_LIT:U+0020>\"\"\" \n queries = self . adapter . list_stored_queries ( request ) \n response = etree . Element ( \"<STR_LIT>\" ) \n for query , description in queries . items ( ) : \n sub = etree . SubElement ( response , \"<STR_LIT>\" ) \n etree . SubElement ( sub , \"<STR_LIT>\" ) . text = query \n for feature_type in description . feature_types : \n etree . SubElement ( sub , '<STR_LIT>' ) . text = feature_type \n return HttpResponse ( etree . tostring ( response , pretty_print = True ) , mimetype = '<STR_LIT>' ) \n class DescribeStoredQueriesMixin ( WFSBase ) : \n class Parameters ( CommonParameters ) : \n stored_query_id = MultipleValueField ( ) \n @ classmethod \n def from_request ( cls , request ) : \n request [ '<STR_LIT>' ] = request . getlist ( '<STR_LIT>' ) \n def DescribeStoredQueries ( self , request , kwargs ) : \n parms = DescribeStoredQueriesMixin . Parameters . create ( kwargs ) \n inspected_queries = parms . cleaned_data [ '<STR_LIT>' ] \n response = etree . Element ( '<STR_LIT>' ) \n for query , description in filter ( lambda ( x , y ) : x in inspected_queries , self . adapter . list_stored_queries ( request ) . items ( ) ) : \n desc = etree . SubElement ( response , \"<STR_LIT>\" ) \n etree . SubElement ( desc , '<STR_LIT>' ) . text = query \n for parameter in description . parameters : \n p = etree . SubElement ( desc , \"<STR_LIT>\" , attrib = { \"<STR_LIT:name>\" : parameter . name , \"<STR_LIT:type>\" : parameter . type } ) \n etree . SubElement ( p , '<STR_LIT>' ) . text = parameter . title \n etree . SubElement ( p , '<STR_LIT>' ) . text = parameter . abstractS \n if parameter . query_expression : \n etree . SubElement ( p , \"<STR_LIT>\" , attrib = { \n \"<STR_LIT>\" : parameter . query_expression . private == True , \n \"<STR_LIT>\" : parameter . query_expression . language , \n \"<STR_LIT>\" : '<STR_LIT:U+0020>' . join ( parameter . query_expression . return_feature_types ) \n } ) . text = parameter . query_expression . text \n return HttpResponse ( etree . tostring ( response , pretty_print = True ) , mimetype = '<STR_LIT>' ) \n class CreateStoredQuery ( WFSBase ) : \n def CreateStoredQuery ( self , request , kwargs ) : \n raise OperationNotSupported . at ( \"<STR_LIT>\" ) \n class DropStoredQuery ( WFSBase ) : \n def DropStoredQuery ( self , request , kwargs ) : \n raise OperationNotSupported . at ( \"<STR_LIT>\" ) \n class TransactionMixin ( WFSBase ) : \n def Transaction ( self , request , kwargs ) : \n \"\"\"<STR_LIT:U+0020>\"\"\" \n raise OperationNotSupported . at ( '<STR_LIT>' ) \n class GetFeatureWithLockMixin ( WFSBase ) : \n def GetFeatureWithLock ( self , request , kwargs ) : \n raise OperationNotSupported . at ( \"<STR_LIT>\" ) \n class LockFeatureMixin ( WFSBase ) : \n def LockFeature ( self , request , kwargs ) : \n raise OperationNotSupported . at ( '<STR_LIT>' ) \n class GetPropertyValueMixin ( WFSBase ) : \n class Parameters ( StoredQueryParameters , AdHocQueryParameters ) : \n value_reference = f . CharField ( ) \n resolve_path = f . CharField ( required = False ) \n def from_request ( cls , request ) : \n request [ '<STR_LIT>' ] = request [ '<STR_LIT>' ] \n request [ '<STR_LIT>' ] = request [ '<STR_LIT>' ] \n def GetPropertyValue ( self , request , kwargs ) : \n raise OperationNotSupported . at ( '<STR_LIT>' ) \n class WFS ( \n common . OWSView , \n GetCapabilitiesMixin , \n DescribeFeatureTypeMixin , \n DescribeStoredQueriesMixin , \n GetFeatureMixin , \n ListStoredQueriesMixin , \n GetPropertyValueMixin \n ) : \n \"\"\"<STR_LIT>\"\"\" \n adapter = None \n models = None \n title = None \n keywords = [ ] \n fees = None \n access_constraints = None \n provider_name = None \n addr_street = None \n addr_city = None \n addr_admin_area = None \n addr_postcode = None \n addr_country = None \n addr_email = None \n def __init__ ( self , ** kwargs ) : \n common . OWSView . __init__ ( self , ** kwargs ) \n if self . models : \n self . adapter = GeoDjangoWFSAdapter ( self . models ) \n def get_capabilities_response ( self , request , params ) : \n return render_to_response ( '<STR_LIT>' , { \n \"<STR_LIT:title>\" : self . title , \n \"<STR_LIT>\" : self . keywords , \n \"<STR_LIT>\" : self . fees , \n \"<STR_LIT>\" : self . access_constraints , \n \"<STR_LIT>\" : request . build_absolute_uri ( ) . split ( '<STR_LIT:?>' ) [ <NUM_LIT:0> ] , \n \"<STR_LIT>\" : [ ogr . GetDriver ( drv ) . GetName ( ) for drv in range ( ogr . GetDriverCount ( ) ) if ogr . GetDriver ( drv ) . TestCapability ( ogr . ODrCCreateDataSource ) ] , \n \"<STR_LIT>\" : self . addr_street , \n \"<STR_LIT>\" : self . addr_city , \n \"<STR_LIT>\" : self . addr_admin_area , \n \"<STR_LIT>\" : self . addr_postcode , \n \"<STR_LIT>\" : self . addr_country , \n \"<STR_LIT>\" : False , \n \"<STR_LIT>\" : False , \n '<STR_LIT>' : self . adapter . get_feature_descriptions ( request ) \n } ) \n class WFST ( WFS , TransactionMixin , GetFeatureWithLockMixin , LockFeatureMixin ) : \n \"\"\"<STR_LIT>\"\"\" \n def get_capabilities_response ( self , request , params ) : \n return render_to_response ( '<STR_LIT>' , { \n \"<STR_LIT:title>\" : self . title , \n \"<STR_LIT>\" : self . keywords , \n \"<STR_LIT>\" : self . fees , \n \"<STR_LIT>\" : self . access_constraints , \n \"<STR_LIT>\" : request . build_absolute_uri ( ) . split ( '<STR_LIT:?>' ) [ <NUM_LIT:0> ] , \n \"<STR_LIT>\" : [ ogr . GetDriver ( drv ) . GetName ( ) for drv in range ( ogr . GetDriverCount ( ) ) if ogr . GetDriver ( drv ) . TestCapability ( ogr . ODrCCreateDataSource ) ] , \n \"<STR_LIT>\" : self . addr_street , \n \"<STR_LIT>\" : self . addr_city , \n \"<STR_LIT>\" : self . addr_admin_area , \n \"<STR_LIT>\" : self . addr_postcode , \n \"<STR_LIT>\" : self . addr_country , \n \"<STR_LIT>\" : self . adapter . supports_feature_versioning ( ) , \n \"<STR_LIT>\" : True , \n '<STR_LIT>' : self . adapter . get_feature_descriptions ( <mask0> ) \n } ) \n", "gt": "request"}
{"input": "\n from sondra . document . valuehandlers import DateTime , Geometry , Now \n from shapely . geometry import Point \n from datetime import datetime \n import rethinkdb as r \n import pytest \n from sondra . tests . api import * \n from sondra . auth import Auth \n s = ConcreteSuite ( ) \n api = SimpleApp ( s ) \n auth = Auth ( s ) \n AuthenticatedApp ( s ) \n AuthorizedApp ( s ) \n s . ensure_database_objects ( ) \n @ pytest . fixture ( scope = '<STR_LIT>' ) \n def simple_doc ( request ) : \n simple_doc = s [ '<STR_LIT>' ] [ '<STR_LIT>' ] . create ( { \n '<STR_LIT:name>' : \"<STR_LIT>\" , \n \"<STR_LIT:date>\" : datetime . now ( ) , \n \"<STR_LIT:value>\" : <NUM_LIT:0> \n } ) \n def teardown ( ) : \n simple_doc . delete ( ) \n request . addfinalizer ( teardown ) \n return simple_doc \n @ pytest . fixture ( scope = '<STR_LIT>' ) \n def fk_doc ( request , simple_doc ) : \n fk_doc = s [ '<STR_LIT>' ] [ '<STR_LIT>' ] . create ( { \n '<STR_LIT:name>' : \"<STR_LIT>\" , \n '<STR_LIT>' : simple_doc , \n '<STR_LIT>' : [ simple_doc ] \n } ) \n def teardown ( ) : \n fk_doc . delete ( ) \n request . addfinalizer ( teardown ) \n return fk_doc \n def test_foreignkey ( fk_doc , simple_doc ) : \n retr_doc = s [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n assert isinstance ( fk_doc . obj [ '<STR_LIT>' ] , str ) \n assert fk_doc . obj [ '<STR_LIT>' ] == simple_doc . url \n assert isinstance ( retr_doc . obj [ '<STR_LIT>' ] , str ) \n assert retr_doc . obj [ '<STR_LIT>' ] == simple_doc . url \n storage_repr = fk_doc . rql_repr ( ) \n assert storage_repr [ '<STR_LIT>' ] == simple_doc . id \n assert isinstance ( fk_doc [ '<STR_LIT>' ] , <mask0> ) \n", "gt": "SimpleDocument"}
{"input": "\n import os \n from PySide . QtGui import * \n from PySide . QtCore import * \n from ui_Event import Ui_Event \n '''<STR_LIT>''' \n class EventWindow ( QDialog , Ui_Event ) : \n def __init__ ( self , parent , eventId ) : \n super ( EventWindow , self ) . __init__ ( parent ) \n self . rent = parent \n self . data = parent . eventData [ eventId ] \n self . deckAssignment = [ ] \n self . setupUi ( self ) \n self . assignWidgets ( ) \n self . setWindowTitle ( unicode ( \"<STR_LIT>\" % eventId ) ) \n def savePressed ( self ) : \n self . data [ \"<STR_LIT>\" ] = self . notesText . toPlainText ( ) \n self . data [ \"<STR_LIT>\" ] = self . deckText . text ( ) \n self . data [ \"<STR_LIT>\" ] = self . placeText . text ( ) \n self . data [ \"<STR_LIT>\" ] = self . eventTypeText . text ( ) \n self . data [ \"<STR_LIT>\" ] = self . playersText . text ( ) \n self . data [ \"<STR_LIT>\" ] = self . formatText . text ( ) \n self . data [ \"<STR_LIT>\" ] = self . locationText . text ( ) \n self . data [ \"<STR_LIT>\" ] = self . dateText . text ( ) \n ourCounter = <NUM_LIT:0> \n for ourRound in self . deckAssignment : \n self . data [ \"<STR_LIT>\" ] [ self . deckAssignment [ ourCounter ] [ <NUM_LIT:0> ] ] [ <NUM_LIT:2> ] = self . deckAssignment [ ourCounter ] [ <NUM_LIT:1> ] \n ourCounter += <NUM_LIT:1> \n self . rent . updateGUI ( ) \n self . rent . messageBox ( \"<STR_LIT>\" ) \n def closePressed ( self ) : \n self . hide ( ) \n def roundSelected ( self , ourRound , ourColumn ) : \n ourIndex = int ( ourRound . text ( <NUM_LIT:0> ) ) - <NUM_LIT:1> \n deckName , ok = QInputDialog . getText ( self , \"<STR_LIT>\" , \n \"<STR_LIT>\" ) \n if ok and deckName : \n self . data [ \"<STR_LIT>\" ] [ ourIndex ] [ <NUM_LIT:3> ] . setData ( <NUM_LIT:3> , <NUM_LIT:0> , deckName ) \n self . deckAssignment . append ( [ ourIndex , deckName ] ) \n def assignWidgets ( self ) : \n self . saveChangesButton . clicked . connect ( self . savePressed ) \n self . closeButton . clicked . connect ( self . closePressed ) \n self . roundTree . itemDoubleClicked . connect ( self . roundSelected ) \n self . notesText . setPlainText ( self . data [ \"<STR_LIT>\" ] ) \n self . deckText . setText ( self . data [ \"<STR_LIT>\" ] ) \n self . placeText . setText ( self . data [ \"<STR_LIT>\" ] ) \n self . eventTypeText . setText ( self . data [ \"<STR_LIT>\" ] ) \n self . playersText . setText ( self . data [ \"<STR_LIT>\" ] ) \n self . formatText . setText ( self . data [ \"<STR_LIT>\" ] ) \n self . locationText . setText ( self . data [ \"<STR_LIT>\" ] ) \n self . dateText . setText ( self . data [ \"<STR_LIT>\" ] ) \n matchItem = TreeWidgetItem ( self . resultsTree ) \n matchItem . setText ( <NUM_LIT:0> , unicode ( self . data [ \"<STR_LIT>\" ] ) ) \n matchItem . setText ( <NUM_LIT:1> , unicode ( self . data [ \"<STR_LIT>\" ] ) ) \n matchItem . setText ( <NUM_LIT:2> , unicode ( self . data [ \"<STR_LIT>\" ] ) ) \n matchItem . setText ( <NUM_LIT:3> , unicode ( self . data [ \"<STR_LIT>\" ] ) ) \n self . resultsTree . addTopLevelItem ( matchItem ) \n for i in range ( <NUM_LIT:4> ) : \n self . resultsTree . resizeColumnToContents ( i ) \n roundCounter = <NUM_LIT:1> \n for opponent in self . data [ \"<STR_LIT>\" ] : \n roundItem = TreeWidgetItem ( self . roundTree ) \n roundItem . setText ( <NUM_LIT:0> , unicode ( roundCounter ) ) \n roundItem . setText ( <NUM_LIT:1> , unicode ( opponent [ <NUM_LIT:0> ] ) ) \n roundItem . setText ( <NUM_LIT:2> , unicode ( opponent [ <NUM_LIT:1> ] ) ) \n roundItem . setText ( <NUM_LIT:3> , unicode ( opponent [ <NUM_LIT:2> ] ) ) \n opponent [ <NUM_LIT:3> ] = roundItem \n self . roundTree . addTopLevelItem ( roundItem ) \n roundCounter += <NUM_LIT:1> \n for i in range ( <NUM_LIT:4> ) : \n self . roundTree . resizeColumnToContents ( i ) \n class TreeWidgetItem ( QTreeWidgetItem ) : \n def __init__ ( self , parent = None ) : \n QTreeWidgetItem . __init__ ( self , parent ) \n def __lt__ ( self , otherItem ) : \n column = self . treeWidget ( ) . sortColumn ( ) \n try : \n return float ( self . text ( column ) ) > float ( otherItem . text ( column ) ) \n except ValueError : \n return self . text ( column ) > otherItem . text ( <mask0> ) \n", "gt": "column"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import numpy as np \n from pylatex import Document , Section , Subsection , Math , Matrix , VectorName \n if __name__ == '<STR_LIT:__main__>' : \n a = np . array ( [ [ <NUM_LIT:100> , <NUM_LIT:10> , <NUM_LIT:20> ] ] ) . T \n doc = Document ( ) \n section = Section ( '<STR_LIT>' ) \n subsection = Subsection ( '<STR_LIT>' ) \n vec = Matrix ( a ) \n vec_name = VectorName ( '<STR_LIT:a>' ) \n math = Math ( data = [ vec_name , '<STR_LIT:=>' , vec ] ) \n subsection . append ( math ) \n section . append ( subsection ) \n subsection = Subsection ( '<STR_LIT>' ) \n M = np . matrix ( [ [ <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:4> ] , \n [ <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1> ] , \n [ <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:2> ] ] ) \n matrix = Matrix ( M , mtype = '<STR_LIT:b>' ) \n math = Math ( data = [ '<STR_LIT>' , matrix ] ) \n subsection . append ( math ) \n section . append ( subsection ) \n subsection = Subsection ( '<STR_LIT>' ) \n math = Math ( data = [ '<STR_LIT:M>' , vec_name , '<STR_LIT:=>' , Matrix ( M * a ) ] ) \n subsection . append ( math ) \n section . append ( subsection ) \n doc . append ( section ) \n doc . <mask0> ( '<STR_LIT>' ) \n", "gt": "generate_pdf"}
{"input": "\n import quantities as pq \n from pylatex . quantities import _dimensionality_to_siunitx , Quantity \n def test_quantity ( ) : \n v = <NUM_LIT:1> * pq . m / pq . s \n q1 = Quantity ( v ) \n assert q1 . dumps ( ) == r'<STR_LIT>' \n q2 = Quantity ( v , format_cb = lambda x : str ( int ( x ) ) ) \n assert q2 . dumps ( ) == r'<STR_LIT>' \n q3 = Quantity ( v , options = { '<STR_LIT>' : '<STR_LIT:true>' } ) \n ref = r'<STR_LIT>' \n assert q3 . dumps ( ) == ref \n def test_quantity_float ( ) : \n q1 = Quantity ( <NUM_LIT> ) \n assert q1 . dumps ( ) == r'<STR_LIT>' \n def test_quantity_uncertain ( ) : \n t = pq . UncertainQuantity ( <NUM_LIT> , pq . second , <NUM_LIT:1.> ) \n q1 = Quantity ( t ) \n assert q1 . dumps ( ) == r'<STR_LIT>' \n def test_dimensionality_to_siunitx ( ) : \n assert _dimensionality_to_siunitx ( ( pq . volt / pq . kelvin ) . dimensionality ) == r'<STR_LIT>' \n if __name__ == '<STR_LIT:__main__>' : \n test_quantity ( ) \n <mask0> ( ) \n", "gt": "test_dimensionality_to_siunitx"}
{"input": "\n from supervisor . medusa import asyncore_25 as asyncore \n from supervisor . medusa import default_handler \n from supervisor . medusa import http_server \n from supervisor . medusa import put_handler \n from supervisor . medusa import auth_handler \n from supervisor . medusa import filesys \n users = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } \n fs = filesys . os_filesystem ( '<STR_LIT>' ) \n dh = default_handler . default_handler ( fs ) \n ph = put_handler . put_handler ( fs , '<STR_LIT>' ) \n ah = auth_handler . auth_handler ( users , ph ) \n hs = http_server . http_server ( ip = '<STR_LIT>' , port = <NUM_LIT> ) \n hs . install_handler ( dh ) \n hs . install_handler ( ah ) \n asyncore . <mask0> ( ) \n", "gt": "loop"}
{"input": "\n import socket \n import string \n from supervisor . medusa import asyncore_25 as asyncore \n from supervisor . medusa import asynchat_25 as asynchat \n class test_client ( asynchat . async_chat ) : \n ac_in_buffer_size = <NUM_LIT> \n ac_out_buffer_size = <NUM_LIT> \n total_in = <NUM_LIT:0> \n concurrent = <NUM_LIT:0> \n max_concurrent = <NUM_LIT:0> \n def __init__ ( self , addr , chain ) : \n asynchat . async_chat . __init__ ( self ) \n self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) \n self . set_terminator ( '<STR_LIT>' ) \n self . connect ( addr ) \n self . push ( chain ) \n def handle_connect ( self ) : \n test_client . concurrent = test_client . concurrent + <NUM_LIT:1> \n if ( test_client . concurrent > test_client . max_concurrent ) : \n test_client . max_concurrent = test_client . concurrent \n def handle_expt ( self ) : \n print '<STR_LIT>' \n self . close ( ) \n def close ( self ) : \n test_client . concurrent = test_client . concurrent - <NUM_LIT:1> \n asynchat . async_chat . close ( self ) \n def collect_incoming_data ( self , data ) : \n test_client . total_in = test_client . total_in + len ( data ) \n def found_terminator ( self ) : \n pass \n def log ( self , * args ) : \n pass \n import time \n class timer : \n def __init__ ( self ) : \n self . start = time . time ( ) \n def end ( self ) : \n return time . time ( ) - self . start \n def build_request_chain ( num , host , request_size ) : \n s = '<STR_LIT>' % ( request_size , host ) \n sl = [ s ] * ( num - <NUM_LIT:1> ) \n sl . append ( \n '<STR_LIT>' % ( \n request_size , host \n ) \n ) \n return string . join ( sl , '<STR_LIT>' ) \n if __name__ == '<STR_LIT:__main__>' : \n import string \n import sys \n if len ( sys . argv ) != <NUM_LIT:6> : \n print '<STR_LIT>' % sys . argv [ <NUM_LIT:0> ] \n else : \n host = sys . argv [ <NUM_LIT:1> ] \n ip = socket . gethostbyname ( host ) \n [ port , request_size , num_requests , num_conns ] = map ( \n string . atoi , sys . argv [ <NUM_LIT:2> : ] \n ) \n chain = build_request_chain ( num_requests , host , request_size ) \n t = timer ( ) \n for i in range ( num_conns ) : \n test_client ( ( host , port ) , chain ) \n asyncore . loop ( ) \n total_time = t . end ( ) \n total_bytes = test_client . total_in \n num_trans = num_requests * num_conns \n throughput = float ( total_bytes ) / total_time \n trans_per_sec = num_trans / total_time \n sys . stderr . write ( '<STR_LIT>' % total_time ) \n sys . stderr . write ( '<STR_LIT>' % num_trans ) \n sys . stderr . write ( '<STR_LIT>' % total_bytes ) \n sys . stderr . write ( '<STR_LIT>' % throughput ) \n sys . stderr . write ( '<STR_LIT>' % trans_per_sec ) \n sys . stderr . write ( '<STR_LIT>' % test_client . max_concurrent ) \n sys . stdout . write ( \n string . join ( \n map ( str , ( num_conns , num_requests , request_size , throughput , trans_per_sec ) ) , \n '<STR_LIT:U+002C>' \n ) + <mask0> \n ) \n", "gt": "'<STR_LIT:\\n>'"}
{"input": "\n from os import * \n from os import _exit \n import os \n class FakeOS : \n def __init__ ( self ) : \n self . orig_uid = os . getuid ( ) \n self . orig_gid = os . getgid ( ) \n def setgroups ( * args ) : \n return \n def getuid ( ) : \n return <NUM_LIT:0> \n def setuid ( arg ) : \n self . uid = arg \n self . setuid_called = <NUM_LIT:1> \n def setgid ( arg ) : \n self . gid = arg \n self . setgid_called = <NUM_LIT:1> \n def clear ( ) : \n self . uid = orig_uid \n self . gid = orig_gid \n self . setuid_called = <NUM_LIT:0> \n self . setgid_called = <NUM_LIT:0> \n fake = FakeOS ( ) \n setgroups = fake . setgroups \n getuid = fake . getuid \n setuid = fake . setuid \n setgid = fake . setgid \n clear = fake . <mask0> \n", "gt": "clear"}
{"input": "\n import toto \n from toto . invocation import * \n from tornado . ioloop import IOLoop \n @ asynchronous \n def invoke ( handler , params ) : \n def receive_message ( message ) : \n handler . respond ( result = { '<STR_LIT:message>' : message } ) \n handler . register_event_handler ( '<STR_LIT:message>' , receive_message , deregister_on_finish = <mask0> ) \n", "gt": "True"}
{"input": "\n import unittest \n from uuid import uuid4 \n from time import time , sleep \n from toto . tasks import TaskQueue , AwaitableInstance , InstancePool \n from tornado . ioloop import IOLoop \n from tornado . gen import coroutine \n class _Instance ( object ) : \n def __init__ ( self ) : \n self . counter = <NUM_LIT:0> \n def increment ( self ) : \n self . counter += <NUM_LIT:1> \n return self . counter \n def value ( self ) : \n return self . counter \n class TestTasks ( unittest . TestCase ) : \n def test_add_task ( self ) : \n queue = TaskQueue ( ) \n self . assertEquals ( len ( queue ) , <NUM_LIT:0> ) \n task_results = [ ] \n task = lambda x : task_results . append ( x ) \n queue . add_task ( task , <NUM_LIT:1> ) \n queue . add_task ( task , <NUM_LIT:2> ) \n queue . add_task ( task , <NUM_LIT:3> ) \n start = time ( ) \n while <NUM_LIT:1> : \n if len ( task_results ) == <NUM_LIT:3> : \n break \n if time ( ) - start > <NUM_LIT:5> : \n break \n sleep ( <NUM_LIT> ) \n self . assertEquals ( len ( task_results ) , <NUM_LIT:3> ) \n self . assertEquals ( task_results , [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ) \n def test_yield_task ( self ) : \n queue = TaskQueue ( ) \n task_results = [ ] \n @ coroutine \n def yield_tasks ( ) : \n task = lambda x : x \n futures = [ ] \n futures . append ( queue . yield_task ( task , <NUM_LIT:1> ) ) \n futures . append ( queue . yield_task ( task , <NUM_LIT:2> ) ) \n futures . append ( queue . yield_task ( task , <NUM_LIT:3> ) ) \n res = yield futures \n task_results [ : ] = res \n loop = IOLoop ( ) \n loop . make_current ( ) \n loop . run_sync ( yield_tasks ) \n self . assertEquals ( len ( task_results ) , <NUM_LIT:3> ) \n self . assertEquals ( task_results , [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ) \n def test_add_task_exception ( self ) : \n queue = TaskQueue ( ) \n self . assertEquals ( len ( queue ) , <NUM_LIT:0> ) \n task_results = [ ] \n def task ( x ) : \n task_results . append ( x ) \n raise Exception ( '<STR_LIT>' ) \n queue . add_task ( task , <NUM_LIT:1> ) \n queue . add_task ( task , <NUM_LIT:2> ) \n queue . add_task ( task , <NUM_LIT:3> ) \n start = time ( ) \n while <NUM_LIT:1> : \n if len ( task_results ) == <NUM_LIT:3> : \n break \n if time ( ) - start > <NUM_LIT:5> : \n break \n sleep ( <NUM_LIT> ) \n self . assertEquals ( len ( task_results ) , <NUM_LIT:3> ) \n self . assertEquals ( task_results , [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ) \n def test_yield_task_exception ( self ) : \n queue = TaskQueue ( ) \n task_results = [ ] \n @ coroutine \n def yield_tasks ( ) : \n def task ( x ) : \n raise Exception ( '<STR_LIT>' ) \n futures = [ ] \n futures . append ( queue . yield_task ( task , <NUM_LIT:1> ) ) \n futures . append ( queue . yield_task ( task , <NUM_LIT:2> ) ) \n futures . append ( queue . yield_task ( task , <NUM_LIT:3> ) ) \n for f in futures : \n try : \n yield f \n except Exception as e : \n task_results . append ( e ) \n loop = IOLoop ( ) \n loop . make_current ( ) \n loop . run_sync ( yield_tasks ) \n self . assertEquals ( len ( task_results ) , <NUM_LIT:3> ) \n for e in task_results : \n self . assertEquals ( e . message , '<STR_LIT>' ) \n def test_awaitable ( self ) : \n instance = _Instance ( ) \n instance . increment ( ) \n self . assertEquals ( instance . value ( ) , <NUM_LIT:1> ) \n awaitable = AwaitableInstance ( instance ) \n @ coroutine \n def yield_tasks ( ) : \n self . assertEquals ( ( yield awaitable . increment ( ) ) , <NUM_LIT:2> ) \n self . assertEquals ( ( yield awaitable . increment ( ) ) , <NUM_LIT:3> ) \n self . assertEquals ( ( yield awaitable . increment ( ) ) , <NUM_LIT:4> ) \n self . assertEquals ( ( yield awaitable . value ( ) ) , <NUM_LIT:4> ) \n loop = IOLoop ( ) \n loop . make_current ( ) \n loop . run_sync ( yield_tasks ) \n self . assertEquals ( instance . value ( ) , <NUM_LIT:4> ) \n def test_instance_pool ( self ) : \n instance1 = _Instance ( ) \n instance2 = _Instance ( ) \n pool = InstancePool ( [ instance1 , instance2 ] ) \n pool . increment ( ) \n pool . increment ( ) \n self . assertEquals ( instance1 . value ( ) , <NUM_LIT:1> ) \n self . assertEquals ( instance2 . value ( ) , <NUM_LIT:1> ) \n pool . transaction ( lambda i : i . increment ( ) ) \n pool . transaction ( lambda i : i . increment ( ) ) \n self . assertEquals ( instance1 . value ( ) , <NUM_LIT:2> ) \n self . assertEquals ( instance2 . value ( ) , <NUM_LIT:2> ) \n @ coroutine \n def yield_tasks ( ) : \n self . assertEquals ( ( yield pool . await ( ) . increment ( ) ) , <NUM_LIT:3> ) \n self . assertEquals ( ( yield pool . await ( ) . increment ( ) ) , <NUM_LIT:3> ) \n self . assertEquals ( instance1 . value ( ) , <NUM_LIT:3> ) \n self . assertEquals ( instance2 . value ( ) , <NUM_LIT:3> ) \n self . assertEquals ( ( yield pool . await_transaction ( lambda i : i . increment ( ) ) ) , <NUM_LIT:4> ) \n self . assertEquals ( ( yield pool . await_transaction ( lambda i : i . increment ( ) ) ) , <NUM_LIT:4> ) \n loop = IOLoop ( ) \n loop . make_current ( ) \n loop . run_sync ( yield_tasks ) \n self . assertEquals ( instance1 . value ( ) , <NUM_LIT:4> ) \n self . assertEquals ( instance2 . <mask0> ( ) , <NUM_LIT:4> ) \n", "gt": "value"}
{"input": "\n '''<STR_LIT>''' \n import cPickle as pickle \n from threading import Thread \n from collections import deque \n from tornado . web import * \n from tornado . ioloop import IOLoop \n from traceback import format_exc \n from tornado . options import options \n import zmq \n import logging \n import zlib \n from random import choice , shuffle \n class EventManager ( ) : \n '''<STR_LIT>''' \n def __init__ ( self , address = None ) : \n self . __handlers = { } \n self . address = address \n self . __zmq_context = zmq . Context ( ) \n self . __remote_servers = { } \n self . __thread = None \n self . __queued_servers = deque ( ) \n def register_server ( self , address ) : \n '''<STR_LIT>''' \n if address in self . __remote_servers : \n raise Exception ( '<STR_LIT>' , address ) \n socket = self . __zmq_context . socket ( zmq . PUSH ) \n socket . connect ( address ) \n self . __remote_servers [ address ] = socket \n self . refresh_server_queue ( ) \n def remove_server ( self , address ) : \n '''<STR_LIT>''' \n del self . __remote_servers [ address ] \n self . refresh_server_queue ( ) \n def remove_all_servers ( self ) : \n '''<STR_LIT>''' \n self . __remote_servers . clear ( ) \n self . refresh_server_queue ( ) \n def refresh_server_queue ( self ) : \n '''<STR_LIT>''' \n self . __queued_servers . clear ( ) \n self . __queued_servers . extend ( self . __remote_servers . itervalues ( ) ) \n shuffle ( self . __queued_servers ) \n def register_handler ( self , event_name , event_handler , run_on_main_loop = False , request_handler = None , persist = False ) : \n '''<STR_LIT>''' \n if not event_name in self . __handlers : \n self . __handlers [ event_name ] = set ( ) \n handler_tuple = ( event_handler , run_on_main_loop , request_handler , persist ) \n self . __handlers [ event_name ] . add ( handler_tuple ) \n return ( event_name , handler_tuple ) \n def remove_handler ( self , handler_sig ) : \n '''<STR_LIT>''' \n self . __handlers [ handler_sig [ <NUM_LIT:0> ] ] . discard ( handler_sig [ <NUM_LIT:1> ] ) \n def start_listening ( self ) : \n '''<STR_LIT>''' \n if self . __thread : \n return \n def receive ( ) : \n context = zmq . Context ( ) \n socket = context . socket ( zmq . PULL ) \n socket . bind ( self . address ) \n while True : \n event = pickle . loads ( zlib . decompress ( socket . recv ( ) ) ) \n event_name = event [ '<STR_LIT:name>' ] \n event_args = event [ '<STR_LIT:args>' ] \n if event_name in self . __handlers : \n handlers = self . __handlers [ event_name ] \n for handler in list ( handlers ) : \n if not handler [ <NUM_LIT:3> ] : \n handlers . remove ( handler ) \n try : \n if handler [ <NUM_LIT:2> ] and handler [ <NUM_LIT:2> ] . _finished : \n continue \n if handler [ <NUM_LIT:1> ] : \n ( lambda h : IOLoop . instance ( ) . add_callback ( lambda : h [ <NUM_LIT:0> ] ( event_args ) ) ) ( handler ) \n else : \n handler [ <NUM_LIT:0> ] ( event_args ) \n except Exception as e : \n logging . error ( format_exc ( ) ) \n self . __thread = Thread ( target = receive ) \n self . __thread . daemon = True \n self . __thread . start ( ) \n def send_to_server ( self , address , event_name , event_args ) : \n '''<STR_LIT>''' \n event = { '<STR_LIT:name>' : event_name , '<STR_LIT:args>' : event_args } \n event_data = zlib . compress ( pickle . dumps ( event ) ) \n self . __remote_servers [ address ] . send ( event_data ) \n def send ( self , event_name , event_args , broadcast = True ) : \n '''<STR_LIT>''' \n if not self . __remote_servers : \n return \n event = { '<STR_LIT:name>' : event_name , '<STR_LIT:args>' : event_args } \n event_data = zlib . compress ( pickle . dumps ( event ) ) \n if not broadcast : \n self . __queued_servers [ <NUM_LIT:0> ] . send ( event_data ) \n self . __queued_servers . rotate ( - <NUM_LIT:1> ) \n return \n for socket in self . __queued_servers : \n socket . send ( event_data ) \n @ classmethod \n def instance ( cls ) : \n '''<STR_LIT>''' \n if not hasattr ( cls , '<STR_LIT>' ) : \n cls . _instance = cls ( ) \n return cls . <mask0> \n", "gt": "_instance"}
{"input": "\n import toto \n import cPickle as pickle \n import zlib \n import logging \n from threading import Thread \n from tornado . options import options \n from tornado . gen import Task \n from collections import deque \n from time import time \n from uuid import uuid4 \n from traceback import format_exc \n from toto . options import safe_define \n safe_define ( \"<STR_LIT>\" , type = str , help = \"<STR_LIT>\" ) \n safe_define ( \"<STR_LIT>\" , type = str , help = \"<STR_LIT>\" ) \n safe_define ( \"<STR_LIT>\" , type = str , default = '<STR_LIT>' , help = \"<STR_LIT>\" ) \n safe_define ( \"<STR_LIT>\" , default = <NUM_LIT> , help = \"<STR_LIT>\" ) \n safe_define ( \"<STR_LIT>\" , default = False , help = \"<STR_LIT>\" ) \n safe_define ( \"<STR_LIT>\" , default = <NUM_LIT:0> , help = \"<STR_LIT>\" ) \n safe_define ( \"<STR_LIT>\" , default = '<STR_LIT>' , help = \"<STR_LIT>\" ) \n safe_define ( \"<STR_LIT>\" , default = '<STR_LIT>' , help = \"<STR_LIT>\" ) \n WORKER_SOCKET_CONNECT = '<STR_LIT>' \n WORKER_SOCKET_DISCONNECT = '<STR_LIT>' \n class WorkerConnection ( object ) : \n '''<STR_LIT>''' \n def __getattr__ ( self , path ) : \n return WorkerInvocation ( path , self ) \n def log_error ( self , error ) : \n logging . error ( repr ( error ) ) \n def enable_traceback_logging ( self ) : \n from new import instancemethod \n from traceback import format_exc \n def log_error ( self , e ) : \n logging . error ( format_exc ( ) ) \n self . log_error = instancemethod ( log_error , self ) \n @ classmethod \n def instance ( cls ) : \n '''<STR_LIT>''' \n if not hasattr ( cls , '<STR_LIT>' ) : \n if options . worker_transport == '<STR_LIT:http>' : \n from toto . httpworkerconnection import HTTPWorkerConnection \n cls . _instance = HTTPWorkerConnection . instance ( ) \n else : \n from toto . zmqworkerconnection import ZMQWorkerConnection \n cls . _instance = ZMQWorkerConnection . instance ( ) \n return cls . _instance \n class WorkerInvocation ( object ) : \n def __init__ ( self , path , connection ) : \n self . _path = path \n self . _connection = connection \n def __call__ ( self , * args , ** kwargs ) : \n return self . _connection . invoke ( self . _path , * args , ** kwargs ) \n def __getattr__ ( self , path ) : \n return getattr ( self . _connection , self . _path + '<STR_LIT:.>' + <mask0> ) \n", "gt": "path"}
{"input": "\n from . import multiarray \n <mask0> = [ ] \n", "gt": "__all__"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n __author__ = '<STR_LIT>' \n import urllib \n from pyactiveresource import connection \n from pyactiveresource import formats \n class Error ( Exception ) : \n \"\"\"<STR_LIT>\"\"\" \n class FakeConnection ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , format = formats . XMLFormat ) : \n \"\"\"<STR_LIT>\"\"\" \n self . format = format \n self . _request_map = { } \n self . _debug_only = False \n def _split_path ( self , path ) : \n \"\"\"<STR_LIT>\"\"\" \n path_only , query_string = urllib . splitquery ( path ) \n if query_string : \n query_dict = dict ( [ i . split ( '<STR_LIT:=>' ) for i in query_string . split ( '<STR_LIT:&>' ) ] ) \n else : \n query_dict = { } \n return path_only , query_dict \n def debug_only ( self , debug = True ) : \n self . _debug_only = debug \n def respond_to ( self , method , path , headers , data , body , \n response_headers = None ) : \n \"\"\"<STR_LIT>\"\"\" \n path_only , query = self . _split_path ( path ) \n if response_headers is None : \n response_headers = { } \n self . _request_map . setdefault ( method , [ ] ) . append ( \n ( ( path_only , query , headers , data ) , ( body , response_headers ) ) ) \n def _lookup_response ( self , method , path , headers , data ) : \n path_only , query = self . _split_path ( path ) \n for key , value in self . _request_map . get ( method , { } ) : \n if key == ( path_only , query , headers , data ) : \n response_body , response_headers = value \n return connection . Response ( <NUM_LIT:200> , response_body , response_headers ) \n raise Error ( '<STR_LIT>' % \n ( path , headers , data ) ) \n def get ( self , path , headers = None ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . format . decode ( \n self . _lookup_response ( '<STR_LIT>' , path , headers , None ) . body ) \n def post ( self , path , headers = None , data = None ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _lookup_response ( '<STR_LIT>' , path , headers , data ) \n def put ( self , path , headers = None , data = None ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _lookup_response ( '<STR_LIT>' , path , headers , data ) \n def delete ( self , path , headers = None ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _lookup_response ( '<STR_LIT>' , path , headers , <mask0> ) \n", "gt": "None"}
{"input": "\n from trac . env import Environment \n from trac . attachment import Attachment \n from tracLib import * \n from ConfigParser import ConfigParser \n import tracLib \n import tracLib . timetracking \n class Client ( object ) : \n def __init__ ( self , env_path ) : \n self . env_path = env_path \n self . env = Environment ( env_path ) \n self . db_cnx = self . env . get_db_cnx ( ) \n self . _registered_users_logins = [ ] \n self . _timetracking_plugins = self . _get_timetracking_plugins ( ) \n def _get_timetracking_plugins ( self ) : \n plugins = { } \n if tracLib . SUPPORT_TIME_TRACKING == '<STR_LIT>' : \n for plugin in tracLib . timetracking . plugins : \n plugin_name = plugin . get_name ( ) \n for com_name , com_enabled in self . env . _component_rules . items ( ) : \n if com_name . startswith ( plugin_name ) and com_enabled and plugin_name not in plugins : \n plugins [ plugin_name ] = plugin ( self . env ) \n else : \n for plugin in tracLib . timetracking . plugins : \n plugin_name = plugin . get_name ( ) \n if plugin_name == tracLib . SUPPORT_TIME_TRACKING : \n plugins [ plugin_name ] = plugin ( self . env ) \n break ; \n for plugin_name in plugins . keys ( ) : \n print \"<STR_LIT>\" % plugin_name \n return plugins . values ( ) \n def get_project_description ( self ) : \n return self . env . project_description \n def get_users ( self ) : \n result = self . env . get_known_users ( ) \n trac_users = list ( [ ] ) \n for user in result : \n user_login = user [ <NUM_LIT:0> ] . lower ( ) \n if user_login in self . _registered_users_logins : \n continue \n u = TracUser ( user_login ) \n u . email = user [ <NUM_LIT:2> ] \n trac_users . append ( u ) \n self . _registered_users_logins . append ( user_login ) \n if not tracLib . ACCEPT_NON_AUTHORISED_USERS : \n return trac_users \n user_fields = [ ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ] \n first = True \n request = \"<STR_LIT>\" \n for column_name , table_name in user_fields : \n if first : \n first = False \n else : \n request += \"<STR_LIT>\" \n request += \"<STR_LIT>\" % ( column_name , table_name ) \n cursor = self . db_cnx . cursor ( ) \n cursor . execute ( request ) \n for row in cursor : \n if row [ <NUM_LIT:0> ] not in self . _registered_users_logins : \n trac_user = self . _get_non_authorised_user ( row [ <NUM_LIT:0> ] ) \n if trac_user is not None : \n trac_users . append ( trac_user ) \n self . _registered_users_logins . append ( trac_user . name ) \n return trac_users \n def _get_non_authorised_user ( self , user_name ) : \n if user_name is None : \n return None \n start = user_name . find ( \"<STR_LIT:<>\" ) \n end = user_name . rfind ( \"<STR_LIT:>>\" ) \n if ( start > - <NUM_LIT:1> ) and ( end > start + <NUM_LIT:1> ) : \n if user_name . find ( \"<STR_LIT:@>\" , start , end ) > <NUM_LIT:0> : \n user = TracUser ( user_name [ start + <NUM_LIT:1> : end ] . replace ( \"<STR_LIT:U+0020>\" , \"<STR_LIT:_>\" ) ) \n user . email = user_name [ start + <NUM_LIT:1> : end ] . replace ( \"<STR_LIT:U+0020>\" , \"<STR_LIT:_>\" ) \n return user \n return None \n def _get_user_login ( self , user_name ) : \n if user_name is None : \n return None \n if user_name in self . _registered_users_logins : \n return user_name \n if not tracLib . ACCEPT_NON_AUTHORISED_USERS : \n return None \n user = self . _get_non_authorised_user ( user_name ) \n if ( user is None ) or ( user . name not in self . _registered_users_logins ) : \n return None \n return user . name \n def get_severities ( self ) : \n return self . _get_data_from_enum ( \"<STR_LIT>\" ) \n def get_issue_types ( self ) : \n return self . _get_data_from_enum ( \"<STR_LIT>\" ) \n def get_issue_priorities ( self ) : \n return self . _get_data_from_enum ( \"<STR_LIT>\" ) \n def get_issue_resolutions ( self ) : \n return [ TracResolution ( name ) for name in self . _get_data_from_enum ( \"<STR_LIT>\" ) ] \n def get_components ( self ) : \n cursor = self . db_cnx . cursor ( ) \n cursor . execute ( \"<STR_LIT>\" ) \n trac_components = list ( [ ] ) \n for row in cursor : \n component = TracComponent ( row [ <NUM_LIT:0> ] ) \n component . owner = self . _get_user_login ( component . owner ) \n if row [ <NUM_LIT:2> ] is not None : \n component . description = row [ <NUM_LIT:2> ] \n trac_components . append ( component ) \n return trac_components \n def get_versions ( self ) : \n cursor = self . db_cnx . cursor ( ) \n cursor . execute ( \"<STR_LIT>\" ) \n trac_versions = list ( [ ] ) \n for row in cursor : \n version = TracVersion ( row [ <NUM_LIT:0> ] ) \n if row [ <NUM_LIT:1> ] : \n version . time = to_unix_time ( row [ <NUM_LIT:1> ] ) \n if row [ <NUM_LIT:2> ] is not None : \n version . description = row [ <NUM_LIT:2> ] \n trac_versions . append ( version ) \n return trac_versions \n def get_issues ( self ) : \n cursor = self . db_cnx . cursor ( ) \n cursor . execute ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n trac_issues = list ( [ ] ) \n for row in cursor : \n issue = TracIssue ( row [ <NUM_LIT:0> ] ) \n issue . time = to_unix_time ( row [ <NUM_LIT:2> ] ) \n issue . changetime = to_unix_time ( row [ <NUM_LIT:3> ] ) \n issue . reporter = self . _get_user_login ( row [ <NUM_LIT:8> ] ) \n if row [ <NUM_LIT:9> ] is not None : \n cc = row [ <NUM_LIT:9> ] . split ( \"<STR_LIT:U+002C>\" ) \n for c in cc : \n if len ( c ) > <NUM_LIT:0> : \n cc_name = self . _get_user_login ( c . strip ( ) ) \n if cc_name is not None : \n issue . cc . add ( cc_name ) \n issue . summary = row [ <NUM_LIT> ] \n issue . description = row [ <NUM_LIT> ] \n issue . custom_fields [ \"<STR_LIT>\" ] = row [ <NUM_LIT:1> ] \n issue . custom_fields [ \"<STR_LIT>\" ] = row [ <NUM_LIT:4> ] \n issue . custom_fields [ \"<STR_LIT>\" ] = row [ <NUM_LIT:5> ] \n issue . custom_fields [ \"<STR_LIT>\" ] = row [ <NUM_LIT:6> ] \n issue . custom_fields [ \"<STR_LIT>\" ] = self . _get_user_login ( row [ <NUM_LIT:7> ] ) \n issue . custom_fields [ \"<STR_LIT>\" ] = row [ <NUM_LIT:10> ] \n issue . custom_fields [ \"<STR_LIT>\" ] = row [ <NUM_LIT:11> ] \n issue . custom_fields [ \"<STR_LIT>\" ] = row [ <NUM_LIT:12> ] \n if row [ <NUM_LIT:15> ] is not None : \n keywords = row [ <NUM_LIT:15> ] . rsplit ( \"<STR_LIT:U+002C>\" ) \n for kw in keywords : \n if len ( kw ) > <NUM_LIT:0> : \n issue . keywords . add ( kw . strip ( ) ) \n custom_field_cursor = self . db_cnx . cursor ( ) \n custom_field_cursor . execute ( \"<STR_LIT>\" , ( str ( row [ <NUM_LIT:0> ] ) , ) ) \n for cf in custom_field_cursor : \n issue . custom_fields [ cf [ <NUM_LIT:0> ] . capitalize ( ) ] = cf [ <NUM_LIT:1> ] \n attachment_cursor = self . db_cnx . cursor ( ) \n attachment_cursor . execute ( \"<STR_LIT>\" \n \"<STR_LIT>\" , ( \"<STR_LIT>\" , str ( issue . id ) ) ) \n for elem in attachment_cursor : \n at = TracAttachment ( Attachment . _get_path ( self . env . path , '<STR_LIT>' , str ( issue . id ) , elem [ <NUM_LIT:0> ] ) ) \n at . name = elem [ <NUM_LIT:0> ] \n at . size = elem [ <NUM_LIT:1> ] \n at . time = to_unix_time ( elem [ <NUM_LIT:2> ] ) \n at . description = elem [ <NUM_LIT:3> ] \n at . author_name = elem [ <NUM_LIT:4> ] \n issue . attachment . add ( at ) \n trac_issues . append ( issue ) \n change_cursor = self . db_cnx . cursor ( ) \n change_cursor . execute ( \"<STR_LIT>\" , ( str ( row [ <NUM_LIT:0> ] ) , \"<STR_LIT>\" , ) ) \n for elem in change_cursor : \n if ( elem [ <NUM_LIT:2> ] is None ) or ( not len ( elem [ <NUM_LIT:2> ] . lstrip ( ) ) ) : \n continue \n comment = TracComment ( to_unix_time ( elem [ <NUM_LIT:0> ] ) ) \n comment . author = str ( elem [ <NUM_LIT:1> ] ) \n comment . content = unicode ( elem [ <NUM_LIT:2> ] ) \n comment . id = elem [ <NUM_LIT:3> ] \n issue . comments . add ( comment ) \n for ttp in self . _timetracking_plugins : \n issue . workitems . update ( set ( ttp [ row [ <NUM_LIT:0> ] ] ) ) \n return trac_issues \n def get_custom_fields_declared ( self ) : \n ini_file_path = self . env_path + \"<STR_LIT>\" \n parser = ConfigParser ( ) \n parser . read ( ini_file_path ) \n if not ( \"<STR_LIT>\" in parser . sections ( ) ) : \n return set ( [ ] ) \n result = parser . items ( \"<STR_LIT>\" ) \n items = dict ( [ ] ) \n for elem in result : \n items [ elem [ <NUM_LIT:0> ] ] = elem [ <NUM_LIT:1> ] \n keys = items . keys ( ) \n custom_fields = list ( [ ] ) \n for k in keys : \n if not ( \"<STR_LIT:.>\" in k ) : \n field = TracCustomFieldDeclaration ( k . capitalize ( ) ) \n field . type = items [ k ] \n options_key = k + \"<STR_LIT>\" \n if options_key in items : \n opts_str = items [ options_key ] \n opts = opts_str . rsplit ( \"<STR_LIT:|>\" ) \n for o in opts : \n field . options . append ( o ) \n value_key = k + \"<STR_LIT>\" \n if value_key in items : \n field . value = items [ value_key ] \n label_key = k + \"<STR_LIT>\" \n if label_key in items : \n field . label = items [ label_key ] \n custom_fields . append ( field ) \n return custom_fields \n def _get_data_from_enum ( self , type_name ) : \n cursor = self . db_cnx . cursor ( ) \n cursor . execute ( \"<STR_LIT>\" , ( type_name , ) ) \n return [ row [ <NUM_LIT:0> ] for row in <mask0> ] \n", "gt": "cursor"}
{"input": "\n import os \n os . <mask0> ( \"<STR_LIT>\" ) \n", "gt": "system"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n from collections import defaultdict \n from bacpypes . debugging import Logging , function_debugging , ModuleLogger \n from bacpypes . consolelogging import ConsoleLogHandler \n from bacpypes . pdu import Address \n from bacpypes . analysis import trace , strftimestamp , Tracer \n from bacpypes . npdu import WhoIsRouterToNetwork \n _debug = <NUM_LIT:0> \n _log = ModuleLogger ( globals ( ) ) \n filterSource = None \n filterDestination = None \n filterHost = None \n requests = defaultdict ( int ) \n networks = defaultdict ( list ) \n @ function_debugging \n def Match ( addr1 , addr2 ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : Match . _debug ( \"<STR_LIT>\" , addr1 , addr2 ) \n if ( addr2 . addrType == Address . localBroadcastAddr ) : \n return ( addr1 . addrType == Address . localStationAddr ) or ( addr1 . addrType == Address . localBroadcastAddr ) \n elif ( addr2 . addrType == Address . localStationAddr ) : \n return ( addr1 . addrType == Address . localStationAddr ) and ( addr1 . addrAddr == addr2 . addrAddr ) \n elif ( addr2 . addrType == Address . remoteBroadcastAddr ) : \n return ( ( addr1 . addrType == Address . remoteStationAddr ) or ( addr1 . addrType == Address . remoteBroadcastAddr ) ) and ( addr1 . addrNet == addr2 . addrNet ) \n elif ( addr2 . addrType == Address . remoteStationAddr ) : \n return ( addr1 . addrType == Address . remoteStationAddr ) and ( addr1 . addrNet == addr2 . addrNet ) and ( addr1 . addrAddr == addr2 . addrAddr ) \n elif ( addr2 . addrType == Address . globalBroadcastAddr ) : \n return ( addr1 . addrType == Address . globalBroadcastAddr ) \n else : \n raise RuntimeError , \"<STR_LIT>\" \n class WhoIsRouterToNetworkSummary ( Tracer , Logging ) : \n def __init__ ( self ) : \n if _debug : IAmRouterToNetworkSummary . _debug ( \"<STR_LIT>\" ) \n Tracer . __init__ ( self , self . Filter ) \n def Filter ( self , pkt ) : \n if _debug : WhoIsRouterToNetworkSummary . _debug ( \"<STR_LIT>\" , pkt ) \n global requests , networks \n if not isinstance ( pkt , WhoIsRouterToNetwork ) : \n return \n if filterSource : \n if not Match ( pkt . pduSource , filterSource ) : \n if _debug : WhoIsRouterToNetworkSummary . _debug ( \"<STR_LIT>\" ) \n return \n if filterDestination : \n if not Match ( pkt . pduDestination , filterDestination ) : \n if _debug : WhoIsRouterToNetworkSummary . _debug ( \"<STR_LIT>\" ) \n return \n if filterHost : \n if ( not Match ( pkt . pduSource , filterHost ) ) and ( not Match ( pkt . pduDestination , filterHost ) ) : \n if _debug : WhoIsRouterToNetworkSummary . _debug ( \"<STR_LIT>\" ) \n return \n requests [ pkt . pduSource ] += <NUM_LIT:1> \n networks [ pkt . pduSource ] . append ( pkt . wirtnNetwork ) \n try : \n if ( '<STR_LIT>' in sys . argv ) : \n indx = sys . argv . index ( '<STR_LIT>' ) \n for i in range ( indx + <NUM_LIT:1> , len ( sys . argv ) ) : \n ConsoleLogHandler ( sys . argv [ i ] ) \n del sys . argv [ indx : ] \n if _debug : _log . debug ( \"<STR_LIT>\" ) \n if ( '<STR_LIT>' in sys . argv ) : \n i = sys . argv . index ( '<STR_LIT>' ) \n filterSource = Address ( sys . argv [ i + <NUM_LIT:1> ] ) \n if _debug : _log . debug ( \"<STR_LIT>\" , filterSource ) \n del sys . argv [ i : i + <NUM_LIT:2> ] \n if ( '<STR_LIT>' in sys . argv ) : \n i = sys . argv . index ( '<STR_LIT>' ) \n filterDestination = Address ( sys . argv [ i + <NUM_LIT:1> ] ) \n if _debug : _log . debug ( \"<STR_LIT>\" , filterDestination ) \n del sys . argv [ i : i + <NUM_LIT:2> ] \n if ( '<STR_LIT>' in sys . argv ) : \n i = sys . argv . index ( '<STR_LIT>' ) \n filterHost = Address ( sys . argv [ i + <NUM_LIT:1> ] ) \n if _debug : _log . debug ( \"<STR_LIT>\" , filterHost ) \n del sys . argv [ i : i + <NUM_LIT:2> ] \n for fname in sys . argv [ <NUM_LIT:1> : ] : \n trace ( fname , [ WhoIsRouterToNetworkSummary ] ) \n items = requests . items ( ) \n items . sort ( lambda x , y : cmp ( y [ <NUM_LIT:1> ] , x [ <NUM_LIT:1> ] ) ) \n print \"<STR_LIT>\" % ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n for key , count in items : \n print \"<STR_LIT>\" % ( key , count ) \n net_count = defaultdict ( int ) \n for net in networks [ key ] : \n net_count [ net ] += <NUM_LIT:1> \n net_count = net_count . items ( ) \n net_count . sort ( lambda x , y : cmp ( y [ <NUM_LIT:1> ] , x [ <NUM_LIT:1> ] ) ) \n for net , count in net_count : \n print \"<STR_LIT>\" % ( net , count ) \n except KeyboardInterrupt : \n pass \n except Exception , e : \n _log . exception ( \"<STR_LIT>\" , e ) \n finally : \n if _debug : _log . <mask0> ( \"<STR_LIT>\" ) \n", "gt": "debug"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import asyncore \n import socket \n import cPickle as pickle \n from time import time as _time , sleep as _sleep \n from StringIO import StringIO \n from . debugging import ModuleLogger , DebugContents , bacpypes_debugging \n from . core import deferred \n from . task import FunctionTask , OneShotFunction \n from . comm import PDU , Client , Server \n from . comm import ServiceAccessPoint , ApplicationServiceElement \n _debug = <NUM_LIT:0> \n _log = ModuleLogger ( globals ( ) ) \n REBIND_SLEEP_INTERVAL = <NUM_LIT> \n class PickleActorMixIn : \n def __init__ ( self , * args ) : \n if _debug : PickleActorMixIn . _debug ( \"<STR_LIT>\" , args ) \n super ( PickleActorMixIn , self ) . __init__ ( * args ) \n self . pickleBuffer = '<STR_LIT>' \n def indication ( self , pdu ) : \n if _debug : PickleActorMixIn . _debug ( \"<STR_LIT>\" , pdu ) \n pdu . pduData = pickle . dumps ( pdu . pduData ) \n super ( PickleActorMixIn , self ) . indication ( pdu ) \n def response ( self , pdu ) : \n if _debug : PickleActorMixIn . _debug ( \"<STR_LIT>\" , pdu ) \n self . pickleBuffer += pdu . pduData \n strm = StringIO ( self . pickleBuffer ) \n pos = <NUM_LIT:0> \n while ( pos < strm . len ) : \n try : \n msg = pickle . load ( strm ) \n except : \n break \n rpdu = PDU ( msg ) \n rpdu . update ( pdu ) \n super ( PickleActorMixIn , self ) . response ( rpdu ) \n pos = strm . tell ( ) \n if ( pos < strm . len ) : \n self . pickleBuffer = self . pickleBuffer [ pos : ] \n else : \n self . pickleBuffer = '<STR_LIT>' \n bacpypes_debugging ( PickleActorMixIn ) \n class TCPClient ( asyncore . dispatcher ) : \n def __init__ ( self , peer ) : \n if _debug : TCPClient . _debug ( \"<STR_LIT>\" , peer ) \n asyncore . dispatcher . __init__ ( self ) \n self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) \n self . peer = peer \n self . request = '<STR_LIT>' \n self . socketError = None \n if _debug : TCPClient . _debug ( \"<STR_LIT>\" ) \n self . connect ( peer ) \n if _debug : TCPClient . _debug ( \"<STR_LIT>\" ) \n def handle_connect ( self ) : \n if _debug : deferred ( TCPClient . _debug , \"<STR_LIT>\" ) \n def handle_expt ( self ) : \n pass \n def readable ( self ) : \n return <NUM_LIT:1> \n def handle_read ( self ) : \n if _debug : deferred ( TCPClient . _debug , \"<STR_LIT>\" ) \n try : \n msg = self . recv ( <NUM_LIT> ) \n if _debug : deferred ( TCPClient . _debug , \"<STR_LIT>\" , len ( msg ) ) \n self . socketError = None \n if not self . socket : \n if _debug : deferred ( TCPClient . _debug , \"<STR_LIT>\" ) \n else : \n deferred ( self . response , PDU ( msg ) ) \n except socket . error , err : \n if ( err . args [ <NUM_LIT:0> ] == <NUM_LIT> ) : \n deferred ( TCPClient . _error , \"<STR_LIT>\" , self . peer ) \n else : \n deferred ( TCPClient . _error , \"<STR_LIT>\" , err ) \n self . socketError = err \n def writable ( self ) : \n return ( len ( self . request ) != <NUM_LIT:0> ) \n def handle_write ( self ) : \n if _debug : deferred ( TCPClient . _debug , \"<STR_LIT>\" ) \n try : \n sent = self . send ( self . request ) \n if _debug : deferred ( TCPClient . _debug , \"<STR_LIT>\" , sent , len ( self . request ) - sent ) \n self . socketError = None \n self . request = self . request [ sent : ] \n except socket . error , err : \n if ( err . args [ <NUM_LIT:0> ] == <NUM_LIT> ) : \n deferred ( TCPClient . _error , \"<STR_LIT>\" , self . peer ) \n else : \n deferred ( TCPClient . _error , \"<STR_LIT>\" , err ) \n self . socketError = err \n def handle_close ( self ) : \n if _debug : deferred ( TCPClient . _debug , \"<STR_LIT>\" ) \n self . close ( ) \n self . socket = None \n def indication ( self , pdu ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : TCPClient . _debug ( \"<STR_LIT>\" , pdu ) \n self . request += pdu . pduData \n bacpypes_debugging ( TCPClient ) \n class TCPClientActor ( TCPClient ) : \n def __init__ ( self , director , peer ) : \n if _debug : TCPClientActor . _debug ( \"<STR_LIT>\" , director , peer ) \n TCPClient . __init__ ( self , peer ) \n self . director = director \n self . timeout = director . timeout \n if self . timeout > <NUM_LIT:0> : \n self . timer = FunctionTask ( self . idle_timeout ) \n self . timer . install_task ( _time ( ) + self . timeout ) \n else : \n self . timer = None \n self . flushTask = None \n self . director . add_actor ( self ) \n def handle_close ( self ) : \n if _debug : TCPClientActor . _debug ( \"<STR_LIT>\" ) \n if self . flushTask : \n self . flushTask . suspend_task ( ) \n if self . timer : \n self . timer . suspend_task ( ) \n self . director . remove_actor ( self ) \n TCPClient . handle_close ( self ) \n def idle_timeout ( self ) : \n if _debug : TCPClientActor . _debug ( \"<STR_LIT>\" ) \n self . handle_close ( ) \n def indication ( self , pdu ) : \n if _debug : TCPClientActor . _debug ( \"<STR_LIT>\" , pdu ) \n if self . flushTask : \n if _debug : TCPServerActor . _debug ( \"<STR_LIT>\" ) \n return \n if self . timer : \n self . timer . install_task ( _time ( ) + self . timeout ) \n TCPClient . indication ( self , pdu ) \n def response ( self , pdu ) : \n if _debug : TCPClientActor . _debug ( \"<STR_LIT>\" , pdu ) \n pdu . pduSource = self . peer \n if self . timer : \n self . timer . install_task ( _time ( ) + self . timeout ) \n self . director . response ( pdu ) \n def flush ( self ) : \n if _debug : TCPClientActor . _debug ( \"<STR_LIT>\" ) \n self . flushTask = None \n if self . request : \n self . flushTask = OneShotFunction ( self . flush ) \n return \n self . handle_close ( ) \n bacpypes_debugging ( TCPClientActor ) \n class TCPPickleClientActor ( PickleActorMixIn , TCPClientActor ) : \n pass \n class TCPClientDirector ( Server , ServiceAccessPoint , DebugContents ) : \n _debug_contents = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n def __init__ ( self , timeout = <NUM_LIT:0> , actorClass = TCPClientActor , sid = None , sapID = None ) : \n if _debug : TCPClientDirector . _debug ( \"<STR_LIT>\" , timeout , actorClass , sid , sapID ) \n Server . __init__ ( self , sid ) \n ServiceAccessPoint . __init__ ( self , sapID ) \n if not issubclass ( actorClass , TCPClientActor ) : \n raise TypeError ( \"<STR_LIT>\" ) \n self . actorClass = actorClass \n self . timeout = timeout \n self . clients = { } \n self . reconnect = { } \n def add_actor ( self , actor ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : TCPClientDirector . _debug ( \"<STR_LIT>\" , actor ) \n self . clients [ actor . peer ] = actor \n if self . serviceElement : \n self . sap_request ( addPeer = actor . peer ) \n def remove_actor ( self , actor ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : TCPClientDirector . _debug ( \"<STR_LIT>\" , actor ) \n del self . clients [ actor . peer ] \n if self . serviceElement : \n self . sap_request ( delPeer = actor . peer ) \n if actor . peer in self . reconnect : \n connect_task = FunctionTask ( self . connect , actor . peer ) \n connect_task . install_task ( _time ( ) + self . reconnect [ actor . peer ] ) \n def get_actor ( self , address ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . clients . get ( address , None ) \n def connect ( self , address , reconnect = <NUM_LIT:0> ) : \n if _debug : TCPClientDirector . _debug ( \"<STR_LIT>\" , address , reconnect ) \n if address in self . clients : \n return \n client = self . actorClass ( self , address ) \n if _debug : TCPClientDirector . _debug ( \"<STR_LIT>\" , client ) \n if reconnect : \n self . reconnect [ address ] = reconnect \n def disconnect ( self , address ) : \n if _debug : TCPClientDirector . _debug ( \"<STR_LIT>\" , address ) \n if address not in self . clients : \n return \n if address in self . reconnect : \n del self . reconnect [ address ] \n self . clients [ address ] . handle_close ( ) \n def indication ( self , pdu ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : TCPClientDirector . _debug ( \"<STR_LIT>\" , pdu ) \n addr = pdu . pduDestination \n client = self . clients . get ( addr , None ) \n if not client : \n client = self . actorClass ( self , addr ) \n client . indication ( pdu ) \n bacpypes_debugging ( TCPClientDirector ) \n class TCPServer ( asyncore . dispatcher ) : \n def __init__ ( self , sock , peer ) : \n if _debug : TCPServer . _debug ( \"<STR_LIT>\" , sock , peer ) \n asyncore . dispatcher . __init__ ( self , sock ) \n self . peer = peer \n self . request = '<STR_LIT>' \n self . socketError = None \n def handle_connect ( self ) : \n if _debug : deferred ( TCPServer . _debug , \"<STR_LIT>\" ) \n def readable ( self ) : \n return <NUM_LIT:1> \n def handle_read ( self ) : \n if _debug : deferred ( TCPServer . _debug , \"<STR_LIT>\" ) \n try : \n msg = self . recv ( <NUM_LIT> ) \n if _debug : deferred ( TCPServer . _debug , \"<STR_LIT>\" , len ( msg ) ) \n self . socketError = None \n if not self . socket : \n if _debug : deferred ( TCPServer . _debug , \"<STR_LIT>\" ) \n else : \n deferred ( self . response , PDU ( msg ) ) \n except socket . error , err : \n if ( err . args [ <NUM_LIT:0> ] == <NUM_LIT> ) : \n deferred ( TCPServer . _error , \"<STR_LIT>\" , self . peer ) \n else : \n deferred ( TCPServer . _error , \"<STR_LIT>\" , err ) \n self . socketError = err \n def writable ( self ) : \n return ( len ( self . request ) != <NUM_LIT:0> ) \n def handle_write ( self ) : \n if _debug : deferred ( TCPServer . _debug , \"<STR_LIT>\" ) \n try : \n sent = self . send ( self . request ) \n if _debug : deferred ( TCPServer . _debug , \"<STR_LIT>\" , sent , len ( self . request ) - sent ) \n self . socketError = None \n self . request = self . request [ sent : ] \n except socket . error , why : \n if ( why . args [ <NUM_LIT:0> ] == <NUM_LIT> ) : \n deferred ( TCPServer . _error , \"<STR_LIT>\" , self . peer ) \n else : \n deferred ( TCPServer . _error , \"<STR_LIT>\" , why ) \n self . socketError = why \n def handle_close ( self ) : \n if _debug : deferred ( TCPServer . _debug , \"<STR_LIT>\" ) \n if not self : \n deferred ( TCPServer . _warning , \"<STR_LIT>\" ) \n return \n if not self . socket : \n deferred ( TCPServer . _warning , \"<STR_LIT>\" ) \n return \n self . close ( ) \n self . socket = None \n def indication ( self , pdu ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : TCPServer . _debug ( \"<STR_LIT>\" , pdu ) \n self . request += pdu . pduData \n bacpypes_debugging ( TCPServer ) \n class TCPServerActor ( TCPServer ) : \n def __init__ ( self , director , sock , peer ) : \n if _debug : TCPServerActor . _debug ( \"<STR_LIT>\" , director , sock , peer ) \n TCPServer . __init__ ( self , sock , peer ) \n self . director = director \n self . timeout = director . timeout \n if self . timeout > <NUM_LIT:0> : \n self . timer = FunctionTask ( self . idle_timeout ) \n self . timer . install_task ( _time ( ) + self . timeout ) \n else : \n self . timer = None \n self . flushTask = None \n self . director . add_actor ( self ) \n def handle_close ( self ) : \n if _debug : TCPServerActor . _debug ( \"<STR_LIT>\" ) \n if self . flushTask : \n self . flushTask . suspend_task ( ) \n self . director . remove_actor ( self ) \n TCPServer . handle_close ( self ) \n def idle_timeout ( self ) : \n if _debug : TCPServerActor . _debug ( \"<STR_LIT>\" ) \n self . handle_close ( ) \n def indication ( self , pdu ) : \n if _debug : TCPServerActor . _debug ( \"<STR_LIT>\" , pdu ) \n if self . flushTask : \n if _debug : TCPServerActor . _debug ( \"<STR_LIT>\" ) \n return \n if self . timer : \n self . timer . install_task ( _time ( ) + self . timeout ) \n TCPServer . indication ( self , pdu ) \n def response ( self , pdu ) : \n if _debug : TCPServerActor . _debug ( \"<STR_LIT>\" , pdu ) \n if self . flushTask : \n if _debug : TCPServerActor . _debug ( \"<STR_LIT>\" ) \n return \n pdu . pduSource = self . peer \n if self . timer : \n self . timer . install_task ( _time ( ) + self . timeout ) \n self . director . response ( pdu ) \n def flush ( self ) : \n if _debug : TCPServerActor . _debug ( \"<STR_LIT>\" ) \n self . flushTask = None \n if self . request : \n self . flushTask = OneShotFunction ( self . flush ) \n return \n self . handle_close ( ) \n bacpypes_debugging ( TCPServerActor ) \n class TCPPickleServerActor ( PickleActorMixIn , TCPServerActor ) : \n pass \n class TCPServerDirector ( asyncore . dispatcher , Server , ServiceAccessPoint , DebugContents ) : \n _debug_contents = ( '<STR_LIT:port>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n def __init__ ( self , address , listeners = <NUM_LIT:5> , timeout = <NUM_LIT:0> , reuse = False , actorClass = TCPServerActor , cid = None , sapID = None ) : \n if _debug : \n TCPServerDirector . _debug ( \"<STR_LIT>\" \n , address , listeners , timeout , reuse , actorClass , cid , sapID \n ) \n Server . __init__ ( self , cid ) \n ServiceAccessPoint . __init__ ( self , sapID ) \n self . port = address \n self . timeout = timeout \n if not issubclass ( actorClass , TCPServerActor ) : \n raise TypeError ( \"<STR_LIT>\" ) \n self . actorClass = actorClass \n self . servers = { } \n asyncore . dispatcher . __init__ ( self ) \n self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) \n if reuse : \n self . set_reuse_addr ( ) \n hadBindErrors = False \n for i in range ( <NUM_LIT:30> ) : \n try : \n self . bind ( address ) \n break \n except socket . error , err : \n hadBindErrors = True \n TCPServerDirector . _warning ( '<STR_LIT>' , err ) \n _sleep ( REBIND_SLEEP_INTERVAL ) \n else : \n TCPServerDirector . _error ( '<STR_LIT>' ) \n raise RuntimeError ( \"<STR_LIT>\" ) \n if hadBindErrors : \n TCPServerDirector . _info ( '<STR_LIT>' ) \n self . listen ( listeners ) \n def handle_accept ( self ) : \n if _debug : TCPServerDirector . _debug ( \"<STR_LIT>\" ) \n try : \n client , addr = self . accept ( ) \n except socket . error : \n TCPServerDirector . _warning ( '<STR_LIT>' ) \n return \n except TypeError : \n TCPServerDirector . _warning ( '<STR_LIT>' ) \n return \n if _debug : TCPServerDirector . _debug ( \"<STR_LIT>\" , client , addr ) \n server = self . actorClass ( self , client , addr ) \n self . servers [ addr ] = server \n return server \n def handle_close ( self ) : \n if _debug : TCPServerDirector . _debug ( \"<STR_LIT>\" ) \n self . close ( ) \n def add_actor ( self , actor ) : \n if _debug : TCPServerDirector . _debug ( \"<STR_LIT>\" , actor ) \n self . servers [ actor . peer ] = actor \n if self . serviceElement : \n self . sap_request ( addPeer = actor . peer ) \n def remove_actor ( self , actor ) : \n if _debug : TCPServerDirector . _debug ( \"<STR_LIT>\" , actor ) \n try : \n del self . servers [ actor . peer ] \n except KeyError : \n TCPServerDirector . _warning ( \"<STR_LIT>\" , actor ) \n if self . serviceElement : \n self . sap_request ( delPeer = actor . peer ) \n def get_actor ( self , address ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . servers . get ( address , None ) \n def indication ( self , pdu ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : TCPServerDirector . _debug ( \"<STR_LIT>\" , pdu ) \n addr = pdu . pduDestination \n server = self . servers . get ( addr , None ) \n if not server : \n raise RuntimeError ( \"<STR_LIT>\" ) \n server . indication ( pdu ) \n bacpypes_debugging ( TCPServerDirector ) \n class StreamToPacket ( Client , Server ) : \n def __init__ ( self , fn , cid = None , sid = None ) : \n if _debug : StreamToPacket . _debug ( \"<STR_LIT>\" , fn , cid , sid ) \n Client . __init__ ( self , cid ) \n Server . __init__ ( self , sid ) \n self . packetFn = fn \n self . upstreamBuffer = { } \n self . downstreamBuffer = { } \n def packetize ( self , pdu , streamBuffer ) : \n if _debug : StreamToPacket . _debug ( \"<STR_LIT>\" , pdu ) \n def chop ( addr ) : \n if _debug : StreamToPacket . _debug ( \"<STR_LIT>\" , addr ) \n buff = streamBuffer . get ( addr , '<STR_LIT>' ) + pdu . pduData \n if _debug : StreamToPacket . _debug ( \"<STR_LIT>\" , buff ) \n while <NUM_LIT:1> : \n packet = self . packetFn ( buff ) \n if packet is None : \n break \n yield PDU ( packet [ <NUM_LIT:0> ] , \n source = pdu . pduSource , \n destination = pdu . pduDestination , \n user_data = pdu . pduUserData , \n ) \n buff = packet [ <NUM_LIT:1> ] \n streamBuffer [ addr ] = buff \n if pdu . pduSource : \n for pdu in chop ( pdu . pduSource ) : \n yield pdu \n if pdu . pduDestination : \n for pdu in chop ( pdu . pduDestination ) : \n yield pdu \n def indication ( self , pdu ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : StreamToPacket . _debug ( \"<STR_LIT>\" , pdu ) \n for packet in self . packetize ( pdu , self . downstreamBuffer ) : \n self . request ( packet ) \n def confirmation ( self , pdu ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : StreamToPacket . _debug ( \"<STR_LIT>\" , pdu ) \n for packet in self . packetize ( pdu , self . upstreamBuffer ) : \n self . response ( packet ) \n bacpypes_debugging ( StreamToPacket ) \n class StreamToPacketSAP ( ApplicationServiceElement , ServiceAccessPoint ) : \n def __init__ ( self , stp , aseID = None , sapID = None ) : \n if _debug : StreamToPacketSAP . _debug ( \"<STR_LIT>\" , stp , aseID , sapID ) \n ApplicationServiceElement . __init__ ( self , aseID ) \n ServiceAccessPoint . __init__ ( self , sapID ) \n self . stp = stp \n def indication ( self , addPeer = None , delPeer = None ) : \n if _debug : StreamToPacketSAP . _debug ( \"<STR_LIT>\" , addPeer , delPeer ) \n if addPeer : \n self . stp . upstreamBuffer [ addPeer ] = '<STR_LIT>' \n self . stp . downstreamBuffer [ addPeer ] = '<STR_LIT>' \n if delPeer : \n del self . stp . upstreamBuffer [ delPeer ] \n del self . stp . downstreamBuffer [ delPeer ] \n if self . serviceElement : \n self . sap_request ( addPeer = addPeer , delPeer = delPeer ) \n bacpypes_debugging ( <mask0> ) \n", "gt": "StreamToPacketSAP"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import random \n from copy import deepcopy \n from . errors import ConfigurationError \n from . debugging import ModuleLogger , bacpypes_debugging \n from . core import deferred \n from . pdu import Address \n from . comm import Server \n _debug = <NUM_LIT:0> \n _log = ModuleLogger ( globals ( ) ) \n @ bacpypes_debugging \n class Network : \n def __init__ ( self , dropPercent = <NUM_LIT:0.0> ) : \n if _debug : Network . _debug ( \"<STR_LIT>\" , dropPercent ) \n self . nodes = [ ] \n self . dropPercent = dropPercent \n def add_node ( self , node ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : Network . _debug ( \"<STR_LIT>\" , node ) \n self . nodes . append ( node ) \n node . lan = self \n def remove_node ( self , node ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : Network . _debug ( \"<STR_LIT>\" , node ) \n self . nodes . remove ( node ) \n node . lan = None \n def process_pdu ( self , pdu ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : Network . _debug ( \"<STR_LIT>\" , pdu ) \n if self . dropPercent != <NUM_LIT:0.0> : \n if ( random . random ( ) * <NUM_LIT> ) < self . dropPercent : \n if _debug : Network . _debug ( \"<STR_LIT>\" ) \n return \n if not pdu . pduDestination or not isinstance ( pdu . pduDestination , Address ) : \n raise RuntimeError ( \"<STR_LIT>\" ) \n elif pdu . pduDestination . addrType == Address . localBroadcastAddr : \n for n in self . nodes : \n if ( pdu . pduSource != n . address ) : \n n . response ( deepcopy ( pdu ) ) \n elif pdu . pduDestination . addrType == Address . localStationAddr : \n for n in self . nodes : \n if n . promiscuous or ( pdu . pduDestination == n . address ) : \n n . response ( deepcopy ( pdu ) ) \n else : \n raise RuntimeError ( \"<STR_LIT>\" ) \n def __len__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : Network . _debug ( \"<STR_LIT>\" ) \n return len ( self . nodes ) \n @ bacpypes_debugging \n class Node ( Server ) : \n def __init__ ( self , addr , lan = None , promiscuous = False , spoofing = False , sid = None ) : \n if _debug : \n Node . _debug ( \"<STR_LIT>\" , \n addr , lan , promiscuous , spoofing , sid \n ) \n Server . __init__ ( self , sid ) \n if not isinstance ( addr , Address ) : \n raise TypeError ( \"<STR_LIT>\" ) \n self . lan = None \n self . address = addr \n if lan : \n self . bind ( lan ) \n self . promiscuous = promiscuous \n self . spoofing = spoofing \n def bind ( self , lan ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : Node . _debug ( \"<STR_LIT>\" , lan ) \n lan . add_node ( self ) \n def indication ( self , pdu ) : \n \"\"\"<STR_LIT>\"\"\" \n if _debug : Node . _debug ( \"<STR_LIT>\" , pdu ) \n if not self . lan : \n raise ConfigurationError ( \"<STR_LIT>\" ) \n if pdu . pduSource is None : \n pdu . pduSource = self . address \n elif ( not self . spoofing ) and ( pdu . pduSource != self . address ) : \n raise RuntimeError ( \"<STR_LIT>\" ) \n deferred ( self . lan . process_pdu , <mask0> ) \n", "gt": "pdu"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n from bacpypes . debugging import bacpypes_debugging , ModuleLogger \n from bacpypes . consolelogging import ConfigArgumentParser \n from bacpypes . consolecmd import ConsoleCmd \n from bacpypes . core import run \n from bacpypes . pdu import Address \n from bacpypes . app import LocalDeviceObject , BIPSimpleApplication \n from bacpypes . apdu import Error , AbortPDU , AtomicReadFileRequest , AtomicReadFileRequestAccessMethodChoice , AtomicReadFileRequestAccessMethodChoiceRecordAccess , AtomicReadFileRequestAccessMethodChoiceStreamAccess , AtomicReadFileACK , AtomicWriteFileRequest , AtomicWriteFileRequestAccessMethodChoice , AtomicWriteFileRequestAccessMethodChoiceRecordAccess , AtomicWriteFileRequestAccessMethodChoiceStreamAccess , AtomicWriteFileACK \n from bacpypes . basetypes import ServicesSupported \n _debug = <NUM_LIT:0> \n _log = ModuleLogger ( globals ( ) ) \n this_application = None \n @ bacpypes_debugging \n class TestApplication ( BIPSimpleApplication ) : \n def request ( self , apdu ) : \n if _debug : TestApplication . _debug ( \"<STR_LIT>\" , apdu ) \n self . _request = apdu \n BIPSimpleApplication . request ( self , apdu ) \n def confirmation ( self , apdu ) : \n if _debug : TestApplication . _debug ( \"<STR_LIT>\" , apdu ) \n if isinstance ( apdu , Error ) : \n sys . stdout . write ( \"<STR_LIT>\" % ( apdu . errorCode , ) ) \n sys . stdout . flush ( ) \n elif isinstance ( apdu , AbortPDU ) : \n apdu . debug_contents ( ) \n elif ( isinstance ( self . _request , AtomicReadFileRequest ) ) and ( isinstance ( apdu , AtomicReadFileACK ) ) : \n if apdu . accessMethod . recordAccess : \n value = apdu . accessMethod . recordAccess . fileRecordData \n elif apdu . accessMethod . streamAccess : \n value = apdu . accessMethod . streamAccess . fileData \n TestApplication . _debug ( \"<STR_LIT>\" , value ) \n sys . stdout . write ( repr ( value ) + '<STR_LIT:\\n>' ) \n sys . stdout . flush ( ) \n elif ( isinstance ( self . _request , AtomicWriteFileRequest ) ) and ( isinstance ( apdu , AtomicWriteFileACK ) ) : \n if apdu . fileStartPosition is not None : \n value = apdu . fileStartPosition \n elif apdu . fileStartRecord is not None : \n value = apdu . fileStartRecord \n TestApplication . _debug ( \"<STR_LIT>\" , value ) \n sys . stdout . write ( repr ( value ) + '<STR_LIT:\\n>' ) \n sys . stdout . flush ( ) \n @ bacpypes_debugging \n class TestConsoleCmd ( ConsoleCmd ) : \n def do_readrecord ( self , args ) : \n \"\"\"<STR_LIT>\"\"\" \n args = args . split ( ) \n if _debug : TestConsoleCmd . _debug ( \"<STR_LIT>\" , args ) \n try : \n addr , obj_inst , start_record , record_count = args \n obj_type = '<STR_LIT:file>' \n obj_inst = int ( obj_inst ) \n start_record = int ( start_record ) \n record_count = int ( record_count ) \n request = AtomicReadFileRequest ( \n fileIdentifier = ( obj_type , obj_inst ) , \n accessMethod = AtomicReadFileRequestAccessMethodChoice ( \n recordAccess = AtomicReadFileRequestAccessMethodChoiceRecordAccess ( \n fileStartRecord = start_record , \n requestedRecordCount = record_count , \n ) , \n ) , \n ) \n request . pduDestination = Address ( addr ) \n if _debug : TestConsoleCmd . _debug ( \"<STR_LIT>\" , request ) \n this_application . request ( request ) \n except Exception , e : \n TestConsoleCmd . _exception ( \"<STR_LIT>\" , e ) \n def do_readstream ( self , args ) : \n \"\"\"<STR_LIT>\"\"\" \n args = args . split ( ) \n if _debug : TestConsoleCmd . _debug ( \"<STR_LIT>\" , args ) \n try : \n addr , obj_inst , start_position , octet_count = args \n obj_type = '<STR_LIT:file>' \n obj_inst = int ( obj_inst ) \n start_position = int ( start_position ) \n octet_count = int ( octet_count ) \n request = AtomicReadFileRequest ( \n fileIdentifier = ( obj_type , obj_inst ) , \n accessMethod = AtomicReadFileRequestAccessMethodChoice ( \n streamAccess = AtomicReadFileRequestAccessMethodChoiceStreamAccess ( \n fileStartPosition = start_position , \n requestedOctetCount = octet_count , \n ) , \n ) , \n ) \n request . pduDestination = Address ( addr ) \n if _debug : TestConsoleCmd . _debug ( \"<STR_LIT>\" , request ) \n this_application . request ( request ) \n except Exception , e : \n TestConsoleCmd . _exception ( \"<STR_LIT>\" , e ) \n def do_writerecord ( self , args ) : \n \"\"\"<STR_LIT>\"\"\" \n args = args . split ( ) \n if _debug : TestConsoleCmd . _debug ( \"<STR_LIT>\" , args ) \n try : \n addr , obj_inst , start_record , record_count = args [ <NUM_LIT:0> : <NUM_LIT:4> ] \n obj_type = '<STR_LIT:file>' \n obj_inst = int ( obj_inst ) \n start_record = int ( start_record ) \n record_count = int ( record_count ) \n record_data = list ( args [ <NUM_LIT:4> : ] ) \n request = AtomicWriteFileRequest ( \n fileIdentifier = ( obj_type , obj_inst ) , \n accessMethod = AtomicWriteFileRequestAccessMethodChoice ( \n recordAccess = AtomicWriteFileRequestAccessMethodChoiceRecordAccess ( \n fileStartRecord = start_record , \n recordCount = record_count , \n fileRecordData = record_data , \n ) , \n ) , \n ) \n request . pduDestination = Address ( addr ) \n if _debug : TestConsoleCmd . _debug ( \"<STR_LIT>\" , request ) \n this_application . request ( request ) \n except Exception , e : \n TestConsoleCmd . _exception ( \"<STR_LIT>\" , e ) \n def do_writestream ( self , args ) : \n \"\"\"<STR_LIT>\"\"\" \n args = args . split ( ) \n if _debug : TestConsoleCmd . _debug ( \"<STR_LIT>\" , args ) \n try : \n addr , obj_inst , start_position , data = args \n obj_type = '<STR_LIT:file>' \n obj_inst = int ( obj_inst ) \n start_position = int ( start_position ) \n request = AtomicWriteFileRequest ( \n fileIdentifier = ( obj_type , obj_inst ) , \n accessMethod = AtomicWriteFileRequestAccessMethodChoice ( \n streamAccess = AtomicWriteFileRequestAccessMethodChoiceStreamAccess ( \n fileStartPosition = start_position , \n fileData = data , \n ) , \n ) , \n ) \n request . pduDestination = Address ( addr ) \n if _debug : TestConsoleCmd . _debug ( \"<STR_LIT>\" , request ) \n this_application . request ( request ) \n except Exception , e : \n TestConsoleCmd . _exception ( \"<STR_LIT>\" , e ) \n try : \n args = ConfigArgumentParser ( description = __doc__ ) . parse_args ( ) \n if _debug : _log . debug ( \"<STR_LIT>\" ) \n if _debug : _log . debug ( \"<STR_LIT>\" , args ) \n this_device = LocalDeviceObject ( \n objectName = args . ini . objectname , \n objectIdentifier = int ( args . ini . objectidentifier ) , \n maxApduLengthAccepted = int ( args . ini . maxapdulengthaccepted ) , \n segmentationSupported = args . ini . segmentationsupported , \n vendorIdentifier = int ( args . ini . vendoridentifier ) , \n ) \n this_application = TestApplication ( this_device , args . ini . address ) \n services_supported = this_application . get_services_supported ( ) \n if _debug : _log . debug ( \"<STR_LIT>\" , services_supported ) \n this_device . protocolServicesSupported = services_supported . value \n this_console = TestConsoleCmd ( ) \n _log . debug ( \"<STR_LIT>\" ) \n run ( ) \n except Exception , e : \n _log . exception ( \"<STR_LIT>\" , e ) \n finally : \n _log . <mask0> ( \"<STR_LIT>\" ) \n", "gt": "debug"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from . import <mask0> \n", "gt": "test_address"}
{"input": "\n from __future__ import division , unicode_literals \n import os \n import hashlib \n import logging \n from collections import defaultdict \n from . bencode import bencode , bdecode \n from . humanize import humanize_bytes \n from . utils import is_unsplitable , get_root_of_unsplitable , Pieces \n logger = logging . getLogger ( '<STR_LIT>' ) \n class Color : \n BLACK = '<STR_LIT>' \n RED = '<STR_LIT>' \n GREEN = '<STR_LIT>' \n YELLOW = '<STR_LIT>' \n BLUE = '<STR_LIT>' \n PINK = '<STR_LIT>' \n CYAN = '<STR_LIT>' \n WHITE = '<STR_LIT>' \n ENDC = '<STR_LIT>' \n COLOR_OK = Color . GREEN \n COLOR_MISSING_FILES = Color . RED \n COLOR_ALREADY_SEEDING = Color . BLUE \n COLOR_FOLDER_EXIST_NOT_SEEDING = Color . YELLOW \n COLOR_FAILED_TO_ADD_TO_CLIENT = Color . PINK \n class Status : \n OK = <NUM_LIT:0> \n MISSING_FILES = <NUM_LIT:1> \n ALREADY_SEEDING = <NUM_LIT:2> \n FOLDER_EXIST_NOT_SEEDING = <NUM_LIT:3> \n FAILED_TO_ADD_TO_CLIENT = <NUM_LIT:4> \n status_messages = { \n Status . OK : '<STR_LIT>' % ( COLOR_OK , Color . ENDC ) , \n Status . MISSING_FILES : '<STR_LIT>' % ( COLOR_MISSING_FILES , Color . ENDC ) , \n Status . ALREADY_SEEDING : '<STR_LIT>' % ( COLOR_ALREADY_SEEDING , Color . ENDC ) , \n Status . FOLDER_EXIST_NOT_SEEDING : '<STR_LIT>' % ( COLOR_FOLDER_EXIST_NOT_SEEDING , Color . ENDC ) , \n Status . FAILED_TO_ADD_TO_CLIENT : '<STR_LIT>' % ( COLOR_FAILED_TO_ADD_TO_CLIENT , Color . ENDC ) , \n } \n CHUNK_SIZE = <NUM_LIT> \n class UnknownLinkTypeException ( Exception ) : \n pass \n class IllegalPathException ( Exception ) : \n pass \n class AutoTorrent ( object ) : \n def __init__ ( self , db , client , store_path , add_limit_size , add_limit_percent , delete_torrents , link_type = '<STR_LIT>' ) : \n self . db = db \n self . client = client \n self . store_path = store_path \n self . add_limit_size = add_limit_size \n self . add_limit_percent = add_limit_percent \n self . delete_torrents = delete_torrents \n self . link_type = link_type \n self . torrents_seeded = set ( ) \n def try_decode ( self , value ) : \n try : \n return value . decode ( '<STR_LIT:utf-8>' ) \n except UnicodeDecodeError : \n logger . debug ( '<STR_LIT>' % value ) \n return value . decode ( '<STR_LIT>' ) \n def is_legal_path ( self , path ) : \n for p in path : \n if p in [ '<STR_LIT:.>' , '<STR_LIT:..>' ] or '<STR_LIT:/>' in p : \n return False \n return True \n def populate_torrents_seeded ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . torrents_seeded = set ( x . lower ( ) for x in self . client . get_torrents ( ) ) \n def get_info_hash ( self , torrent ) : \n \"\"\"<STR_LIT>\"\"\" \n return hashlib . sha1 ( bencode ( torrent [ b'<STR_LIT:info>' ] ) ) . hexdigest ( ) \n def find_hash_checks ( self , torrent , result ) : \n \"\"\"<STR_LIT>\"\"\" \n modified_result = False \n pieces = Pieces ( torrent ) \n if self . db . hash_slow_mode : \n logger . info ( '<STR_LIT>' ) \n self . db . build_hash_size_table ( ) \n start_size = <NUM_LIT:0> \n end_size = <NUM_LIT:0> \n logger . info ( '<STR_LIT>' ) \n for f in result : \n start_size = end_size \n end_size += f [ '<STR_LIT>' ] \n if f [ '<STR_LIT>' ] : \n continue \n files_to_check = [ ] \n logger . debug ( '<STR_LIT>' ) \n if self . db . hash_size_mode : \n logger . debug ( '<STR_LIT>' ) \n files_to_check += self . db . find_hash_size ( f [ '<STR_LIT>' ] ) \n if self . db . hash_name_mode : \n logger . debug ( '<STR_LIT>' ) \n name = f [ '<STR_LIT:path>' ] [ - <NUM_LIT:1> ] \n files_to_check += self . db . find_hash_name ( name ) \n if self . db . hash_slow_mode : \n logger . debug ( '<STR_LIT>' ) \n files_to_check += self . db . find_hash_varying_size ( f [ '<STR_LIT>' ] ) \n logger . debug ( '<STR_LIT>' % len ( files_to_check ) ) \n checked_files = set ( ) \n for db_file in files_to_check : \n if db_file in checked_files : \n logger . debug ( '<STR_LIT>' % db_file ) \n checked_files . add ( db_file ) \n logger . info ( '<STR_LIT>' % db_file ) \n match_start , match_end = pieces . match_file ( db_file , start_size , end_size ) \n logger . info ( '<STR_LIT>' % ( db_file , match_start , match_end ) ) \n if match_start or match_end : \n size = os . path . getsize ( db_file ) \n if size != f [ '<STR_LIT>' ] : \n logger . debug ( '<STR_LIT>' ) \n if match_start and match_end : \n logger . debug ( '<STR_LIT>' ) \n modification_point = pieces . find_piece_breakpoint ( db_file , start_size , end_size ) \n elif match_start : \n logger . debug ( '<STR_LIT>' ) \n modification_point = min ( f [ '<STR_LIT>' ] , size ) \n elif match_end : \n logger . debug ( '<STR_LIT>' ) \n modification_point = <NUM_LIT:0> \n if size > f [ '<STR_LIT>' ] : \n modification_action = '<STR_LIT>' \n else : \n modification_action = '<STR_LIT>' \n f [ '<STR_LIT>' ] = False \n f [ '<STR_LIT>' ] = ( '<STR_LIT>' , modification_action , modification_point ) \n modified_result = True \n else : \n logger . debug ( '<STR_LIT>' ) \n f [ '<STR_LIT>' ] = True \n f [ '<STR_LIT>' ] = db_file \n break \n return modified_result , result \n def index_torrent ( self , torrent ) : \n \"\"\"<STR_LIT>\"\"\" \n torrent_name = torrent [ b'<STR_LIT:info>' ] [ b'<STR_LIT:name>' ] \n logger . debug ( '<STR_LIT>' % ( torrent_name , ) ) \n torrent_name = self . try_decode ( torrent_name ) \n if not self . is_legal_path ( [ torrent_name ] ) : \n raise IllegalPathException ( '<STR_LIT>' % torrent_name ) \n logger . info ( '<STR_LIT>' % torrent_name ) \n if self . db . exact_mode : \n prefix = '<STR_LIT:d>' if b'<STR_LIT>' in torrent [ b'<STR_LIT:info>' ] else '<STR_LIT:f>' \n paths = self . db . find_exact_file_path ( prefix , torrent_name ) \n if paths : \n for path in paths : \n logger . debug ( '<STR_LIT>' % path ) \n if prefix == '<STR_LIT:f>' : \n logger . info ( '<STR_LIT>' ) \n size = os . path . getsize ( path ) \n if torrent [ b'<STR_LIT:info>' ] [ b'<STR_LIT>' ] != size : \n continue \n return { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : os . path . dirname ( path ) , \n '<STR_LIT>' : [ { \n '<STR_LIT>' : path , \n '<STR_LIT>' : size , \n '<STR_LIT:path>' : [ torrent_name ] , \n '<STR_LIT>' : True , \n } ] } \n else : \n result = [ ] \n for f in torrent [ b'<STR_LIT:info>' ] [ b'<STR_LIT>' ] : \n orig_path = [ self . try_decode ( x ) for x in f [ b'<STR_LIT:path>' ] ] \n p = os . path . join ( path , * orig_path ) \n if not os . path . isfile ( p ) : \n logger . debug ( '<STR_LIT>' % p ) \n break \n size = os . path . getsize ( p ) \n if size != f [ b'<STR_LIT>' ] : \n logger . debug ( '<STR_LIT>' % ( p , size , f [ b'<STR_LIT>' ] ) ) \n break \n result . append ( { \n '<STR_LIT>' : p , \n '<STR_LIT>' : f [ b'<STR_LIT>' ] , \n '<STR_LIT:path>' : orig_path , \n '<STR_LIT>' : True , \n } ) \n else : \n logger . info ( '<STR_LIT>' ) \n return { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : path , \n '<STR_LIT>' : result } \n result = [ ] \n if b'<STR_LIT>' in torrent [ b'<STR_LIT:info>' ] : \n files_sorted = { } \n files = { } \n if b'<STR_LIT>' in torrent [ b'<STR_LIT:info>' ] : \n i = <NUM_LIT:0> \n path_files = defaultdict ( list ) \n for f in torrent [ b'<STR_LIT:info>' ] [ b'<STR_LIT>' ] : \n logger . debug ( '<STR_LIT>' % ( f , ) ) \n orig_path = [ self . try_decode ( x ) for x in f [ b'<STR_LIT:path>' ] if x ] \n if not self . is_legal_path ( orig_path ) : \n raise IllegalPathException ( '<STR_LIT>' % orig_path ) \n path = [ torrent_name ] + orig_path \n name = path . pop ( ) \n path_files [ os . path . join ( * path ) ] . append ( { \n '<STR_LIT:path>' : orig_path , \n '<STR_LIT>' : f [ b'<STR_LIT>' ] , \n } ) \n files_sorted [ '<STR_LIT:/>' . join ( orig_path ) ] = i \n i += <NUM_LIT:1> \n if self . db . unsplitable_mode : \n unsplitable_paths = set ( ) \n for path , files in path_files . items ( ) : \n if is_unsplitable ( f [ '<STR_LIT:path>' ] [ - <NUM_LIT:1> ] for f in files ) : \n path = path . split ( os . sep ) \n name = get_root_of_unsplitable ( path ) \n if not name : \n continue \n while path [ - <NUM_LIT:1> ] != name : \n path . pop ( ) \n unsplitable_paths . add ( os . path . join ( * path ) ) \n for path , files in path_files . items ( ) : \n if self . db . unsplitable_mode : \n path = path . split ( os . sep ) \n while path and os . path . join ( * path ) not in unsplitable_paths : \n path . pop ( ) \n else : \n path = None \n if path : \n name = path [ - <NUM_LIT:1> ] \n for f in files : \n actual_path = self . db . find_unsplitable_file_path ( name , f [ '<STR_LIT:path>' ] , f [ '<STR_LIT>' ] ) \n f [ '<STR_LIT>' ] = actual_path \n f [ '<STR_LIT>' ] = actual_path is not None \n result += files \n else : \n for f in files : \n actual_path = self . db . find_file_path ( f [ '<STR_LIT:path>' ] [ - <NUM_LIT:1> ] , f [ '<STR_LIT>' ] ) \n f [ '<STR_LIT>' ] = actual_path \n f [ '<STR_LIT>' ] = actual_path is not None \n result += files \n result = sorted ( result , key = lambda x : files_sorted [ '<STR_LIT:/>' . join ( x [ '<STR_LIT:path>' ] ) ] ) \n else : \n length = torrent [ b'<STR_LIT:info>' ] [ b'<STR_LIT>' ] \n actual_path = self . db . find_file_path ( torrent_name , length ) \n result . append ( { \n '<STR_LIT>' : actual_path , \n '<STR_LIT>' : length , \n '<STR_LIT:path>' : [ torrent_name ] , \n '<STR_LIT>' : actual_path is not None , \n } ) \n mode = '<STR_LIT>' \n if self . db . hash_mode : \n modified_result , result = self . find_hash_checks ( torrent , result ) \n if modified_result : \n mode = '<STR_LIT>' \n return { '<STR_LIT>' : mode , '<STR_LIT>' : result } \n def parse_torrent ( self , torrent ) : \n \"\"\"<STR_LIT>\"\"\" \n files = self . index_torrent ( torrent ) \n found_size , missing_size = <NUM_LIT:0> , <NUM_LIT:0> \n for f in files [ '<STR_LIT>' ] : \n if f [ '<STR_LIT>' ] or f . get ( '<STR_LIT>' ) : \n found_size += f [ '<STR_LIT>' ] \n else : \n missing_size += f [ '<STR_LIT>' ] \n return found_size , missing_size , files \n def link_files ( self , destination_path , files ) : \n \"\"\"<STR_LIT>\"\"\" \n if not os . path . isdir ( destination_path ) : \n os . makedirs ( destination_path ) \n for f in files : \n if f [ '<STR_LIT>' ] : \n destination = os . path . join ( destination_path , * f [ '<STR_LIT:path>' ] ) \n file_path = os . path . dirname ( destination ) \n if not os . path . isdir ( file_path ) : \n logger . debug ( '<STR_LIT>' % file_path ) \n os . makedirs ( file_path ) \n logger . debug ( '<STR_LIT>' % ( self . link_type , f [ '<STR_LIT>' ] , destination ) ) \n if self . link_type == '<STR_LIT>' : \n os . symlink ( f [ '<STR_LIT>' ] , destination ) \n elif self . link_type == '<STR_LIT>' : \n os . link ( f [ '<STR_LIT>' ] , destination ) \n else : \n raise UnknownLinkTypeException ( '<STR_LIT>' % self . link_type ) \n def rewrite_hashed_files ( self , destination_path , files ) : \n \"\"\"<STR_LIT>\"\"\" \n if not os . path . isdir ( destination_path ) : \n os . makedirs ( destination_path ) \n for f in files : \n if not f [ '<STR_LIT>' ] and '<STR_LIT>' in f : \n destination = os . path . join ( destination_path , * f [ '<STR_LIT:path>' ] ) \n file_path = os . path . dirname ( destination ) \n if not os . path . isdir ( file_path ) : \n logger . debug ( '<STR_LIT>' % file_path ) \n os . makedirs ( file_path ) \n logger . debug ( '<STR_LIT>' % ( f [ '<STR_LIT>' ] , destination ) ) \n _ , modification_action , modification_point = f [ '<STR_LIT>' ] \n current_size = os . path . getsize ( f [ '<STR_LIT>' ] ) \n expected_size = f [ '<STR_LIT>' ] \n diff = abs ( current_size - expected_size ) \n modified = False \n bytes_written = <NUM_LIT:0> \n with open ( destination , '<STR_LIT:wb>' ) as output_fp : \n with open ( f [ '<STR_LIT>' ] , '<STR_LIT:rb>' ) as input_fp : \n logger . debug ( '<STR_LIT>' % ( f [ '<STR_LIT>' ] , destination , modification_point ) ) \n while True : \n if not modified and bytes_written == modification_point : \n logger . debug ( '<STR_LIT>' % ( modification_action , diff ) ) \n modified = True \n if modification_action == '<STR_LIT>' : \n seek_point = bytes_written + diff \n logger . debug ( '<STR_LIT>' % ( seek_point , ) ) \n input_fp . seek ( seek_point ) \n elif modification_action == '<STR_LIT>' : \n logger . debug ( '<STR_LIT>' % diff ) \n while diff > <NUM_LIT:0> : \n write_bytes = min ( CHUNK_SIZE , diff ) \n output_fp . write ( b'<STR_LIT:\\x00>' * write_bytes ) \n diff -= write_bytes \n read_bytes = CHUNK_SIZE \n if not modified : \n read_bytes = min ( read_bytes , modification_point - bytes_written ) \n logger . debug ( '<STR_LIT>' % ( read_bytes , ) ) \n data = input_fp . read ( read_bytes ) \n if not data : \n break \n output_fp . write ( data ) \n bytes_written += read_bytes \n logger . debug ( '<STR_LIT>' ) \n def handle_torrentfile ( self , path , dry_run = False ) : \n \"\"\"<STR_LIT>\"\"\" \n logger . info ( '<STR_LIT>' % path ) \n torrent = self . open_torrentfile ( path ) \n if self . check_torrent_in_client ( torrent ) : \n self . print_status ( Status . ALREADY_SEEDING , path , '<STR_LIT>' ) \n if self . delete_torrents : \n logger . info ( '<STR_LIT>' % path ) \n os . remove ( path ) \n return Status . ALREADY_SEEDING \n found_size , missing_size , files = self . parse_torrent ( torrent ) \n missing_percent = ( missing_size / ( found_size + missing_size ) ) * <NUM_LIT:100> \n found_percent = <NUM_LIT:100> - missing_percent \n would_not_add = missing_size and missing_percent > self . add_limit_percent or missing_size > self . add_limit_size \n if dry_run : \n return found_size , missing_size , would_not_add , [ f [ '<STR_LIT>' ] for f in files [ '<STR_LIT>' ] if f . get ( '<STR_LIT>' ) ] \n if would_not_add : \n logger . info ( '<STR_LIT>' % ( path , found_percent , humanize_bytes ( missing_size ) ) ) \n self . print_status ( Status . MISSING_FILES , path , '<STR_LIT>' % ( found_percent , humanize_bytes ( missing_size ) ) ) \n return Status . MISSING_FILES \n if files [ '<STR_LIT>' ] == '<STR_LIT>' or files [ '<STR_LIT>' ] == '<STR_LIT>' : \n logger . info ( '<STR_LIT>' ) \n destination_path = os . path . join ( self . store_path , os . path . splitext ( os . path . basename ( path ) ) [ <NUM_LIT:0> ] ) \n if os . path . isdir ( destination_path ) : \n logger . info ( '<STR_LIT>' % destination_path ) \n self . print_status ( Status . FOLDER_EXIST_NOT_SEEDING , path , '<STR_LIT>' ) \n return Status . FOLDER_EXIST_NOT_SEEDING \n self . link_files ( destination_path , files [ '<STR_LIT>' ] ) \n elif files [ '<STR_LIT>' ] == '<STR_LIT>' : \n logger . info ( '<STR_LIT>' ) \n destination_path = files [ '<STR_LIT>' ] \n fast_resume = True \n if files [ '<STR_LIT>' ] == '<STR_LIT>' : \n fast_resume = False \n logger . info ( '<STR_LIT>' ) \n self . rewrite_hashed_files ( destination_path , files [ '<STR_LIT>' ] ) \n if self . delete_torrents : \n logger . info ( '<STR_LIT>' % path ) \n os . remove ( path ) \n if self . client . add_torrent ( torrent , destination_path , files [ '<STR_LIT>' ] , fast_resume ) : \n self . print_status ( Status . OK , path , '<STR_LIT>' ) \n return Status . OK \n else : \n self . print_status ( Status . FAILED_TO_ADD_TO_CLIENT , path , '<STR_LIT>' ) \n return Status . FAILED_TO_ADD_TO_CLIENT \n def check_torrent_in_client ( self , torrent ) : \n \"\"\"<STR_LIT>\"\"\" \n info_hash = self . get_info_hash ( torrent ) \n return info_hash in self . torrents_seeded \n def open_torrentfile ( self , path ) : \n \"\"\"<STR_LIT>\"\"\" \n with open ( path , '<STR_LIT:rb>' ) as f : \n return bdecode ( f . read ( ) ) \n def print_status ( self , status , torrentfile , message ) : \n print ( '<STR_LIT>' % ( '<STR_LIT>' % status_messages [ status ] , os . path . splitext ( os . path . basename ( torrentfile ) ) [ <NUM_LIT:0> ] , <mask0> ) ) \n", "gt": "message"}
{"input": "\n import pytest \n import exceptions \n def test_exceptions ( ) : \n with pytest . raises ( Exception ) : \n raise exceptions . CardinalException \n with pytest . raises ( exceptions . CardinalException ) : \n raise exceptions . InternalError \n with pytest . raises ( exceptions . CardinalException ) : \n raise exceptions . PluginError \n with pytest . raises ( exceptions . CardinalException ) : \n raise exceptions . CommandNotFoundError \n with pytest . raises ( exceptions . CardinalException ) : \n raise exceptions . ConfigNotFoundError \n with pytest . raises ( exceptions . CardinalException ) : \n raise exceptions . AmbiguousConfigError \n with pytest . raises ( exceptions . CardinalException ) : \n raise exceptions . EventAlreadyExistsError \n with pytest . raises ( exceptions . CardinalException ) : \n raise exceptions . EventDoesNotExistError \n with pytest . raises ( exceptions . CardinalException ) : \n raise exceptions . EventCallbackError \n with pytest . raises ( exceptions . CardinalException ) : \n raise exceptions . <mask0> \n", "gt": "EventRejectedMessage"}
{"input": "\n import os \n import legofy \n import tkinter as tk \n import tkinter . ttk as ttk \n from tkinter import filedialog \n import tkinter . messagebox as tkmsg \n LEGO_PALETTE = ( '<STR_LIT:none>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT:all>' , ) \n class LegofyGui ( tk . Tk ) : \n def __init__ ( self , * args , ** kwargs ) : \n super ( ) . __init__ ( * args , ** kwargs ) \n self . wm_title ( \"<STR_LIT>\" ) \n self . iconbitmap ( os . path . dirname ( os . path . realpath ( __file__ ) ) + '<STR_LIT>' ) \n self . resizable ( False , False ) \n self . body = LegofyGuiMainFrame ( self ) \n self . body . grid ( row = <NUM_LIT:0> , column = <NUM_LIT:0> , padx = <NUM_LIT:10> , pady = <NUM_LIT:10> ) \n class LegofyGuiMainFrame ( tk . Frame ) : \n def __init__ ( self , * args , ** kwargs ) : \n super ( ) . __init__ ( * args , ** kwargs ) \n self . chosenFile = None \n self . chosenFilePath = tk . StringVar ( ) \n self . pathField = tk . Entry ( self , width = <NUM_LIT> , textvariable = self . chosenFilePath , state = tk . DISABLED ) \n self . pathField . grid ( row = <NUM_LIT:0> , column = <NUM_LIT:0> , padx = <NUM_LIT:10> ) \n self . selectFile = tk . Button ( self , text = \"<STR_LIT>\" , command = self . choose_a_file ) \n self . selectFile . grid ( row = <NUM_LIT:0> , column = <NUM_LIT:1> ) \n self . groupFrame = tk . LabelFrame ( self , text = \"<STR_LIT>\" , padx = <NUM_LIT:5> , pady = <NUM_LIT:5> ) \n self . groupFrame . grid ( row = <NUM_LIT:1> , column = <NUM_LIT:0> , columnspan = <NUM_LIT:2> , ) \n self . colorPaletteLabel = tk . Label ( self . groupFrame , text = '<STR_LIT>' ) \n self . colorPaletteLabel . grid ( row = <NUM_LIT:0> , column = <NUM_LIT:0> ) \n self . colorPalette = ttk . Combobox ( self . groupFrame ) \n self . colorPalette [ '<STR_LIT>' ] = LEGO_PALETTE \n self . colorPalette . current ( <NUM_LIT:0> ) \n self . colorPalette . grid ( row = <NUM_LIT:0> , column = <NUM_LIT:1> ) \n self . brickNumberScale = tk . Scale ( self . groupFrame , from_ = <NUM_LIT:1> , to = <NUM_LIT:200> , orient = tk . HORIZONTAL , label = \"<STR_LIT>\" , length = <NUM_LIT> ) \n self . brickNumberScale . set ( <NUM_LIT:30> ) \n self . brickNumberScale . grid ( row = <NUM_LIT:1> , column = <NUM_LIT:0> , columnspan = <NUM_LIT:2> , ) \n self . convertFile = tk . Button ( text = \"<STR_LIT>\" , command = self . convert_file ) \n self . convertFile . grid ( row = <NUM_LIT:2> , column = <NUM_LIT:0> , columnspan = <NUM_LIT:2> ) \n def choose_a_file ( self ) : \n options = { } \n options [ '<STR_LIT>' ] = '<STR_LIT>' \n options [ '<STR_LIT>' ] = [ ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , ] \n options [ '<STR_LIT>' ] = os . path . realpath ( \"<STR_LIT:\\\\>\" ) \n options [ '<STR_LIT>' ] = '<STR_LIT>' \n options [ '<STR_LIT>' ] = self \n options [ '<STR_LIT:title>' ] = '<STR_LIT>' \n self . chosenFile = filedialog . askopenfile ( mode = '<STR_LIT:r>' , ** options ) \n if self . chosenFile : \n self . chosenFilePath . set ( self . chosenFile . name ) \n def convert_file ( self ) : \n try : \n if self . chosenFile is not None : \n palette = self . colorPalette . get ( ) \n if palette in LEGO_PALETTE and palette != '<STR_LIT:none>' : \n legofy . main ( self . chosenFile . name , size = self . brickNumberScale . get ( ) , palette_mode = palette ) \n else : \n legofy . main ( self . chosenFile . name , size = self . brickNumberScale . get ( ) ) \n tkmsg . showinfo ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n else : \n tkmsg . showerror ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n except Exception as e : \n tkmsg . showerror ( \"<STR_LIT>\" , str ( e ) ) \n if __name__ == '<STR_LIT:__main__>' : \n app = LegofyGui ( ) \n app . <mask0> ( ) \n", "gt": "mainloop"}
{"input": "\n from distutils . core import setup \n from condent import __version__ \n with open ( \"<STR_LIT>\" ) as readme : \n long_description = readme . read ( ) \n classifiers = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n setup ( \n name = \"<STR_LIT>\" , \n version = __version__ , \n py_modules = [ \"<STR_LIT>\" ] , \n scripts = [ \"<STR_LIT>\" ] , \n author = \"<STR_LIT>\" , \n author_email = \"<STR_LIT>\" , \n classifiers = classifiers , \n description = \"<STR_LIT>\" , \n license = \"<STR_LIT>\" , \n long_description = long_description , \n <mask0> = \"<STR_LIT>\" , \n ) \n", "gt": "url"}
{"input": "\n from pyvi import window \n from pyvi . modes import normal \n class Editor ( object ) : \n _command = None \n active_tab = None \n def __init__ ( self , tabs = None , config = None , normal = normal ) : \n self . config = config \n self . mode = self . normal = normal \n self . count = None \n if tabs is None : \n tabs = self . tabs = [ window . Tab ( self ) ] \n else : \n tabs = self . tabs = list ( tabs ) \n if tabs : \n self . active_tab = tabs [ <NUM_LIT:0> ] \n @ property \n def active_window ( self ) : \n return self . active_tab . active_window \n def keypress ( self , keys ) : \n return self . mode . keypress ( self , <mask0> ) \n", "gt": "keys"}
{"input": "\n from collections import deque \n from contextlib import contextmanager \n import json \n from jsonschema import FormatChecker , ValidationError \n from jsonschema . tests . compat import mock , unittest \n from jsonschema . validators import ( \n RefResolutionError , UnknownType , Draft3Validator , \n Draft4Validator , RefResolver , create , extend , validator_for , validate , \n ) \n class TestCreateAndExtend ( unittest . TestCase ) : \n def setUp ( self ) : \n self . meta_schema = { u\"<STR_LIT>\" : { u\"<STR_LIT>\" : { } } } \n self . smelly = mock . MagicMock ( ) \n self . validators = { u\"<STR_LIT>\" : self . smelly } \n self . types = { u\"<STR_LIT>\" : dict } \n self . Validator = create ( \n meta_schema = self . meta_schema , \n validators = self . validators , \n default_types = self . types , \n ) \n self . validator_value = <NUM_LIT:12> \n self . schema = { u\"<STR_LIT>\" : self . validator_value } \n self . validator = self . Validator ( self . schema ) \n def test_attrs ( self ) : \n self . assertEqual ( self . Validator . VALIDATORS , self . validators ) \n self . assertEqual ( self . Validator . META_SCHEMA , self . meta_schema ) \n self . assertEqual ( self . Validator . DEFAULT_TYPES , self . types ) \n def test_init ( self ) : \n self . assertEqual ( self . validator . schema , self . schema ) \n def test_iter_errors ( self ) : \n instance = \"<STR_LIT:hello>\" \n self . smelly . return_value = [ ] \n self . assertEqual ( list ( self . validator . iter_errors ( instance ) ) , [ ] ) \n error = mock . Mock ( ) \n self . smelly . return_value = [ error ] \n self . assertEqual ( list ( self . validator . iter_errors ( instance ) ) , [ error ] ) \n self . smelly . assert_called_with ( \n self . validator , self . validator_value , instance , self . schema , \n ) \n def test_if_a_version_is_provided_it_is_registered ( self ) : \n with mock . patch ( \"<STR_LIT>\" ) as validates : \n validates . side_effect = lambda version : lambda cls : cls \n Validator = create ( meta_schema = { u\"<STR_LIT:id>\" : \"<STR_LIT>\" } , version = \"<STR_LIT>\" ) \n validates . assert_called_once_with ( \"<STR_LIT>\" ) \n self . assertEqual ( Validator . __name__ , \"<STR_LIT>\" ) \n def test_if_a_version_is_not_provided_it_is_not_registered ( self ) : \n with mock . patch ( \"<STR_LIT>\" ) as validates : \n create ( meta_schema = { u\"<STR_LIT:id>\" : \"<STR_LIT:id>\" } ) \n self . assertFalse ( validates . called ) \n def test_extend ( self ) : \n validators = dict ( self . Validator . VALIDATORS ) \n new = mock . Mock ( ) \n Extended = extend ( self . Validator , validators = { u\"<STR_LIT>\" : new } ) \n validators . update ( [ ( u\"<STR_LIT>\" , new ) ] ) \n self . assertEqual ( Extended . VALIDATORS , validators ) \n self . assertNotIn ( u\"<STR_LIT>\" , self . Validator . VALIDATORS ) \n self . assertEqual ( Extended . META_SCHEMA , self . Validator . META_SCHEMA ) \n self . assertEqual ( Extended . DEFAULT_TYPES , self . Validator . DEFAULT_TYPES ) \n class TestIterErrors ( unittest . TestCase ) : \n def setUp ( self ) : \n self . validator = Draft3Validator ( { } ) \n def test_iter_errors ( self ) : \n instance = [ <NUM_LIT:1> , <NUM_LIT:2> ] \n schema = { \n u\"<STR_LIT>\" : u\"<STR_LIT>\" , \n u\"<STR_LIT>\" : [ [ \"<STR_LIT:a>\" , \"<STR_LIT:b>\" , \"<STR_LIT:c>\" ] , [ \"<STR_LIT:d>\" , \"<STR_LIT:e>\" , \"<STR_LIT:f>\" ] ] , \n u\"<STR_LIT>\" : <NUM_LIT:3> \n } \n got = ( e . message for e in self . validator . iter_errors ( instance , schema ) ) \n expected = [ \n \"<STR_LIT>\" % ( schema [ \"<STR_LIT>\" ] , ) , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" % ( schema [ \"<STR_LIT>\" ] , ) , \n ] \n self . assertEqual ( sorted ( got ) , sorted ( expected ) ) \n def test_iter_errors_multiple_failures_one_validator ( self ) : \n instance = { \"<STR_LIT:foo>\" : <NUM_LIT:2> , \"<STR_LIT:bar>\" : [ <NUM_LIT:1> ] , \"<STR_LIT>\" : <NUM_LIT:15> , \"<STR_LIT>\" : \"<STR_LIT>\" } \n schema = { \n u\"<STR_LIT>\" : { \n \"<STR_LIT:foo>\" : { u\"<STR_LIT:type>\" : \"<STR_LIT:string>\" } , \n \"<STR_LIT:bar>\" : { u\"<STR_LIT>\" : <NUM_LIT:2> } , \n \"<STR_LIT>\" : { u\"<STR_LIT>\" : <NUM_LIT:10> , u\"<STR_LIT>\" : [ <NUM_LIT:2> , <NUM_LIT:4> , <NUM_LIT:6> , <NUM_LIT:8> ] } , \n } \n } \n errors = list ( self . validator . iter_errors ( instance , schema ) ) \n self . assertEqual ( len ( errors ) , <NUM_LIT:4> ) \n class TestValidationErrorMessages ( unittest . TestCase ) : \n def message_for ( self , instance , schema , * args , ** kwargs ) : \n kwargs . setdefault ( \"<STR_LIT>\" , Draft3Validator ) \n with self . assertRaises ( ValidationError ) as e : \n validate ( instance , schema , * args , ** kwargs ) \n return e . exception . message \n def test_single_type_failure ( self ) : \n message = self . message_for ( instance = <NUM_LIT:1> , schema = { u\"<STR_LIT:type>\" : u\"<STR_LIT:string>\" } ) \n self . assertEqual ( message , \"<STR_LIT>\" % u\"<STR_LIT:string>\" ) \n def test_single_type_list_failure ( self ) : \n message = self . message_for ( instance = <NUM_LIT:1> , schema = { u\"<STR_LIT:type>\" : [ u\"<STR_LIT:string>\" ] } ) \n self . assertEqual ( message , \"<STR_LIT>\" % u\"<STR_LIT:string>\" ) \n def test_multiple_type_failure ( self ) : \n types = u\"<STR_LIT:string>\" , u\"<STR_LIT:object>\" \n message = self . message_for ( instance = <NUM_LIT:1> , schema = { u\"<STR_LIT:type>\" : list ( types ) } ) \n self . assertEqual ( message , \"<STR_LIT>\" % types ) \n def test_object_without_title_type_failure ( self ) : \n type = { u\"<STR_LIT:type>\" : [ { u\"<STR_LIT>\" : <NUM_LIT:3> } ] } \n message = self . message_for ( instance = <NUM_LIT:1> , schema = { u\"<STR_LIT:type>\" : [ type ] } ) \n self . assertEqual ( message , \"<STR_LIT>\" % ( type , ) ) \n def test_object_with_name_type_failure ( self ) : \n name = \"<STR_LIT>\" \n schema = { u\"<STR_LIT:type>\" : [ { u\"<STR_LIT:name>\" : name , u\"<STR_LIT>\" : <NUM_LIT:3> } ] } \n message = self . message_for ( instance = <NUM_LIT:1> , schema = schema ) \n self . assertEqual ( message , \"<STR_LIT>\" % ( name , ) ) \n def test_minimum ( self ) : \n message = self . message_for ( instance = <NUM_LIT:1> , schema = { \"<STR_LIT>\" : <NUM_LIT:2> } ) \n self . assertEqual ( message , \"<STR_LIT>\" ) \n def test_maximum ( self ) : \n message = self . message_for ( instance = <NUM_LIT:1> , schema = { \"<STR_LIT>\" : <NUM_LIT:0> } ) \n self . assertEqual ( message , \"<STR_LIT>\" ) \n def test_dependencies_failure_has_single_element_not_list ( self ) : \n depend , on = \"<STR_LIT:bar>\" , \"<STR_LIT:foo>\" \n schema = { u\"<STR_LIT>\" : { depend : on } } \n message = self . message_for ( { \"<STR_LIT:bar>\" : <NUM_LIT:2> } , schema ) \n self . assertEqual ( message , \"<STR_LIT>\" % ( on , depend ) ) \n def test_additionalItems_single_failure ( self ) : \n message = self . message_for ( \n [ <NUM_LIT:2> ] , { u\"<STR_LIT>\" : [ ] , u\"<STR_LIT>\" : False } , \n ) \n self . assertIn ( \"<STR_LIT>\" , message ) \n def test_additionalItems_multiple_failures ( self ) : \n message = self . message_for ( \n [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] , { u\"<STR_LIT>\" : [ ] , u\"<STR_LIT>\" : False } \n ) \n self . assertIn ( \"<STR_LIT>\" , message ) \n def test_additionalProperties_single_failure ( self ) : \n additional = \"<STR_LIT:foo>\" \n schema = { u\"<STR_LIT>\" : False } \n message = self . message_for ( { additional : <NUM_LIT:2> } , schema ) \n self . assertIn ( \"<STR_LIT>\" % ( additional , ) , message ) \n def test_additionalProperties_multiple_failures ( self ) : \n schema = { u\"<STR_LIT>\" : False } \n message = self . message_for ( dict . fromkeys ( [ \"<STR_LIT:foo>\" , \"<STR_LIT:bar>\" ] ) , schema ) \n self . assertIn ( repr ( \"<STR_LIT:foo>\" ) , message ) \n self . assertIn ( repr ( \"<STR_LIT:bar>\" ) , message ) \n self . assertIn ( \"<STR_LIT>\" , message ) \n def test_invalid_format_default_message ( self ) : \n checker = FormatChecker ( formats = ( ) ) \n check_fn = mock . Mock ( return_value = False ) \n checker . checks ( u\"<STR_LIT>\" ) ( check_fn ) \n schema = { u\"<STR_LIT>\" : u\"<STR_LIT>\" } \n message = self . message_for ( \"<STR_LIT>\" , schema , format_checker = checker ) \n self . assertIn ( repr ( \"<STR_LIT>\" ) , message ) \n self . assertIn ( repr ( \"<STR_LIT>\" ) , message ) \n self . assertIn ( \"<STR_LIT>\" , message ) \n class TestValidationErrorDetails ( unittest . TestCase ) : \n def test_anyOf ( self ) : \n instance = <NUM_LIT:5> \n schema = { \n \"<STR_LIT>\" : [ \n { \"<STR_LIT>\" : <NUM_LIT:20> } , \n { \"<STR_LIT:type>\" : \"<STR_LIT:string>\" } \n ] \n } \n validator = Draft4Validator ( schema ) \n errors = list ( validator . iter_errors ( instance ) ) \n self . assertEqual ( len ( errors ) , <NUM_LIT:1> ) \n e = errors [ <NUM_LIT:0> ] \n self . assertEqual ( e . validator , \"<STR_LIT>\" ) \n self . assertEqual ( e . validator_value , schema [ \"<STR_LIT>\" ] ) \n self . assertEqual ( e . instance , instance ) \n self . assertEqual ( e . schema , schema ) \n self . assertIsNone ( e . parent ) \n self . assertEqual ( e . path , deque ( [ ] ) ) \n self . assertEqual ( e . relative_path , deque ( [ ] ) ) \n self . assertEqual ( e . absolute_path , deque ( [ ] ) ) \n self . assertEqual ( e . schema_path , deque ( [ \"<STR_LIT>\" ] ) ) \n self . assertEqual ( e . relative_schema_path , deque ( [ \"<STR_LIT>\" ] ) ) \n self . assertEqual ( e . absolute_schema_path , deque ( [ \"<STR_LIT>\" ] ) ) \n self . assertEqual ( len ( e . context ) , <NUM_LIT:2> ) \n e1 , e2 = sorted_errors ( e . context ) \n self . assertEqual ( e1 . validator , \"<STR_LIT>\" ) \n self . assertEqual ( e1 . validator_value , schema [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] [ \"<STR_LIT>\" ] ) \n self . assertEqual ( e1 . instance , instance ) \n self . assertEqual ( e1 . schema , schema [ \"<STR_LIT>\" ] [ <NUM_LIT:0> ] ) \n self . assertIs ( e1 . parent , e ) \n self . assertEqual ( e1 . path , deque ( [ ] ) ) \n self . assertEqual ( e1 . absolute_path , deque ( [ ] ) ) \n self . assertEqual ( e1 . relative_path , deque ( [ ] ) ) \n self . assertEqual ( e1 . schema_path , deque ( [ <NUM_LIT:0> , \"<STR_LIT>\" ] ) ) \n self . assertEqual ( e1 . relative_schema_path , deque ( [ <NUM_LIT:0> , \"<STR_LIT>\" ] ) ) \n self . assertEqual ( \n e1 . absolute_schema_path , deque ( [ \"<STR_LIT>\" , <NUM_LIT:0> , \"<STR_LIT>\" ] ) , \n ) \n self . assertFalse ( e1 . context ) \n self . assertEqual ( e2 . validator , \"<STR_LIT:type>\" ) \n self . assertEqual ( e2 . validator_value , schema [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] [ \"<STR_LIT:type>\" ] ) \n self . assertEqual ( e2 . instance , instance ) \n self . assertEqual ( e2 . schema , schema [ \"<STR_LIT>\" ] [ <NUM_LIT:1> ] ) \n self . assertIs ( e2 . parent , e ) \n self . assertEqual ( e2 . path , deque ( [ ] ) ) \n self . assertEqual ( e2 . relative_path , deque ( [ ] ) ) \n self . assertEqual ( e2 . absolute_path , deque ( [ ] ) ) \n self . assertEqual ( e2 . schema_path , deque ( [ <NUM_LIT:1> , \"<STR_LIT:type>\" ] ) ) \n self . assertEqual ( e2 . relative_schema_path , deque ( [ <NUM_LIT:1> , \"<STR_LIT:type>\" ] ) ) \n self . assertEqual ( e2 . absolute_schema_path , deque ( [ \"<STR_LIT>\" , <NUM_LIT:1> , \"<STR_LIT:type>\" ] ) ) \n self . assertEqual ( len ( e2 . context ) , <NUM_LIT:0> ) \n def test_type ( self ) : \n instance = { \"<STR_LIT:foo>\" : <NUM_LIT:1> } \n schema = { \n \"<STR_LIT:type>\" : [ \n { \"<STR_LIT:type>\" : \"<STR_LIT>\" } , \n { \n \"<STR_LIT:type>\" : \"<STR_LIT:object>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT:foo>\" : { \"<STR_LIT>\" : [ <NUM_LIT:2> ] } \n } \n } \n ] \n } \n validator = Draft3Validator ( schema ) \n errors = list ( validator . iter_errors ( instance ) ) \n self . assertEqual ( len ( errors ) , <NUM_LIT:1> ) \n e = errors [ <NUM_LIT:0> ] \n self . assertEqual ( e . validator , \"<STR_LIT:type>\" ) \n self . assertEqual ( e . validator_value , schema [ \"<STR_LIT:type>\" ] ) \n self . assertEqual ( e . instance , instance ) \n self . assertEqual ( e . schema , schema ) \n self . assertIsNone ( e . parent ) \n self . assertEqual ( e . path , deque ( [ ] ) ) \n self . assertEqual ( e . relative_path , deque ( [ ] ) ) \n self . assertEqual ( e . absolute_path , deque ( [ ] ) ) \n self . assertEqual ( e . schema_path , deque ( [ \"<STR_LIT:type>\" ] ) ) \n self . assertEqual ( e . relative_schema_path , deque ( [ \"<STR_LIT:type>\" ] ) ) \n self . assertEqual ( e . absolute_schema_path , deque ( [ \"<STR_LIT:type>\" ] ) ) \n self . assertEqual ( len ( e . context ) , <NUM_LIT:2> ) \n e1 , e2 = sorted_errors ( e . context ) \n self . assertEqual ( e1 . validator , \"<STR_LIT:type>\" ) \n self . assertEqual ( e1 . validator_value , schema [ \"<STR_LIT:type>\" ] [ <NUM_LIT:0> ] [ \"<STR_LIT:type>\" ] ) \n self . assertEqual ( e1 . instance , instance ) \n self . assertEqual ( e1 . schema , schema [ \"<STR_LIT:type>\" ] [ <NUM_LIT:0> ] ) \n self . assertIs ( e1 . parent , e ) \n self . assertEqual ( e1 . path , deque ( [ ] ) ) \n self . assertEqual ( e1 . relative_path , deque ( [ ] ) ) \n self . assertEqual ( e1 . absolute_path , deque ( [ ] ) ) \n self . assertEqual ( e1 . schema_path , deque ( [ <NUM_LIT:0> , \"<STR_LIT:type>\" ] ) ) \n self . assertEqual ( e1 . relative_schema_path , deque ( [ <NUM_LIT:0> , \"<STR_LIT:type>\" ] ) ) \n self . assertEqual ( e1 . absolute_schema_path , deque ( [ \"<STR_LIT:type>\" , <NUM_LIT:0> , \"<STR_LIT:type>\" ] ) ) \n self . assertFalse ( e1 . context ) \n self . assertEqual ( e2 . validator , \"<STR_LIT>\" ) \n self . assertEqual ( e2 . validator_value , [ <NUM_LIT:2> ] ) \n self . assertEqual ( e2 . instance , <NUM_LIT:1> ) \n self . assertEqual ( e2 . schema , { u\"<STR_LIT>\" : [ <NUM_LIT:2> ] } ) \n self . assertIs ( e2 . parent , e ) \n self . assertEqual ( e2 . path , deque ( [ \"<STR_LIT:foo>\" ] ) ) \n self . assertEqual ( e2 . relative_path , deque ( [ \"<STR_LIT:foo>\" ] ) ) \n self . assertEqual ( e2 . absolute_path , deque ( [ \"<STR_LIT:foo>\" ] ) ) \n self . assertEqual ( \n e2 . schema_path , deque ( [ <NUM_LIT:1> , \"<STR_LIT>\" , \"<STR_LIT:foo>\" , \"<STR_LIT>\" ] ) , \n ) \n self . assertEqual ( \n e2 . relative_schema_path , deque ( [ <NUM_LIT:1> , \"<STR_LIT>\" , \"<STR_LIT:foo>\" , \"<STR_LIT>\" ] ) , \n ) \n self . assertEqual ( \n e2 . absolute_schema_path , \n deque ( [ \"<STR_LIT:type>\" , <NUM_LIT:1> , \"<STR_LIT>\" , \"<STR_LIT:foo>\" , \"<STR_LIT>\" ] ) , \n ) \n self . assertFalse ( e2 . context ) \n def test_single_nesting ( self ) : \n instance = { \"<STR_LIT:foo>\" : <NUM_LIT:2> , \"<STR_LIT:bar>\" : [ <NUM_LIT:1> ] , \"<STR_LIT>\" : <NUM_LIT:15> , \"<STR_LIT>\" : \"<STR_LIT>\" } \n schema = { \n \"<STR_LIT>\" : { \n \"<STR_LIT:foo>\" : { \"<STR_LIT:type>\" : \"<STR_LIT:string>\" } , \n \"<STR_LIT:bar>\" : { \"<STR_LIT>\" : <NUM_LIT:2> } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : <NUM_LIT:10> , \"<STR_LIT>\" : [ <NUM_LIT:2> , <NUM_LIT:4> , <NUM_LIT:6> , <NUM_LIT:8> ] } , \n } \n } \n validator = Draft3Validator ( schema ) \n errors = validator . iter_errors ( instance ) \n e1 , e2 , e3 , e4 = sorted_errors ( errors ) \n self . assertEqual ( e1 . path , deque ( [ \"<STR_LIT:bar>\" ] ) ) \n self . assertEqual ( e2 . path , deque ( [ \"<STR_LIT>\" ] ) ) \n self . assertEqual ( e3 . path , deque ( [ \"<STR_LIT>\" ] ) ) \n self . assertEqual ( e4 . path , deque ( [ \"<STR_LIT:foo>\" ] ) ) \n self . assertEqual ( e1 . relative_path , deque ( [ \"<STR_LIT:bar>\" ] ) ) \n self . assertEqual ( e2 . relative_path , deque ( [ \"<STR_LIT>\" ] ) ) \n self . assertEqual ( e3 . relative_path , deque ( [ \"<STR_LIT>\" ] ) ) \n self . assertEqual ( e4 . relative_path , deque ( [ \"<STR_LIT:foo>\" ] ) ) \n self . assertEqual ( e1 . absolute_path , deque ( [ \"<STR_LIT:bar>\" ] ) ) \n self . assertEqual ( e2 . absolute_path , deque ( [ \"<STR_LIT>\" ] ) ) \n self . assertEqual ( e3 . absolute_path , deque ( [ \"<STR_LIT>\" ] ) ) \n self . assertEqual ( e4 . absolute_path , deque ( [ \"<STR_LIT:foo>\" ] ) ) \n self . assertEqual ( e1 . validator , \"<STR_LIT>\" ) \n self . assertEqual ( e2 . validator , \"<STR_LIT>\" ) \n self . assertEqual ( e3 . validator , \"<STR_LIT>\" ) \n self . assertEqual ( e4 . validator , \"<STR_LIT:type>\" ) \n def test_multiple_nesting ( self ) : \n instance = [ <NUM_LIT:1> , { \"<STR_LIT:foo>\" : <NUM_LIT:2> , \"<STR_LIT:bar>\" : { \"<STR_LIT>\" : [ <NUM_LIT:1> ] } } , \"<STR_LIT>\" ] \n schema = { \n \"<STR_LIT:type>\" : \"<STR_LIT:string>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT:type>\" : [ \"<STR_LIT:string>\" , \"<STR_LIT:object>\" ] , \n \"<STR_LIT>\" : { \n \"<STR_LIT:foo>\" : { \"<STR_LIT>\" : [ <NUM_LIT:1> , <NUM_LIT:3> ] } , \n \"<STR_LIT:bar>\" : { \n \"<STR_LIT:type>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT:bar>\" : { \"<STR_LIT>\" : True } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : <NUM_LIT:2> } , \n } \n } \n } \n } \n } \n validator = Draft3Validator ( schema ) \n errors = validator . iter_errors ( instance ) \n e1 , e2 , e3 , e4 , e5 , e6 = sorted_errors ( errors ) \n self . assertEqual ( e1 . path , deque ( [ ] ) ) \n self . assertEqual ( e2 . path , deque ( [ <NUM_LIT:0> ] ) ) \n self . assertEqual ( e3 . path , deque ( [ <NUM_LIT:1> , \"<STR_LIT:bar>\" ] ) ) \n self . assertEqual ( e4 . path , deque ( [ <NUM_LIT:1> , \"<STR_LIT:bar>\" , \"<STR_LIT:bar>\" ] ) ) \n self . assertEqual ( e5 . path , deque ( [ <NUM_LIT:1> , \"<STR_LIT:bar>\" , \"<STR_LIT>\" ] ) ) \n self . assertEqual ( e6 . path , deque ( [ <NUM_LIT:1> , \"<STR_LIT:foo>\" ] ) ) \n self . assertEqual ( e1 . schema_path , deque ( [ \"<STR_LIT:type>\" ] ) ) \n self . assertEqual ( e2 . schema_path , deque ( [ \"<STR_LIT>\" , \"<STR_LIT:type>\" ] ) ) \n self . assertEqual ( \n list ( e3 . schema_path ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT:bar>\" , \"<STR_LIT:type>\" ] , \n ) \n self . assertEqual ( \n list ( e4 . schema_path ) , \n [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT:bar>\" , \"<STR_LIT>\" , \"<STR_LIT:bar>\" , \"<STR_LIT>\" ] , \n ) \n self . assertEqual ( \n list ( e5 . schema_path ) , \n [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT:bar>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n ) \n self . assertEqual ( \n list ( e6 . schema_path ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT:foo>\" , \"<STR_LIT>\" ] , \n ) \n self . assertEqual ( e1 . validator , \"<STR_LIT:type>\" ) \n self . assertEqual ( e2 . validator , \"<STR_LIT:type>\" ) \n self . assertEqual ( e3 . validator , \"<STR_LIT:type>\" ) \n self . assertEqual ( e4 . validator , \"<STR_LIT>\" ) \n self . assertEqual ( e5 . validator , \"<STR_LIT>\" ) \n self . assertEqual ( e6 . validator , \"<STR_LIT>\" ) \n def test_recursive ( self ) : \n schema = { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ { \n \"<STR_LIT:type>\" : \"<STR_LIT:object>\" , \n \"<STR_LIT>\" : [ \"<STR_LIT:name>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : { \n \"<STR_LIT:name>\" : { \n \"<STR_LIT:type>\" : \"<STR_LIT:string>\" , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT:type>\" : \"<STR_LIT:object>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n } , \n } , \n } , \n } ] , \n } , \n } , \n \"<STR_LIT:type>\" : \"<STR_LIT:object>\" , \n \"<STR_LIT>\" : [ \"<STR_LIT:root>\" ] , \n \"<STR_LIT>\" : { \n \"<STR_LIT:root>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n } \n } \n instance = { \n \"<STR_LIT:root>\" : { \n \"<STR_LIT:name>\" : \"<STR_LIT:root>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT:a>\" : { \n \"<STR_LIT:name>\" : \"<STR_LIT:a>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT:name>\" : \"<STR_LIT>\" , \n } \n } \n } , \n } , \n } , \n } \n validator = Draft4Validator ( schema ) \n e , = validator . iter_errors ( instance ) \n self . assertEqual ( e . absolute_path , deque ( [ \"<STR_LIT:root>\" ] ) ) \n self . assertEqual ( \n e . absolute_schema_path , deque ( [ \"<STR_LIT>\" , \"<STR_LIT:root>\" , \"<STR_LIT>\" ] ) , \n ) \n e1 , = e . context \n self . assertEqual ( e1 . absolute_path , deque ( [ \"<STR_LIT:root>\" , \"<STR_LIT>\" , \"<STR_LIT:a>\" ] ) ) \n self . assertEqual ( \n e1 . absolute_schema_path , deque ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT:root>\" , \n \"<STR_LIT>\" , \n <NUM_LIT:0> , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n ) , \n ) \n e2 , = e1 . context \n self . assertEqual ( \n e2 . absolute_path , deque ( \n [ \"<STR_LIT:root>\" , \"<STR_LIT>\" , \"<STR_LIT:a>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n ) , \n ) \n self . assertEqual ( \n e2 . absolute_schema_path , deque ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT:root>\" , \n \"<STR_LIT>\" , \n <NUM_LIT:0> , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n <NUM_LIT:0> , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" \n ] , \n ) , \n ) \n def test_additionalProperties ( self ) : \n instance = { \"<STR_LIT:bar>\" : \"<STR_LIT:bar>\" , \"<STR_LIT:foo>\" : <NUM_LIT:2> } \n schema = { \n \"<STR_LIT>\" : { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT:5> } \n } \n validator = Draft3Validator ( schema ) \n errors = validator . iter_errors ( instance ) \n e1 , e2 = sorted_errors ( errors ) \n self . assertEqual ( e1 . path , deque ( [ \"<STR_LIT:bar>\" ] ) ) \n self . assertEqual ( e2 . path , deque ( [ \"<STR_LIT:foo>\" ] ) ) \n self . assertEqual ( e1 . validator , \"<STR_LIT:type>\" ) \n self . assertEqual ( e2 . validator , \"<STR_LIT>\" ) \n def test_patternProperties ( self ) : \n instance = { \"<STR_LIT:bar>\" : <NUM_LIT:1> , \"<STR_LIT:foo>\" : <NUM_LIT:2> } \n schema = { \n \"<STR_LIT>\" : { \n \"<STR_LIT:bar>\" : { \"<STR_LIT:type>\" : \"<STR_LIT:string>\" } , \n \"<STR_LIT:foo>\" : { \"<STR_LIT>\" : <NUM_LIT:5> } \n } \n } \n validator = Draft3Validator ( schema ) \n errors = validator . iter_errors ( instance ) \n e1 , e2 = sorted_errors ( errors ) \n self . assertEqual ( e1 . path , deque ( [ \"<STR_LIT:bar>\" ] ) ) \n self . assertEqual ( e2 . path , deque ( [ \"<STR_LIT:foo>\" ] ) ) \n self . assertEqual ( e1 . validator , \"<STR_LIT:type>\" ) \n self . assertEqual ( e2 . validator , \"<STR_LIT>\" ) \n def test_additionalItems ( self ) : \n instance = [ \"<STR_LIT:foo>\" , <NUM_LIT:1> ] \n schema = { \n \"<STR_LIT>\" : [ ] , \n \"<STR_LIT>\" : { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT:5> } \n } \n validator = Draft3Validator ( schema ) \n errors = validator . iter_errors ( instance ) \n e1 , e2 = sorted_errors ( errors ) \n self . assertEqual ( e1 . path , deque ( [ <NUM_LIT:0> ] ) ) \n self . assertEqual ( e2 . path , deque ( [ <NUM_LIT:1> ] ) ) \n self . assertEqual ( e1 . validator , \"<STR_LIT:type>\" ) \n self . assertEqual ( e2 . validator , \"<STR_LIT>\" ) \n def test_additionalItems_with_items ( self ) : \n instance = [ \"<STR_LIT:foo>\" , \"<STR_LIT:bar>\" , <NUM_LIT:1> ] \n schema = { \n \"<STR_LIT>\" : [ { } ] , \n \"<STR_LIT>\" : { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT:5> } \n } \n validator = Draft3Validator ( schema ) \n errors = validator . iter_errors ( instance ) \n e1 , e2 = sorted_errors ( errors ) \n self . assertEqual ( e1 . path , deque ( [ <NUM_LIT:1> ] ) ) \n self . assertEqual ( e2 . path , deque ( [ <NUM_LIT:2> ] ) ) \n self . assertEqual ( e1 . validator , \"<STR_LIT:type>\" ) \n self . assertEqual ( e2 . validator , \"<STR_LIT>\" ) \n class ValidatorTestMixin ( object ) : \n def setUp ( self ) : \n self . instance = mock . Mock ( ) \n self . schema = { } \n self . resolver = mock . Mock ( ) \n self . validator = self . validator_class ( self . schema ) \n def test_valid_instances_are_valid ( self ) : \n errors = iter ( [ ] ) \n with mock . patch . object ( \n self . validator , \"<STR_LIT>\" , return_value = errors , \n ) : \n self . assertTrue ( \n self . validator . is_valid ( self . instance , self . schema ) \n ) \n def test_invalid_instances_are_not_valid ( self ) : \n errors = iter ( [ mock . Mock ( ) ] ) \n with mock . patch . object ( \n self . validator , \"<STR_LIT>\" , return_value = errors , \n ) : \n self . assertFalse ( \n self . validator . is_valid ( self . instance , self . schema ) \n ) \n def test_non_existent_properties_are_ignored ( self ) : \n instance , my_property , my_value = mock . Mock ( ) , mock . Mock ( ) , mock . Mock ( ) \n validate ( instance = instance , schema = { my_property : my_value } ) \n def test_it_creates_a_ref_resolver_if_not_provided ( self ) : \n self . assertIsInstance ( self . validator . resolver , RefResolver ) \n def test_it_delegates_to_a_ref_resolver ( self ) : \n resolver = RefResolver ( \"<STR_LIT>\" , { } ) \n schema = { \"<STR_LIT>\" : mock . Mock ( ) } \n with mock . patch . object ( resolver , \"<STR_LIT>\" ) as resolve : \n resolve . return_value = \"<STR_LIT:url>\" , { \"<STR_LIT:type>\" : \"<STR_LIT>\" } \n with self . assertRaises ( ValidationError ) : \n self . validator_class ( schema , resolver = resolver ) . validate ( None ) \n resolve . assert_called_once_with ( schema [ \"<STR_LIT>\" ] ) \n def test_it_delegates_to_a_legacy_ref_resolver ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n class LegacyRefResolver ( object ) : \n @ contextmanager \n def resolving ( this , ref ) : \n self . assertEqual ( ref , \"<STR_LIT>\" ) \n yield { \"<STR_LIT:type>\" : \"<STR_LIT>\" } \n resolver = LegacyRefResolver ( ) \n schema = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n with self . assertRaises ( ValidationError ) : \n self . validator_class ( schema , resolver = resolver ) . validate ( None ) \n def test_is_type_is_true_for_valid_type ( self ) : \n self . assertTrue ( self . validator . is_type ( \"<STR_LIT:foo>\" , \"<STR_LIT:string>\" ) ) \n def test_is_type_is_false_for_invalid_type ( self ) : \n self . assertFalse ( self . validator . is_type ( \"<STR_LIT:foo>\" , \"<STR_LIT>\" ) ) \n def test_is_type_evades_bool_inheriting_from_int ( self ) : \n self . assertFalse ( self . validator . is_type ( True , \"<STR_LIT>\" ) ) \n self . assertFalse ( self . validator . is_type ( True , \"<STR_LIT>\" ) ) \n def test_is_type_raises_exception_for_unknown_type ( self ) : \n with self . assertRaises ( UnknownType ) : \n self . validator . is_type ( \"<STR_LIT:foo>\" , object ( ) ) \n class TestDraft3Validator ( ValidatorTestMixin , unittest . TestCase ) : \n validator_class = Draft3Validator \n def test_is_type_is_true_for_any_type ( self ) : \n self . assertTrue ( self . validator . is_valid ( mock . Mock ( ) , { \"<STR_LIT:type>\" : \"<STR_LIT>\" } ) ) \n def test_is_type_does_not_evade_bool_if_it_is_being_tested ( self ) : \n self . assertTrue ( self . validator . is_type ( True , \"<STR_LIT>\" ) ) \n self . assertTrue ( self . validator . is_valid ( True , { \"<STR_LIT:type>\" : \"<STR_LIT>\" } ) ) \n def test_non_string_custom_types ( self ) : \n schema = { '<STR_LIT:type>' : [ None ] } \n cls = self . validator_class ( schema , types = { None : type ( None ) } ) \n cls . validate ( None , schema ) \n class TestDraft4Validator ( ValidatorTestMixin , unittest . TestCase ) : \n validator_class = Draft4Validator \n class TestBuiltinFormats ( unittest . TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n for format in FormatChecker . checkers : \n def test ( self , format = format ) : \n v = Draft4Validator ( { \"<STR_LIT>\" : format } , format_checker = FormatChecker ( ) ) \n v . validate ( <NUM_LIT> ) \n name = \"<STR_LIT>\" . format ( format ) \n test . __name__ = name \n setattr ( TestBuiltinFormats , name , test ) \n del test \n class TestValidatorFor ( unittest . TestCase ) : \n def test_draft_3 ( self ) : \n schema = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n self . assertIs ( validator_for ( schema ) , Draft3Validator ) \n schema = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n self . assertIs ( validator_for ( schema ) , Draft3Validator ) \n def test_draft_4 ( self ) : \n schema = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n self . assertIs ( validator_for ( schema ) , Draft4Validator ) \n schema = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n self . assertIs ( validator_for ( schema ) , Draft4Validator ) \n def test_custom_validator ( self ) : \n Validator = create ( meta_schema = { \"<STR_LIT:id>\" : \"<STR_LIT>\" } , version = \"<STR_LIT>\" ) \n schema = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n self . assertIs ( validator_for ( schema ) , Validator ) \n def test_validator_for_jsonschema_default ( self ) : \n self . assertIs ( validator_for ( { } ) , Draft4Validator ) \n def test_validator_for_custom_default ( self ) : \n self . assertIs ( validator_for ( { } , default = None ) , None ) \n class TestValidate ( unittest . TestCase ) : \n def test_draft3_validator_is_chosen ( self ) : \n schema = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n with mock . patch . object ( Draft3Validator , \"<STR_LIT>\" ) as chk_schema : \n validate ( { } , schema ) \n chk_schema . assert_called_once_with ( schema ) \n schema = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n with mock . patch . object ( Draft3Validator , \"<STR_LIT>\" ) as chk_schema : \n validate ( { } , schema ) \n chk_schema . assert_called_once_with ( schema ) \n def test_draft4_validator_is_chosen ( self ) : \n schema = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n with mock . patch . object ( Draft4Validator , \"<STR_LIT>\" ) as chk_schema : \n validate ( { } , schema ) \n chk_schema . assert_called_once_with ( schema ) \n def test_draft4_validator_is_the_default ( self ) : \n with mock . patch . object ( Draft4Validator , \"<STR_LIT>\" ) as chk_schema : \n validate ( { } , { } ) \n chk_schema . assert_called_once_with ( { } ) \n class TestRefResolver ( unittest . TestCase ) : \n base_uri = \"<STR_LIT>\" \n stored_uri = \"<STR_LIT>\" \n stored_schema = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n def setUp ( self ) : \n self . referrer = { } \n self . store = { self . stored_uri : self . stored_schema } \n self . resolver = RefResolver ( self . base_uri , self . referrer , self . store ) \n def test_it_does_not_retrieve_schema_urls_from_the_network ( self ) : \n ref = Draft3Validator . META_SCHEMA [ \"<STR_LIT:id>\" ] \n with mock . patch . object ( self . resolver , \"<STR_LIT>\" ) as remote : \n with self . resolver . resolving ( ref ) as resolved : \n self . assertEqual ( resolved , Draft3Validator . META_SCHEMA ) \n self . assertFalse ( remote . called ) \n def test_it_resolves_local_refs ( self ) : \n ref = \"<STR_LIT>\" \n self . referrer [ \"<STR_LIT>\" ] = { \"<STR_LIT:foo>\" : object ( ) } \n with self . resolver . resolving ( ref ) as resolved : \n self . assertEqual ( resolved , self . referrer [ \"<STR_LIT>\" ] [ \"<STR_LIT:foo>\" ] ) \n def test_it_resolves_local_refs_with_id ( self ) : \n schema = { \"<STR_LIT:id>\" : \"<STR_LIT>\" , \"<STR_LIT:a>\" : { \"<STR_LIT:foo>\" : \"<STR_LIT:bar>\" } } \n resolver = RefResolver . from_schema ( schema ) \n with resolver . resolving ( \"<STR_LIT>\" ) as resolved : \n self . assertEqual ( resolved , schema [ \"<STR_LIT:a>\" ] ) \n with resolver . resolving ( \"<STR_LIT>\" ) as resolved : \n self . assertEqual ( resolved , schema [ \"<STR_LIT:a>\" ] ) \n def test_it_retrieves_stored_refs ( self ) : \n with self . resolver . resolving ( self . stored_uri ) as resolved : \n self . assertIs ( resolved , self . stored_schema ) \n self . resolver . store [ \"<STR_LIT>\" ] = { \"<STR_LIT:foo>\" : <NUM_LIT:12> } \n with self . resolver . resolving ( \"<STR_LIT>\" ) as resolved : \n self . assertEqual ( resolved , <NUM_LIT:12> ) \n def test_it_retrieves_unstored_refs_via_requests ( self ) : \n ref = \"<STR_LIT>\" \n schema = { \"<STR_LIT>\" : <NUM_LIT:12> } \n with mock . patch ( \"<STR_LIT>\" ) as requests : \n requests . get . return_value . json . return_value = schema \n with self . resolver . resolving ( ref ) as resolved : \n self . assertEqual ( resolved , <NUM_LIT:12> ) \n requests . get . assert_called_once_with ( \"<STR_LIT>\" ) \n def test_it_retrieves_unstored_refs_via_urlopen ( self ) : \n ref = \"<STR_LIT>\" \n schema = { \"<STR_LIT>\" : <NUM_LIT:12> } \n with mock . patch ( \"<STR_LIT>\" , None ) : \n with mock . patch ( \"<STR_LIT>\" ) as urlopen : \n urlopen . return_value . read . return_value = ( \n json . dumps ( schema ) . encode ( \"<STR_LIT:utf8>\" ) ) \n with self . resolver . resolving ( ref ) as resolved : \n self . assertEqual ( resolved , <NUM_LIT:12> ) \n urlopen . assert_called_once_with ( \"<STR_LIT>\" ) \n def test_it_can_construct_a_base_uri_from_a_schema ( self ) : \n schema = { \"<STR_LIT:id>\" : \"<STR_LIT:foo>\" } \n resolver = RefResolver . from_schema ( schema ) \n self . assertEqual ( resolver . base_uri , \"<STR_LIT:foo>\" ) \n self . assertEqual ( resolver . resolution_scope , \"<STR_LIT:foo>\" ) \n with resolver . resolving ( \"<STR_LIT>\" ) as resolved : \n self . assertEqual ( resolved , schema ) \n with resolver . resolving ( \"<STR_LIT:#>\" ) as resolved : \n self . assertEqual ( resolved , schema ) \n with resolver . resolving ( \"<STR_LIT:foo>\" ) as resolved : \n self . assertEqual ( resolved , schema ) \n with resolver . resolving ( \"<STR_LIT>\" ) as resolved : \n self . assertEqual ( resolved , schema ) \n def test_it_can_construct_a_base_uri_from_a_schema_without_id ( self ) : \n schema = { } \n resolver = RefResolver . from_schema ( schema ) \n self . assertEqual ( resolver . base_uri , \"<STR_LIT>\" ) \n self . assertEqual ( resolver . resolution_scope , \"<STR_LIT>\" ) \n with resolver . resolving ( \"<STR_LIT>\" ) as resolved : \n self . assertEqual ( resolved , schema ) \n with resolver . resolving ( \"<STR_LIT:#>\" ) as resolved : \n self . assertEqual ( resolved , schema ) \n def test_custom_uri_scheme_handlers ( self ) : \n schema = { \"<STR_LIT:foo>\" : \"<STR_LIT:bar>\" } \n ref = \"<STR_LIT>\" \n foo_handler = mock . Mock ( return_value = schema ) \n resolver = RefResolver ( \"<STR_LIT>\" , { } , handlers = { \"<STR_LIT:foo>\" : foo_handler } ) \n with resolver . resolving ( ref ) as resolved : \n self . assertEqual ( resolved , schema ) \n foo_handler . assert_called_once_with ( ref ) \n def test_cache_remote_on ( self ) : \n ref = \"<STR_LIT>\" \n foo_handler = mock . Mock ( ) \n resolver = RefResolver ( \n \"<STR_LIT>\" , { } , cache_remote = True , handlers = { \"<STR_LIT:foo>\" : foo_handler } , \n ) \n with resolver . resolving ( ref ) : \n pass \n with resolver . resolving ( ref ) : \n pass \n foo_handler . assert_called_once_with ( ref ) \n def test_cache_remote_off ( self ) : \n ref = \"<STR_LIT>\" \n foo_handler = mock . Mock ( ) \n resolver = RefResolver ( \n \"<STR_LIT>\" , { } , cache_remote = False , handlers = { \"<STR_LIT:foo>\" : foo_handler } , \n ) \n with resolver . resolving ( ref ) : \n pass \n self . assertEqual ( foo_handler . call_count , <NUM_LIT:1> ) \n def test_if_you_give_it_junk_you_get_a_resolution_error ( self ) : \n ref = \"<STR_LIT>\" \n foo_handler = mock . Mock ( side_effect = ValueError ( \"<STR_LIT>\" ) ) \n resolver = RefResolver ( \"<STR_LIT>\" , { } , handlers = { \"<STR_LIT:foo>\" : foo_handler } ) \n with self . assertRaises ( RefResolutionError ) as err : \n with resolver . resolving ( ref ) : \n pass \n self . assertEqual ( str ( err . exception ) , \"<STR_LIT>\" ) \n def test_helpful_error_message_on_failed_pop_scope ( self ) : \n resolver = RefResolver ( \"<STR_LIT>\" , { } ) \n resolver . pop_scope ( ) \n with self . assertRaises ( RefResolutionError ) as exc : \n resolver . pop_scope ( ) \n self . assertIn ( \"<STR_LIT>\" , str ( exc . exception ) ) \n class UniqueTupleItemsMixin ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def test_it_properly_formats_an_error_message ( self ) : \n validator = self . validator_class ( \n schema = { \"<STR_LIT>\" : True } , \n types = { \"<STR_LIT>\" : ( tuple , ) } , \n ) \n with self . assertRaises ( ValidationError ) as e : \n validator . validate ( ( <NUM_LIT:1> , <NUM_LIT:1> ) ) \n self . assertIn ( \"<STR_LIT>\" , str ( e . exception ) ) \n class TestDraft4UniqueTupleItems ( UniqueTupleItemsMixin , unittest . TestCase ) : \n validator_class = Draft4Validator \n class TestDraft3UniqueTupleItems ( UniqueTupleItemsMixin , unittest . TestCase ) : \n validator_class = Draft3Validator \n def sorted_errors ( errors ) : \n def key ( error ) : \n return ( \n [ str ( e ) for e in error . path ] , \n [ str ( e ) for e in error . schema_path ] \n ) \n return sorted ( errors , key = <mask0> ) \n", "gt": "key"}
{"input": "\n '''<STR_LIT>''' \n import unittest \n import os \n from jnpr . openclos . report import ResourceAllocationReport , L2Report , L3Report \n from test_dao import InMemoryDao \n class Test ( unittest . TestCase ) : \n def setUp ( self ) : \n '''<STR_LIT>''' \n self . __conf = { } \n self . __conf [ '<STR_LIT>' ] = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , '<STR_LIT>' ) \n self . __conf [ '<STR_LIT>' ] = '<STR_LIT>' \n self . __conf [ '<STR_LIT>' ] = '<STR_LIT:false>' \n self . __conf [ '<STR_LIT>' ] = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n self . __conf [ '<STR_LIT>' ] = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] } \n self . __conf [ '<STR_LIT>' ] = { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : '<STR_LIT>' \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : '<STR_LIT>' , \n \"<STR_LIT>\" : '<STR_LIT>' \n } \n } \n self . _dao = InMemoryDao . getInstance ( ) \n def tearDown ( self ) : \n self . _dao = None \n InMemoryDao . _destroy ( ) \n '''<STR_LIT>''' \n def testGenerateL2Report ( self ) : \n l2Report = L2Report ( self . __conf , self . _dao ) \n from test_model import createPod \n with self . _dao . getReadSession ( ) as session : \n pod = createPod ( \"<STR_LIT:test>\" , session ) \n l2Report . generateReport ( pod . id , True , False ) \n def testGenerateL3Report ( self ) : \n l3Report = L3Report ( self . __conf , self . _dao ) \n from test_model import createPod \n with self . _dao . getReadSession ( ) as session : \n pod = createPod ( \"<STR_LIT:test>\" , session ) \n l3Report . generateReport ( pod . id , True , False ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import yaml \n import os . path \n from jnpr . junos . factory . factory_loader import FactoryLoader \n __all__ = [ '<STR_LIT>' , '<STR_LIT>' ] \n def loadyaml ( path ) : \n \"\"\"<STR_LIT>\"\"\" \n if os . path . splitext ( path ) [ <NUM_LIT:1> ] == '<STR_LIT>' : \n path += '<STR_LIT>' \n return FactoryLoader ( ) . load ( yaml . load ( open ( path , <mask0> ) ) ) \n", "gt": "'<STR_LIT:r>'"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from jnpr . junos . factory import loadyaml \n from os . path import splitext \n _YAML_ = splitext ( __file__ ) [ <NUM_LIT:0> ] + '<STR_LIT>' \n globals ( ) . update ( <mask0> ( _YAML_ ) ) \n", "gt": "loadyaml"}
{"input": "\n '''<STR_LIT>''' \n import unittest \n from nose . plugins . attrib import attr \n from jnpr . junos import Device \n @ attr ( '<STR_LIT>' ) \n class TestDeviceSsh ( unittest . TestCase ) : \n def tearDown ( self ) : \n self . dev . close ( ) \n def test_device_open_default_key ( self ) : \n self . dev = Device ( '<STR_LIT>' ) \n self . dev . open ( ) \n self . assertEqual ( self . dev . connected , True ) \n def test_device_open_key_pass ( self ) : \n self . dev = Device ( host = '<STR_LIT>' , ssh_private_key_file = '<STR_LIT>' , passwd = '<STR_LIT:password>' ) \n self . dev . open ( ) \n self . assertEqual ( self . dev . connected , <mask0> ) \n", "gt": "True"}
{"input": "\n __author__ = \"<STR_LIT>\" \n __credits__ = \"<STR_LIT>\" \n import unittest \n from nose . plugins . attrib import attr \n from jnpr . junos import Device \n from jnpr . junos . utils . util import Util \n from mock import patch \n @ attr ( '<STR_LIT>' ) \n class TestUtil ( unittest . TestCase ) : \n @ patch ( '<STR_LIT>' ) \n def setUp ( self , mock_connect ) : \n self . dev = Device ( host = '<STR_LIT>' , user = '<STR_LIT>' , password = '<STR_LIT>' , \n gather_facts = False ) \n self . dev . open ( ) \n self . util = Util ( self . dev ) \n def test_repr ( self ) : \n self . assertEqual ( repr ( self . util ) , '<STR_LIT>' ) \n def test_dev_setter_exception ( self ) : \n def mod_dev ( ) : \n self . util . dev = '<STR_LIT:abc>' \n self . assertRaises ( RuntimeError , mod_dev ) \n def test_rpc_setter_exception ( self ) : \n def mod_rpc ( ) : \n self . util . rpc = '<STR_LIT:abc>' \n self . assertRaises ( RuntimeError , <mask0> ) \n", "gt": "mod_rpc"}
{"input": "\n import unittest \n from openmdao . main . api import set_as_top , Assembly \n from openmdao . util . testutil import assert_rel_error \n from openmdao . lib . drivers . api import BroydenSolver \n from hyperloop . tube_wall_temp import TubeWallTemp \n class TubeHeatBalance ( Assembly ) : \n def configure ( self ) : \n tm = self . add ( '<STR_LIT>' , TubeWallTemp ( ) ) \n driver = self . add ( '<STR_LIT>' , BroydenSolver ( ) ) \n driver . add_parameter ( '<STR_LIT>' , low = <NUM_LIT:0.> , high = <NUM_LIT> ) \n driver . add_constraint ( '<STR_LIT>' ) \n driver . workflow . add ( [ '<STR_LIT>' ] ) \n class TubeWallTestCase ( unittest . TestCase ) : \n def test_tube_temp ( self ) : \n test = set_as_top ( TubeHeatBalance ( ) ) \n test . tm . nozzle_air . setTotalTP ( <NUM_LIT> , <NUM_LIT> ) \n test . tm . nozzle_air . W = <NUM_LIT> \n test . tm . bearing_air . W = <NUM_LIT:0.> \n test . tm . diameter_outer_tube = <NUM_LIT> \n test . tm . length_tube = <NUM_LIT> \n test . tm . num_pods = <NUM_LIT> \n test . tm . temp_boundary = <NUM_LIT> \n test . tm . temp_outside_ambient = <NUM_LIT> \n test . run ( ) \n assert_rel_error ( self , test . tm . heat_rate_pod , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . total_heat_rate_pods , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . GrDelTL3 , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . Pr , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . Gr , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . Ra , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . Nu , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . k , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . h , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . area_convection , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . q_per_area_nat_conv , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . total_q_nat_conv , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . area_viewing , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . q_per_area_solar , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . q_total_solar , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . area_rad , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . q_rad_per_area , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . q_rad_tot , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , test . tm . q_total_out , <NUM_LIT> , <NUM_LIT> ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import os \n from setuptools import setup , find_packages \n with open ( os . path . join ( os . path . dirname ( __file__ ) , '<STR_LIT>' ) ) as f : \n required = f . read ( ) . splitlines ( ) \n setup ( \n name = '<STR_LIT>' , \n version = '<STR_LIT>' , \n description = '<STR_LIT>' , \n author = '<STR_LIT>' , \n author_email = '<STR_LIT>' , \n packages = find_packages ( exclude = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) , \n include_package_data = True , \n setup_requires = [ \n '<STR_LIT>' , \n ] , \n install_requires = required , \n entry_points = { \n '<STR_LIT>' : [ \n '<STR_LIT>' , \n ] , \n } , \n ) \n del <mask0> \n", "gt": "required"}
{"input": "\n import re \n import os \n import sys \n import time \n import hmac \n import base64 \n import hashlib \n import threading \n import logging \n import requests \n from yubico_client . otp import OTP \n from yubico_client . yubico_exceptions import ( StatusCodeError , \n InvalidClientIdError , \n InvalidValidationResponse , \n SignatureVerificationError ) \n from yubico_client . py3 import b \n from yubico_client . py3 import urlencode \n from yubico_client . py3 import unquote \n logger = logging . getLogger ( '<STR_LIT>' ) \n COMMON_CA_LOCATIONS = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] \n DEFAULT_API_URLS = ( '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' ) \n DEFAULT_TIMEOUT = <NUM_LIT:10> \n DEFAULT_MAX_TIME_WINDOW = <NUM_LIT:5> \n BAD_STATUS_CODES = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' ] \n class Yubico ( object ) : \n def __init__ ( self , client_id , key = None , verify_cert = True , \n translate_otp = True , api_urls = DEFAULT_API_URLS , \n ca_certs_bundle_path = None ) : \n if ca_certs_bundle_path and not self . _is_valid_ca_bundle_file ( ca_certs_bundle_path ) : \n raise ValueError ( ( '<STR_LIT>' \n '<STR_LIT>' ) ) \n self . client_id = client_id \n if key is not None : \n key = base64 . b64decode ( key . encode ( '<STR_LIT:ascii>' ) ) \n self . key = key \n self . verify_cert = verify_cert \n self . translate_otp = translate_otp \n self . api_urls = self . _init_request_urls ( api_urls = api_urls ) \n self . ca_certs_bundle_path = ca_certs_bundle_path \n def verify ( self , otp , timestamp = False , sl = None , timeout = None , \n return_response = False ) : \n \"\"\"<STR_LIT>\"\"\" \n ca_bundle_path = self . _get_ca_bundle_path ( ) \n otp = OTP ( otp , self . translate_otp ) \n rand_str = b ( os . urandom ( <NUM_LIT:30> ) ) \n nonce = base64 . b64encode ( rand_str , b ( '<STR_LIT>' ) ) [ : <NUM_LIT> ] . decode ( '<STR_LIT:utf-8>' ) \n query_string = self . generate_query_string ( otp . otp , nonce , timestamp , \n sl , timeout ) \n threads = [ ] \n timeout = timeout or DEFAULT_TIMEOUT \n for url in self . api_urls : \n thread = URLThread ( '<STR_LIT>' % ( url , query_string ) , timeout , \n self . verify_cert , ca_bundle_path ) \n thread . start ( ) \n threads . append ( thread ) \n start_time = time . time ( ) \n while threads and ( start_time + timeout ) > time . time ( ) : \n for thread in threads : \n if not thread . is_alive ( ) : \n if thread . exception : \n raise thread . exception \n elif thread . response : \n status = self . verify_response ( thread . response , \n otp . otp , nonce , \n return_response ) \n if status : \n if return_response : \n return status \n else : \n return True \n threads . remove ( thread ) \n time . sleep ( <NUM_LIT:0.1> ) \n raise Exception ( '<STR_LIT>' ) \n def verify_multi ( self , otp_list , max_time_window = DEFAULT_MAX_TIME_WINDOW , \n sl = None , timeout = None ) : \n \"\"\"<STR_LIT>\"\"\" \n otps = [ ] \n for otp in otp_list : \n otps . append ( OTP ( otp , self . translate_otp ) ) \n if len ( otp_list ) < <NUM_LIT:2> : \n raise ValueError ( '<STR_LIT>' ) \n device_ids = set ( ) \n for otp in otps : \n device_ids . add ( otp . device_id ) \n if len ( device_ids ) != <NUM_LIT:1> : \n raise Exception ( '<STR_LIT>' ) \n for otp in otps : \n response = self . verify ( otp . otp , True , sl , timeout , \n return_response = True ) \n if not response : \n return False \n otp . timestamp = int ( response [ '<STR_LIT>' ] ) \n count = len ( otps ) \n delta = otps [ count - <NUM_LIT:1> ] . timestamp - otps [ <NUM_LIT:0> ] . timestamp \n delta = delta / <NUM_LIT:8> \n if delta < <NUM_LIT:0> : \n raise Exception ( '<STR_LIT>' \n '<STR_LIT>' ) \n if delta > max_time_window : \n raise Exception ( ( '<STR_LIT>' \n '<STR_LIT>' ) % \n ( max_time_window ) ) \n return True \n def verify_response ( self , response , otp , nonce , return_response = False ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n status = re . search ( r'<STR_LIT>' , response ) . groups ( ) \n if len ( status ) > <NUM_LIT:1> : \n message = '<STR_LIT>' \n raise InvalidValidationResponse ( message , response ) \n status = status [ <NUM_LIT:0> ] \n except ( AttributeError , IndexError ) : \n return False \n signature , parameters = self . parse_parameters_from_response ( response ) \n if self . key : \n generated_signature = self . generate_message_signature ( parameters ) \n if signature != generated_signature : \n logger . warn ( \"<STR_LIT>\" , parameters ) \n raise SignatureVerificationError ( generated_signature , \n signature ) \n param_dict = self . get_parameters_as_dictionary ( parameters ) \n if '<STR_LIT>' in param_dict and param_dict [ '<STR_LIT>' ] != otp : \n message = '<STR_LIT>' \n raise InvalidValidationResponse ( message , response , param_dict ) \n if '<STR_LIT>' in param_dict and param_dict [ '<STR_LIT>' ] != nonce : \n message = '<STR_LIT>' \n raise InvalidValidationResponse ( message , response , param_dict ) \n if status == '<STR_LIT:OK>' : \n if return_response : \n return param_dict \n else : \n return True \n elif status == '<STR_LIT>' : \n raise InvalidClientIdError ( self . client_id ) \n elif status == '<STR_LIT>' : \n raise StatusCodeError ( status ) \n return False \n def generate_query_string ( self , otp , nonce , timestamp = False , sl = None , \n timeout = None ) : \n \"\"\"<STR_LIT>\"\"\" \n data = [ ( '<STR_LIT:id>' , self . client_id ) , \n ( '<STR_LIT>' , otp ) , \n ( '<STR_LIT>' , nonce ) ] \n if timestamp : \n data . append ( ( '<STR_LIT>' , '<STR_LIT:1>' ) ) \n if sl is not None : \n if sl not in range ( <NUM_LIT:0> , <NUM_LIT> ) and sl not in [ '<STR_LIT>' , '<STR_LIT>' ] : \n raise Exception ( '<STR_LIT>' \n '<STR_LIT>' ) \n data . append ( ( '<STR_LIT>' , sl ) ) \n if timeout : \n data . append ( ( '<STR_LIT>' , timeout ) ) \n query_string = urlencode ( data ) \n if self . key : \n hmac_signature = self . generate_message_signature ( query_string ) \n hmac_signature = hmac_signature \n query_string += '<STR_LIT>' % ( hmac_signature . replace ( '<STR_LIT:+>' , '<STR_LIT>' ) ) \n return query_string \n def generate_message_signature ( self , query_string ) : \n \"\"\"<STR_LIT>\"\"\" \n pairs = query_string . split ( '<STR_LIT:&>' ) \n pairs = [ pair . split ( '<STR_LIT:=>' , <NUM_LIT:1> ) for pair in pairs ] \n pairs_sorted = sorted ( pairs ) \n pairs_string = '<STR_LIT:&>' . join ( [ '<STR_LIT:=>' . join ( pair ) for pair in pairs_sorted ] ) \n digest = hmac . new ( self . key , b ( pairs_string ) , hashlib . sha1 ) . digest ( ) \n signature = base64 . b64encode ( digest ) . decode ( '<STR_LIT:utf-8>' ) \n return signature \n def parse_parameters_from_response ( self , response ) : \n \"\"\"<STR_LIT>\"\"\" \n lines = response . splitlines ( ) \n pairs = [ line . strip ( ) . split ( '<STR_LIT:=>' , <NUM_LIT:1> ) for line in lines if '<STR_LIT:=>' in line ] \n pairs = sorted ( pairs ) \n signature = ( [ unquote ( v ) for k , v in pairs if k == '<STR_LIT:h>' ] or [ None ] ) [ <NUM_LIT:0> ] \n query_string = '<STR_LIT:&>' . join ( [ k + '<STR_LIT:=>' + v for k , v in pairs if k != '<STR_LIT:h>' ] ) \n return ( signature , query_string ) \n def get_parameters_as_dictionary ( self , query_string ) : \n \"\"\"<STR_LIT>\"\"\" \n pairs = ( x . split ( '<STR_LIT:=>' , <NUM_LIT:1> ) for x in query_string . split ( '<STR_LIT:&>' ) ) \n return dict ( ( k , unquote ( v ) ) for k , v in pairs ) \n def _init_request_urls ( self , api_urls ) : \n \"\"\"<STR_LIT>\"\"\" \n if not isinstance ( api_urls , ( str , list , tuple ) ) : \n raise TypeError ( '<STR_LIT>' ) \n if isinstance ( api_urls , str ) : \n api_urls = ( api_urls , ) \n api_urls = list ( api_urls ) \n for url in api_urls : \n if not url . startswith ( '<STR_LIT>' ) and not url . startswith ( '<STR_LIT>' ) : \n raise ValueError ( ( '<STR_LIT>' \n '<STR_LIT>' % ( url ) ) ) \n return list ( api_urls ) \n def _get_ca_bundle_path ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . ca_certs_bundle_path : \n return self . ca_certs_bundle_path \n for file_path in COMMON_CA_LOCATIONS : \n if self . _is_valid_ca_bundle_file ( file_path = file_path ) : \n return file_path \n return None \n def _is_valid_ca_bundle_file ( self , file_path ) : \n return os . path . exists ( file_path ) and os . path . isfile ( file_path ) \n class URLThread ( threading . Thread ) : \n def __init__ ( self , url , timeout , verify_cert , ca_bundle_path = None ) : \n super ( URLThread , self ) . __init__ ( ) \n self . url = url \n self . timeout = timeout \n self . verify_cert = verify_cert \n self . ca_bundle_path = ca_bundle_path \n self . exception = None \n self . request = None \n self . response = None \n def run ( self ) : \n logger . debug ( '<STR_LIT>' % ( self . url , \n self . name ) ) \n verify = self . verify_cert \n if self . ca_bundle_path is not None : \n verify = self . ca_bundle_path \n logger . debug ( '<STR_LIT>' % ( self . ca_bundle_path ) ) \n try : \n self . request = requests . get ( url = self . url , timeout = self . timeout , \n verify = verify ) \n self . response = self . request . content . decode ( '<STR_LIT:utf-8>' ) \n except requests . exceptions . SSLError : \n e = sys . exc_info ( ) [ <NUM_LIT:1> ] \n self . exception = e \n self . response = None \n except Exception : \n e = sys . exc_info ( ) [ <NUM_LIT:1> ] \n logger . error ( '<STR_LIT>' + str ( e ) ) \n self . response = None \n args = ( self . url , self . name , self . response ) \n logger . debug ( '<STR_LIT>' % <mask0> ) \n", "gt": "args"}
{"input": "\n import logging \n from app import app , logger \n root = logging . getLogger ( ) \n root . setLevel ( logging . DEBUG ) \n logging . getLogger ( \"<STR_LIT>\" ) . setLevel ( logging . INFO ) \n if __name__ == '<STR_LIT:__main__>' : \n logger . warn ( \"<STR_LIT>\" ) \n app . run ( host = '<STR_LIT>' , debug = <mask0> ) \n", "gt": "True"}
{"input": "\n from setuptools import setup \n setup ( \n name = '<STR_LIT>' , \n version = '<STR_LIT>' , \n py_modules = [ '<STR_LIT>' ] , \n install_requires = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] , \n <mask0> = '''<STR_LIT>''' , \n ) \n", "gt": "entry_points"}
{"input": "\n from nose . tools import ok_ , raises \n from linot import config \n from linot . interfaces . line_interface import LineClientP , LineInterface \n class TestLineClientP : \n def setUp ( self ) : \n self . line_cfg = config [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n self . lineclient = LineClientP ( self . line_cfg [ '<STR_LIT>' ] , \n self . line_cfg [ '<STR_LIT:password>' ] ) \n def test_find_contact_by_id ( self ) : \n contact = self . lineclient . find_contact_by_id ( self . line_cfg [ '<STR_LIT>' ] ) \n ok_ ( contact . id == self . line_cfg [ '<STR_LIT>' ] ) \n @ raises ( ValueError ) \n def test_find_contact_by_id_exception ( self ) : \n self . lineclient . find_contact_by_id ( self . line_cfg [ '<STR_LIT>' ] [ : - <NUM_LIT:2> ] ) \n class TestLineInterface : \n def setUp ( self ) : \n self . line_interface = LineInterface ( ) \n def test_polling_command ( self ) : \n test_str = '<STR_LIT>' \n me = self . line_interface . _client . getProfile ( ) \n me . sendMessage ( test_str ) \n result = self . line_interface . polling_command ( ) \n ok_ ( len ( result ) == <NUM_LIT:1> , result ) \n submitter , msg = result [ <NUM_LIT:0> ] \n ok_ ( submitter . code == me . id , submitter ) \n ok_ ( msg == test_str , \n '<STR_LIT>' . format ( msg , test_str ) ) \n def test_get_contact_by_id ( self ) : \n me = self . line_interface . _client . getProfile ( ) \n contact = self . line_interface . _get_contact_by_id ( me . id ) \n ok_ ( me . id == contact . id , '<STR_LIT>' . format ( me . id , contact . id ) ) \n def test_send_message ( self ) : \n test_str = '<STR_LIT>' \n me = self . line_interface . _client . getProfile ( ) \n me . sendMessage ( test_str ) \n result = self . line_interface . polling_command ( ) \n me , msg = result [ <NUM_LIT:0> ] \n self . line_interface . send_message ( me , test_str ) \n result = self . line_interface . polling_command ( ) \n me , msg = result [ <NUM_LIT:0> ] \n ok_ ( msg == test_str , '<STR_LIT>' . format ( msg , test_str ) ) \n def test_send_message_to_id ( self ) : \n test_str = '<STR_LIT>' \n me = self . line_interface . _client . getProfile ( ) \n me . sendMessage ( test_str ) \n result = self . line_interface . polling_command ( ) \n me , msg = result [ <NUM_LIT:0> ] \n self . line_interface . _send_message_to_id ( me . code , test_str ) \n result = self . line_interface . polling_command ( ) \n me , msg = result [ <NUM_LIT:0> ] \n ok_ ( msg == test_str , '<STR_LIT>' . format ( msg , test_str ) ) \n def test_get_display_name ( self ) : \n test_str = '<STR_LIT>' \n me = self . line_interface . _client . getProfile ( ) \n me . sendMessage ( test_str ) \n result = self . line_interface . polling_command ( ) \n me_submitter , msg = result [ <NUM_LIT:0> ] \n me_display_name = self . line_interface . get_display_name ( me_submitter ) \n ok_ ( me_display_name == me . <mask0> ) \n", "gt": "name"}
{"input": "\n import pytest \n import socket \n from aiohttp . parsers import StreamWriter , CORK \n from unittest import mock \n def test_nodelay_default ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n assert not writer . tcp_nodelay \n assert not s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n def test_set_nodelay_no_change ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_nodelay ( False ) \n assert not writer . tcp_nodelay \n assert not s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n def test_set_nodelay_enable ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_nodelay ( True ) \n assert writer . tcp_nodelay \n assert s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n def test_set_nodelay_enable_and_disable ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_nodelay ( True ) \n writer . set_tcp_nodelay ( False ) \n assert not writer . tcp_nodelay \n assert not s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n def test_set_nodelay_enable_ipv6 ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET6 , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_nodelay ( True ) \n assert writer . tcp_nodelay \n assert s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n @ pytest . mark . skipif ( not hasattr ( socket , '<STR_LIT>' ) , \n reason = \"<STR_LIT>\" ) \n def test_set_nodelay_enable_unix ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_UNIX , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_nodelay ( True ) \n assert writer . tcp_nodelay \n def test_set_nodelay_enable_no_socket ( loop ) : \n transport = mock . Mock ( ) \n transport . get_extra_info . return_value = None \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_nodelay ( True ) \n assert writer . tcp_nodelay \n assert writer . _socket is None \n @ pytest . mark . skipif ( CORK is None , reason = \"<STR_LIT>\" ) \n def test_cork_default ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n assert not writer . tcp_cork \n assert not s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n @ pytest . mark . skipif ( CORK is None , reason = \"<STR_LIT>\" ) \n def test_set_cork_no_change ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_cork ( False ) \n assert not writer . tcp_cork \n assert not s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n @ pytest . mark . skipif ( CORK is None , reason = \"<STR_LIT>\" ) \n def test_set_cork_enable ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_cork ( True ) \n assert writer . tcp_cork \n assert s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n @ pytest . mark . skipif ( CORK is None , reason = \"<STR_LIT>\" ) \n def test_set_cork_enable_and_disable ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_cork ( True ) \n writer . set_tcp_cork ( False ) \n assert not writer . tcp_cork \n assert not s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n @ pytest . mark . skipif ( CORK is None , reason = \"<STR_LIT>\" ) \n def test_set_cork_enable_ipv6 ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET6 , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_cork ( True ) \n assert writer . tcp_cork \n assert s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n @ pytest . mark . skipif ( not hasattr ( socket , '<STR_LIT>' ) , \n reason = \"<STR_LIT>\" ) \n @ pytest . mark . skipif ( CORK is None , reason = \"<STR_LIT>\" ) \n def test_set_cork_enable_unix ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_UNIX , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_cork ( True ) \n assert writer . tcp_cork \n @ pytest . mark . skipif ( CORK is None , reason = \"<STR_LIT>\" ) \n def test_set_cork_enable_no_socket ( loop ) : \n transport = mock . Mock ( ) \n transport . get_extra_info . return_value = None \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_cork ( True ) \n assert writer . tcp_cork \n assert writer . _socket is None \n @ pytest . mark . skipif ( CORK is None , reason = \"<STR_LIT>\" ) \n def test_set_enabling_cork_disables_nodelay ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_nodelay ( True ) \n writer . set_tcp_cork ( True ) \n assert not writer . tcp_nodelay \n assert not s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n assert writer . tcp_cork \n assert s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n @ pytest . mark . skipif ( CORK is None , reason = \"<STR_LIT>\" ) \n def test_set_enabling_nodelay_disables_cork ( loop ) : \n transport = mock . Mock ( ) \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n transport . get_extra_info . return_value = s \n proto = mock . Mock ( ) \n reader = mock . Mock ( ) \n writer = StreamWriter ( transport , proto , reader , loop ) \n writer . set_tcp_cork ( True ) \n writer . set_tcp_nodelay ( True ) \n assert writer . tcp_nodelay \n assert s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n assert not writer . tcp_cork \n assert not s . getsockopt ( <mask0> . IPPROTO_TCP , CORK ) \n", "gt": "socket"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import argparse \n import logging \n from collections import namedtuple \n from . import placeholder \n logger = logging . getLogger ( ) \n ShortenerSettings = namedtuple ( '<STR_LIT>' , [ \n '<STR_LIT:name>' , \n '<STR_LIT>' \n ] ) \n Settings = namedtuple ( '<STR_LIT>' , [ \n '<STR_LIT:source>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT:strict>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] ) \n def default_settings ( ) : \n return Settings ( \n verbose = False , \n strict = True , \n force = False , \n source = '<STR_LIT:src>' , \n destination = '<STR_LIT:target>' , \n templates = '<STR_LIT>' , \n images = '<STR_LIT>' , \n right_to_left = [ '<STR_LIT>' , '<STR_LIT>' ] , \n pattern = '<STR_LIT>' , \n shortener = { } , \n exclusive = None , \n default_locale = '<STR_LIT>' , \n workers_pool = <NUM_LIT:10> , \n local_images = '<STR_LIT>' , \n save = None , \n cms_service_host = \"<STR_LIT>\" \n ) \n def read_args ( argsargs = argparse . ArgumentParser ) : \n settings = default_settings ( ) \n logger . debug ( '<STR_LIT>' ) \n args = argsargs ( epilog = '<STR_LIT>' ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , help = '<STR_LIT>' % settings . source ) \n args . add_argument ( \n '<STR_LIT>' , '<STR_LIT>' , help = '<STR_LIT>' % settings . exclusive ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , \n help = '<STR_LIT>' % settings . destination ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , help = '<STR_LIT>' % settings . templates ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , \n help = '<STR_LIT>' % settings . right_to_left ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , help = '<STR_LIT>' % settings . images ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , help = '<STR_LIT>' % settings . pattern ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , \n help = '<STR_LIT>' , \n action = '<STR_LIT>' ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , help = '<STR_LIT>' , action = '<STR_LIT:store_true>' ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , \n help = '<STR_LIT>' % settings . workers_pool , type = int ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , help = '<STR_LIT>' , action = '<STR_LIT:store_true>' ) \n args . add_argument ( '<STR_LIT>' , '<STR_LIT>' , help = '<STR_LIT>' , action = '<STR_LIT:store_true>' ) \n subparsers = args . add_subparsers ( help = '<STR_LIT>' , dest = '<STR_LIT>' ) \n template_parser = subparsers . add_parser ( '<STR_LIT>' ) \n template_parser . add_argument ( '<STR_LIT>' , help = '<STR_LIT>' ) \n template_parser . add_argument ( '<STR_LIT>' , \n help = '<STR_LIT>' ) \n config_parser = subparsers . add_parser ( '<STR_LIT>' ) \n config_parser . add_argument ( '<STR_LIT>' , help = '<STR_LIT>' ) \n gui_parser = subparsers . add_parser ( '<STR_LIT>' ) \n gui_parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , type = int , help = '<STR_LIT>' , default = <NUM_LIT> ) \n gui_parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , type = str , help = '<STR_LIT>' , \n default = '<STR_LIT>' ) \n gui_parser . add_argument ( '<STR_LIT>' , type = str , help = '<STR_LIT>' ) \n gui_parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , type = str , help = '<STR_LIT>' ) \n return args . parse_args ( ) \n def read_settings ( args ) : \n args = vars ( args ) \n settings = default_settings ( ) . _asdict ( ) \n for k in settings : \n if k in args and args [ k ] is not None : \n settings [ k ] = args [ k ] \n return Settings ( ** settings ) \n def print_version ( ) : \n import pkg_resources \n version = pkg_resources . require ( '<STR_LIT>' ) [ <NUM_LIT:0> ] . version \n print ( version ) \n return True \n def generate_config ( args ) : \n if args . config_name == '<STR_LIT>' : \n logger . info ( '<STR_LIT>' ) \n settings = read_settings ( args ) \n placeholder . generate_config ( settings ) \n return True \n return False \n def execute_command ( args ) : \n if args . command == '<STR_LIT>' : \n return generate_config ( args ) \n elif args . command == '<STR_LIT>' : \n from . gui . gui import serve \n serve ( args ) \n return True \n return <mask0> \n", "gt": "False"}
{"input": "\n from ldap3 import Server , Connection , ALL \n \"\"\"<STR_LIT>\"\"\" \n def rotate ( record , newpassword ) : \n result = False \n host = record . get ( '<STR_LIT>' ) \n user_dn = record . get ( '<STR_LIT>' ) \n try : \n server = Server ( \n host = host , \n use_ssl = True , \n get_info = ALL ) \n conn = Connection ( \n server = server , \n user = user_dn , \n password = record . password , \n auto_bind = True ) \n changePwdResult = conn . extend . microsoft . modify_password ( user_dn , newpassword ) \n if ( changePwdResult == True ) : \n print ( '<STR_LIT>' ) \n record . password = newpassword \n result = True \n else : \n print ( \"<STR_LIT>\" % ( changePwdResult ) ) \n conn . unbind ( ) \n except : \n print ( \"<STR_LIT>\" ) \n return <mask0> \n", "gt": "result"}
{"input": "\n from keepercommander . record import Record \n def sample_record ( ) : \n record = Record ( ) \n record . folder = '<STR_LIT>' \n record . title = '<STR_LIT:title>' \n record . login = '<STR_LIT>' \n record . password = '<STR_LIT:password>' \n record . login_url = '<STR_LIT>' \n record . notes = '<STR_LIT>' \n record . custom_fields = [ \n { '<STR_LIT:name>' : '<STR_LIT>' , '<STR_LIT:value>' : '<STR_LIT>' , '<STR_LIT:type>' : '<STR_LIT:text>' } , \n { '<STR_LIT:name>' : '<STR_LIT>' , '<STR_LIT:value>' : '<STR_LIT>' , '<STR_LIT:type>' : '<STR_LIT:text>' } ] \n return record \n class TestRecord : \n def test_to_tab_delimited ( self ) : \n assert sample_record ( ) . to_tab_delimited ( ) == '<STR_LIT>' \n def test_to_tab_dictionary ( self ) : \n assert sample_record ( ) . to_dictionary ( ) == { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:title>' : '<STR_LIT:title>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:password>' : '<STR_LIT:password>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : [ \n { '<STR_LIT:name>' : '<STR_LIT>' , '<STR_LIT:value>' : '<STR_LIT>' , '<STR_LIT:type>' : '<STR_LIT:text>' } , \n { '<STR_LIT:name>' : '<STR_LIT>' , '<STR_LIT:value>' : '<STR_LIT>' , '<STR_LIT:type>' : <mask0> } ] , \n } \n", "gt": "'<STR_LIT:text>'"}
{"input": "\n from filesize import size \n from filesize import traditional , alternative , verbose , iec , <mask0> \n", "gt": "si"}
{"input": "\n import os \n import logbook \n import pytest \n import pyshark \n @ pytest . fixture \n def caps_directory ( ) : \n return os . path . join ( os . path . dirname ( __file__ ) , '<STR_LIT>' ) \n @ pytest . fixture \n def lazy_simple_capture ( request , caps_directory ) : \n \"\"\"<STR_LIT>\"\"\" \n cap_path = os . path . join ( caps_directory , '<STR_LIT>' ) \n cap = pyshark . FileCapture ( cap_path ) \n cap . log . level = logbook . DEBUG \n def finalizer ( ) : \n cap . close ( ) \n cap . eventloop . stop ( ) \n request . addfinalizer ( finalizer ) \n return cap \n @ pytest . fixture \n def simple_capture ( lazy_simple_capture ) : \n \"\"\"<STR_LIT>\"\"\" \n lazy_simple_capture . load_packets ( ) \n return <mask0> \n", "gt": "lazy_simple_capture"}
{"input": "\n from cornice import Service \n from pyramid import httpexceptions \n from pyramid . security import NO_PERMISSION_REQUIRED \n from kinto . events import ServerFlushed \n flush = Service ( name = '<STR_LIT>' , \n description = '<STR_LIT>' , \n path = '<STR_LIT>' ) \n @ flush . post ( permission = NO_PERMISSION_REQUIRED ) \n def flush_post ( request ) : \n request . registry . storage . flush ( ) \n request . registry . permission . flush ( ) \n request . registry . cache . flush ( ) \n event = ServerFlushed ( request ) \n request . registry . notify ( event ) \n return httpexceptions . <mask0> ( ) \n", "gt": "HTTPAccepted"}
{"input": "\n import os \n os . environ [ '<STR_LIT>' ] = os . environ . get ( '<STR_LIT>' , '<STR_LIT>' ) \n from tests import base \n def setUpModule ( ) : \n \"\"\"<STR_LIT>\"\"\" \n base . enabledPlugins . append ( '<STR_LIT>' ) \n base . enabledPlugins . append ( '<STR_LIT>' ) \n base . enabledPlugins . append ( '<STR_LIT>' ) \n base . enabledPlugins . append ( '<STR_LIT>' ) \n base . startServer ( False ) \n def tearDownModule ( ) : \n \"\"\"<STR_LIT>\"\"\" \n base . stopServer ( ) \n class SourceTestCase ( base . TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n super ( SourceTestCase , self ) . setUp ( ) \n self . _user = self . model ( '<STR_LIT:user>' ) . createUser ( \n '<STR_LIT>' , '<STR_LIT:password>' , '<STR_LIT>' , '<STR_LIT:user>' , \n '<STR_LIT>' ) \n def testSource ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n path = '<STR_LIT>' \n params = { \n '<STR_LIT>' : self . _user [ '<STR_LIT>' ] , \n } \n response = self . request ( path = path , method = '<STR_LIT:GET>' , params = params ) \n self . assertStatusOk ( response ) \n folder = response . json [ '<STR_LIT>' ] \n self . assertEquals ( folder , None ) \n response = self . request ( path = path , method = '<STR_LIT:POST>' , params = params ) \n self . assertStatus ( response , <NUM_LIT> ) \n response = self . request ( path = path , method = '<STR_LIT:POST>' , params = params , user = self . _user ) \n self . assertStatusOk ( response ) \n folder = response . json [ '<STR_LIT>' ] \n self . assertNotEquals ( folder , None ) \n self . assertEquals ( folder [ '<STR_LIT>' ] , '<STR_LIT:user>' ) \n self . assertEquals ( folder [ '<STR_LIT>' ] , str ( self . _user [ '<STR_LIT>' ] ) ) \n response = self . request ( path = path , method = '<STR_LIT:GET>' , params = params ) \n self . assertStatusOk ( response ) \n folder = response . json [ '<STR_LIT>' ] \n self . assertEquals ( folder , None ) \n response = self . request ( path = path , method = '<STR_LIT:GET>' , params = params , user = self . _user ) \n self . assertStatusOk ( response ) \n folder = response . json [ '<STR_LIT>' ] \n self . assertNotEquals ( folder , None ) \n self . assertEquals ( folder [ '<STR_LIT>' ] , '<STR_LIT:user>' ) \n self . assertEquals ( folder [ '<STR_LIT>' ] , str ( self . _user [ '<STR_LIT>' ] ) ) \n params = { \n '<STR_LIT:name>' : '<STR_LIT>' , \n '<STR_LIT>' : folder [ '<STR_LIT>' ] \n } \n response = self . request ( path = '<STR_LIT>' , method = '<STR_LIT:POST>' , params = params , \n user = self . _user ) \n item1Id = response . json [ '<STR_LIT>' ] \n params = { \n '<STR_LIT:name>' : '<STR_LIT>' , \n '<STR_LIT>' : folder [ '<STR_LIT>' ] \n } \n response = self . request ( path = '<STR_LIT>' , method = '<STR_LIT:POST>' , params = params , \n user = self . _user ) \n item2Id = response . json [ '<STR_LIT>' ] \n path = '<STR_LIT>' \n params = { \n '<STR_LIT>' : self . _user [ '<STR_LIT>' ] , \n } \n response = self . request ( path = path , method = '<STR_LIT:GET>' , params = params ) \n self . assertStatusOk ( response ) \n self . assertEquals ( len ( response . json ) , <NUM_LIT:0> ) \n response = self . request ( path = path , method = '<STR_LIT:GET>' , params = params , user = self . _user ) \n self . assertStatusOk ( response ) \n self . assertEquals ( len ( response . json ) , <NUM_LIT:2> ) \n sourceIds = [ d [ '<STR_LIT>' ] for d in response . json ] \n self . assertTrue ( item1Id in sourceIds , \"<STR_LIT>\" ) \n self . assertTrue ( item2Id in <mask0> , \"<STR_LIT>\" ) \n", "gt": "sourceIds"}
{"input": "\n from girder . api import access \n from girder . api . describe import Description \n from girder . api . rest import loadmodel , RestException \n from girder . constants import AccessType \n from girder . plugins . minerva . rest . dataset import Dataset \n from girder . plugins . minerva . utility . minerva_utility import findDatasetFolder , updateMinervaMetadata \n class GeojsonDataset ( Dataset ) : \n def __init__ ( self ) : \n self . resourceName = '<STR_LIT>' \n self . route ( '<STR_LIT:POST>' , ( ) , self . createGeojsonDataset ) \n @ access . user \n @ loadmodel ( map = { '<STR_LIT>' : '<STR_LIT>' } , model = '<STR_LIT>' , \n level = AccessType . WRITE ) \n def createGeojsonDataset ( self , item , params ) : \n user = self . getCurrentUser ( ) \n folder = findDatasetFolder ( user , user , create = True ) \n if folder is None : \n raise RestException ( '<STR_LIT>' ) \n if folder [ '<STR_LIT>' ] != item [ '<STR_LIT>' ] : \n raise RestException ( \"<STR_LIT>\" + \n \"<STR_LIT>\" ) \n minerva_metadata = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n } \n for file in self . model ( '<STR_LIT>' ) . childFiles ( item = item , limit = <NUM_LIT:0> ) : \n if '<STR_LIT>' in file [ '<STR_LIT>' ] or '<STR_LIT>' in file [ '<STR_LIT>' ] : \n minerva_metadata [ '<STR_LIT>' ] = [ { \n '<STR_LIT:name>' : file [ '<STR_LIT:name>' ] , '<STR_LIT>' : file [ '<STR_LIT>' ] } ] \n minerva_metadata [ '<STR_LIT>' ] = { \n '<STR_LIT:name>' : file [ '<STR_LIT:name>' ] , '<STR_LIT>' : file [ '<STR_LIT>' ] } \n break \n if '<STR_LIT>' not in minerva_metadata : \n raise RestException ( '<STR_LIT>' ) \n updateMinervaMetadata ( item , minerva_metadata ) \n return item \n createGeojsonDataset . description = ( \n Description ( '<STR_LIT>' ) \n . responseClass ( '<STR_LIT>' ) \n . param ( '<STR_LIT>' , '<STR_LIT>' , required = True ) \n . errorResponse ( '<STR_LIT>' ) \n . <mask0> ( '<STR_LIT>' , <NUM_LIT> ) ) \n", "gt": "errorResponse"}
{"input": "\n from setuptools import setup , find_packages \n import re \n import os \n from os . path import join as opj \n curdir = os . path . dirname ( os . path . realpath ( __file__ ) ) \n def read ( fname ) : \n contents = '<STR_LIT>' \n with open ( fname ) as f : \n contents = f . read ( ) \n return contents \n package_name = '<STR_LIT>' \n def version ( ) : \n text = read ( opj ( curdir , package_name , '<STR_LIT>' ) ) \n matches = re . findall ( \"<STR_LIT>\" , text ) \n return matches [ <NUM_LIT:0> ] [ <NUM_LIT:1> ] \n install_requires = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n test_requires = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n if __name__ == '<STR_LIT:__main__>' : \n setup ( \n name = package_name , \n packages = [ package_name ] , \n include_package_data = True , \n author = '<STR_LIT>' , \n author_email = '<STR_LIT>' , \n version = version ( ) , \n description = \"<STR_LIT>\" , \n long_description = read ( opj ( curdir , '<STR_LIT>' ) ) , \n url = '<STR_LIT>' , \n install_requires = install_requires , \n license = '<STR_LIT>' , \n classifiers = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n package_data = { '<STR_LIT>' : [ '<STR_LIT>' ] } , \n zip_safe = False , \n tests_require = <mask0> , \n ) \n", "gt": "test_requires"}
{"input": "\n import unittest \n import utils \n import sdk \n change_file_permissions = [ '<STR_LIT>' ] \n change_folder_permissions = [ '<STR_LIT>' , '<STR_LIT>' ] \n list_permissions = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n readonly_permissions = [ '<STR_LIT>' , '<STR_LIT>' ] \n class Permissions ( unittest . TestCase ) : \n new_roles = { } \n @ utils . allow ( services = list_permissions ) \n def setUp ( self ) : \n acc = self . account \n if acc . service in list_permissions : \n self . test_folder = utils . create_or_get_test_folder ( acc ) \n self . test_file = utils . create_test_file ( acc ) \n new_roles = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n if acc . service in change_folder_permissions : \n self . new_roles = new_roles \n self . test_folder . permissions . create ( data = self . new_roles ) \n if acc . service in change_file_permissions : \n self . new_roles = new_roles \n self . test_file . permissions . create ( data = self . new_roles ) \n def list_helper ( self , data ) : \n result = data . permissions . all ( ) \n self . assertIsInstance ( result , sdk . resources . AnnotatedList ) \n owner_exists = False \n for perm in result : \n self . assertIsInstance ( perm , sdk . resources . Permission ) \n if self . account . service not in readonly_permissions : \n if perm . role == \"<STR_LIT>\" : \n owner_exists = True \n else : \n self . assertIn ( perm . email , self . new_roles ) \n self . assertEqual ( perm . role , self . new_roles . get ( perm . email ) ) \n self . assertTrue ( owner_exists ) \n def test_folder_permissions_list ( self ) : \n if self . account . service in list_permissions : \n self . list_helper ( self . test_folder ) \n def test_folder_permissions_set ( self ) : \n if self . account . service in change_folder_permissions : \n self . new_roles = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n result = self . test_folder . permissions . create ( data = self . new_roles ) \n self . assertIsInstance ( result . permissions , list ) \n self . list_helper ( self . test_folder ) \n def test_folder_permissions_update ( self ) : \n if self . account . service in change_folder_permissions : \n self . new_roles . update ( { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } ) \n result = self . test_folder . permissions . update ( data = self . new_roles ) \n self . assertIsInstance ( result . permissions , list ) \n self . list_helper ( self . test_folder ) \n def test_file_permissions_list ( self ) : \n if self . account . service in list_permissions : \n self . list_helper ( self . test_file ) \n def test_file_permissions_set ( self ) : \n if self . account . service in change_file_permissions : \n self . new_roles = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n result = self . test_file . permissions . create ( data = self . new_roles ) \n self . assertIsInstance ( result . permissions , list ) \n self . list_helper ( self . test_file ) \n def test_file_permissions_update ( self ) : \n if self . account . service in change_file_permissions : \n self . new_roles . update ( { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } ) \n result = self . test_file . permissions . update ( data = self . new_roles ) \n self . assertIsInstance ( result . permissions , list ) \n self . list_helper ( self . test_file ) \n def test_cases ( ) : \n return [ utils . create_test_case ( acc , Permissions ) for acc in utils . accounts ] \n if __name__ == '<STR_LIT:__main__>' : \n suite = utils . create_suite ( test_cases ( ) ) \n unittest . TextTestRunner ( verbosity = <NUM_LIT:2> ) . run ( <mask0> ) \n", "gt": "suite"}
{"input": "\n try : \n basestring = basestring \n except NameError : \n basestring = str \n from . nmea import NMEASentence \n class NMEAFile ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , f , * args , ** kwargs ) : \n super ( NMEAFile , self ) . __init__ ( ) \n if isinstance ( f , basestring ) or args or kwargs : \n self . _file = self . open ( f , * args , ** kwargs ) \n else : \n self . _file = f \n self . _context = None \n def open ( self , fp , mode = '<STR_LIT:r>' ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _file = open ( fp , mode = mode ) \n return self . _file \n def close ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _file . close ( ) \n def __iter__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for line in self . _file : \n yield self . parse ( line ) \n def __enter__ ( self ) : \n if hasattr ( self . _file , '<STR_LIT>' ) : \n self . _context = self . _file . __enter__ ( ) \n return self \n def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n if self . _context : \n ctx = self . _context \n self . _context = None \n ctx . __exit__ ( exc_type , exc_val , exc_tb ) \n def next ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n data = self . _file . readline ( ) \n return self . parse ( data ) \n def parse ( self , s ) : \n return NMEASentence . parse ( s ) \n def readline ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n data = self . _file . readline ( ) \n s = self . parse ( data ) \n return s \n def read ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return [ s for s in <mask0> ] \n", "gt": "self"}
{"input": "\n from optparse import make_option \n from django . core . management . base import BaseCommand \n from django . utils . translation import ugettext as _ \n from django_q . cluster import Cluster \n class Command ( BaseCommand ) : \n help = _ ( \"<STR_LIT>\" ) \n option_list = BaseCommand . option_list + ( \n make_option ( '<STR_LIT>' , \n action = '<STR_LIT:store_true>' , \n dest = '<STR_LIT>' , \n default = False , \n help = '<STR_LIT>' ) , \n ) \n def handle ( self , * args , ** options ) : \n q = Cluster ( ) \n q . start ( ) \n if options . get ( '<STR_LIT>' , False ) : \n q . <mask0> ( ) \n", "gt": "stop"}
{"input": "\n sources = \"\"\"<STR_LIT>\"\"\" \n import sys \n import base64 \n import zlib \n class DictImporter ( object ) : \n def __init__ ( self , sources ) : \n self . sources = sources \n def find_module ( self , fullname , path = None ) : \n if fullname == \"<STR_LIT>\" and sys . version_info >= ( <NUM_LIT:2> , <NUM_LIT:7> ) : \n return None \n if fullname in self . sources : \n return self \n if fullname + '<STR_LIT>' in self . sources : \n return self \n return None \n def load_module ( self , fullname ) : \n from types import ModuleType \n try : \n s = self . sources [ fullname ] \n is_pkg = False \n except KeyError : \n s = self . sources [ fullname + '<STR_LIT>' ] \n is_pkg = True \n co = compile ( s , fullname , '<STR_LIT>' ) \n module = sys . modules . setdefault ( fullname , ModuleType ( fullname ) ) \n module . __file__ = \"<STR_LIT>\" % ( __file__ , fullname ) \n module . __loader__ = self \n if is_pkg : \n module . __path__ = [ fullname ] \n do_exec ( co , module . __dict__ ) \n return sys . modules [ fullname ] \n def get_source ( self , name ) : \n res = self . sources . get ( name ) \n if res is None : \n res = self . sources . get ( name + '<STR_LIT>' ) \n return res \n if __name__ == \"<STR_LIT:__main__>\" : \n try : \n import pkg_resources \n except ImportError : \n sys . stderr . write ( \"<STR_LIT>\" ) \n sys . exit ( <NUM_LIT:2> ) \n if sys . version_info >= ( <NUM_LIT:3> , <NUM_LIT:0> ) : \n exec ( \"<STR_LIT>\" ) \n import pickle \n sources = sources . encode ( \"<STR_LIT:ascii>\" ) \n sources = pickle . loads ( zlib . decompress ( base64 . decodebytes ( sources ) ) ) \n else : \n import cPickle as pickle \n exec ( \"<STR_LIT>\" ) \n sources = pickle . loads ( zlib . decompress ( base64 . decodestring ( sources ) ) ) \n importer = DictImporter ( sources ) \n sys . meta_path . insert ( <NUM_LIT:0> , importer ) \n entry = \"<STR_LIT>\" \n do_exec ( entry , <mask0> ( ) ) \n", "gt": "locals"}
{"input": "\n from __future__ import with_statement \n from contextlib import contextmanager \n from datetime import datetime \n from UserDict import DictMixin \n import bcrypt \n from pyramid . location import lineage \n from pyramid . security import view_execution_permitted \n from six import string_types \n from sqlalchemy import Boolean , bindparam \n from sqlalchemy import Column \n from sqlalchemy import DateTime \n from sqlalchemy import func \n from sqlalchemy import Integer \n from sqlalchemy import Unicode \n from sqlalchemy . orm . exc import NoResultFound \n from sqlalchemy . sql . expression import and_ \n from sqlalchemy . sql . expression import or_ \n from zope . deprecation . deprecation import deprecated \n from kotti import Base \n from kotti import DBSession \n from kotti import get_settings \n from kotti . sqla import bakery \n from kotti . sqla import JsonType \n from kotti . sqla import MutationList \n from kotti . util import _ \n from kotti . util import request_cache \n from kotti . util import DontCache \n def get_principals ( ) : \n return get_settings ( ) [ '<STR_LIT>' ] [ <NUM_LIT:0> ] ( ) \n @ request_cache ( lambda request : None ) \n def get_user ( request ) : \n userid = request . unauthenticated_userid \n return get_principals ( ) . get ( userid ) \n def has_permission ( permission , context , request ) : \n \"\"\"<STR_LIT>\"\"\" \n return request . has_permission ( permission , context ) \n deprecated ( u'<STR_LIT>' , \n u\"<STR_LIT>\" \n u\"<STR_LIT>\" \n u\"<STR_LIT>\" ) \n class Principal ( Base ) : \n \"\"\"<STR_LIT>\"\"\" \n id = Column ( Integer , primary_key = True ) \n name = Column ( Unicode ( <NUM_LIT:100> ) , unique = True ) \n password = Column ( Unicode ( <NUM_LIT:100> ) ) \n active = Column ( Boolean ) \n confirm_token = Column ( Unicode ( <NUM_LIT:100> ) ) \n title = Column ( Unicode ( <NUM_LIT:100> ) , nullable = False ) \n email = Column ( Unicode ( <NUM_LIT:100> ) , unique = True ) \n groups = Column ( MutationList . as_mutable ( JsonType ) , nullable = False ) \n creation_date = Column ( DateTime ( ) , nullable = False ) \n last_login_date = Column ( DateTime ( ) ) \n __tablename__ = '<STR_LIT>' \n __mapper_args__ = dict ( \n order_by = name , \n ) \n def __init__ ( self , name , password = None , active = True , confirm_token = None , \n title = u\"<STR_LIT>\" , email = None , groups = None ) : \n self . name = name \n if password is not None : \n password = get_principals ( ) . hash_password ( password ) \n self . password = password \n self . active = active \n self . confirm_token = confirm_token \n self . title = title \n self . email = email \n if groups is None : \n groups = [ ] \n self . groups = groups \n self . creation_date = datetime . now ( ) \n self . last_login_date = None \n def __repr__ ( self ) : \n return u'<STR_LIT>' . format ( self . name ) \n class AbstractPrincipals ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __getitem__ ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n def __setitem__ ( self , name , principal ) : \n \"\"\"<STR_LIT>\"\"\" \n def __delitem__ ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n def keys ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n def search ( self , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n def hash_password ( self , password ) : \n \"\"\"<STR_LIT>\"\"\" \n def validate_password ( self , clear , hashed ) : \n \"\"\"<STR_LIT>\"\"\" \n ROLES = { \n u'<STR_LIT>' : Principal ( u'<STR_LIT>' , title = _ ( u'<STR_LIT>' ) ) , \n u'<STR_LIT>' : Principal ( u'<STR_LIT>' , title = _ ( u'<STR_LIT>' ) ) , \n u'<STR_LIT>' : Principal ( u'<STR_LIT>' , title = _ ( u'<STR_LIT>' ) ) , \n u'<STR_LIT>' : Principal ( u'<STR_LIT>' , title = _ ( u'<STR_LIT>' ) ) , \n } \n _DEFAULT_ROLES = ROLES . copy ( ) \n SHARING_ROLES = [ u'<STR_LIT>' , u'<STR_LIT>' , u'<STR_LIT>' ] \n USER_MANAGEMENT_ROLES = SHARING_ROLES + [ '<STR_LIT>' ] \n _DEFAULT_SHARING_ROLES = SHARING_ROLES [ : ] \n _DEFAULT_USER_MANAGEMENT_ROLES = USER_MANAGEMENT_ROLES [ : ] \n SITE_ACL = [ \n [ '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] ] , \n [ '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] ] , \n [ '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] , \n [ '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] , \n ] \n def set_roles ( roles_dict ) : \n ROLES . clear ( ) \n ROLES . update ( roles_dict ) \n def set_sharing_roles ( role_names ) : \n SHARING_ROLES [ : ] = role_names \n def set_user_management_roles ( role_names ) : \n USER_MANAGEMENT_ROLES [ : ] = role_names \n def reset_roles ( ) : \n ROLES . clear ( ) \n ROLES . update ( _DEFAULT_ROLES ) \n def reset_sharing_roles ( ) : \n SHARING_ROLES [ : ] = _DEFAULT_SHARING_ROLES \n def reset_user_management_roles ( ) : \n USER_MANAGEMENT_ROLES [ : ] = _DEFAULT_USER_MANAGEMENT_ROLES \n def reset ( ) : \n reset_roles ( ) \n reset_sharing_roles ( ) \n reset_user_management_roles ( ) \n class PersistentACLMixin ( object ) : \n def _get_acl ( self ) : \n if self . _acl is None : \n raise AttributeError ( '<STR_LIT>' ) \n return self . _acl \n def _set_acl ( self , value ) : \n self . _acl = value \n def _del_acl ( self ) : \n self . _acl = None \n __acl__ = property ( _get_acl , _set_acl , _del_acl ) \n def _cachekey_list_groups_raw ( name , context ) : \n context_id = context is not None and getattr ( context , '<STR_LIT:id>' , id ( context ) ) \n return name , context_id \n @ request_cache ( _cachekey_list_groups_raw ) \n def list_groups_raw ( name , context ) : \n \"\"\"<STR_LIT>\"\"\" \n from kotti . resources import Node \n if isinstance ( context , Node ) : \n return set ( \n r . group_name for r in context . local_groups \n if r . principal_name == name \n ) \n return set ( ) \n def list_groups ( name , context = None ) : \n \"\"\"<STR_LIT>\"\"\" \n return list_groups_ext ( name , context ) [ <NUM_LIT:0> ] \n def _cachekey_list_groups_ext ( name , context = None , _seen = None , _inherited = None ) : \n if _seen is not None or _inherited is not None : \n raise DontCache \n else : \n context_id = getattr ( context , '<STR_LIT:id>' , id ( context ) ) \n return unicode ( name ) , context_id \n @ request_cache ( _cachekey_list_groups_ext ) \n def list_groups_ext ( name , context = None , _seen = None , _inherited = None ) : \n name = unicode ( name ) \n groups = set ( ) \n recursing = _inherited is not None \n _inherited = _inherited or set ( ) \n principal = get_principals ( ) . get ( name ) \n if principal is not None : \n groups . update ( principal . groups ) \n if context is not None or ( context is None and _seen is not None ) : \n _inherited . update ( principal . groups ) \n if _seen is None : \n _seen = { name } \n if context is not None : \n items = lineage ( context ) \n for idx , item in enumerate ( items ) : \n group_names = [ i for i in list_groups_raw ( name , item ) \n if i not in _seen ] \n groups . update ( group_names ) \n if recursing or idx != <NUM_LIT:0> : \n _inherited . update ( group_names ) \n new_groups = groups - _seen \n _seen . update ( new_groups ) \n for group_name in new_groups : \n g , i = list_groups_ext ( \n group_name , context , _seen = _seen , _inherited = _inherited ) \n groups . update ( g ) \n _inherited . update ( i ) \n return list ( groups ) , list ( _inherited ) \n def set_groups ( name , context , groups_to_set = ( ) ) : \n \"\"\"<STR_LIT>\"\"\" \n from kotti . resources import LocalGroup \n name = unicode ( name ) \n context . local_groups = [ \n lg for lg in context . local_groups \n if lg . principal_name != name \n ] + [ \n LocalGroup ( context , name , unicode ( group_name ) ) \n for group_name in groups_to_set \n ] \n def list_groups_callback ( name , request ) : \n \"\"\"<STR_LIT>\"\"\" \n if not is_user ( name ) : \n return None \n if name in get_principals ( ) : \n context = request . environ . get ( \n '<STR_LIT>' , getattr ( request , '<STR_LIT>' , None ) ) \n if context is None : \n from kotti . resources import get_root \n context = get_root ( request ) \n return list_groups ( name , context ) \n @ contextmanager \n def authz_context ( context , request ) : \n before = request . environ . pop ( '<STR_LIT>' , None ) \n request . environ [ '<STR_LIT>' ] = context \n try : \n yield \n finally : \n del request . environ [ '<STR_LIT>' ] \n if before is not None : \n request . environ [ '<STR_LIT>' ] = before \n @ contextmanager \n def request_method ( request , method ) : \n before = request . method \n request . method = method \n try : \n yield \n finally : \n request . method = before \n def view_permitted ( context , request , name = '<STR_LIT>' , method = '<STR_LIT:GET>' ) : \n with authz_context ( context , request ) : \n with request_method ( request , method ) : \n return view_execution_permitted ( context , request , name ) \n def principals_with_local_roles ( context , inherit = True ) : \n \"\"\"<STR_LIT>\"\"\" \n principals = set ( ) \n items = [ context ] \n if inherit : \n items = lineage ( context ) \n for item in items : \n principals . update ( \n r . principal_name for r in item . local_groups \n if not r . principal_name . startswith ( '<STR_LIT>' ) \n ) \n return list ( principals ) \n def map_principals_with_local_roles ( context ) : \n principals = get_principals ( ) \n value = [ ] \n for principal_name in principals_with_local_roles ( context ) : \n try : \n principal = principals [ principal_name ] \n except KeyError : \n continue \n else : \n all , inherited = list_groups_ext ( principal_name , context ) \n value . append ( ( principal , ( all , inherited ) ) ) \n return sorted ( value , key = lambda t : t [ <NUM_LIT:0> ] . name ) \n def is_user ( principal ) : \n if not isinstance ( principal , string_types ) : \n principal = principal . name \n return '<STR_LIT::>' not in principal \n class Principals ( DictMixin ) : \n \"\"\"<STR_LIT>\"\"\" \n factory = Principal \n @ classmethod \n def _principal_by_name ( cls , name ) : \n query = bakery ( lambda session : session . query ( cls . factory ) . filter ( \n cls . factory . name == bindparam ( '<STR_LIT:name>' ) ) ) \n return query ( DBSession ( ) ) . params ( name = name ) . one ( ) \n @ request_cache ( lambda self , name : unicode ( name ) ) \n def __getitem__ ( self , name ) : \n name = unicode ( name ) \n if name . startswith ( '<STR_LIT>' ) : \n raise KeyError ( name ) \n try : \n return self . _principal_by_name ( name ) \n except NoResultFound : \n raise KeyError ( name ) \n def __setitem__ ( self , name , principal ) : \n name = unicode ( name ) \n if isinstance ( principal , dict ) : \n principal = self . factory ( ** principal ) \n DBSession . add ( principal ) \n def __delitem__ ( self , name ) : \n name = unicode ( name ) \n try : \n principal = self . _principal_by_name ( name ) \n DBSession . delete ( principal ) \n except NoResultFound : \n raise KeyError ( name ) \n def iterkeys ( self ) : \n for ( principal_name , ) in DBSession . query ( self . factory . name ) : \n yield principal_name \n def keys ( self ) : \n return list ( self . iterkeys ( ) ) \n def search ( self , match = '<STR_LIT>' , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n if not kwargs : \n return [ ] \n filters = [ ] \n for key , value in kwargs . items ( ) : \n col = getattr ( self . factory , key ) \n if isinstance ( value , string_types ) and '<STR_LIT:*>' in value : \n value = value . replace ( '<STR_LIT:*>' , '<STR_LIT:%>' ) . lower ( ) \n filters . append ( func . lower ( col ) . like ( value ) ) \n else : \n filters . append ( col == value ) \n query = DBSession . query ( self . factory ) \n if match == '<STR_LIT>' : \n query = query . filter ( or_ ( * filters ) ) \n elif match == '<STR_LIT:all>' : \n query = query . filter ( and_ ( * filters ) ) \n else : \n raise ValueError ( '<STR_LIT>' ) \n return query \n log_rounds = <NUM_LIT:10> \n def hash_password ( self , password , hashed = None ) : \n if hashed is None : \n hashed = bcrypt . gensalt ( self . log_rounds ) \n return unicode ( \n bcrypt . hashpw ( password . encode ( '<STR_LIT:utf-8>' ) , hashed . encode ( '<STR_LIT:utf-8>' ) ) ) \n def validate_password ( self , clear , hashed ) : \n try : \n return self . hash_password ( clear , hashed ) == hashed \n except ValueError : \n return False \n def principals_factory ( ) : \n return <mask0> ( ) \n", "gt": "Principals"}
{"input": "\n import json \n from mechanize . _mechanize import LinkNotFoundError \n from pytest import raises \n from kotti . testing import BASE_URL \n from kotti . testing import user \n from kotti . views . edit . upload import UploadView \n def test_upload_anonymous ( root , dummy_request , browser ) : \n view = UploadView ( root , dummy_request ) \n assert view . factories == [ ] \n link = browser . getLink \n browser . open ( u'<STR_LIT>' . format ( BASE_URL ) ) \n with raises ( LinkNotFoundError ) : \n link ( '<STR_LIT>' ) . click ( ) \n browser . open ( u'<STR_LIT>' . format ( BASE_URL ) ) \n assert browser . url . startswith ( u'<STR_LIT>' . format ( BASE_URL ) ) \n browser . open ( u'<STR_LIT>' . format ( BASE_URL ) ) \n assert browser . url . startswith ( u'<STR_LIT>' . format ( BASE_URL ) ) \n @ user ( '<STR_LIT>' ) \n def test_upload_authenticated_wo_mimetype ( root , dummy_request , browser ) : \n with raises ( KeyError ) : \n browser . open ( u'<STR_LIT>' . format ( BASE_URL ) ) \n @ user ( '<STR_LIT>' ) \n def test_upload_authenticated_text ( root , dummy_request , browser ) : \n browser . open ( u'<STR_LIT>' . format ( BASE_URL ) ) \n j = json . loads ( browser . contents ) \n assert '<STR_LIT>' in j \n types = j [ '<STR_LIT>' ] \n assert len ( types ) == <NUM_LIT:1> \n assert types [ <NUM_LIT:0> ] [ '<STR_LIT:name>' ] == <mask0> \n", "gt": "u'<STR_LIT>'"}
{"input": "\n import os \n import sys \n from setuptools import setup \n from setuptools import find_packages \n here = os . path . abspath ( os . path . dirname ( __file__ ) ) \n try : \n README = open ( os . path . join ( here , '<STR_LIT>' ) ) . read ( ) \n AUTHORS = open ( os . path . join ( here , '<STR_LIT>' ) ) . read ( ) \n CHANGES = open ( os . path . join ( here , '<STR_LIT>' ) ) . read ( ) \n except IOError : \n README = AUTHORS = CHANGES = '<STR_LIT>' \n install_requires = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n tests_require = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n development_requires = [ ] \n docs_require = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n if sys . version_info [ : <NUM_LIT:3> ] < ( <NUM_LIT:2> , <NUM_LIT:7> , <NUM_LIT:0> ) : \n install_requires . append ( '<STR_LIT>' ) \n setup ( name = '<STR_LIT>' , \n version = '<STR_LIT>' , \n description = \"<STR_LIT>\" , \n long_description = '<STR_LIT>' . join ( [ README , AUTHORS , CHANGES ] ) , \n classifiers = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n author = '<STR_LIT>' , \n author_email = '<STR_LIT>' , \n url = '<STR_LIT>' , \n keywords = '<STR_LIT>' , \n license = \"<STR_LIT>\" , \n packages = find_packages ( ) , \n include_package_data = True , \n zip_safe = False , \n install_requires = install_requires , \n tests_require = tests_require , \n dependency_links = [ ] , \n entry_points = \"\"\"<STR_LIT>\"\"\" , \n extras_require = { \n '<STR_LIT>' : tests_require , \n '<STR_LIT>' : development_requires , \n '<STR_LIT>' : <mask0> , \n } , \n ) \n", "gt": "docs_require"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import division , unicode_literals \n from . markers import list_marker_layout \n from . min_max import handle_min_max_width \n from . percentages import resolve_percentages , resolve_position_percentages \n from . preferred import shrink_to_fit \n from . tables import table_wrapper_width \n from . . formatting_structure import boxes \n @ handle_min_max_width \n def float_width ( box , context , containing_block ) : \n if box . width == '<STR_LIT>' : \n box . width = shrink_to_fit ( context , box , containing_block . width ) \n def float_layout ( context , box , containing_block , device_size , absolute_boxes , \n fixed_boxes ) : \n \"\"\"<STR_LIT>\"\"\" \n from . blocks import block_container_layout \n from . inlines import inline_replaced_box_width_height \n resolve_percentages ( box , ( containing_block . width , containing_block . height ) ) \n resolve_position_percentages ( \n box , ( containing_block . width , containing_block . height ) ) \n if box . margin_left == '<STR_LIT>' : \n box . margin_left = <NUM_LIT:0> \n if box . margin_right == '<STR_LIT>' : \n box . margin_right = <NUM_LIT:0> \n if box . margin_top == '<STR_LIT>' : \n box . margin_top = <NUM_LIT:0> \n if box . margin_bottom == '<STR_LIT>' : \n box . margin_bottom = <NUM_LIT:0> \n clearance = get_clearance ( context , box ) \n if clearance is not None : \n box . position_y += clearance \n if isinstance ( box , boxes . BlockReplacedBox ) : \n inline_replaced_box_width_height ( box , device_size = None ) \n elif box . width == '<STR_LIT>' : \n float_width ( box , context , containing_block ) \n if box . is_table_wrapper : \n table_wrapper_width ( \n context , box , ( containing_block . width , containing_block . height ) ) \n if isinstance ( box , boxes . BlockBox ) : \n context . create_block_formatting_context ( ) \n box , _ , _ , _ , _ = block_container_layout ( \n context , box , max_position_y = float ( '<STR_LIT>' ) , \n skip_stack = None , device_size = device_size , page_is_empty = False , \n absolute_boxes = absolute_boxes , fixed_boxes = fixed_boxes , \n adjoining_margins = None ) \n list_marker_layout ( context , box ) \n context . finish_block_formatting_context ( box ) \n else : \n assert isinstance ( box , boxes . BlockReplacedBox ) \n box = find_float_position ( context , box , containing_block ) \n context . excluded_shapes . append ( box ) \n return box \n def find_float_position ( context , box , containing_block ) : \n \"\"\"<STR_LIT>\"\"\" \n if context . excluded_shapes : \n highest_y = context . excluded_shapes [ - <NUM_LIT:1> ] . position_y \n if box . position_y < highest_y : \n box . translate ( <NUM_LIT:0> , highest_y - box . position_y ) \n position_x , position_y , available_width = avoid_collisions ( \n context , box , containing_block ) \n if box . style . float == '<STR_LIT:right>' : \n position_x += available_width - box . margin_width ( ) \n box . translate ( position_x - box . position_x , position_y - box . position_y ) \n return box \n def get_clearance ( context , box , collapsed_margin = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n clearance = None \n hypothetical_position = box . position_y + collapsed_margin \n for excluded_shape in context . excluded_shapes : \n if box . style . clear in ( excluded_shape . style . float , '<STR_LIT>' ) : \n y , h = excluded_shape . position_y , excluded_shape . margin_height ( ) \n if hypothetical_position < y + h : \n clearance = max ( \n ( clearance or <NUM_LIT:0> ) , y + h - hypothetical_position ) \n return clearance \n def avoid_collisions ( context , box , containing_block , outer = True ) : \n excluded_shapes = context . excluded_shapes \n position_y = box . position_y if outer else box . border_box_y ( ) \n box_width = box . margin_width ( ) if outer else box . border_width ( ) \n box_height = box . margin_height ( ) if outer else box . border_height ( ) \n if box . border_height ( ) == <NUM_LIT:0> and box . is_floated ( ) : \n return <NUM_LIT:0> , <NUM_LIT:0> , containing_block . width \n while True : \n colliding_shapes = [ \n shape for shape in excluded_shapes \n if ( shape . position_y < position_y < \n shape . position_y + shape . margin_height ( ) ) or \n ( shape . position_y < position_y + box_height < \n shape . position_y + shape . margin_height ( ) ) or \n ( shape . position_y >= position_y and \n shape . position_y + shape . margin_height ( ) <= \n position_y + box_height ) \n ] \n left_bounds = [ \n shape . position_x + shape . margin_width ( ) \n for shape in colliding_shapes \n if shape . style . float == '<STR_LIT:left>' ] \n right_bounds = [ \n shape . position_x \n for shape in colliding_shapes \n if shape . style . float == '<STR_LIT:right>' ] \n max_left_bound = containing_block . content_box_x ( ) \n max_right_bound = containing_block . content_box_x ( ) + containing_block . width \n if not outer : \n max_left_bound += box . margin_left \n max_right_bound -= box . margin_right \n if left_bounds or right_bounds : \n if left_bounds : \n max_left_bound = max ( max ( left_bounds ) , max_left_bound ) \n if right_bounds : \n max_right_bound = min ( min ( right_bounds ) , max_right_bound ) \n if box_width > max_right_bound - max_left_bound : \n new_positon_y = min ( \n shape . position_y + shape . margin_height ( ) \n for shape in colliding_shapes ) \n if new_positon_y > position_y : \n position_y = new_positon_y \n continue \n break \n position_x = max_left_bound \n available_width = max_right_bound - max_left_bound \n if not outer : \n position_x -= box . margin_left \n position_y -= box . margin_top \n return position_x , position_y , <mask0> \n", "gt": "available_width"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from setuptools import setup , find_packages \n VERSION = '<STR_LIT>' \n options = dict ( \n name = \"<STR_LIT>\" , \n version = VERSION , \n description = \"<STR_LIT>\" , \n long_description = __doc__ , \n author = \"<STR_LIT>\" , \n author_email = \"<STR_LIT>\" , \n license = \"<STR_LIT>\" , \n platforms = \"<STR_LIT>\" , \n install_requires = [ '<STR_LIT>' ] , \n provides = [ '<STR_LIT>' ] , \n packages = find_packages ( ) , \n use_2to3 = True , \n classifiers = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ] ) \n setup ( ** <mask0> ) \n", "gt": "options"}
{"input": "\n from django . conf . urls import patterns , include , url \n import health_check \n health_check . autodiscover ( ) \n urlpatterns = patterns ( '<STR_LIT>' , \n url ( r'<STR_LIT>' , '<STR_LIT>' , <mask0> = '<STR_LIT>' ) , \n ) \n", "gt": "name"}
{"input": "\n import django \n from django . db import connection \n from django . db . models import Count \n from django . db . models . query_utils import Q \n from django . utils import translation \n from hvad . test_utils . data import NORMAL , STANDARD \n from hvad . test_utils . testcase import HvadTestCase , minimumDjangoVersion \n from hvad . test_utils . project . app . models import Normal , AggregateModel , Standard , SimpleRelated \n from hvad . test_utils . fixtures import NormalFixture , StandardFixture \n class FilterTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n def test_simple_filter ( self ) : \n qs = Normal . objects . language ( '<STR_LIT>' ) . filter ( shared_field__contains = '<STR_LIT:2>' ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n obj = qs [ <NUM_LIT:0> ] \n self . assertEqual ( obj . shared_field , NORMAL [ <NUM_LIT:2> ] . shared_field ) \n self . assertEqual ( obj . translated_field , NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) \n qs = Normal . objects . language ( '<STR_LIT>' ) . filter ( shared_field__contains = '<STR_LIT:1>' ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n obj = qs [ <NUM_LIT:0> ] \n self . assertEqual ( obj . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( obj . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n def test_translated_filter ( self ) : \n qs = Normal . objects . language ( '<STR_LIT>' ) . filter ( translated_field__contains = '<STR_LIT>' ) \n self . assertEqual ( qs . count ( ) , self . normal_count ) \n obj1 , obj2 = qs \n self . assertEqual ( obj1 . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( obj1 . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( obj2 . shared_field , NORMAL [ <NUM_LIT:2> ] . shared_field ) \n self . assertEqual ( obj2 . translated_field , NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) \n def test_fallbacks_filter ( self ) : \n ( Normal . objects . language ( '<STR_LIT>' ) \n . filter ( shared_field = NORMAL [ <NUM_LIT:1> ] . shared_field ) \n . delete_translations ( ) ) \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) . fallbacks ( ) \n with self . assertNumQueries ( <NUM_LIT:2> ) : \n self . assertEqual ( qs . count ( ) , self . normal_count ) \n self . assertEqual ( len ( qs ) , self . normal_count ) \n with self . assertNumQueries ( <NUM_LIT:0> ) : \n self . assertCountEqual ( ( obj . pk for obj in qs ) , tuple ( self . normal_id . values ( ) ) ) \n self . assertCountEqual ( ( obj . language_code for obj in qs ) , self . translations ) \n def test_all_languages_filter ( self ) : \n with self . assertNumQueries ( <NUM_LIT:2> ) : \n qs = Normal . objects . language ( '<STR_LIT:all>' ) . filter ( shared_field__contains = '<STR_LIT>' ) \n self . assertEqual ( qs . count ( ) , self . normal_count * len ( self . translations ) ) \n self . assertCountEqual ( ( obj . shared_field for obj in qs ) , \n ( NORMAL [ <NUM_LIT:1> ] . shared_field , \n NORMAL [ <NUM_LIT:2> ] . shared_field ) * <NUM_LIT:2> ) \n self . assertCountEqual ( ( obj . translated_field for obj in qs ) , \n ( NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] , \n NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] , \n NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] , \n NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) ) \n with self . assertNumQueries ( <NUM_LIT:2> ) : \n qs = Normal . objects . language ( '<STR_LIT:all>' ) . filter ( translated_field__contains = '<STR_LIT>' ) \n self . assertEqual ( qs . count ( ) , self . normal_count ) \n self . assertCountEqual ( ( obj . shared_field for obj in qs ) , \n ( NORMAL [ <NUM_LIT:1> ] . shared_field , \n NORMAL [ <NUM_LIT:2> ] . shared_field ) ) \n self . assertCountEqual ( ( obj . translated_field for obj in qs ) , \n ( NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] , \n NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) ) \n with self . assertNumQueries ( <NUM_LIT:2> ) : \n qs = Normal . objects . language ( '<STR_LIT:all>' ) . filter ( translated_field__contains = '<STR_LIT:1>' ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n obj = qs [ <NUM_LIT:0> ] \n self . assertEqual ( obj . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( obj . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n def test_deferred_language_filter ( self ) : \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) . filter ( translated_field__contains = '<STR_LIT>' ) \n with translation . override ( '<STR_LIT>' ) : \n self . assertEqual ( qs . count ( ) , self . normal_count ) \n obj1 , obj2 = qs \n self . assertEqual ( obj1 . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( obj1 . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( obj2 . shared_field , NORMAL [ <NUM_LIT:2> ] . shared_field ) \n self . assertEqual ( obj2 . translated_field , NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) \n class ExtraTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n def test_simple_extra ( self ) : \n qs = Normal . objects . language ( '<STR_LIT>' ) . extra ( select = { '<STR_LIT>' : '<STR_LIT>' } ) \n self . assertEqual ( qs . count ( ) , self . normal_count ) \n self . assertEqual ( int ( qs [ <NUM_LIT:0> ] . test_extra ) , <NUM_LIT:4> ) \n class QueryCachingTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n def _try_all_cache_using_methods ( self , qs , length ) : \n with self . assertNumQueries ( <NUM_LIT:0> ) : \n x = <NUM_LIT:0> \n for obj in qs : x += <NUM_LIT:1> \n self . assertEqual ( x , length ) \n with self . assertNumQueries ( <NUM_LIT:0> ) : \n qs [ <NUM_LIT:0> ] \n with self . assertNumQueries ( <NUM_LIT:0> ) : \n self . assertEqual ( qs . exists ( ) , length != <NUM_LIT:0> ) \n with self . assertNumQueries ( <NUM_LIT:0> ) : \n self . assertEqual ( qs . count ( ) , length ) \n with self . assertNumQueries ( <NUM_LIT:0> ) : \n self . assertEqual ( len ( qs ) , length ) \n with self . assertNumQueries ( <NUM_LIT:0> ) : \n self . assertEqual ( bool ( qs ) , length != <NUM_LIT:0> ) \n def test_iter_caches ( self ) : \n with translation . override ( '<STR_LIT>' ) : \n index = <NUM_LIT:0> \n qs = Normal . objects . language ( ) . filter ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n for obj in qs : \n index += <NUM_LIT:1> \n self . assertEqual ( index , <NUM_LIT:1> ) \n self . _try_all_cache_using_methods ( qs , <NUM_LIT:1> ) \n def test_pickling_caches ( self ) : \n import pickle \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) . filter ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n pickle . dumps ( qs ) \n self . _try_all_cache_using_methods ( qs , <NUM_LIT:1> ) \n def test_len_caches ( self ) : \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) . filter ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n self . assertEqual ( len ( qs ) , <NUM_LIT:1> ) \n self . _try_all_cache_using_methods ( qs , <NUM_LIT:1> ) \n def test_bool_caches ( self ) : \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) . filter ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n self . assertTrue ( qs ) \n self . _try_all_cache_using_methods ( qs , <NUM_LIT:1> ) \n class IterTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n def test_simple_iter ( self ) : \n with translation . override ( '<STR_LIT>' ) : \n with self . assertNumQueries ( <NUM_LIT:1> ) : \n for index , obj in enumerate ( Normal . objects . language ( ) , <NUM_LIT:1> ) : \n self . assertEqual ( obj . shared_field , NORMAL [ index ] . shared_field ) \n self . assertEqual ( obj . translated_field , NORMAL [ index ] . translated_field [ '<STR_LIT>' ] ) \n with translation . override ( '<STR_LIT>' ) : \n with self . assertNumQueries ( <NUM_LIT:1> ) : \n for index , obj in enumerate ( Normal . objects . language ( ) , <NUM_LIT:1> ) : \n self . assertEqual ( obj . shared_field , NORMAL [ index ] . shared_field ) \n self . assertEqual ( obj . translated_field , NORMAL [ index ] . translated_field [ '<STR_LIT>' ] ) \n def test_iter_unique_reply ( self ) : \n with translation . override ( '<STR_LIT>' ) : \n self . assertEqual ( len ( Normal . objects . all ( ) ) , len ( Normal . objects . untranslated ( ) ) ) \n def test_iter_deferred_language ( self ) : \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) \n with translation . override ( '<STR_LIT>' ) : \n for index , obj in enumerate ( qs , <NUM_LIT:1> ) : \n self . assertEqual ( obj . shared_field , NORMAL [ index ] . shared_field ) \n self . assertEqual ( obj . translated_field , NORMAL [ index ] . translated_field [ '<STR_LIT>' ] ) \n class UpdateTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n def test_update_shared ( self ) : \n NEW_SHARED = '<STR_LIT>' \n n1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n n2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n ja1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n ja2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n with self . assertNumQueries ( <NUM_LIT:1> if connection . features . update_can_self_select else <NUM_LIT:2> ) : \n Normal . objects . language ( '<STR_LIT>' ) . update ( shared_field = NEW_SHARED ) \n new1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n new2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n self . assertEqual ( new1 . shared_field , NEW_SHARED ) \n self . assertEqual ( new1 . translated_field , n1 . translated_field ) \n self . assertEqual ( new2 . shared_field , NEW_SHARED ) \n self . assertEqual ( new2 . translated_field , n2 . translated_field ) \n newja1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n newja2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n self . assertEqual ( newja1 . shared_field , NEW_SHARED ) \n self . assertEqual ( newja2 . shared_field , NEW_SHARED ) \n self . assertEqual ( newja1 . translated_field , ja1 . translated_field ) \n self . assertEqual ( newja2 . translated_field , ja2 . translated_field ) \n def test_update_translated ( self ) : \n NEW_TRANSLATED = '<STR_LIT>' \n n1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n n2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n ja1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n ja2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n with self . assertNumQueries ( <NUM_LIT:1> ) : \n Normal . objects . language ( '<STR_LIT>' ) . update ( translated_field = NEW_TRANSLATED ) \n new1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n new2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n self . assertEqual ( new1 . shared_field , n1 . shared_field ) \n self . assertEqual ( new2 . shared_field , n2 . shared_field ) \n self . assertEqual ( new1 . translated_field , NEW_TRANSLATED ) \n self . assertEqual ( new2 . translated_field , NEW_TRANSLATED ) \n newja1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n newja2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n self . assertEqual ( newja1 . shared_field , ja1 . shared_field ) \n self . assertEqual ( newja2 . shared_field , ja2 . shared_field ) \n self . assertEqual ( newja1 . translated_field , ja1 . translated_field ) \n self . assertEqual ( newja2 . translated_field , ja2 . translated_field ) \n def test_update_mixed ( self ) : \n NEW_SHARED = '<STR_LIT>' \n NEW_TRANSLATED = '<STR_LIT>' \n ja1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n ja2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n with self . assertNumQueries ( <NUM_LIT:2> if connection . features . update_can_self_select else <NUM_LIT:3> ) : \n Normal . objects . language ( '<STR_LIT>' ) . update ( \n shared_field = NEW_SHARED , translated_field = NEW_TRANSLATED \n ) \n new1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n new2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n self . assertEqual ( new1 . shared_field , NEW_SHARED ) \n self . assertEqual ( new1 . translated_field , NEW_TRANSLATED ) \n self . assertEqual ( new2 . shared_field , NEW_SHARED ) \n self . assertEqual ( new2 . translated_field , NEW_TRANSLATED ) \n newja1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n newja2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n self . assertEqual ( newja1 . shared_field , NEW_SHARED ) \n self . assertEqual ( newja2 . shared_field , NEW_SHARED ) \n self . assertEqual ( newja1 . translated_field , ja1 . translated_field ) \n self . assertEqual ( newja2 . translated_field , ja2 . translated_field ) \n def test_update_deferred_language ( self ) : \n NEW_TRANSLATED = '<STR_LIT>' \n n1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n n2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n ja1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n ja2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) \n with translation . override ( '<STR_LIT>' ) : \n with self . assertNumQueries ( <NUM_LIT:1> ) : \n qs . update ( translated_field = NEW_TRANSLATED ) \n new1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n new2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n self . assertEqual ( new1 . shared_field , n1 . shared_field ) \n self . assertEqual ( new2 . shared_field , n2 . shared_field ) \n self . assertEqual ( new1 . translated_field , NEW_TRANSLATED ) \n self . assertEqual ( new2 . translated_field , NEW_TRANSLATED ) \n newja1 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:1> ] ) \n newja2 = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = self . normal_id [ <NUM_LIT:2> ] ) \n self . assertEqual ( newja1 . shared_field , ja1 . shared_field ) \n self . assertEqual ( newja2 . shared_field , ja2 . shared_field ) \n self . assertEqual ( newja1 . translated_field , ja1 . translated_field ) \n self . assertEqual ( newja2 . translated_field , ja2 . translated_field ) \n def test_update_fallbacks ( self ) : \n qs = Normal . objects . language ( ) . fallbacks ( ) \n with self . assertNumQueries ( <NUM_LIT:1> if connection . features . update_can_self_select else <NUM_LIT:2> ) : \n qs . filter ( shared_field = NORMAL [ <NUM_LIT:1> ] . shared_field ) . update ( shared_field = '<STR_LIT>' ) \n self . assertEqual ( Normal . objects . language ( '<STR_LIT>' ) . get ( shared_field = '<STR_LIT>' ) . pk , self . normal_id [ <NUM_LIT:1> ] ) \n self . assertEqual ( Normal . objects . language ( '<STR_LIT>' ) . get ( shared_field = '<STR_LIT>' ) . pk , self . normal_id [ <NUM_LIT:1> ] ) \n class ValuesListTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n def test_values_list_translated ( self ) : \n values = Normal . objects . language ( '<STR_LIT>' ) . values_list ( '<STR_LIT>' , flat = True ) \n values_list = list ( values ) \n self . assertCountEqual ( values_list , [ NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] , \n NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ] ) \n def test_values_list_shared ( self ) : \n values = Normal . objects . language ( '<STR_LIT>' ) . values_list ( '<STR_LIT>' , flat = True ) \n values_list = list ( values ) \n self . assertCountEqual ( values_list , [ NORMAL [ <NUM_LIT:1> ] . shared_field , \n NORMAL [ <NUM_LIT:2> ] . shared_field ] ) \n def test_values_list_mixed ( self ) : \n values = Normal . objects . language ( '<STR_LIT>' ) . values_list ( '<STR_LIT>' , '<STR_LIT>' ) \n values_list = list ( values ) \n check = [ \n ( NORMAL [ <NUM_LIT:1> ] . shared_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) , \n ( NORMAL [ <NUM_LIT:2> ] . shared_field , NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) , \n ] \n self . assertCountEqual ( values_list , check ) \n def test_values_list_deferred_language ( self ) : \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) \n with translation . override ( '<STR_LIT>' ) : \n values = qs . values_list ( '<STR_LIT>' , '<STR_LIT>' ) \n values_list = list ( values ) \n check = [ \n ( NORMAL [ <NUM_LIT:1> ] . shared_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) , \n ( NORMAL [ <NUM_LIT:2> ] . shared_field , NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) , \n ] \n self . assertCountEqual ( values_list , check ) \n def test_values_list_language_all ( self ) : \n values = ( Normal . objects . language ( '<STR_LIT:all>' ) . filter ( shared_field = NORMAL [ <NUM_LIT:1> ] . shared_field ) \n . values_list ( '<STR_LIT>' , '<STR_LIT>' ) ) \n values_list = list ( values ) \n check = [ \n ( NORMAL [ <NUM_LIT:1> ] . shared_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) , \n ( NORMAL [ <NUM_LIT:1> ] . shared_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) , \n ] \n self . assertCountEqual ( values_list , check ) \n class ValuesTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n def test_values_shared ( self ) : \n values = Normal . objects . language ( '<STR_LIT>' ) . values ( '<STR_LIT>' ) \n values_list = list ( values ) \n check = [ \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . shared_field } , \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:2> ] . shared_field } , \n ] \n self . assertCountEqual ( values_list , check ) \n def test_values_translated ( self ) : \n values = Normal . objects . language ( '<STR_LIT>' ) . values ( '<STR_LIT>' ) \n values_list = list ( values ) \n check = [ \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] } , \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] } , \n ] \n self . assertCountEqual ( values_list , check ) \n def test_values_mixed ( self ) : \n values = Normal . objects . language ( '<STR_LIT>' ) . values ( '<STR_LIT>' , '<STR_LIT>' ) \n values_list = list ( values ) \n check = [ \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] , \n '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . shared_field } , \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] , \n '<STR_LIT>' : NORMAL [ <NUM_LIT:2> ] . shared_field } , \n ] \n self . assertCountEqual ( values_list , check ) \n def test_values_post_language ( self ) : \n values = Normal . objects . language ( ) . values ( '<STR_LIT>' ) . language ( '<STR_LIT>' ) \n values_list = list ( values ) \n check = [ \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . shared_field } , \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:2> ] . shared_field } , \n ] \n self . assertCountEqual ( values_list , check ) \n def test_values_post_filter ( self ) : \n qs = Normal . objects . language ( '<STR_LIT>' ) . values ( '<STR_LIT>' ) \n values = qs . filter ( shared_field = NORMAL [ <NUM_LIT:1> ] . shared_field ) \n values_list = list ( values ) \n check = [ \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . shared_field } , \n ] \n self . assertCountEqual ( values_list , check ) \n def test_values_deferred_language ( self ) : \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) \n with translation . override ( '<STR_LIT>' ) : \n values = qs . values ( '<STR_LIT>' ) \n values_list = list ( values ) \n check = [ \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] } , \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] } , \n ] \n self . assertCountEqual ( values_list , check ) \n def test_values_language_all ( self ) : \n values = ( Normal . objects . language ( '<STR_LIT:all>' ) . filter ( shared_field = NORMAL [ <NUM_LIT:1> ] . shared_field ) \n . values ( '<STR_LIT>' , '<STR_LIT>' ) ) \n values_list = list ( values ) \n check = [ \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . shared_field , \n '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] } , \n { '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . shared_field , \n '<STR_LIT>' : NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] } , \n ] \n self . assertCountEqual ( values_list , check ) \n class InBulkTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n def test_empty_in_bulk ( self ) : \n with self . assertNumQueries ( <NUM_LIT:0> ) : \n result = Normal . objects . language ( '<STR_LIT>' ) . in_bulk ( [ ] ) \n self . assertEqual ( len ( result ) , <NUM_LIT:0> ) \n def test_in_bulk ( self ) : \n pk1 , pk2 = self . normal_id [ <NUM_LIT:1> ] , self . normal_id [ <NUM_LIT:2> ] \n with self . assertNumQueries ( <NUM_LIT:1> ) : \n result = Normal . objects . language ( '<STR_LIT>' ) . in_bulk ( [ pk1 , pk2 ] ) \n self . assertCountEqual ( ( pk1 , pk2 ) , result ) \n self . assertEqual ( result [ pk1 ] . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( result [ pk1 ] . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( result [ pk1 ] . language_code , '<STR_LIT>' ) \n self . assertEqual ( result [ pk2 ] . shared_field , NORMAL [ <NUM_LIT:2> ] . shared_field ) \n self . assertEqual ( result [ pk2 ] . translated_field , NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( result [ pk2 ] . language_code , '<STR_LIT>' ) \n def test_untranslated_in_bulk ( self ) : \n pk1 = self . normal_id [ <NUM_LIT:1> ] \n with translation . override ( '<STR_LIT>' ) : \n with self . assertNumQueries ( <NUM_LIT:2> ) : \n result = Normal . objects . untranslated ( ) . in_bulk ( [ pk1 ] ) \n self . assertCountEqual ( ( pk1 , ) , result ) \n self . assertEqual ( result [ pk1 ] . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( result [ pk1 ] . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( result [ pk1 ] . language_code , '<STR_LIT>' ) \n def test_fallbacks_in_bulk ( self ) : \n ( Normal . objects . language ( '<STR_LIT>' ) \n . filter ( shared_field = NORMAL [ <NUM_LIT:2> ] . shared_field ) \n . delete_translations ( ) ) \n with self . assertNumQueries ( <NUM_LIT:1> ) : \n pk1 , pk2 = self . normal_id [ <NUM_LIT:1> ] , self . normal_id [ <NUM_LIT:2> ] \n result = Normal . objects . language ( '<STR_LIT>' ) . fallbacks ( '<STR_LIT>' , '<STR_LIT>' ) . in_bulk ( [ pk1 , pk2 ] ) \n self . assertCountEqual ( ( pk1 , pk2 ) , result ) \n self . assertEqual ( result [ pk1 ] . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( result [ pk1 ] . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( result [ pk1 ] . language_code , '<STR_LIT>' ) \n self . assertEqual ( result [ pk2 ] . shared_field , NORMAL [ <NUM_LIT:2> ] . shared_field ) \n self . assertEqual ( result [ pk2 ] . translated_field , NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( result [ pk2 ] . language_code , '<STR_LIT>' ) \n def test_all_languages_in_bulk ( self ) : \n with self . assertRaises ( ValueError ) : \n Normal . objects . language ( '<STR_LIT:all>' ) . in_bulk ( [ self . normal_id [ <NUM_LIT:1> ] ] ) \n def test_in_bulk_deferred_language ( self ) : \n pk1 = self . normal_id [ <NUM_LIT:1> ] \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) \n with translation . override ( '<STR_LIT>' ) : \n result = qs . in_bulk ( [ pk1 ] ) \n self . assertCountEqual ( ( pk1 , ) , result ) \n self . assertEqual ( result [ pk1 ] . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( result [ pk1 ] . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( result [ pk1 ] . language_code , '<STR_LIT>' ) \n class DeleteTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n def test_delete_all ( self ) : \n Normal . objects . all ( ) . delete ( ) \n self . assertEqual ( Normal . objects . count ( ) , <NUM_LIT:0> ) \n self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , <NUM_LIT:0> ) \n def test_delete_translation ( self ) : \n self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , <NUM_LIT:4> ) \n Normal . objects . language ( '<STR_LIT>' ) . delete_translations ( ) \n self . assertEqual ( Normal . objects . untranslated ( ) . count ( ) , <NUM_LIT:2> ) \n self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , <NUM_LIT:2> ) \n Normal . objects . language ( '<STR_LIT>' ) . delete_translations ( ) \n self . assertEqual ( Normal . objects . untranslated ( ) . count ( ) , <NUM_LIT:2> ) \n self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , <NUM_LIT:0> ) \n def test_filtered_delete_translation ( self ) : \n self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , <NUM_LIT:4> ) \n ( Normal . objects . language ( '<STR_LIT>' ) \n . filter ( shared_field = NORMAL [ <NUM_LIT:1> ] . shared_field ) \n . delete_translations ( ) ) \n self . assertEqual ( Normal . objects . untranslated ( ) . count ( ) , <NUM_LIT:2> ) \n self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , <NUM_LIT:3> ) \n ( Normal . objects . language ( '<STR_LIT>' ) \n . filter ( translated_field = NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) \n . delete_translations ( ) ) \n self . assertEqual ( Normal . objects . untranslated ( ) . count ( ) , <NUM_LIT:2> ) \n self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , <NUM_LIT:2> ) \n def test_delete_translation_deferred_language ( self ) : \n self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , <NUM_LIT:4> ) \n with translation . override ( '<STR_LIT>' ) : \n qs = Normal . objects . language ( ) \n with translation . override ( '<STR_LIT>' ) : \n qs . delete_translations ( ) \n self . assertEqual ( Normal . objects . language ( '<STR_LIT>' ) . count ( ) , <NUM_LIT:2> ) \n self . assertEqual ( Normal . objects . language ( '<STR_LIT>' ) . count ( ) , <NUM_LIT:0> ) \n def test_delete_fallbacks ( self ) : \n qs = Normal . objects . language ( ) . fallbacks ( ) \n qs . filter ( shared_field = NORMAL [ <NUM_LIT:1> ] . shared_field ) . delete ( ) \n self . assertEqual ( Normal . objects . language ( '<STR_LIT>' ) . count ( ) , self . normal_count - <NUM_LIT:1> ) \n self . assertEqual ( Normal . objects . language ( '<STR_LIT>' ) . count ( ) , self . normal_count - <NUM_LIT:1> ) \n class GetTranslationFromInstanceTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:1> \n def test_simple ( self ) : \n en = Normal . objects . language ( '<STR_LIT>' ) . get ( ) \n ja_trans = en . translations . get_language ( '<STR_LIT>' ) \n ja = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = en . pk ) \n self . assertEqual ( en . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( en . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertRaises ( AttributeError , getattr , ja_trans , '<STR_LIT>' ) \n self . assertEqual ( ja_trans . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( ja . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( ja . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n def test_cached ( self ) : \n en = Normal . objects . untranslated ( ) . prefetch_related ( '<STR_LIT>' ) . get ( ) \n with self . assertNumQueries ( <NUM_LIT:0> ) : \n ja_trans = en . translations . get_language ( '<STR_LIT>' ) \n ja = Normal . objects . language ( '<STR_LIT>' ) . get ( pk = en . pk ) \n self . assertEqual ( en . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( en . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertRaises ( AttributeError , getattr , ja_trans , '<STR_LIT>' ) \n self . assertEqual ( ja_trans . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( ja . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( ja . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n def test_not_exist ( self ) : \n en = Normal . objects . untranslated ( ) . get ( ) \n with self . assertRaises ( Normal . DoesNotExist ) : \n en . translations . get_language ( '<STR_LIT>' ) \n en = Normal . objects . untranslated ( ) . prefetch_related ( '<STR_LIT>' ) . get ( ) \n with self . assertRaises ( Normal . DoesNotExist ) : \n en . translations . get_language ( '<STR_LIT>' ) \n class AggregateTests ( HvadTestCase ) : \n def test_aggregate ( self ) : \n from django . db . models import Avg \n AggregateModel . objects . language ( \"<STR_LIT>\" ) . create ( number = <NUM_LIT:10> , translated_number = <NUM_LIT:20> ) \n AggregateModel . objects . language ( \"<STR_LIT>\" ) . create ( number = <NUM_LIT:0> , translated_number = <NUM_LIT:0> ) \n self . assertEqual ( AggregateModel . objects . language ( \"<STR_LIT>\" ) . aggregate ( Avg ( \"<STR_LIT>\" ) ) , { '<STR_LIT>' : <NUM_LIT:5> } ) \n self . assertEqual ( AggregateModel . objects . language ( \"<STR_LIT>\" ) . aggregate ( Avg ( \"<STR_LIT>\" ) ) , { '<STR_LIT>' : <NUM_LIT:10> } ) \n self . assertEqual ( AggregateModel . objects . language ( \"<STR_LIT>\" ) . aggregate ( num = Avg ( \"<STR_LIT>\" ) ) , { '<STR_LIT>' : <NUM_LIT:5> } ) \n self . assertEqual ( AggregateModel . objects . language ( \"<STR_LIT>\" ) . aggregate ( tnum = Avg ( \"<STR_LIT>\" ) ) , { '<STR_LIT>' : <NUM_LIT:10> } ) \n class AnnotateTests ( HvadTestCase , StandardFixture , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n standard_count = <NUM_LIT:4> \n def test_annotate ( self ) : \n qs = Normal . objects . language ( '<STR_LIT>' ) . annotate ( Count ( '<STR_LIT>' ) ) \n self . assertEqual ( len ( qs ) , self . normal_count ) \n self . assertEqual ( qs [ <NUM_LIT:0> ] . standards__count , <NUM_LIT:2> ) \n self . assertEqual ( qs [ <NUM_LIT:1> ] . standards__count , <NUM_LIT:2> ) \n qs = Normal . objects . language ( '<STR_LIT>' ) . annotate ( foo = Count ( '<STR_LIT>' ) ) \n self . assertEqual ( len ( qs ) , self . normal_count ) \n self . assertEqual ( qs [ <NUM_LIT:0> ] . foo , <NUM_LIT:2> ) \n self . assertEqual ( qs [ <NUM_LIT:1> ] . foo , <NUM_LIT:2> ) \n with self . assertRaises ( ValueError ) : \n qs = Normal . objects . language ( '<STR_LIT>' ) . annotate ( Count ( '<STR_LIT>' ) , standards__count = Count ( '<STR_LIT>' ) ) \n class NotImplementedTests ( HvadTestCase ) : \n def test_notimplemented ( self ) : \n baseqs = SimpleRelated . objects . language ( '<STR_LIT>' ) \n self . assertRaises ( NotImplementedError , baseqs . defer , '<STR_LIT>' ) \n self . assertRaises ( NotImplementedError , baseqs . only ) \n self . assertRaises ( NotImplementedError , baseqs . bulk_create , [ ] ) \n self . assertRaises ( NotImplementedError , baseqs . select_related ) \n if django . VERSION >= ( <NUM_LIT:1> , <NUM_LIT:7> ) : \n self . assertRaises ( NotImplementedError , baseqs . update_or_create ) \n class MinimumVersionTests ( HvadTestCase ) : \n def test_versions ( self ) : \n qs = SimpleRelated . objects . language ( '<STR_LIT>' ) \n if django . VERSION < ( <NUM_LIT:1> , <NUM_LIT:7> ) : \n self . assertRaises ( AttributeError , getattr , qs , '<STR_LIT>' ) \n class ExcludeTests ( HvadTestCase , NormalFixture ) : \n normal_count = <NUM_LIT:1> \n def test_defer ( self ) : \n qs = Normal . objects . language ( '<STR_LIT>' ) . exclude ( translated_field = NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:0> ) \n def test_fallbacks_exclude ( self ) : \n ( Normal . objects . language ( '<STR_LIT>' ) \n . filter ( shared_field = NORMAL [ <NUM_LIT:1> ] . shared_field ) \n . delete_translations ( ) ) \n qs = ( Normal . objects . language ( '<STR_LIT>' ) \n . fallbacks ( '<STR_LIT>' , '<STR_LIT>' ) \n . exclude ( shared_field = NORMAL [ <NUM_LIT:1> ] . shared_field ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:0> ) \n def test_all_languages_exclude ( self ) : \n qs = Normal . objects . language ( '<STR_LIT:all>' ) . exclude ( translated_field = NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n self . assertEqual ( qs [ <NUM_LIT:0> ] . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n def test_invalid_all_languages_exclude ( self ) : \n with self . assertRaises ( ValueError ) : \n Normal . objects . language ( ) . exclude ( language_code = '<STR_LIT:all>' ) \n class ComplexFilterTests ( HvadTestCase , StandardFixture , NormalFixture ) : \n normal_count = <NUM_LIT:2> \n standard_count = <NUM_LIT:2> \n def test_qobject_filter ( self ) : \n shared_contains_one = Q ( shared_field__contains = '<STR_LIT:1>' ) \n shared_contains_two = Q ( shared_field__contains = '<STR_LIT:2>' ) \n qs = Normal . objects . language ( '<STR_LIT>' ) . filter ( shared_contains_two ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n obj = qs [ <NUM_LIT:0> ] \n self . assertEqual ( obj . shared_field , NORMAL [ <NUM_LIT:2> ] . shared_field ) \n self . assertEqual ( obj . translated_field , NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) \n qs = ( Normal . objects . language ( '<STR_LIT>' ) . filter ( Q ( shared_contains_one | shared_contains_two ) ) \n . order_by ( '<STR_LIT>' ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:2> ) \n obj = qs [ <NUM_LIT:0> ] \n self . assertEqual ( obj . shared_field , NORMAL [ <NUM_LIT:1> ] . shared_field ) \n self . assertEqual ( obj . translated_field , NORMAL [ <NUM_LIT:1> ] . translated_field [ '<STR_LIT>' ] ) \n obj = qs [ <NUM_LIT:1> ] \n self . assertEqual ( obj . shared_field , NORMAL [ <NUM_LIT:2> ] . shared_field ) \n self . assertEqual ( obj . translated_field , NORMAL [ <NUM_LIT:2> ] . translated_field [ '<STR_LIT>' ] ) \n def test_aware_qobject_filter ( self ) : \n from hvad . utils import get_translation_aware_manager \n manager = get_translation_aware_manager ( Standard ) \n normal_one = Q ( normal_field = STANDARD [ <NUM_LIT:1> ] . normal_field ) \n normal_two = Q ( normal_field = STANDARD [ <NUM_LIT:2> ] . normal_field ) \n shared_one = Q ( normal__shared_field = NORMAL [ STANDARD [ <NUM_LIT:1> ] . normal ] . shared_field ) \n translated_one_en = Q ( normal__translated_field = NORMAL [ STANDARD [ <NUM_LIT:1> ] . normal ] . translated_field [ '<STR_LIT>' ] ) \n translated_two_en = Q ( normal__translated_field = NORMAL [ STANDARD [ <NUM_LIT:2> ] . normal ] . translated_field [ '<STR_LIT>' ] ) \n with translation . override ( '<STR_LIT>' ) : \n qs = manager . filter ( shared_one ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n obj = qs [ <NUM_LIT:0> ] \n self . assertEqual ( obj . normal_field , STANDARD [ <NUM_LIT:1> ] . normal_field ) \n qs = manager . filter ( translated_one_en ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n obj = qs [ <NUM_LIT:0> ] \n self . assertEqual ( obj . normal_field , STANDARD [ <NUM_LIT:1> ] . normal_field ) \n qs = manager . filter ( Q ( normal_one & shared_one & translated_one_en ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n obj = qs [ <NUM_LIT:0> ] \n self . assertEqual ( obj . normal_field , STANDARD [ <NUM_LIT:1> ] . normal_field ) \n qs = manager . filter ( Q ( normal_one & translated_two_en ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:0> ) \n qs = manager . filter ( Q ( shared_one & translated_two_en ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:0> ) \n qs = manager . filter ( Q ( translated_one_en & translated_two_en ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:0> ) \n qs = manager . filter ( Q ( normal_one | translated_one_en ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n qs = manager . filter ( Q ( shared_one | translated_one_en ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n qs = manager . filter ( Q ( normal_one | translated_two_en ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:2> ) \n qs = manager . filter ( Q ( shared_one | translated_two_en ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:2> ) \n qs = manager . filter ( Q ( translated_one_en | translated_two_en ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:2> ) \n qs = manager . filter ( Q ( normal_one & ( translated_one_en | translated_two_en ) ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n qs = manager . filter ( Q ( normal_two & ( translated_one_en | translated_two_en ) ) ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n qs = manager . filter ( shared_one & ~ translated_one_en ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:0> ) \n qs = manager . filter ( shared_one & ~ translated_two_en ) \n self . assertEqual ( qs . count ( ) , <NUM_LIT:1> ) \n def test_defer ( self ) : \n qs = Normal . objects . language ( '<STR_LIT>' ) . complex_filter ( { } ) \n self . assertEqual ( qs . count ( ) , self . normal_count ) \n self . assertRaises ( NotImplementedError , \n Normal . objects . language ( '<STR_LIT>' ) . complex_filter , \n Q ( shared_field = NORMAL [ <NUM_LIT:1> ] . <mask0> ) ) \n", "gt": "shared_field"}
{"input": "\n import json \n import threading \n import sublime \n import sublime_plugin \n import analytics \n import uuid \n from elasticsearch import Elasticsearch \n from elasticsearch_connections import CustomHeadersConnection \n from abc import ABCMeta , abstractmethod \n from . . panel import IndexListPanel \n from . . panel import DocTypeListPanel \n from . . panel import SwitchServerListPanel \n from . . panel import AnalyzerListPanel \n from . . panel import ScriptListPanel \n from . . panel import SearchTemplateListPanel \n from . . panel import AliasListPanel \n from . . panel import IndexTemplateListPanel \n from . . panel import WarmerListPanel \n from . . panel import FieldListPanel \n from . . panel import RepositoryListPanel \n from . . panel import SnapshotListPanel \n ANALYTICS_WRITE_KEY = \"<STR_LIT>\" \n def track_command ( user_id , command_name ) : \n analytics . write_key = ANALYTICS_WRITE_KEY \n analytics . identify ( user_id ) \n analytics . track ( user_id , \"<STR_LIT>\" , { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT:label>\" : command_name , \n } ) \n def track_activate ( user_id ) : \n analytics . write_key = ANALYTICS_WRITE_KEY \n analytics . identify ( user_id ) \n analytics . track ( user_id , \"<STR_LIT>\" , { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT:label>\" : sublime . platform ( ) , \n } ) \n class Settings ( object ) : \n SETTINGS_FILE = '<STR_LIT>' \n def __init__ ( self ) : \n self . settings = sublime . load_settings ( self . SETTINGS_FILE ) \n @ property \n def base_url ( self ) : \n base_url = self . settings . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if base_url . endswith ( \"<STR_LIT:/>\" ) : \n return base_url [ : - <NUM_LIT:1> ] \n return base_url \n @ property \n def index ( self ) : \n return self . settings . get ( \"<STR_LIT:index>\" , \"<STR_LIT>\" ) \n @ property \n def doc_type ( self ) : \n return self . settings . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n @ property \n def scroll_size ( self ) : \n return self . settings . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n @ property \n def headers ( self ) : \n return self . settings . get ( \"<STR_LIT>\" , { } ) \n @ property \n def servers ( self ) : \n def _normalize_servers ( servers ) : \n items = [ ] \n for name , server in servers . items ( ) : \n server [ \"<STR_LIT:name>\" ] = name \n items . append ( server ) \n servers = sorted ( items , key = lambda k : k [ \"<STR_LIT:name>\" ] ) \n return servers \n servers = self . settings . get ( \"<STR_LIT>\" , [ ] ) \n if isinstance ( servers , dict ) : \n servers = _normalize_servers ( servers ) \n return servers \n @ property \n def active_server ( self ) : \n return dict ( \n base_url = self . base_url , \n index = self . index , \n doc_type = self . doc_type , \n scroll_size = self . scroll_size , \n ) \n @ property \n def ab_command ( self ) : \n return self . settings . get ( \"<STR_LIT>\" ) \n @ property \n def ab_requests ( self ) : \n return str ( self . settings . get ( \"<STR_LIT>\" ) ) \n @ property \n def ab_concurrency ( self ) : \n return str ( self . settings . get ( \"<STR_LIT>\" ) ) \n @ property \n def analytics ( self ) : \n return self . settings . get ( \"<STR_LIT>\" , True ) \n @ property \n def user_id ( self ) : \n return self . settings . get ( \"<STR_LIT>\" , None ) \n @ property \n def dump_file ( self ) : \n return self . settings . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n @ property \n def chunk_size ( self ) : \n return self . settings . get ( \"<STR_LIT>\" , <NUM_LIT> ) \n def set ( self , key , value ) : \n self . settings . set ( key , value ) \n def save ( self ) : \n sublime . save_settings ( self . SETTINGS_FILE ) \n class BaseCommand ( sublime_plugin . WindowCommand ) : \n __metaclass__ = ABCMeta \n command_name = None \n def __init__ ( self , * args , ** kwargs ) : \n self . settings = Settings ( ) \n sublime_plugin . WindowCommand . __init__ ( self , * args , ** kwargs ) \n @ property \n def view ( self ) : \n return self . window . active_view ( ) \n def is_valid_json ( self ) : \n try : \n json . loads ( self . get_text ( ) ) \n except ValueError : \n return False \n return True \n def is_enabled ( self ) : \n return self . is_valid_json ( ) \n def get_text ( self ) : \n return self . view . substr ( sublime . Region ( <NUM_LIT:0> , self . view . size ( ) ) ) \n def init_client ( self ) : \n self . _client = Elasticsearch ( \n self . settings . base_url , \n send_get_body_as = '<STR_LIT:POST>' , \n connection_class = CustomHeadersConnection , \n headers = self . settings . headers \n ) \n return self . _client \n def save_settings ( self ) : \n self . settings . save ( ) \n self . init_client ( ) \n @ property \n def client ( self ) : \n return self . init_client ( ) \n def track_command ( self ) : \n if self . settings . analytics : \n user_id = self . settings . user_id \n if not user_id : \n user_id = str ( uuid . uuid4 ( ) ) \n self . settings . set ( \"<STR_LIT>\" , user_id ) \n self . settings . save ( ) \n track_activate ( user_id ) \n track_command ( user_id , self . command_name ) \n def show_input_panel ( self , label , default , callback ) : \n self . window . show_input_panel ( label , default , callback , None , None ) \n def show_response ( self , response , title = \"<STR_LIT>\" ) : \n title = title or self . command_name \n text = json . dumps ( response , indent = <NUM_LIT:2> , ensure_ascii = False ) \n self . window . run_command ( \n \"<STR_LIT>\" , { \"<STR_LIT:title>\" : title , \"<STR_LIT:text>\" : text } ) \n def show_index_list_panel ( self , callback ) : \n list_panel = IndexListPanel ( \n self . window , self . client , self . settings . index ) \n list_panel . show ( callback ) \n def show_doc_type_list_panel ( self , callback ) : \n list_panel = DocTypeListPanel ( \n self . window , self . client , self . settings . index ) \n list_panel . show ( callback ) \n def show_analyzer_list_panel ( self , callback ) : \n list_panel = AnalyzerListPanel ( \n self . window , self . client , self . settings . index ) \n list_panel . show ( callback ) \n def show_switch_server_list_panel ( self , callback ) : \n list_panel = SwitchServerListPanel ( self . window , self . settings . servers ) \n list_panel . show ( callback ) \n def show_script_list_panel ( self , callback ) : \n list_panel = ScriptListPanel ( self . window , self . client ) \n list_panel . show ( callback ) \n def show_search_template_list_panel ( self , callback ) : \n list_panel = SearchTemplateListPanel ( self . window , self . client ) \n list_panel . show ( callback ) \n def show_alias_list_panel ( self , callback ) : \n list_panel = AliasListPanel ( \n self . window , self . client , self . settings . index ) \n list_panel . show ( callback ) \n def show_index_template_list_panel ( self , callback ) : \n list_panel = IndexTemplateListPanel ( self . window , self . client ) \n list_panel . show ( callback ) \n def show_warmer_list_panel ( self , callback ) : \n list_panel = WarmerListPanel ( \n self . window , self . client , self . settings . index ) \n list_panel . show ( callback ) \n def show_field_list_panel ( self , callback ) : \n list_panel = FieldListPanel ( \n self . window , self . client , \n self . settings . index , self . settings . doc_type ) \n list_panel . show ( callback ) \n def show_repository_list_panel ( self , callback ) : \n list_panel = RepositoryListPanel ( self . window , self . client ) \n list_panel . show ( callback ) \n def show_snapshot_list_panel ( self , repository , callback ) : \n list_panel = SnapshotListPanel ( self . window , self . client , repository ) \n list_panel . show ( callback ) \n def show_output_panel ( self , text , syntax = None ) : \n self . window . run_command ( \n \"<STR_LIT>\" , { \"<STR_LIT:text>\" : text , \"<STR_LIT>\" : syntax } ) \n def show_object_output_panel ( self , obj ) : \n options = dict ( \n indent = <NUM_LIT:4> , \n ensure_ascii = False \n ) \n self . show_output_panel ( \n json . dumps ( obj , ** options ) , \n syntax = \"<STR_LIT>\" ) \n def show_active_server ( self ) : \n self . window . run_command ( \"<STR_LIT>\" ) \n @ abstractmethod \n def run_request ( self , * args , ** kwargs ) : \n raise NotImplementedError ( ) \n def run_request_wrapper ( self , * args , ** kwargs ) : \n try : \n response = self . run_request ( * args , ** kwargs ) \n except Exception as e : \n sublime . error_message ( \"<STR_LIT>\" . format ( e ) ) \n return \n if response is not None : \n self . show_response ( response ) \n self . track_command ( ) \n def request_thread ( self , * args , ** kwargs ) : \n thread = threading . Thread ( \n target = self . run_request_wrapper , args = args , kwargs = kwargs ) \n thread . start ( ) \n def run ( self , * args , ** kwargs ) : \n self . request_thread ( * args , ** kwargs ) \n class CreateBaseCommand ( BaseCommand ) : \n def run_request_wrapper ( self , * args , ** kwargs ) : \n try : \n response = self . run_request ( * args , ** kwargs ) \n except Exception as e : \n sublime . error_message ( \"<STR_LIT>\" . format ( e ) ) \n return \n if response is not None : \n self . show_object_output_panel ( response ) \n self . track_command ( ) \n class DeleteBaseCommand ( CreateBaseCommand ) : \n pass \n class CatBaseCommand ( CreateBaseCommand ) : \n def is_enabled ( self ) : \n return True \n def run_request_wrapper ( self , * args , ** kwargs ) : \n try : \n response = self . run_request ( * args , ** kwargs ) \n except Exception as e : \n sublime . error_message ( \"<STR_LIT>\" . format ( e ) ) \n return \n if response is not None : \n self . show_output_panel ( response ) \n self . track_command ( ) \n class SearchBaseCommand ( BaseCommand ) : \n def extend_options ( self , options , search_type = None ) : \n if search_type : \n self . command_name = \"<STR_LIT>\" . format ( \n base = self . command_name , \n search_type = search_type . lower ( ) \n ) \n if search_type == \"<STR_LIT>\" : \n options [ \"<STR_LIT>\" ] = dict ( \n search_type = search_type , \n scroll = self . settings . scroll_size \n ) \n elif search_type is not None : \n options [ \"<STR_LIT>\" ] = dict ( \n search_type = search_type \n ) \n return options \n class SettingsBaseCommand ( BaseCommand ) : \n def is_enabled ( self ) : \n return <mask0> \n", "gt": "True"}
{"input": "\n import sublime \n from . base import DeleteBaseCommand \n class DeleteDocumentCommand ( DeleteBaseCommand ) : \n command_name = \"<STR_LIT>\" \n def is_enabled ( self ) : \n return True \n def run_request ( self , id = None ) : \n if not id : \n self . show_input_panel ( '<STR_LIT>' , '<STR_LIT>' , self . run ) \n return \n options = dict ( \n index = self . settings . index , \n doc_type = self . settings . doc_type , \n id = id \n ) \n if sublime . ok_cancel_dialog ( \"<STR_LIT>\" , ok_title = '<STR_LIT>' ) : \n return self . client . delete ( ** <mask0> ) \n", "gt": "options"}
{"input": "\n import sublime \n from . base import DeleteBaseCommand \n class IndicesDeleteAliasCommand ( DeleteBaseCommand ) : \n command_name = \"<STR_LIT>\" \n def is_enabled ( self ) : \n return True \n def run_request ( self , index = None , name = None ) : \n if not index or not name : \n self . show_alias_list_panel ( self . run ) \n return \n options = dict ( \n index = index , \n name = name \n ) \n if sublime . ok_cancel_dialog ( \"<STR_LIT>\" , ok_title = '<STR_LIT>' ) : \n return self . client . indices . delete_alias ( ** <mask0> ) \n", "gt": "options"}
{"input": "\n from . base import BaseCommand \n class IndicesStatsCommand ( BaseCommand ) : \n command_name = \"<STR_LIT>\" \n def is_enabled ( self ) : \n return True \n def run_request ( self , index = None ) : \n if index is None : \n self . show_index_list_panel ( self . run ) \n return \n options = dict ( \n index = index , \n params = dict ( human = True ) \n ) \n return self . client . indices . stats ( ** <mask0> ) \n", "gt": "options"}
{"input": "\n import sublime_plugin \n class ShowOutputPanelCommand ( sublime_plugin . WindowCommand ) : \n default_syntax = \"<STR_LIT>\" \n def run ( self , text , syntax = None ) : \n if syntax is None : \n syntax = self . default_syntax \n panel = self . window . create_output_panel ( \"<STR_LIT>\" ) \n self . window . run_command ( \n \"<STR_LIT>\" , { \"<STR_LIT>\" : \"<STR_LIT>\" } ) \n panel . set_syntax_file ( syntax ) \n panel . settings ( ) . set ( '<STR_LIT>' , True ) \n panel . settings ( ) . set ( '<STR_LIT>' , False ) \n panel . set_read_only ( False ) \n panel . run_command ( '<STR_LIT>' , { '<STR_LIT>' : text } ) \n panel . set_read_only ( <mask0> ) \n", "gt": "True"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import datetime \n __all__ = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n EASTER_JULIAN = <NUM_LIT:1> \n EASTER_ORTHODOX = <NUM_LIT:2> \n EASTER_WESTERN = <NUM_LIT:3> \n def easter ( year , method = EASTER_WESTERN ) : \n \"\"\"<STR_LIT>\"\"\" \n if not ( <NUM_LIT:1> <= method <= <NUM_LIT:3> ) : \n raise ValueError ( \"<STR_LIT>\" ) \n y = year \n g = y % <NUM_LIT> \n e = <NUM_LIT:0> \n if method < <NUM_LIT:3> : \n i = ( <NUM_LIT> * g + <NUM_LIT:15> ) % <NUM_LIT:30> \n j = ( y + y // <NUM_LIT:4> + i ) % <NUM_LIT:7> \n if method == <NUM_LIT:2> : \n e = <NUM_LIT:10> \n if y > <NUM_LIT> : \n e = e + y // <NUM_LIT:100> - <NUM_LIT:16> - ( y // <NUM_LIT:100> - <NUM_LIT:16> ) // <NUM_LIT:4> \n else : \n c = y // <NUM_LIT:100> \n h = ( c - c // <NUM_LIT:4> - ( <NUM_LIT:8> * c + <NUM_LIT> ) // <NUM_LIT> + <NUM_LIT> * g + <NUM_LIT:15> ) % <NUM_LIT:30> \n i = h - ( h // <NUM_LIT> ) * ( <NUM_LIT:1> - ( h // <NUM_LIT> ) * ( <NUM_LIT> // ( h + <NUM_LIT:1> ) ) * ( ( <NUM_LIT> - g ) // <NUM_LIT:11> ) ) \n j = ( y + y // <NUM_LIT:4> + i + <NUM_LIT:2> - c + c // <NUM_LIT:4> ) % <NUM_LIT:7> \n p = i - j + e \n d = <NUM_LIT:1> + ( p + <NUM_LIT> + ( p + <NUM_LIT:6> ) // <NUM_LIT> ) % <NUM_LIT> \n m = <NUM_LIT:3> + ( p + <NUM_LIT> ) // <NUM_LIT:30> \n return datetime . date ( int ( y ) , int ( m ) , int ( <mask0> ) ) \n", "gt": "d"}
{"input": "\n __all__ = [ \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' \n ] \n class ImproperlyConfigured ( Exception ) : \n \"\"\"<STR_LIT>\"\"\" \n class ElasticsearchException ( Exception ) : \n \"\"\"<STR_LIT>\"\"\" \n class SerializationError ( ElasticsearchException ) : \n \"\"\"<STR_LIT>\"\"\" \n class TransportError ( ElasticsearchException ) : \n \"\"\"<STR_LIT>\"\"\" \n @ property \n def status_code ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . args [ <NUM_LIT:0> ] \n @ property \n def error ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . args [ <NUM_LIT:1> ] \n @ property \n def info ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . args [ <NUM_LIT:2> ] \n def __str__ ( self ) : \n return '<STR_LIT>' % ( self . status_code , self . error ) \n class ConnectionError ( TransportError ) : \n \"\"\"<STR_LIT>\"\"\" \n def __str__ ( self ) : \n return '<STR_LIT>' % ( \n self . error , self . info . __class__ . __name__ , self . info ) \n class SSLError ( ConnectionError ) : \n \"\"\"<STR_LIT>\"\"\" \n class ConnectionTimeout ( ConnectionError ) : \n \"\"\"<STR_LIT>\"\"\" \n def __str__ ( self ) : \n return '<STR_LIT>' % ( \n self . info . __class__ . __name__ , self . info ) \n class NotFoundError ( TransportError ) : \n \"\"\"<STR_LIT>\"\"\" \n class ConflictError ( TransportError ) : \n \"\"\"<STR_LIT>\"\"\" \n class RequestError ( TransportError ) : \n \"\"\"<STR_LIT>\"\"\" \n class AuthenticationException ( TransportError ) : \n \"\"\"<STR_LIT>\"\"\" \n class AuthorizationException ( TransportError ) : \n \"\"\"<STR_LIT>\"\"\" \n HTTP_EXCEPTIONS = { \n <NUM_LIT> : RequestError , \n <NUM_LIT> : AuthenticationException , \n <NUM_LIT> : AuthorizationException , \n <NUM_LIT> : NotFoundError , \n <NUM_LIT> : <mask0> , \n } \n", "gt": "ConflictError"}
{"input": "\n from . alias_list_panel import AliasListPanel \n from . analyzer_list_panel import AnalyzerListPanel \n from . doc_type_list_panel import DocTypeListPanel \n from . field_list_panel import FieldListPanel \n from . index_list_panel import IndexListPanel \n from . index_template_list_panel import IndexTemplateListPanel \n from . repository_list_panel import RepositoryListPanel \n from . script_list_panel import ScriptListPanel \n from . search_template_list_panel import SearchTemplateListPanel \n from . snapshot_list_panel import SnapshotListPanel \n from . switch_server_list_panel import SwitchServerListPanel \n from . warmer_list_panel import WarmerListPanel \n <mask0> = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n", "gt": "__all__"}
{"input": "\n import unittest \n from test . asserting . policy import PolicyAssertion , get_fixture_path \n from vint . linting . level import Level \n from vint . linting . policy . prohibit_command_with_unintended_side_effect import ProhibitCommandWithUnintendedSideEffect \n PATH_VALID_VIM_SCRIPT = get_fixture_path ( '<STR_LIT>' ) \n PATH_INVALID_VIM_SCRIPT = get_fixture_path ( '<STR_LIT>' ) \n class TestProhibitCommandWithUnintendedSideEffect ( PolicyAssertion , unittest . TestCase ) : \n def _create_violation_by_line_number ( self , line_number ) : \n return { \n '<STR_LIT:name>' : '<STR_LIT>' , \n '<STR_LIT>' : Level . WARNING , \n '<STR_LIT>' : { \n '<STR_LIT>' : line_number , \n '<STR_LIT>' : <NUM_LIT:1> , \n '<STR_LIT:path>' : PATH_INVALID_VIM_SCRIPT \n } \n } \n def test_get_violation_if_found_with_valid_file ( self ) : \n self . assertFoundNoViolations ( PATH_VALID_VIM_SCRIPT , \n ProhibitCommandWithUnintendedSideEffect ) \n def test_get_violation_if_found_with_invalid_file ( self ) : \n expected_violations = [ self . _create_violation_by_line_number ( line_number ) \n for line_number in range ( <NUM_LIT:1> , <NUM_LIT> ) ] \n expected_violations [ <NUM_LIT:3> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] = <NUM_LIT:2> \n expected_violations [ <NUM_LIT:4> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] = <NUM_LIT:6> \n self . assertFoundViolationsEqual ( PATH_INVALID_VIM_SCRIPT , \n ProhibitCommandWithUnintendedSideEffect , \n expected_violations ) \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import unittest \n from test . asserting . config_source import ConfigSourceAssertion \n from test . asserting . config_source import get_fixture_path \n from vint . linting . config . config_file_source import ConfigFileSource \n from vint . linting . level import Level \n FIXTURE_CONFIG_FILE = get_fixture_path ( '<STR_LIT>' ) \n class TestConfigFileSource ( ConfigSourceAssertion , unittest . TestCase ) : \n class ConcreteConfigFileSource ( ConfigFileSource ) : \n def get_file_path ( self , env ) : \n return FIXTURE_CONFIG_FILE \n def test_get_config_dict ( self ) : \n expected_config_dict = { \n '<STR_LIT>' : { \n '<STR_LIT>' : True , \n '<STR_LIT>' : Level . WARNING , \n '<STR_LIT>' : <NUM_LIT:10> , \n } , \n '<STR_LIT>' : { \n '<STR_LIT>' : { \n '<STR_LIT>' : False , \n } , \n '<STR_LIT>' : { \n '<STR_LIT>' : True , \n } , \n } \n } \n config_source = self . initialize_config_source_with_env ( \n TestConfigFileSource . ConcreteConfigFileSource ) \n self . assertConfigDict ( config_source , expected_config_dict ) \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n from vint . ast . plugin . scope_plugin . scope_detector import ( \n detect_scope_visibility , \n normalize_variable_name , \n is_builtin_variable , \n ) \n from vint . ast . plugin . scope_plugin . scope_linker import ScopeLinker \n from vint . ast . plugin . scope_plugin . identifier_classifier import ( \n IdentifierClassifier , \n is_function_identifier , \n ) \n REACHABILITY_FLAG = '<STR_LIT>' \n REFERECED_FLAG = '<STR_LIT>' \n class ReferenceReachabilityTester ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n class TwoWayScopeReferenceAttacher ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n @ classmethod \n def attach ( cls , root_scope_tree ) : \n root_scope_tree [ '<STR_LIT>' ] = None \n return cls . _attach_recursively ( root_scope_tree ) \n @ classmethod \n def _attach_recursively ( cls , scope_tree ) : \n for child_scope in scope_tree [ '<STR_LIT>' ] : \n child_scope [ '<STR_LIT>' ] = scope_tree \n cls . _attach_recursively ( child_scope ) \n return scope_tree \n def process ( self , ast ) : \n scope_linker = ScopeLinker ( ) \n scope_linker . process ( ast ) \n id_collector = IdentifierClassifier . IdentifierCollector ( ) \n classified_id_group = id_collector . collect_identifiers ( ast ) \n dec_id_nodes = classified_id_group [ '<STR_LIT>' ] \n ref_id_nodes = classified_id_group [ '<STR_LIT>' ] \n self . _scope_tree = scope_linker . scope_tree \n self . _link_registry = scope_linker . link_registry \n ReferenceReachabilityTester . TwoWayScopeReferenceAttacher . attach ( self . _scope_tree ) \n for dec_id_node in dec_id_nodes : \n dec_id_node [ REFERECED_FLAG ] = False \n for ref_id_node in ref_id_nodes : \n is_reachable = self . check_reachability ( ref_id_node ) \n ref_id_node [ REACHABILITY_FLAG ] = is_reachable \n def get_objective_scope_visibility ( self , node ) : \n \"\"\"<STR_LIT>\"\"\" \n context_scope = self . _link_registry . get_scope_by_referencing_identifier ( node ) \n return detect_scope_visibility ( node , context_scope ) [ '<STR_LIT>' ] \n def _reset_referenced_flag ( self , scope_tree ) : \n for child_scope in scope_tree [ '<STR_LIT>' ] : \n for functions in child_scope [ '<STR_LIT>' ] . values ( ) : \n for function in functions : \n function [ REFERECED_FLAG ] = False \n for variables in child_scope [ '<STR_LIT>' ] . values ( ) : \n for variable in variables : \n variable [ REFERECED_FLAG ] = False \n self . _reset_referenced_flag ( child_scope ) \n def check_reachability ( self , ref_id_node ) : \n scope = self . _link_registry . get_context_scope_by_identifier ( ref_id_node ) \n var_name = normalize_variable_name ( ref_id_node , scope ) \n is_func_id = is_function_identifier ( ref_id_node ) \n while scope is not None : \n if is_func_id : \n functions_list = scope [ '<STR_LIT>' ] \n if var_name in functions_list : \n for variable in functions_list [ var_name ] : \n declaring_id_node = self . _link_registry . get_declarative_identifier_by_variable ( variable ) \n declaring_id_node [ REFERECED_FLAG ] = True \n return True \n else : \n pass \n variables_list = scope [ '<STR_LIT>' ] \n if var_name in variables_list : \n for variable in variables_list [ var_name ] : \n declaring_id_node = self . _link_registry . get_declarative_identifier_by_variable ( variable ) \n declaring_id_node [ REFERECED_FLAG ] = True \n return True \n scope = scope [ '<STR_LIT>' ] \n return is_builtin_variable ( ref_id_node ) \n def is_reference_identifier ( node ) : \n return REACHABILITY_FLAG in node \n def is_reachable_reference_identifier ( node ) : \n return node . get ( REACHABILITY_FLAG , False ) \n def is_declarative_identifier ( node ) : \n return REFERECED_FLAG in node \n def is_referenced_declarative_identifier ( node ) : \n return node . get ( REFERECED_FLAG , <mask0> ) \n", "gt": "False"}
{"input": "\n import re \n from vint . ast . node_type import NodeType \n from vint . linting . level import Level \n from vint . linting . policy . abstract_policy import AbstractPolicy \n from vint . linting . policy_registry import register_policy \n from vint . ast . dictionary . abbreviations import ( \n Abbreviations , \n AbbreviationsIncludingInvertPrefix , \n ) \n SetCommandFamily = { \n '<STR_LIT>' : True , \n '<STR_LIT>' : True , \n '<STR_LIT>' : True , \n } \n @ register_policy \n class ProhibitAbbreviationOption ( AbstractPolicy ) : \n def __init__ ( self ) : \n super ( ProhibitAbbreviationOption , self ) . __init__ ( ) \n self . description = '<STR_LIT>' \n self . reference = '<STR_LIT>' \n self . level = Level . STYLE_PROBLEM \n self . was_scriptencoding_found = False \n self . has_encoding_opt_after_scriptencoding = False \n def listen_node_types ( self ) : \n return [ NodeType . EXCMD , NodeType . OPTION ] \n def is_valid ( self , node , lint_context ) : \n \"\"\"<STR_LIT>\"\"\" \n node_type = NodeType ( node [ '<STR_LIT:type>' ] ) \n if node_type is NodeType . OPTION : \n option_name = node [ '<STR_LIT:value>' ] [ <NUM_LIT:1> : ] \n is_valid = option_name not in Abbreviations \n if not is_valid : \n self . _make_description_by_option_name ( option_name ) \n return is_valid \n excmd_node = node \n is_set_cmd = excmd_node [ '<STR_LIT>' ] [ '<STR_LIT>' ] . get ( '<STR_LIT:name>' ) in SetCommandFamily \n if not is_set_cmd : \n return True \n option_expr = excmd_node [ '<STR_LIT:str>' ] . split ( ) [ <NUM_LIT:1> ] \n option_name = re . match ( r'<STR_LIT>' , option_expr ) . group ( <NUM_LIT:0> ) \n is_valid = option_name not in AbbreviationsIncludingInvertPrefix \n if not is_valid : \n self . _make_description_by_option_name ( option_name ) \n return is_valid \n def _make_description_by_option_name ( self , option_name ) : \n param = { \n '<STR_LIT>' : AbbreviationsIncludingInvertPrefix [ option_name ] , \n '<STR_LIT>' : option_name , \n } \n self . description = ( '<STR_LIT>' \n '<STR_LIT>' . format ( ** <mask0> ) ) \n", "gt": "param"}
{"input": "\n from serfclient import result \n class TestSerfResult ( object ) : \n def test_initialises_to_none ( self ) : \n r = result . SerfResult ( ) \n assert r . head is None \n assert r . body is None \n def test_provides_a_pretty_printed_form_for_repl_use ( self ) : \n r = result . SerfResult ( head = { \"<STR_LIT:a>\" : <NUM_LIT:1> } , body = ( '<STR_LIT:foo>' , '<STR_LIT:bar>' ) ) \n assert str ( r ) == \"<STR_LIT>\" \n def test_can_convert_to_list ( self ) : \n r = result . SerfResult ( head = <NUM_LIT:1> , body = <NUM_LIT:2> ) \n assert sorted ( list ( r ) ) == [ <NUM_LIT:1> , <NUM_LIT:2> ] \n def test_can_convert_to_tuple ( self ) : \n r = result . SerfResult ( head = <NUM_LIT:1> , body = <NUM_LIT:2> ) \n assert sorted ( tuple ( <mask0> ) ) == [ <NUM_LIT:1> , <NUM_LIT:2> ] \n", "gt": "r"}
{"input": "\n import os \n import logging \n class AttrDict ( dict ) : \n \"\"\"<STR_LIT>\"\"\" \n def __getattr__ ( self , name ) : \n if name in self : \n return self [ name ] \n raise AttributeError ( '<STR_LIT>' % name ) \n def __setattr__ ( self , name , val ) : \n self [ name ] = val \n def get_logger ( name , level = None ) : \n \"\"\"<STR_LIT>\"\"\" \n logger = logging . getLogger ( name ) \n if not logger . handlers : \n stderr = logging . StreamHandler ( ) \n stderr . setFormatter ( logging . Formatter ( \n '<STR_LIT>' ) ) \n logger . addHandler ( stderr ) \n level = level if level else os . environ . get ( '<STR_LIT>' , '<STR_LIT>' ) \n logger . setLevel ( getattr ( logging , level ) ) \n return <mask0> \n", "gt": "logger"}
{"input": "\n from hacksport . problem import Challenge \n class Problem ( Challenge ) : \n def setup ( self ) : \n self . <mask0> = '<STR_LIT>' \n", "gt": "flag"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from math import * \n def AirDensity ( RH , Tc , P = <NUM_LIT> ) : \n Rd = <NUM_LIT> \n q = <NUM_LIT> * ( RH * SatVapor ( Tc ) ) / P \n Tv = ( Tc + <NUM_LIT> ) * ( <NUM_LIT:1.0> + <NUM_LIT> * q ) \n P *= <NUM_LIT> \n rho_a = P / ( Rd * Tv ) \n return rho_a \n def PsychConst ( P , cP = <NUM_LIT> , lambda_v = <NUM_LIT> ) : \n gamma = ( cP * P / ( <NUM_LIT> * lambda_v ) ) \n return gamma \n def SatVaporPress ( Tc ) : \n eSat = <NUM_LIT> * exp ( <NUM_LIT> * Tc / ( <NUM_LIT> + Tc ) ) \n return eSat \n def SlopeSatVaporPress ( Tc ) : \n delta = <NUM_LIT> * SatVaporPress ( Tc ) / ( <NUM_LIT> + Tc ) ** <NUM_LIT:2> \n return delta \n def AeroReist ( um , zm , z0 , d , zmp = zm ) : \n k = <NUM_LIT> \n r_a = <NUM_LIT:1.0> / ( k ** <NUM_LIT:2> * um ) * log ( ( zm - d ) / z0 ) * log ( ( zmp - d ) / ( z0 / <NUM_LIT> ) ) \n return r_a \n def SurfResist ( g0 , S , D , Tc , SM , SM0 ) : \n g_c = Gee_C ( ) \n g_R = Gee_R ( S ) \n g_D = Gee_D ( D ) \n g_T = Gee_T ( Tc + <NUM_LIT> ) \n g_M = Gee_M ( SM , SM0 ) \n g_s = g0 * g_c * g_R * g_D * g_T * g_M \n r_s = <NUM_LIT:1.0> / g_s \n return r_s \n def Gee_c ( ) : \n g_c = <NUM_LIT:1.0> \n return g_c \n def Gee_R ( S , K_R = <NUM_LIT> ) : \n g_R = ( S * ( <NUM_LIT> + K_R ) ) / ( <NUM_LIT> * ( S + K_R ) ) \n return g_R \n def Gee_D ( D , K_D1 = - <NUM_LIT> , K_D2 = <NUM_LIT> ) : \n g_D = <NUM_LIT:1.0> + K_D1 * D + K_D2 * D ** <NUM_LIT:2> \n return g_D \n def Gee_T ( TK , TL = <NUM_LIT> , TH = <NUM_LIT> , T0 = <NUM_LIT> ) : \n alpha_T = ( TH - T0 ) / ( T0 - TL ) \n g_T = ( ( TK - TL ) * ( TH - TK ) ** alpha_T ) / ( ( T0 - TL ) * ( TH - T0 ) ** alpha_T ) \n return g_T \n def Gee_M ( SM , SM0 , K_M1 , K_M2 ) : \n g_SM = <NUM_LIT:1.0> - K_M1 * exp ( K_M2 * ( SM - SM0 ) ) \n return g_SM \n def PenmanMonteithPET ( Tc , RH , Rn , S , SM , um , z0 , d , g0 , SM0 , P = <NUM_LIT> , zm = <NUM_LIT> ) : \n cP = <NUM_LIT> \n rho_a = AirDensity ( RH , Tc , P ) \n D = ( <NUM_LIT:1.0> - RH ) * SatVaporPress ( Tc ) \n delta = SlopeSatVaporPress ( Tc ) \n gamma = PsychConst ( P ) \n r_a = AeroReist ( um , zm , z0 , d ) \n r_s = SurfResist ( g0 , S , D , Tc , SM , SM0 ) \n LE = ( delta * Rn + ( rho_a * cP * D ) / r_a ) / ( delta + gamma * ( <NUM_LIT:1.0> + r_s / r_a ) ) \n <mask0> LE \n", "gt": "return"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from contextlib import contextmanager \n from OpenGL . GL import * \n @ contextmanager \n def glSection ( type ) : \n glBegin ( type ) \n yield \n glEnd ( ) \n @ contextmanager \n def glMatrix ( ) : \n glPushMatrix ( ) \n yield \n glPopMatrix ( ) \n @ contextmanager \n def glModeMatrix ( type ) : \n glMatrixMode ( type ) \n glPushMatrix ( ) \n yield \n glMatrixMode ( type ) \n glPopMatrix ( ) \n @ contextmanager \n def attributes ( * glBits ) : \n for bit in glBits : \n glPushAttrib ( bit ) \n yield \n for bit in glBits : \n glPopAttrib ( ) \n @ contextmanager \n def enabled ( * glBits ) : \n for bit in glBits : \n glEnable ( bit ) \n yield \n for bit in glBits : \n glDisable ( bit ) \n @ contextmanager \n def disabled ( * glBits ) : \n for bit in glBits : \n glDisable ( bit ) \n yield \n for bit in glBits : \n glEnable ( bit ) \n @ contextmanager \n def overlays2D ( width , height , background_color ) : \n \"\"\"<STR_LIT>\"\"\" \n glDisable ( GL_LIGHTING ) \n glDisable ( GL_LIGHT0 ) \n glDisable ( GL_BLEND ) \n glEnable ( GL_SCISSOR_TEST ) \n with glModeMatrix ( GL_PROJECTION ) : \n yield \n glViewport ( <NUM_LIT:0> , <NUM_LIT:0> , width , height ) \n glDisable ( GL_SCISSOR_TEST ) \n glMatrixMode ( GL_MODELVIEW ) \n glLoadIdentity ( ) \n glEnable ( GL_LIGHTING ) \n glEnable ( GL_LIGHT0 ) \n glEnable ( GL_BLEND ) \n glClearColor ( * background_color ) \n def setup_overlay2D ( x , y , width , height ) : \n \"\"\"<STR_LIT>\"\"\" \n glMatrixMode ( GL_PROJECTION ) \n glLoadIdentity ( ) \n glScissor ( x , y , width , height ) \n glViewport ( x , y , width , height ) \n glOrtho ( x , x + width , y , y + height , - <NUM_LIT:1> , <NUM_LIT:1> ) \n glMatrixMode ( GL_MODELVIEW ) \n cyltrigs = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ; \n def notGlePolyCylinder ( points , color , radius ) : \n trigs = [ radius * x for x in cyltrigs ] ; \n if abs ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] - points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] ) > <NUM_LIT> : \n with glSection ( GL_QUAD_STRIP ) : \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1.> ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , <NUM_LIT:0> , radius ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , <NUM_LIT:0> , radius ) \n glNormal3f ( <NUM_LIT:0> , cyltrigs [ <NUM_LIT:0> ] , cyltrigs [ <NUM_LIT:1> ] ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:1> ] ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:1> ] ) \n glNormal3f ( <NUM_LIT:0> , cyltrigs [ <NUM_LIT:2> ] , cyltrigs [ <NUM_LIT:3> ] ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:2> ] , trigs [ <NUM_LIT:3> ] ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:2> ] , trigs [ <NUM_LIT:3> ] ) \n glNormal3f ( <NUM_LIT:0> , cyltrigs [ <NUM_LIT:2> ] , - cyltrigs [ <NUM_LIT:3> ] ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:2> ] , - trigs [ <NUM_LIT:3> ] ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:2> ] , - trigs [ <NUM_LIT:3> ] ) \n glNormal3f ( <NUM_LIT:0> , cyltrigs [ <NUM_LIT:0> ] , - cyltrigs [ <NUM_LIT:1> ] ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:1> ] ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:1> ] ) \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:0> , - <NUM_LIT:1.> ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , <NUM_LIT:0> , - radius ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , <NUM_LIT:0> , - radius ) \n glNormal3f ( <NUM_LIT:0> , - cyltrigs [ <NUM_LIT:0> ] , - cyltrigs [ <NUM_LIT:1> ] ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:1> ] ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:1> ] ) \n glNormal3f ( <NUM_LIT:0> , - cyltrigs [ <NUM_LIT:2> ] , - cyltrigs [ <NUM_LIT:3> ] ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:2> ] , - trigs [ <NUM_LIT:3> ] ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:2> ] , - trigs [ <NUM_LIT:3> ] ) \n glNormal3f ( <NUM_LIT:0> , - cyltrigs [ <NUM_LIT:2> ] , cyltrigs [ <NUM_LIT:3> ] ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:2> ] , trigs [ <NUM_LIT:3> ] ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:2> ] , trigs [ <NUM_LIT:3> ] ) \n glNormal3f ( <NUM_LIT:0> , - cyltrigs [ <NUM_LIT:0> ] , cyltrigs [ <NUM_LIT:1> ] ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:1> ] ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:1> ] ) \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1.> ) \n glVertex ( points [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , <NUM_LIT:0> , radius ) \n glVertex ( points [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , <NUM_LIT:0> , radius ) \n elif abs ( points [ <NUM_LIT:1> ] [ <NUM_LIT:1> ] - points [ <NUM_LIT:2> ] [ <NUM_LIT:1> ] ) > <NUM_LIT> : \n p1 = points [ <NUM_LIT:1> ] [ <NUM_LIT:1> ] \n p2 = points [ <NUM_LIT:2> ] [ <NUM_LIT:1> ] \n with glSection ( GL_QUAD_STRIP ) : \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1.> ) \n glVertex ( <NUM_LIT:0> , p1 , radius ) \n glVertex ( <NUM_LIT:0> , p2 , radius ) \n glNormal3f ( cyltrigs [ <NUM_LIT:0> ] , <NUM_LIT:0> , cyltrigs [ <NUM_LIT:1> ] ) \n glVertex ( trigs [ <NUM_LIT:0> ] , p1 , trigs [ <NUM_LIT:1> ] ) \n glVertex ( trigs [ <NUM_LIT:0> ] , p2 , trigs [ <NUM_LIT:1> ] ) \n glNormal3f ( cyltrigs [ <NUM_LIT:2> ] , <NUM_LIT:0> , cyltrigs [ <NUM_LIT:3> ] ) \n glVertex ( trigs [ <NUM_LIT:2> ] , p1 , trigs [ <NUM_LIT:3> ] ) \n glVertex ( trigs [ <NUM_LIT:2> ] , p2 , trigs [ <NUM_LIT:3> ] ) \n glNormal3f ( cyltrigs [ <NUM_LIT:2> ] , <NUM_LIT:0> , - cyltrigs [ <NUM_LIT:3> ] ) \n glVertex ( trigs [ <NUM_LIT:2> ] , p1 , - trigs [ <NUM_LIT:3> ] ) \n glVertex ( trigs [ <NUM_LIT:2> ] , p2 , - trigs [ <NUM_LIT:3> ] ) \n glNormal3f ( cyltrigs [ <NUM_LIT:0> ] , <NUM_LIT:0> , - cyltrigs [ <NUM_LIT:1> ] ) \n glVertex ( trigs [ <NUM_LIT:0> ] , p1 , - trigs [ <NUM_LIT:1> ] ) \n glVertex ( trigs [ <NUM_LIT:0> ] , p2 , - trigs [ <NUM_LIT:1> ] ) \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:0> , - <NUM_LIT:1.> ) \n glVertex ( <NUM_LIT:0> , p1 , - radius ) \n glVertex ( <NUM_LIT:0> , p2 , - radius ) \n glNormal3f ( - cyltrigs [ <NUM_LIT:0> ] , <NUM_LIT:0> , - cyltrigs [ <NUM_LIT:1> ] ) \n glVertex ( - trigs [ <NUM_LIT:0> ] , p1 , - trigs [ <NUM_LIT:1> ] ) \n glVertex ( - trigs [ <NUM_LIT:0> ] , p2 , - trigs [ <NUM_LIT:1> ] ) \n glNormal3f ( - cyltrigs [ <NUM_LIT:2> ] , <NUM_LIT:0> , - cyltrigs [ <NUM_LIT:3> ] ) \n glVertex ( - trigs [ <NUM_LIT:2> ] , p1 , - trigs [ <NUM_LIT:3> ] ) \n glVertex ( - trigs [ <NUM_LIT:2> ] , p2 , - trigs [ <NUM_LIT:3> ] ) \n glNormal3f ( - cyltrigs [ <NUM_LIT:2> ] , <NUM_LIT:0> , cyltrigs [ <NUM_LIT:3> ] ) \n glVertex ( - trigs [ <NUM_LIT:2> ] , p1 , trigs [ <NUM_LIT:3> ] ) \n glVertex ( - trigs [ <NUM_LIT:2> ] , p2 , trigs [ <NUM_LIT:3> ] ) \n glNormal3f ( - cyltrigs [ <NUM_LIT:0> ] , <NUM_LIT:0> , cyltrigs [ <NUM_LIT:1> ] ) \n glVertex ( - trigs [ <NUM_LIT:0> ] , p1 , trigs [ <NUM_LIT:1> ] ) \n glVertex ( - trigs [ <NUM_LIT:0> ] , p2 , trigs [ <NUM_LIT:1> ] ) \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1.> ) \n glVertex ( <NUM_LIT:0> , p1 , radius ) \n glVertex ( <NUM_LIT:0> , p2 , radius ) \n else : \n p1 = points [ <NUM_LIT:1> ] [ <NUM_LIT:2> ] \n p2 = points [ <NUM_LIT:2> ] [ <NUM_LIT:2> ] \n with glSection ( GL_QUAD_STRIP ) : \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:1.> , <NUM_LIT:0> ) \n glVertex ( <NUM_LIT:0> , radius , p1 ) \n glVertex ( <NUM_LIT:0> , radius , p2 ) \n glNormal3f ( cyltrigs [ <NUM_LIT:0> ] , cyltrigs [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n glVertex ( trigs [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:1> ] , p1 ) \n glVertex ( trigs [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:1> ] , p2 ) \n glNormal3f ( cyltrigs [ <NUM_LIT:2> ] , cyltrigs [ <NUM_LIT:3> ] , <NUM_LIT:0> ) \n glVertex ( trigs [ <NUM_LIT:2> ] , trigs [ <NUM_LIT:3> ] , p1 ) \n glVertex ( trigs [ <NUM_LIT:2> ] , trigs [ <NUM_LIT:3> ] , p2 ) \n glNormal3f ( cyltrigs [ <NUM_LIT:2> ] , - cyltrigs [ <NUM_LIT:3> ] , <NUM_LIT:0> ) \n glVertex ( trigs [ <NUM_LIT:2> ] , - trigs [ <NUM_LIT:3> ] , p1 ) \n glVertex ( trigs [ <NUM_LIT:2> ] , - trigs [ <NUM_LIT:3> ] , p2 ) \n glNormal3f ( cyltrigs [ <NUM_LIT:0> ] , - cyltrigs [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n glVertex ( trigs [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:1> ] , p1 ) \n glVertex ( trigs [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:1> ] , p2 ) \n glNormal3f ( <NUM_LIT:0> , - <NUM_LIT:1.> , <NUM_LIT:0> ) \n glVertex ( <NUM_LIT:0> , - radius , p1 ) \n glVertex ( <NUM_LIT:0> , - radius , p2 ) \n glNormal3f ( - cyltrigs [ <NUM_LIT:0> ] , - cyltrigs [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n glVertex ( - trigs [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:1> ] , p1 ) \n glVertex ( - trigs [ <NUM_LIT:0> ] , - trigs [ <NUM_LIT:1> ] , p2 ) \n glNormal3f ( - cyltrigs [ <NUM_LIT:2> ] , - cyltrigs [ <NUM_LIT:3> ] , <NUM_LIT:0> ) \n glVertex ( - trigs [ <NUM_LIT:2> ] , - trigs [ <NUM_LIT:3> ] , p1 ) \n glVertex ( - trigs [ <NUM_LIT:2> ] , - trigs [ <NUM_LIT:3> ] , p2 ) \n glNormal3f ( - cyltrigs [ <NUM_LIT:2> ] , cyltrigs [ <NUM_LIT:3> ] , <NUM_LIT:0> ) \n glVertex ( - trigs [ <NUM_LIT:2> ] , trigs [ <NUM_LIT:3> ] , p1 ) \n glVertex ( - trigs [ <NUM_LIT:2> ] , trigs [ <NUM_LIT:3> ] , p2 ) \n glNormal3f ( - cyltrigs [ <NUM_LIT:0> ] , cyltrigs [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n glVertex ( - trigs [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:1> ] , p1 ) \n glVertex ( - trigs [ <NUM_LIT:0> ] , trigs [ <NUM_LIT:1> ] , p2 ) \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:1.> , <NUM_LIT:0> ) \n glVertex ( <NUM_LIT:0> , radius , p1 ) \n glVertex ( <NUM_LIT:0> , radius , p2 ) \n def notGlutSolidCube ( size ) : \n p = size / <NUM_LIT:2> \n n = - <NUM_LIT:1> * p \n with glSection ( GL_QUADS ) : \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1.> ) \n glVertex ( n , p , n ) \n glVertex ( n , n , n ) \n glVertex ( p , n , n ) \n glVertex ( p , p , n ) \n with glSection ( GL_QUADS ) : \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:1.> , <NUM_LIT:0> ) \n glVertex ( n , p , p ) \n glVertex ( n , p , n ) \n glVertex ( p , p , n ) \n glVertex ( p , p , p ) \n with glSection ( GL_QUADS ) : \n glNormal3f ( <NUM_LIT:1.> , <NUM_LIT:0> , <NUM_LIT:0> ) \n glVertex ( p , p , n ) \n glVertex ( p , n , n ) \n glVertex ( p , n , p ) \n glVertex ( p , p , p ) \n with glSection ( GL_QUADS ) : \n glNormal3f ( <NUM_LIT:0> , <NUM_LIT:0> , - <NUM_LIT:1.> ) \n glVertex ( p , p , p ) \n glVertex ( p , n , p ) \n glVertex ( n , n , p ) \n glVertex ( n , p , p ) \n with glSection ( GL_QUADS ) : \n glNormal3f ( <NUM_LIT:0> , - <NUM_LIT:1.> , <NUM_LIT:0> ) \n glVertex ( p , n , p ) \n glVertex ( p , n , n ) \n glVertex ( n , n , n ) \n glVertex ( n , n , p ) \n with glSection ( GL_QUADS ) : \n glNormal3f ( - <NUM_LIT:1.> , <NUM_LIT:0> , <NUM_LIT:0> ) \n glVertex ( n , p , p ) \n glVertex ( n , n , p ) \n glVertex ( n , n , n ) \n glVertex ( n , p , n ) \n class DisplayList ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , renderFunction ) : \n self . renderFunction = renderFunction \n self . needsUpdate = True \n self . listId = None \n def update ( self ) : \n self . needsUpdate = True \n def __call__ ( self , * args ) : \n if self . needsUpdate : \n if self . listId : \n glDeleteLists ( self . listId , <NUM_LIT:1> ) \n self . listId = glGenLists ( <NUM_LIT:1> ) \n glNewList ( self . listId , GL_COMPILE_AND_EXECUTE ) \n self . renderFunction ( * args ) \n glEndList ( ) \n self . needsUpdate = False \n else : \n glCallList ( self . <mask0> ) \n", "gt": "listId"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from lantz import Q_ \n from lantz . drivers . examples . dummydrivers import DummyOsci , DummyFunGen , DummyShutter \n from myapps import AmplitudeScannerShutter \n fungen = DummyFunGen ( '<STR_LIT>' ) \n osci = DummyOsci ( '<STR_LIT>' ) \n shutter = DummyShutter ( '<STR_LIT>' ) \n with AmplitudeScannerShutter ( fungen = fungen , osci = osci , shutter = shutter ) as app : \n print ( '<STR_LIT>' ) \n data = list ( app . scan_amplitude ( Q_ ( range ( <NUM_LIT:1> , <NUM_LIT:10> ) , '<STR_LIT>' ) ) ) \n print ( <mask0> ) \n", "gt": "data"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from . cobolt0601 import Cobolt0601 \n <mask0> = [ '<STR_LIT>' ] \n", "gt": "__all__"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import warnings \n from . import Q_ \n from . log import LOGGER as _LOG \n from stringparser import Parser \n class DimensionalityWarning ( Warning ) : \n pass \n def _do_nothing ( value ) : \n return value \n def _getitem ( a , b ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n return a [ b ] \n except KeyError : \n return a [ type ( b ) ] \n getitem = _getitem \n def convert_to ( units , on_dimensionless = '<STR_LIT>' , on_incompatible = '<STR_LIT>' , \n return_float = False ) : \n \"\"\"<STR_LIT>\"\"\" \n if on_dimensionless not in ( '<STR_LIT:ignore>' , '<STR_LIT>' , '<STR_LIT>' ) : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( on_dimensionless ) ) \n if on_incompatible not in ( '<STR_LIT:ignore>' , '<STR_LIT>' , '<STR_LIT>' ) : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( on_dimensionless ) ) \n if isinstance ( units , str ) : \n units = Q_ ( <NUM_LIT:1> , units ) \n elif not isinstance ( units , Q_ ) : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n if return_float : \n def _inner ( value ) : \n if isinstance ( value , Q_ ) : \n try : \n return value . to ( units ) . magnitude \n except ValueError as e : \n if on_incompatible == '<STR_LIT>' : \n raise ValueError ( e ) \n elif on_incompatible == '<STR_LIT>' : \n msg = '<STR_LIT>' . format ( value , units ) \n warnings . warn ( msg , DimensionalityWarning ) \n _LOG . warn ( msg ) \n return value . magnitude \n else : \n if not units . dimensionless : \n if on_dimensionless == '<STR_LIT>' : \n raise ValueError ( '<STR_LIT>' . format ( value , units ) ) \n elif on_dimensionless == '<STR_LIT>' : \n msg = '<STR_LIT>' . format ( value , units ) \n warnings . warn ( msg , DimensionalityWarning ) \n _LOG . warn ( msg ) \n return float ( value ) \n return _inner \n else : \n def _inner ( value ) : \n if isinstance ( value , Q_ ) : \n try : \n return value . to ( units ) \n except ValueError as e : \n if on_incompatible == '<STR_LIT>' : \n raise ValueError ( e ) \n elif on_incompatible == '<STR_LIT>' : \n msg = '<STR_LIT>' . format ( value , units ) \n warnings . warn ( msg , DimensionalityWarning ) \n _LOG . warn ( msg ) \n return float ( value . magnitude ) * units \n else : \n if not units . dimensionless : \n if on_dimensionless == '<STR_LIT>' : \n raise ValueError ( '<STR_LIT>' . format ( value , units ) ) \n elif on_dimensionless == '<STR_LIT>' : \n msg = '<STR_LIT>' . format ( value , units ) \n warnings . warn ( msg , DimensionalityWarning ) \n _LOG . warn ( msg ) \n return float ( value ) * units \n return _inner \n class Processor ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __new__ ( cls , processors ) : \n if isinstance ( processors , ( tuple , list ) ) : \n if len ( processors ) > <NUM_LIT:1> : \n inst = super ( ) . __new__ ( cls ) \n inst . processors = tuple ( cls . _to_callable ( processor ) \n for processor in processors ) \n return inst \n else : \n return cls . _to_callable ( processors [ <NUM_LIT:0> ] ) \n else : \n return cls . _to_callable ( processors ) \n def __call__ ( self , values ) : \n return tuple ( processor ( value ) \n for processor , value in zip ( self . processors , values ) ) \n @ classmethod \n def _to_callable ( cls , obj ) : \n if callable ( obj ) : \n return obj \n if obj is None : \n return _do_nothing \n return cls . to_callable ( obj ) \n @ classmethod \n def to_callable ( cls , obj ) : \n raise TypeError ( '<STR_LIT>' . format ( obj ) ) \n def __len__ ( self ) : \n if isinstance ( self . processors , tuple ) : \n return len ( self . processors ) \n return <NUM_LIT:1> \n class FromQuantityProcessor ( Processor ) : \n \"\"\"<STR_LIT>\"\"\" \n @ classmethod \n def to_callable ( cls , obj ) : \n if isinstance ( obj , ( str , Q_ ) ) : \n return convert_to ( obj , return_float = True ) \n raise TypeError ( '<STR_LIT>' \n '<STR_LIT>' . format ( obj ) ) \n class ToQuantityProcessor ( Processor ) : \n \"\"\"<STR_LIT>\"\"\" \n @ classmethod \n def to_callable ( cls , obj ) : \n if isinstance ( obj , ( str , Q_ ) ) : \n return convert_to ( obj , on_dimensionless = '<STR_LIT:ignore>' ) \n raise TypeError ( '<STR_LIT>' \n '<STR_LIT>' . format ( obj ) ) \n class ParseProcessor ( Processor ) : \n \"\"\"<STR_LIT>\"\"\" \n @ classmethod \n def to_callable ( cls , obj ) : \n if isinstance ( obj , str ) : \n return Parser ( obj ) \n raise TypeError ( '<STR_LIT>' \n '<STR_LIT>' . format ( obj ) ) \n class MapProcessor ( Processor ) : \n \"\"\"<STR_LIT>\"\"\" \n @ classmethod \n def to_callable ( cls , obj ) : \n if isinstance ( obj , dict ) : \n return get_mapping ( obj ) \n if isinstance ( obj , set ) : \n return check_membership ( obj ) \n raise TypeError ( '<STR_LIT>' \n '<STR_LIT>' . format ( obj ) ) \n class ReverseMapProcessor ( Processor ) : \n \"\"\"<STR_LIT>\"\"\" \n __reversed_cache = { } \n @ classmethod \n def to_callable ( cls , obj ) : \n if isinstance ( obj , dict ) : \n obj = cls . __reversed_cache . setdefault ( id ( obj ) , \n { value : key for key , value \n in obj . items ( ) } ) \n return get_mapping ( obj ) \n if isinstance ( obj , set ) : \n return check_membership ( obj ) \n raise TypeError ( '<STR_LIT>' \n '<STR_LIT>' . format ( obj ) ) \n class RangeProcessor ( Processor ) : \n \"\"\"<STR_LIT>\"\"\" \n @ classmethod \n def to_callable ( cls , obj ) : \n if not isinstance ( obj , ( list , tuple ) ) : \n raise TypeError ( '<STR_LIT>' \n '<STR_LIT>' . format ( obj ) ) \n if not len ( obj ) in ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) : \n raise TypeError ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' . format ( len ( obj ) ) ) \n if len ( obj ) == <NUM_LIT:1> : \n return check_range_and_coerce_step ( <NUM_LIT:0> , * obj ) \n return check_range_and_coerce_step ( * obj ) \n def check_range_and_coerce_step ( low , high , step = None ) : \n \"\"\"<STR_LIT>\"\"\" \n def _inner ( value ) : \n if not ( low <= value <= high ) : \n raise ValueError ( '<STR_LIT>' . format ( value , low , high ) ) \n if step : \n value = round ( ( value - low ) / step ) * step + low \n return value \n return _inner \n def check_membership ( container ) : \n \"\"\"<STR_LIT>\"\"\" \n def _inner ( value ) : \n if value not in container : \n raise ValueError ( '<STR_LIT>' . format ( value , container ) ) \n return value \n return _inner \n def get_mapping ( container ) : \n \"\"\"<STR_LIT>\"\"\" \n def _inner ( key ) : \n if key not in container : \n raise ValueError ( \"<STR_LIT>\" . format ( key , tuple ( container . keys ( ) ) ) ) \n return container [ key ] \n return <mask0> \n", "gt": "_inner"}
{"input": "\n try : \n from setuptools import setup \n except ImportError : \n print ( '<STR_LIT>' ) \n sys . exit ( <NUM_LIT:1> ) \n import os \n import sys \n import codecs \n def read ( filename ) : \n return codecs . open ( filename , encoding = '<STR_LIT:utf-8>' ) . read ( ) \n long_description = '<STR_LIT>' . join ( [ read ( '<STR_LIT>' ) , \n read ( '<STR_LIT>' ) , \n read ( '<STR_LIT>' ) ] ) \n __doc__ = long_description \n requirements = [ ] \n if sys . version_info < ( <NUM_LIT:3> , <NUM_LIT:4> ) : \n requirements . append ( '<STR_LIT>' ) \n root_folder = os . path . dirname ( os . path . abspath ( __file__ ) ) \n folder = os . path . join ( root_folder , '<STR_LIT>' , '<STR_LIT>' ) \n paths = os . listdir ( folder ) \n companies = [ path for path in paths \n if os . path . isdir ( os . path . join ( folder , path ) ) \n and os . path . exists ( os . path . join ( folder , path , '<STR_LIT>' ) ) ] \n folder = os . path . join ( root_folder , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n paths = os . listdir ( folder ) \n legacy_companies = [ path for path in paths \n if os . path . isdir ( os . path . join ( folder , path ) ) \n and os . path . exists ( os . path . join ( folder , path , '<STR_LIT>' ) ) ] \n setup ( name = '<STR_LIT>' , \n version = '<STR_LIT>' , \n license = '<STR_LIT>' , \n description = '<STR_LIT>' , \n long_description = long_description , \n keywords = '<STR_LIT>' , \n author = '<STR_LIT>' , \n author_email = '<STR_LIT>' , \n url = '<STR_LIT>' , \n packages = [ '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' ] + \n [ '<STR_LIT>' + company for company in companies ] + \n [ '<STR_LIT>' + company for company in legacy_companies ] , \n test_suite = '<STR_LIT>' , \n install_requires = [ '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] + requirements , \n zip_safe = False , \n platforms = '<STR_LIT>' , \n entry_points = { \n '<STR_LIT>' : [ \n '<STR_LIT>' , \n ] , \n '<STR_LIT>' : [ \n '<STR_LIT>' , \n ] \n } , \n classifiers = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] , \n <mask0> = [ '<STR_LIT>' , \n ] , \n ) \n", "gt": "scripts"}
{"input": "\n __author__ = '<STR_LIT>' \n from learnpy . Problem import Problem \n pro = Problem ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n pro . set_label ( '<STR_LIT:Name>' ) \n pro . set_model ( \"<STR_LIT>\" ) \n pro . model . fit ( None ) \n pro . set_testing ( \"<STR_LIT>\" ) \n pro . predict ( ) \n pro2 = Problem ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n pro2 . set_label ( '<STR_LIT:Name>' ) \n pro2 . set_model ( \"<STR_LIT>\" ) \n pro2 . model . fit ( None ) \n pro2 . set_testing ( \"<STR_LIT>\" ) \n pro2 . <mask0> ( ) \n", "gt": "predict"}
{"input": "\n from collections import OrderedDict \n import theano . tensor as T \n from . . import utils \n __all__ = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n class Layer ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , incoming , name = None ) : \n if isinstance ( incoming , tuple ) : \n self . input_shape = incoming \n self . input_layer = None \n else : \n self . input_shape = incoming . output_shape \n self . input_layer = incoming \n self . name = name \n self . params = OrderedDict ( ) \n self . get_output_kwargs = [ ] \n if any ( d is not None and d <= <NUM_LIT:0> for d in self . input_shape ) : \n raise ValueError ( ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" ) % ( \n self . input_shape , self . name ) ) \n @ property \n def output_shape ( self ) : \n shape = self . get_output_shape_for ( self . input_shape ) \n if any ( isinstance ( s , T . Variable ) for s in shape ) : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" % ( self . __class__ . __name__ , shape ) ) \n return shape \n def get_params ( self , ** tags ) : \n \"\"\"<STR_LIT>\"\"\" \n result = list ( self . params . keys ( ) ) \n only = set ( tag for tag , value in tags . items ( ) if value ) \n if only : \n result = [ param for param in result \n if not ( only - self . params [ param ] ) ] \n exclude = set ( tag for tag , value in tags . items ( ) if not value ) \n if exclude : \n result = [ param for param in result \n if not ( self . params [ param ] & exclude ) ] \n return utils . collect_shared_vars ( result ) \n def get_output_shape_for ( self , input_shape ) : \n \"\"\"<STR_LIT>\"\"\" \n return input_shape \n def get_output_for ( self , input , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n raise NotImplementedError \n def add_param ( self , spec , shape , name = None , ** tags ) : \n \"\"\"<STR_LIT>\"\"\" \n if name is not None : \n if self . name is not None : \n name = \"<STR_LIT>\" % ( self . name , name ) \n param = utils . create_param ( spec , shape , name ) \n tags [ '<STR_LIT>' ] = tags . get ( '<STR_LIT>' , True ) \n tags [ '<STR_LIT>' ] = tags . get ( '<STR_LIT>' , True ) \n self . params [ param ] = set ( tag for tag , value in tags . items ( ) if value ) \n return param \n class MergeLayer ( Layer ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , incomings , name = None ) : \n self . input_shapes = [ incoming if isinstance ( incoming , tuple ) \n else incoming . output_shape \n for incoming in incomings ] \n self . input_layers = [ None if isinstance ( incoming , tuple ) \n else incoming \n for incoming in incomings ] \n self . name = name \n self . params = OrderedDict ( ) \n self . get_output_kwargs = [ ] \n @ Layer . output_shape . getter \n def output_shape ( self ) : \n shape = self . get_output_shape_for ( self . input_shapes ) \n if any ( isinstance ( s , T . Variable ) for s in shape ) : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" % ( self . __class__ . __name__ , shape ) ) \n return shape \n def get_output_shape_for ( self , input_shapes ) : \n \"\"\"<STR_LIT>\"\"\" \n raise NotImplementedError \n def get_output_for ( self , inputs , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n raise <mask0> \n", "gt": "NotImplementedError"}
{"input": "\n from mock import Mock \n import numpy \n import pytest \n import theano \n class TestAutocrop : \n def test_autocrop_array_shapes ( self ) : \n from lasagne . layers . merge import autocrop_array_shapes \n crop0 = None \n crop1 = [ None , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n crop2 = [ '<STR_LIT>' , '<STR_LIT>' ] \n crop_bad = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n assert autocrop_array_shapes ( \n [ ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:4> ) , ( <NUM_LIT:5> , <NUM_LIT:6> , <NUM_LIT:7> , <NUM_LIT:8> ) , ( <NUM_LIT:5> , <NUM_LIT:4> , <NUM_LIT:3> , <NUM_LIT:2> ) ] , crop0 ) == [ ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:4> ) , ( <NUM_LIT:5> , <NUM_LIT:6> , <NUM_LIT:7> , <NUM_LIT:8> ) , ( <NUM_LIT:5> , <NUM_LIT:4> , <NUM_LIT:3> , <NUM_LIT:2> ) ] \n assert autocrop_array_shapes ( \n [ ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:4> ) , ( <NUM_LIT:5> , <NUM_LIT:6> , <NUM_LIT:7> , <NUM_LIT:8> ) , ( <NUM_LIT:5> , <NUM_LIT:4> , <NUM_LIT:3> , <NUM_LIT:2> ) ] , crop1 ) == [ ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:2> ) , ( <NUM_LIT:5> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:2> ) , ( <NUM_LIT:5> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:2> ) ] \n assert autocrop_array_shapes ( \n [ ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:4> ) , ( <NUM_LIT:5> , <NUM_LIT:6> , <NUM_LIT:7> , <NUM_LIT:8> ) , ( <NUM_LIT:5> , <NUM_LIT:4> , <NUM_LIT:3> , <NUM_LIT:2> ) ] , crop2 ) == [ ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:4> ) , ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:7> , <NUM_LIT:8> ) , ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:2> ) ] \n with pytest . raises ( ValueError ) : \n autocrop_array_shapes ( \n [ ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:4> ) , ( <NUM_LIT:5> , <NUM_LIT:6> , <NUM_LIT:7> , <NUM_LIT:8> ) , ( <NUM_LIT:5> , <NUM_LIT:4> , <NUM_LIT:3> , <NUM_LIT:2> ) ] , crop_bad ) \n with pytest . raises ( ValueError ) : \n autocrop_array_shapes ( \n [ ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:4> ) , ( <NUM_LIT:5> , <NUM_LIT:6> , <NUM_LIT:7> ) , ( <NUM_LIT:5> , <NUM_LIT:4> , <NUM_LIT:3> , <NUM_LIT:2> , <NUM_LIT:10> ) ] , crop1 ) \n def test_crop_inputs ( self ) : \n from lasagne . layers . merge import autocrop \n from numpy . testing import assert_array_equal \n crop_0 = None \n crop_1 = [ None , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n crop_l = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n crop_c = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n crop_u = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n crop_x = [ '<STR_LIT>' , '<STR_LIT>' ] \n crop_bad = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n x0 = numpy . random . random ( ( <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:5> , <NUM_LIT:7> ) ) \n x1 = numpy . random . random ( ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:4> ) ) \n x2 = numpy . random . random ( ( <NUM_LIT:6> , <NUM_LIT:3> , <NUM_LIT:4> , <NUM_LIT:2> ) ) \n def crop_test ( cropping , inputs , expected ) : \n inputs = [ theano . shared ( x ) for x in inputs ] \n outs = autocrop ( inputs , cropping ) \n outs = [ o . eval ( ) for o in outs ] \n assert len ( outs ) == len ( expected ) \n for o , e in zip ( outs , expected ) : \n assert_array_equal ( o , e ) \n crop_test ( crop_0 , [ x0 , x1 ] , \n [ x0 , x1 ] ) \n crop_test ( crop_1 , [ x0 , x1 ] , \n [ x0 [ : , : <NUM_LIT:2> , <NUM_LIT:1> : <NUM_LIT:4> , <NUM_LIT:3> : ] , x1 [ : , : , : , : ] ] ) \n crop_test ( crop_l , [ x0 , x1 ] , \n [ x0 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : <NUM_LIT:3> , : <NUM_LIT:4> ] , x1 [ : , : , : , : ] ] ) \n crop_test ( crop_c , [ x0 , x1 ] , \n [ x0 [ : <NUM_LIT:1> , : <NUM_LIT:2> , <NUM_LIT:1> : <NUM_LIT:4> , <NUM_LIT:1> : <NUM_LIT:5> ] , x1 [ : , : , : , : ] ] ) \n crop_test ( crop_u , [ x0 , x1 ] , \n [ x0 [ <NUM_LIT:1> : , <NUM_LIT:1> : , <NUM_LIT:2> : , <NUM_LIT:3> : ] , x1 [ : , : , : , : ] ] ) \n crop_test ( crop_0 , [ x0 , x2 ] , \n [ x0 , x2 ] ) \n crop_test ( crop_1 , [ x0 , x2 ] , \n [ x0 [ : , : , : <NUM_LIT:4> , <NUM_LIT:5> : ] , x2 [ : , : , : , : ] ] ) \n crop_test ( crop_l , [ x0 , x2 ] , \n [ x0 [ : , : , : <NUM_LIT:4> , : <NUM_LIT:2> ] , x2 [ : <NUM_LIT:2> , : , : , : ] ] ) \n crop_test ( crop_c , [ x0 , x2 ] , \n [ x0 [ : , : , : <NUM_LIT:4> , <NUM_LIT:2> : <NUM_LIT:4> ] , x2 [ <NUM_LIT:2> : <NUM_LIT:4> , : , : , : ] ] ) \n crop_test ( crop_u , [ x0 , x2 ] , \n [ x0 [ : , : , <NUM_LIT:1> : , <NUM_LIT:5> : ] , x2 [ <NUM_LIT:4> : , : , : , : ] ] ) \n crop_test ( crop_0 , [ x0 , x1 , x2 ] , \n [ x0 , x1 , x2 ] ) \n crop_test ( crop_1 , [ x0 , x1 , x2 ] , \n [ x0 [ : , : <NUM_LIT:2> , <NUM_LIT:1> : <NUM_LIT:4> , <NUM_LIT:5> : ] , x1 [ : , : , : , <NUM_LIT:2> : ] , x2 [ : , : <NUM_LIT:2> , : <NUM_LIT:3> , : ] ] ) \n crop_test ( crop_l , [ x0 , x1 , x2 ] , \n [ x0 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : <NUM_LIT:3> , : <NUM_LIT:2> ] , x1 [ : , : , : , : <NUM_LIT:2> ] , x2 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : <NUM_LIT:3> , : ] ] ) \n crop_test ( crop_c , [ x0 , x1 , x2 ] , \n [ x0 [ : <NUM_LIT:1> , : <NUM_LIT:2> , <NUM_LIT:1> : <NUM_LIT:4> , <NUM_LIT:2> : <NUM_LIT:4> ] , x1 [ : , : , : , <NUM_LIT:1> : <NUM_LIT:3> ] , x2 [ <NUM_LIT:2> : <NUM_LIT:3> , : <NUM_LIT:2> , : <NUM_LIT:3> , : ] ] ) \n crop_test ( crop_u , [ x0 , x1 , x2 ] , \n [ x0 [ <NUM_LIT:1> : , <NUM_LIT:1> : , <NUM_LIT:2> : , <NUM_LIT:5> : ] , x1 [ : , : , : , <NUM_LIT:2> : ] , x2 [ <NUM_LIT:5> : , <NUM_LIT:1> : , <NUM_LIT:1> : , : ] ] ) \n crop_test ( crop_x , [ x0 , x1 , x2 ] , \n [ x0 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , x1 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , x2 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] ] ) \n crop_test ( crop_x , [ x0 , x1 , x2 , x0 , x1 , x2 ] , \n [ x0 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , x1 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , x2 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , \n x0 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , x1 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , x2 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] ] ) \n with pytest . raises ( ValueError ) : \n crop_test ( crop_bad , [ x0 , x1 , x2 ] , \n [ x0 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , x1 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , x2 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] ] ) \n with pytest . raises ( ValueError ) : \n crop_test ( crop_bad , [ x0 [ : , : , : , <NUM_LIT:0> ] , x1 , x2 [ : , : , : , : , None ] ] , \n [ x0 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , x1 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] , x2 [ : <NUM_LIT:1> , : <NUM_LIT:2> , : , : ] ] ) \n class TestConcatLayer : \n @ pytest . fixture \n def layer ( self ) : \n from lasagne . layers . merge import ConcatLayer \n return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = <NUM_LIT:1> ) \n @ pytest . fixture \n def crop_layer_0 ( self ) : \n from lasagne . layers . merge import ConcatLayer \n return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = <NUM_LIT:0> , \n cropping = [ '<STR_LIT>' ] * <NUM_LIT:2> ) \n @ pytest . fixture \n def crop_layer_1 ( self ) : \n from lasagne . layers . merge import ConcatLayer \n return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = <NUM_LIT:1> , \n cropping = [ '<STR_LIT>' ] * <NUM_LIT:2> ) \n def test_get_output_shape_for ( self , layer ) : \n assert layer . get_output_shape_for ( [ ( <NUM_LIT:3> , <NUM_LIT:2> ) , ( <NUM_LIT:3> , <NUM_LIT:5> ) ] ) == ( <NUM_LIT:3> , <NUM_LIT:7> ) \n assert layer . get_output_shape_for ( [ ( <NUM_LIT:3> , <NUM_LIT:2> ) , ( <NUM_LIT:3> , None ) ] ) == ( <NUM_LIT:3> , None ) \n assert layer . get_output_shape_for ( [ ( None , <NUM_LIT:2> ) , ( <NUM_LIT:3> , <NUM_LIT:5> ) ] ) == ( <NUM_LIT:3> , <NUM_LIT:7> ) \n assert layer . get_output_shape_for ( [ ( None , <NUM_LIT:2> ) , ( None , <NUM_LIT:5> ) ] ) == ( None , <NUM_LIT:7> ) \n with pytest . raises ( ValueError ) : \n layer . get_output_shape_for ( [ ( <NUM_LIT:4> , None ) , ( <NUM_LIT:3> , <NUM_LIT:5> ) ] ) \n with pytest . raises ( ValueError ) : \n layer . get_output_shape_for ( [ ( <NUM_LIT:3> , <NUM_LIT:2> ) , ( <NUM_LIT:4> , None ) ] ) \n with pytest . raises ( ValueError ) : \n layer . get_output_shape_for ( [ ( None , <NUM_LIT:2> ) , ( <NUM_LIT:3> , <NUM_LIT:5> ) , ( <NUM_LIT:4> , <NUM_LIT:5> ) ] ) \n def test_get_output_shape_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n input_shapes = [ ( <NUM_LIT:3> , <NUM_LIT:2> ) , ( <NUM_LIT:4> , <NUM_LIT:5> ) ] \n result_0 = crop_layer_0 . get_output_shape_for ( input_shapes ) \n result_1 = crop_layer_1 . get_output_shape_for ( input_shapes ) \n assert result_0 == ( <NUM_LIT:7> , <NUM_LIT:2> ) \n assert result_1 == ( <NUM_LIT:3> , <NUM_LIT:7> ) \n def test_get_output_for ( self , layer ) : \n inputs = [ theano . shared ( numpy . ones ( ( <NUM_LIT:3> , <NUM_LIT:3> ) ) ) , \n theano . shared ( numpy . ones ( ( <NUM_LIT:3> , <NUM_LIT:2> ) ) ) ] \n result = layer . get_output_for ( inputs ) \n result_eval = result . eval ( ) \n desired_result = numpy . hstack ( [ input . get_value ( ) for input in inputs ] ) \n assert ( result_eval == desired_result ) . all ( ) \n def test_get_output_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n x0 = numpy . random . random ( ( <NUM_LIT:5> , <NUM_LIT:3> ) ) \n x1 = numpy . random . random ( ( <NUM_LIT:4> , <NUM_LIT:2> ) ) \n inputs = [ theano . shared ( x0 ) , \n theano . shared ( x1 ) ] \n result_0 = crop_layer_0 . get_output_for ( inputs ) . eval ( ) \n result_1 = crop_layer_1 . get_output_for ( inputs ) . eval ( ) \n desired_result_0 = numpy . concatenate ( [ x0 [ : , : <NUM_LIT:2> ] , x1 [ : , : <NUM_LIT:2> ] ] , axis = <NUM_LIT:0> ) \n desired_result_1 = numpy . concatenate ( [ x0 [ : <NUM_LIT:4> , : ] , x1 [ : <NUM_LIT:4> , : ] ] , axis = <NUM_LIT:1> ) \n assert ( result_0 == desired_result_0 ) . all ( ) \n assert ( result_1 == desired_result_1 ) . all ( ) \n class TestElemwiseSumLayer : \n @ pytest . fixture \n def layer ( self ) : \n from lasagne . layers . merge import ElemwiseSumLayer \n return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ <NUM_LIT:2> , - <NUM_LIT:1> ] ) \n @ pytest . fixture \n def crop_layer ( self ) : \n from lasagne . layers . merge import ElemwiseSumLayer \n return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ <NUM_LIT:2> , - <NUM_LIT:1> ] , \n cropping = [ '<STR_LIT>' ] * <NUM_LIT:2> ) \n def test_get_output_shape_for ( self , layer ) : \n assert layer . get_output_shape_for ( [ ( <NUM_LIT:3> , <NUM_LIT:2> ) , ( <NUM_LIT:3> , <NUM_LIT:2> ) ] ) == ( <NUM_LIT:3> , <NUM_LIT:2> ) \n assert layer . get_output_shape_for ( [ ( <NUM_LIT:3> , <NUM_LIT:2> ) , ( <NUM_LIT:3> , None ) ] ) == ( <NUM_LIT:3> , <NUM_LIT:2> ) \n assert layer . get_output_shape_for ( [ ( None , <NUM_LIT:2> ) , ( <NUM_LIT:3> , <NUM_LIT:2> ) ] ) == ( <NUM_LIT:3> , <NUM_LIT:2> ) \n assert layer . get_output_shape_for ( [ ( None , <NUM_LIT:2> ) , ( None , <NUM_LIT:2> ) ] ) == ( None , <NUM_LIT:2> ) \n with pytest . raises ( ValueError ) : \n layer . get_output_shape_for ( [ ( <NUM_LIT:3> , None ) , ( <NUM_LIT:4> , <NUM_LIT:2> ) ] ) \n with pytest . raises ( ValueError ) : \n layer . get_output_shape_for ( [ ( <NUM_LIT:3> , <NUM_LIT:2> ) , ( <NUM_LIT:4> , None ) ] ) \n with pytest . raises ( ValueError ) : \n layer . get_output_shape_for ( [ ( None , <NUM_LIT:2> ) , ( <NUM_LIT:3> , <NUM_LIT:2> ) , ( <NUM_LIT:4> , <NUM_LIT:2> ) ] ) \n def test_get_output_for ( self , layer ) : \n a = numpy . array ( [ [ <NUM_LIT:0> , <NUM_LIT:1> ] , [ <NUM_LIT:2> , <NUM_LIT:3> ] ] ) \n b = numpy . array ( [ [ <NUM_LIT:1> , <NUM_LIT:2> ] , [ <NUM_LIT:4> , <NUM_LIT:5> ] ] ) \n inputs = [ theano . shared ( a ) , \n theano . shared ( b ) ] \n result = layer . get_output_for ( inputs ) \n result_eval = result . eval ( ) \n desired_result = <NUM_LIT:2> * a - b \n assert ( result_eval == desired_result ) . all ( ) \n def test_get_output_for_cropped ( self , crop_layer ) : \n from numpy . testing import assert_array_almost_equal as aeq \n x0 = numpy . random . random ( ( <NUM_LIT:5> , <NUM_LIT:3> ) ) \n x1 = numpy . random . random ( ( <NUM_LIT:4> , <NUM_LIT:2> ) ) \n inputs = [ theano . shared ( x0 ) , \n theano . shared ( x1 ) ] \n result = crop_layer . get_output_for ( inputs ) . eval ( ) \n desired_result = <NUM_LIT:2> * x0 [ : <NUM_LIT:4> , : <NUM_LIT:2> ] - x1 [ : <NUM_LIT:4> , : <NUM_LIT:2> ] \n aeq ( result , desired_result ) \n def test_bad_coeffs_fails ( self , layer ) : \n from lasagne . layers . merge import ElemwiseSumLayer \n with pytest . raises ( ValueError ) : \n ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ <NUM_LIT:2> , <NUM_LIT:3> , - <NUM_LIT:1> ] ) \n class TestElemwiseMergeLayerMul : \n @ pytest . fixture \n def layer ( self ) : \n import theano . tensor as T \n from lasagne . layers . merge import ElemwiseMergeLayer \n return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . mul ) \n def test_get_output_for ( self , layer ) : \n a = numpy . array ( [ [ <NUM_LIT:0> , <NUM_LIT:1> ] , [ <NUM_LIT:2> , <NUM_LIT:3> ] ] ) \n b = numpy . array ( [ [ <NUM_LIT:1> , <NUM_LIT:2> ] , [ <NUM_LIT:4> , <NUM_LIT:5> ] ] ) \n inputs = [ theano . shared ( a ) , \n theano . shared ( b ) ] \n result = layer . get_output_for ( inputs ) \n result_eval = result . eval ( ) \n desired_result = a * b \n assert ( result_eval == desired_result ) . all ( ) \n class TestElemwiseMergeLayerMaximum : \n @ pytest . fixture \n def layer ( self ) : \n import theano . tensor as T \n from lasagne . layers . merge import ElemwiseMergeLayer \n return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . maximum ) \n def test_get_output_for ( self , layer ) : \n a = numpy . array ( [ [ <NUM_LIT:0> , <NUM_LIT:1> ] , [ <NUM_LIT:2> , <NUM_LIT:3> ] ] ) \n b = numpy . array ( [ [ <NUM_LIT:1> , <NUM_LIT:2> ] , [ <NUM_LIT:4> , <NUM_LIT:5> ] ] ) \n inputs = [ theano . shared ( a ) , \n theano . shared ( b ) ] \n result = layer . get_output_for ( inputs ) \n result_eval = result . eval ( ) \n desired_result = numpy . maximum ( a , b ) \n assert ( result_eval == desired_result ) . <mask0> ( ) \n", "gt": "all"}
{"input": "\n from gevent import monkey ; monkey . patch_all ( ) \n import gevent \n from ws4py . client . geventclient import WebSocketClient \n if __name__ == '<STR_LIT:__main__>' : \n ws = WebSocketClient ( '<STR_LIT>' , protocols = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n ws . connect ( ) \n ws . send ( \"<STR_LIT>\" ) \n print ( ( ws . receive ( ) , ) ) \n ws . send ( \"<STR_LIT>\" ) \n print ( ( ws . receive ( ) , ) ) \n def incoming ( ) : \n while True : \n m = ws . receive ( ) \n if m is not None : \n m = str ( m ) \n print ( ( m , len ( m ) ) ) \n if len ( m ) == <NUM_LIT> : \n ws . close ( ) \n break \n else : \n break \n print ( ( \"<STR_LIT>\" , ) ) \n def outgoing ( ) : \n for i in range ( <NUM_LIT:0> , <NUM_LIT> , <NUM_LIT:5> ) : \n ws . send ( \"<STR_LIT:*>\" * i ) \n ws . send ( \"<STR_LIT>\" ) \n greenlets = [ \n gevent . spawn ( incoming ) , \n gevent . spawn ( outgoing ) , \n ] \n gevent . joinall ( <mask0> ) \n", "gt": "greenlets"}
{"input": "\n import os \n import struct \n from ws4py . framing import Frame , OPCODE_CONTINUATION , OPCODE_TEXT , OPCODE_BINARY , OPCODE_CLOSE , OPCODE_PING , OPCODE_PONG \n from ws4py . compat import unicode , py3k \n __all__ = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' ] \n class Message ( object ) : \n def __init__ ( self , opcode , data = b'<STR_LIT>' , encoding = '<STR_LIT:utf-8>' ) : \n \"\"\"<STR_LIT>\"\"\" \n self . opcode = opcode \n self . _completed = False \n self . encoding = encoding \n if isinstance ( data , unicode ) : \n if not encoding : \n raise TypeError ( \"<STR_LIT>\" ) \n data = data . encode ( encoding ) \n elif isinstance ( data , bytearray ) : \n data = bytes ( data ) \n elif not isinstance ( data , bytes ) : \n raise TypeError ( \"<STR_LIT>\" % type ( data ) ) \n self . data = data \n def single ( self , mask = False ) : \n \"\"\"<STR_LIT>\"\"\" \n mask = os . urandom ( <NUM_LIT:4> ) if mask else None \n return Frame ( body = self . data , opcode = self . opcode , \n masking_key = mask , fin = <NUM_LIT:1> ) . build ( ) \n def fragment ( self , first = False , last = False , mask = False ) : \n \"\"\"<STR_LIT>\"\"\" \n fin = <NUM_LIT:1> if last is True else <NUM_LIT:0> \n opcode = self . opcode if first is True else OPCODE_CONTINUATION \n mask = os . urandom ( <NUM_LIT:4> ) if mask else None \n return Frame ( body = self . data , \n opcode = opcode , masking_key = mask , \n fin = fin ) . build ( ) \n @ property \n def completed ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _completed \n @ completed . setter \n def completed ( self , state ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _completed = state \n def extend ( self , data ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( data , bytes ) : \n self . data += data \n elif isinstance ( data , bytearray ) : \n self . data += bytes ( data ) \n elif isinstance ( data , unicode ) : \n self . data += data . encode ( self . encoding ) \n else : \n raise TypeError ( \"<STR_LIT>\" % type ( data ) ) \n def __len__ ( self ) : \n return len ( self . __unicode__ ( ) ) \n def __str__ ( self ) : \n if py3k : \n return self . data . decode ( self . encoding ) \n return self . data \n def __unicode__ ( self ) : \n return self . data . decode ( self . encoding ) \n class TextMessage ( Message ) : \n def __init__ ( self , text = None ) : \n Message . __init__ ( self , OPCODE_TEXT , text ) \n @ property \n def is_binary ( self ) : \n return False \n @ property \n def is_text ( self ) : \n return True \n class BinaryMessage ( Message ) : \n def __init__ ( self , bytes = None ) : \n Message . __init__ ( self , OPCODE_BINARY , bytes , encoding = None ) \n @ property \n def is_binary ( self ) : \n return True \n @ property \n def is_text ( self ) : \n return False \n def __len__ ( self ) : \n return len ( self . data ) \n class CloseControlMessage ( Message ) : \n def __init__ ( self , code = <NUM_LIT:1000> , reason = '<STR_LIT>' ) : \n data = b\"<STR_LIT>\" \n if code : \n data += struct . pack ( \"<STR_LIT>\" , code ) \n if reason is not None : \n if isinstance ( reason , unicode ) : \n reason = reason . encode ( '<STR_LIT:utf-8>' ) \n data += reason \n Message . __init__ ( self , OPCODE_CLOSE , data , '<STR_LIT:utf-8>' ) \n self . code = code \n self . reason = reason \n def __str__ ( self ) : \n if py3k : \n return self . reason . decode ( '<STR_LIT:utf-8>' ) \n return self . reason \n def __unicode__ ( self ) : \n return self . reason . decode ( self . encoding ) \n class PingControlMessage ( Message ) : \n def __init__ ( self , data = None ) : \n Message . __init__ ( self , OPCODE_PING , data ) \n class PongControlMessage ( Message ) : \n def __init__ ( self , data ) : \n Message . __init__ ( self , OPCODE_PONG , <mask0> ) \n", "gt": "data"}
{"input": "\n import sys \n import base64 \n import time \n import urllib \n from struct import unpack \n from threading import Lock \n from binascii import hexlify \n from urlparse import urlparse \n from mod_python import apache \n from PyAuthenNTLM2 . ntlm_dc_proxy import NTLM_DC_Proxy \n from PyAuthenNTLM2 . ntlm_ad_proxy import NTLM_AD_Proxy \n use_basic_auth = True \n try : \n from PyAuthenNTLM2 . ntlm_client import NTLM_Client \n except ImportError : \n use_basic_auth = False \n class CacheConnections : \n def __init__ ( self ) : \n self . _mutex = Lock ( ) \n self . _cache = { } \n def __len__ ( self ) : \n return len ( self . _cache ) \n def remove ( self , id ) : \n self . _mutex . acquire ( ) \n ( proxy , ts ) = self . _cache . get ( id , ( None , None ) ) \n if proxy : \n proxy . close ( ) \n del self . _cache [ id ] \n self . _mutex . release ( ) \n def add ( self , id , proxy ) : \n self . _mutex . acquire ( ) \n self . _cache [ id ] = ( proxy , int ( time . time ( ) ) ) \n self . _mutex . release ( ) \n def clean ( self ) : \n now = int ( time . time ( ) ) \n self . _mutex . acquire ( ) \n for id , conn in self . _cache . items ( ) : \n if conn [ <NUM_LIT:1> ] + <NUM_LIT> < now : \n conn [ <NUM_LIT:0> ] . close ( ) \n del self . _cache [ id ] \n self . _mutex . release ( ) \n def has_key ( self , id ) : \n return self . _cache . has_key ( id ) \n def get_proxy ( self , id ) : \n self . _mutex . acquire ( ) \n proxy = self . _cache [ id ] [ <NUM_LIT:0> ] \n self . _mutex . release ( ) \n return proxy \n class CacheGroups : \n def __init__ ( self ) : \n self . _mutex = Lock ( ) \n self . _cache = { } \n def __len__ ( self ) : \n return len ( self . _cache ) \n def add ( self , group , user ) : \n self . _mutex . acquire ( ) \n if not self . _cache . has_key ( group ) : \n self . _cache [ group ] = { } \n self . _cache [ group ] [ user ] = int ( time . time ( ) ) \n self . _mutex . release ( ) \n def clean ( self ) : \n now = int ( time . time ( ) ) \n self . _mutex . acquire ( ) \n old = [ ] \n for group , members in self . _cache . items ( ) : \n for user in members : \n if members [ user ] + <NUM_LIT:3> * <NUM_LIT> * <NUM_LIT> < now : \n old . append ( ( group , user ) ) \n for group , user in old : \n del self . _cache [ group ] [ user ] \n self . _mutex . release ( ) \n def has ( self , group , user ) : \n if not self . _cache . has_key ( group ) : \n return False \n return self . _cache [ group ] . has_key ( user ) \n cache = CacheConnections ( ) \n cacheGroups = CacheGroups ( ) \n def ntlm_message_type ( msg ) : \n if not msg . startswith ( '<STR_LIT>' ) or len ( msg ) < <NUM_LIT:12> : \n raise RuntimeError ( \"<STR_LIT>\" % hexlify ( msg ) ) \n msg_type = unpack ( '<STR_LIT>' , msg [ <NUM_LIT:8> : <NUM_LIT:8> + <NUM_LIT:4> ] ) [ <NUM_LIT:0> ] \n if msg_type not in ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) : \n raise RuntimeError ( \"<STR_LIT>\" % msg_type ) \n return msg_type \n def parse_ntlm_authenticate ( msg ) : \n '''<STR_LIT>''' \n NTLMSSP_NEGOTIATE_UNICODE = <NUM_LIT> \n idx = <NUM_LIT> \n length , offset = unpack ( '<STR_LIT>' , msg [ idx : idx + <NUM_LIT:8> ] ) \n domain = msg [ offset : offset + length ] \n idx += <NUM_LIT:8> \n length , offset = unpack ( '<STR_LIT>' , msg [ idx : idx + <NUM_LIT:8> ] ) \n username = msg [ offset : offset + length ] \n idx += <NUM_LIT> \n flags = unpack ( '<STR_LIT>' , msg [ idx : idx + <NUM_LIT:4> ] ) [ <NUM_LIT:0> ] \n if flags & NTLMSSP_NEGOTIATE_UNICODE : \n domain = str ( domain . decode ( '<STR_LIT>' ) ) \n username = str ( username . decode ( '<STR_LIT>' ) ) \n return username , domain \n def set_remote_user ( req , username , domain ) : \n format = req . get_options ( ) . get ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) \n if format == '<STR_LIT>' : \n req . user = domain + '<STR_LIT:\\\\>' + username \n else : \n req . user = username \n def decode_http_authorization_header ( auth ) : \n '''<STR_LIT>''' \n ah = auth . split ( '<STR_LIT:U+0020>' ) \n if len ( ah ) == <NUM_LIT:2> : \n b64 = base64 . b64decode ( ah [ <NUM_LIT:1> ] ) \n if ah [ <NUM_LIT:0> ] == '<STR_LIT>' : \n return ( '<STR_LIT>' , b64 ) \n elif ah [ <NUM_LIT:0> ] == '<STR_LIT>' and use_basic_auth : \n ( user , password ) = b64 . split ( '<STR_LIT::>' ) \n return ( '<STR_LIT>' , user , password ) \n return False \n def handle_unauthorized ( req ) : \n '''<STR_LIT>''' \n req . err_headers_out . add ( '<STR_LIT>' , '<STR_LIT>' ) \n if use_basic_auth : \n req . err_headers_out . add ( '<STR_LIT>' , '<STR_LIT>' % req . auth_name ( ) ) \n req . err_headers_out . add ( '<STR_LIT>' , '<STR_LIT>' ) \n return apache . HTTP_UNAUTHORIZED \n def connect_to_proxy ( req , type1 ) : \n '''<STR_LIT>''' \n try : \n domain = req . get_options ( ) [ '<STR_LIT>' ] \n pdc = req . get_options ( ) [ '<STR_LIT>' ] \n bdc = req . get_options ( ) . get ( '<STR_LIT>' , False ) \n except KeyError , e : \n req . log_error ( '<STR_LIT>' % str ( e ) , apache . APLOG_CRIT ) \n raise \n ntlm_challenge = None \n for server in ( pdc , bdc ) : \n if not server : continue \n try : \n if server . startswith ( '<STR_LIT>' ) : \n url = urlparse ( server ) \n decoded_path = urllib . unquote ( url . path ) [ <NUM_LIT:1> : ] \n req . log_error ( '<STR_LIT>' % \n ( url . netloc , domain , decoded_path ) , apache . APLOG_INFO ) \n proxy = NTLM_AD_Proxy ( url . netloc , domain , base = decoded_path ) \n else : \n req . log_error ( '<STR_LIT>' % \n ( server , domain ) , apache . APLOG_INFO ) \n proxy = NTLM_DC_Proxy ( server , domain ) \n ntlm_challenge = proxy . negotiate ( type1 ) \n except Exception , e : \n req . log_error ( '<STR_LIT>' % ( server , str ( e ) ) , apache . APLOG_CRIT ) \n if ntlm_challenge : break \n proxy . close ( ) \n else : \n raise RuntimeError ( \"<STR_LIT>\" ) \n return ( proxy , ntlm_challenge ) \n def handle_type1 ( req , ntlm_message ) : \n '''<STR_LIT>''' \n cache . remove ( req . connection . id ) \n cache . clean ( ) \n try : \n ( proxy , ntlm_challenge ) = connect_to_proxy ( req , ntlm_message ) \n except Exception , e : \n return apache . HTTP_INTERNAL_SERVER_ERROR \n cache . add ( req . connection . id , proxy ) \n req . err_headers_out . add ( '<STR_LIT>' , \"<STR_LIT>\" + base64 . b64encode ( ntlm_challenge ) ) \n return apache . HTTP_UNAUTHORIZED \n def check_authorization ( req , username , proxy ) : \n '''<STR_LIT>''' \n rules = '<STR_LIT>' . join ( req . requires ( ) ) . strip ( ) \n if rules == '<STR_LIT>' or cacheGroups . has ( rules , username ) : \n return True \n groups = [ ] \n for r in req . requires ( ) : \n if r . lower ( ) . startswith ( \"<STR_LIT>\" ) : \n users = [ u . strip ( ) for u in r [ <NUM_LIT:5> : ] . split ( \"<STR_LIT:U+002C>\" ) ] \n if username in users : \n req . log_error ( '<STR_LIT>' % \n ( username , req . unparsed_uri ) , apache . APLOG_INFO ) \n return True \n if r . lower ( ) . startswith ( \"<STR_LIT>\" ) : \n groups += [ g . strip ( ) for g in r [ <NUM_LIT:6> : ] . split ( \"<STR_LIT:U+002C>\" ) ] \n if groups : \n try : \n res = proxy . check_membership ( username , groups ) \n except Exception , e : \n req . log_error ( '<STR_LIT>' % ( username , str ( groups ) , req . unparsed_uri , str ( e ) ) ) \n if res : \n cacheGroups . add ( rules , username ) \n req . log_error ( '<STR_LIT>' % \n ( username , str ( groups ) , req . unparsed_uri ) , apache . APLOG_INFO ) \n return True \n req . log_error ( '<STR_LIT>' % \n ( username , str ( groups ) , req . unparsed_uri ) ) \n else : \n req . log_error ( '<STR_LIT>' % \n ( username , req . unparsed_uri ) ) \n return False \n def handle_type3 ( req , ntlm_message ) : \n '''<STR_LIT>''' \n proxy = cache . get_proxy ( req . connection . id ) \n try : \n user , domain = parse_ntlm_authenticate ( ntlm_message ) \n if not domain : \n domain = req . get_options ( ) . get ( '<STR_LIT>' , req . auth_name ( ) ) \n result = proxy . authenticate ( ntlm_message ) \n except Exception , e : \n req . log_error ( '<STR_LIT>' % str ( e ) , apache . APLOG_CRIT ) \n user , domain = '<STR_LIT>' , '<STR_LIT>' \n result = False \n if not result : \n cache . remove ( req . connection . id ) \n req . log_error ( '<STR_LIT>' % ( \n domain , user , req . unparsed_uri ) ) \n return handle_unauthorized ( req ) \n req . log_error ( '<STR_LIT>' % ( user , domain , req . unparsed_uri ) , apache . APLOG_NOTICE ) \n set_remote_user ( req , user , domain ) \n result = check_authorization ( req , user , proxy ) \n cache . remove ( req . connection . id ) \n if not result : \n return apache . HTTP_FORBIDDEN \n req . connection . notes . add ( '<STR_LIT>' , req . user ) \n return apache . OK \n def handle_basic ( req , user , password ) : \n '''<STR_LIT>''' \n req . log_error ( '<STR_LIT>' % ( req . unparsed_uri ) ) \n domain = req . get_options ( ) . get ( '<STR_LIT>' , req . auth_name ( ) ) \n client = NTLM_Client ( user , domain , password ) \n type1 = client . make_ntlm_negotiate ( ) \n try : \n ( proxy , type2 ) = connect_to_proxy ( req , type1 ) \n except Exception , e : \n return apache . HTTP_INTERNAL_SERVER_ERROR \n client . parse_ntlm_challenge ( type2 ) \n type3 = client . make_ntlm_authenticate ( ) \n if not proxy . authenticate ( type3 ) : \n proxy . close ( ) \n req . log_error ( '<STR_LIT>' % ( \n user , domain , req . unparsed_uri ) ) \n return handle_unauthorized ( req ) \n req . log_error ( '<STR_LIT>' % ( user , domain , req . unparsed_uri ) , apache . APLOG_NOTICE ) \n set_remote_user ( req , user , domain ) \n result = check_authorization ( req , user , proxy ) \n proxy . close ( ) \n if not result : \n return apache . HTTP_FORBIDDEN \n req . connection . notes . add ( '<STR_LIT>' , user + password ) \n return apache . OK \n def authenhandler ( req ) : \n '''<STR_LIT>''' \n req . log_error ( \"<STR_LIT>\" % ( \n req . connection . id , req . method , req . unparsed_uri , len ( cache ) ) , apache . APLOG_INFO ) \n auth_headers = req . headers_in . get ( '<STR_LIT>' , [ ] ) \n if not isinstance ( auth_headers , list ) : \n auth_headers = [ auth_headers ] \n user = req . connection . notes . get ( '<STR_LIT>' , None ) \n if user : \n req . user = user \n if auth_headers : \n req . log_error ( '<STR_LIT>' % ( \n req . connection . id , req . method , req . clength , auth_headers ) , apache . APLOG_INFO ) \n if req . method != '<STR_LIT:POST>' or req . clength > <NUM_LIT:0> : \n return apache . OK \n else : \n return apache . OK \n if not auth_headers : \n return handle_unauthorized ( req ) \n try : \n for ah in auth_headers : \n ah_data = decode_http_authorization_header ( ah ) \n if ah_data : \n break \n except : \n ah_data = False \n if not ah_data : \n req . log_error ( '<STR_LIT>' % req . unparsed_uri , apache . APLOG_ERR ) \n return apache . HTTP_BAD_REQUEST \n if ah_data [ <NUM_LIT:0> ] == '<STR_LIT>' : \n userpwd = req . connection . notes . get ( '<STR_LIT>' , None ) \n if userpwd : \n if userpwd != ah_data [ <NUM_LIT:1> ] + ah_data [ <NUM_LIT:2> ] : \n return handle_unauthorized ( req ) \n domain = req . get_options ( ) . get ( '<STR_LIT>' , req . auth_name ( ) ) \n set_remote_user ( req , ah_data [ <NUM_LIT:1> ] , domain ) \n return apache . OK \n return handle_basic ( req , ah_data [ <NUM_LIT:1> ] , ah_data [ <NUM_LIT:2> ] ) \n try : \n ntlm_version = ntlm_message_type ( ah_data [ <NUM_LIT:1> ] ) \n if ntlm_version == <NUM_LIT:1> : \n return handle_type1 ( req , ah_data [ <NUM_LIT:1> ] ) \n if ntlm_version == <NUM_LIT:3> : \n if cache . has_key ( req . connection . id ) : \n return handle_type3 ( req , ah_data [ <NUM_LIT:1> ] ) \n req . log_error ( '<STR_LIT>' % \n ( req . unparsed_uri ) , apache . APLOG_INFO ) \n return handle_unauthorized ( req ) \n error = '<STR_LIT>' \n except Exception , e : \n error = str ( e ) \n req . log_error ( '<STR_LIT>' % \n ( req . unparsed_uri , error ) , apache . APLOG_ERR ) \n return <mask0> . HTTP_BAD_REQUEST \n", "gt": "apache"}
{"input": "\n from celery import Celery \n def create_celery_app ( app ) : \n if app . config . get ( '<STR_LIT>' ) : \n app . celery = Celery ( __name__ , broker = app . config [ '<STR_LIT>' ] ) \n app . celery . conf . update ( app . config ) \n taskbase = app . celery . Task \n class ContextTask ( taskbase ) : \n abstract = True \n def __call__ ( self , * args , ** kwargs ) : \n with app . app_context ( ) : \n return taskbase . __call__ ( self , * args , ** kwargs ) \n app . celery . Task = <mask0> \n", "gt": "ContextTask"}
{"input": "\n import unittest \n import os \n import sys \n import json \n sys . path . append ( os . path . dirname ( os . path . realpath ( __file__ ) . rsplit ( '<STR_LIT:/>' , <NUM_LIT:2> ) [ <NUM_LIT:0> ] ) ) \n from app import create_app \n app = create_app ( '<STR_LIT>' ) \n add_data = \"\"\"<STR_LIT>\"\"\" \n update_data = \"\"\"<STR_LIT>\"\"\" \n class TestUsers ( unittest . TestCase ) : \n def setUp ( self ) : \n self . app = app . test_client ( ) \n def test_01_add ( self ) : \n rv = self . app . post ( '<STR_LIT>' , data = add_data , \n content_type = \"<STR_LIT:application/json>\" ) \n assert rv . status_code == <NUM_LIT> \n def test_02_read_update ( self ) : \n request = self . app . get ( '<STR_LIT>' ) \n dict = json . loads ( request . data . decode ( '<STR_LIT:utf-8>' ) ) \n id = dict [ '<STR_LIT:data>' ] [ <NUM_LIT:0> ] [ '<STR_LIT:id>' ] \n rv = self . app . patch ( '<STR_LIT>' . format ( id ) , \n data = update_data , content_type = \"<STR_LIT:application/json>\" ) \n assert rv . status_code == <NUM_LIT:200> \n def test_03_delete ( self ) : \n request = self . app . get ( '<STR_LIT>' ) \n dict = json . loads ( request . data . decode ( '<STR_LIT:utf-8>' ) ) \n id = dict [ '<STR_LIT:data>' ] [ <NUM_LIT:0> ] [ '<STR_LIT:id>' ] \n rv = self . app . delete ( '<STR_LIT>' . format ( id ) ) \n assert rv . status_code == <NUM_LIT> \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n def tokenProgressFunc ( state = \"<STR_LIT>\" , action = None , text = None , tick = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n print ( \"<STR_LIT>\" % ( state , str ( title ) , str ( text ) , str ( tick ) ) ) \n def build ( \n documentPath , \n outputUFOFormatVersion = <NUM_LIT:2> , \n roundGeometry = True , \n verbose = True , \n logPath = None , \n progressFunc = None , \n ) : \n \"\"\"<STR_LIT>\"\"\" \n from mutatorMath . ufo . document import DesignSpaceDocumentReader \n import os , glob \n if os . path . isdir ( documentPath ) : \n todo = glob . glob ( os . path . join ( documentPath , \"<STR_LIT>\" ) ) \n else : \n todo = [ documentPath ] \n results = [ ] \n for path in todo : \n reader = DesignSpaceDocumentReader ( \n path , \n ufoVersion = outputUFOFormatVersion , \n roundGeometry = roundGeometry , \n verbose = verbose , \n logPath = logPath , \n progressFunc = progressFunc \n ) \n reader . process ( ) \n results . append ( reader . results ) \n reader = None \n return <mask0> \n", "gt": "results"}
{"input": "\n from django . core . management . base import BaseCommand \n from chronam . core . management . commands import configure_logging \n from chronam . core . index import index_pages \n configure_logging ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n class Command ( BaseCommand ) : \n def handle ( self , ** options ) : \n <mask0> ( ) \n", "gt": "index_pages"}
{"input": "\n import os \n from django . conf import settings \n from django . http import HttpResponse \n class HttpResponseServiceUnavailable ( HttpResponse ) : \n status_code = <NUM_LIT> \n class TooBusyMiddleware ( object ) : \n def process_request ( self , request ) : \n one , five , fifteen = os . getloadavg ( ) \n if one > settings . TOO_BUSY_LOAD_AVERAGE : \n return HttpResponseServiceUnavailable ( \"\"\"<STR_LIT>\"\"\" ) \n return <mask0> \n", "gt": "None"}
{"input": "\n from os . path import dirname , join \n from django . test import TestCase \n from chronam . core . ocr_extractor import ocr_extractor \n class OcrExtractorTests ( TestCase ) : \n def test_extractor ( self ) : \n dir = join ( dirname ( dirname ( __file__ ) ) , '<STR_LIT>' ) \n ocr_file = join ( dir , '<STR_LIT>' ) \n text , coord_info = ocr_extractor ( ocr_file ) \n coords = coord_info [ \"<STR_LIT>\" ] \n expected_text = { \"<STR_LIT>\" : file ( join ( dir , '<STR_LIT>' ) ) . read ( ) . decode ( '<STR_LIT:utf-8>' ) } \n self . assertEqual ( text , expected_text ) \n self . assertEqual ( len ( coords . keys ( ) ) , <NUM_LIT> ) \n self . assertEqual ( len ( coords [ '<STR_LIT>' ] ) , <NUM_LIT:3> ) \n self . assertTrue ( coords . has_key ( '<STR_LIT>' ) ) \n self . assertTrue ( not coords . <mask0> ( '<STR_LIT>' ) ) \n", "gt": "has_key"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import logging \n from . tests import * \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n logger . setLevel ( logging . DEBUG ) \n handler = logging . StreamHandler ( ) \n formatter = logging . Formatter ( \"<STR_LIT>\" ) \n handler . setFormatter ( formatter ) \n logger . addHandler ( <mask0> ) \n", "gt": "handler"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import print_function \n import argparse \n import datetime \n import json \n import multiprocessing \n import os \n import random \n import sys \n import threading \n import time \n import numpy as np \n from PIL import Image \n import six \n import six . moves . cPickle as pickle \n from six . moves import queue \n import chainer \n from chainer import computational_graph \n from chainer import cuda \n from chainer import optimizers \n from chainer import serializers \n parser = argparse . ArgumentParser ( \n description = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT:train>' , help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , default = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , default = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , type = int , default = <NUM_LIT:32> , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , type = int , default = <NUM_LIT> , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , default = <NUM_LIT:10> , type = int , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , default = - <NUM_LIT:1> , type = int , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , default = <NUM_LIT:20> , type = int , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , default = '<STR_LIT:.>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , default = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , default = '<STR_LIT:state>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , default = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , default = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n args = parser . parse_args ( ) \n if args . gpu >= <NUM_LIT:0> : \n cuda . check_cuda_available ( ) \n xp = cuda . cupy if args . gpu >= <NUM_LIT:0> else np \n assert <NUM_LIT> % args . val_batchsize == <NUM_LIT:0> \n def load_image_list ( path , root ) : \n tuples = [ ] \n for line in open ( path ) : \n pair = line . strip ( ) . split ( ) \n tuples . append ( ( os . path . join ( root , pair [ <NUM_LIT:0> ] ) , np . int32 ( pair [ <NUM_LIT:1> ] ) ) ) \n return tuples \n train_list = load_image_list ( args . train , args . root ) \n val_list = load_image_list ( args . val , args . root ) \n mean_image = pickle . load ( open ( args . mean , '<STR_LIT:rb>' ) ) \n if args . arch == '<STR_LIT>' : \n import nin \n model = nin . NIN ( ) \n elif args . arch == '<STR_LIT>' : \n import alex \n model = alex . Alex ( ) \n elif args . arch == '<STR_LIT>' : \n import alexbn \n model = alexbn . AlexBN ( ) \n elif args . arch == '<STR_LIT>' : \n import googlenet \n model = googlenet . GoogLeNet ( ) \n elif args . arch == '<STR_LIT>' : \n import googlenetbn \n model = googlenetbn . GoogLeNetBN ( ) \n else : \n raise ValueError ( '<STR_LIT>' ) \n if args . gpu >= <NUM_LIT:0> : \n cuda . get_device ( args . gpu ) . use ( ) \n model . to_gpu ( ) \n optimizer = optimizers . MomentumSGD ( lr = <NUM_LIT> , momentum = <NUM_LIT> ) \n optimizer . setup ( model ) \n if args . initmodel : \n print ( '<STR_LIT>' , args . initmodel ) \n serializers . load_hdf5 ( args . initmodel , model ) \n if args . resume : \n print ( '<STR_LIT>' , args . resume ) \n serializers . load_hdf5 ( args . resume , optimizer ) \n data_q = queue . Queue ( maxsize = <NUM_LIT:1> ) \n res_q = queue . Queue ( ) \n cropwidth = <NUM_LIT> - model . insize \n def read_image ( path , center = False , flip = False ) : \n image = np . asarray ( Image . open ( path ) ) . transpose ( <NUM_LIT:2> , <NUM_LIT:0> , <NUM_LIT:1> ) \n if center : \n top = left = cropwidth / <NUM_LIT:2> \n else : \n top = random . randint ( <NUM_LIT:0> , cropwidth - <NUM_LIT:1> ) \n left = random . randint ( <NUM_LIT:0> , cropwidth - <NUM_LIT:1> ) \n bottom = model . insize + top \n right = model . insize + left \n image = image [ : , top : bottom , left : right ] . astype ( np . float32 ) \n image -= mean_image [ : , top : bottom , left : right ] \n image /= <NUM_LIT:255> \n if flip and random . randint ( <NUM_LIT:0> , <NUM_LIT:1> ) == <NUM_LIT:0> : \n return image [ : , : , : : - <NUM_LIT:1> ] \n else : \n return image \n def feed_data ( ) : \n i = <NUM_LIT:0> \n count = <NUM_LIT:0> \n x_batch = np . ndarray ( \n ( args . batchsize , <NUM_LIT:3> , model . insize , model . insize ) , dtype = np . float32 ) \n y_batch = np . ndarray ( ( args . batchsize , ) , dtype = np . int32 ) \n val_x_batch = np . ndarray ( \n ( args . val_batchsize , <NUM_LIT:3> , model . insize , model . insize ) , dtype = np . float32 ) \n val_y_batch = np . ndarray ( ( args . val_batchsize , ) , dtype = np . int32 ) \n batch_pool = [ None ] * args . batchsize \n val_batch_pool = [ None ] * args . val_batchsize \n pool = multiprocessing . Pool ( args . loaderjob ) \n data_q . put ( '<STR_LIT:train>' ) \n for epoch in six . moves . range ( <NUM_LIT:1> , <NUM_LIT:1> + args . epoch ) : \n print ( '<STR_LIT>' , epoch , file = sys . stderr ) \n print ( '<STR_LIT>' , optimizer . lr , file = sys . stderr ) \n perm = np . random . permutation ( len ( train_list ) ) \n for idx in perm : \n path , label = train_list [ idx ] \n batch_pool [ i ] = pool . apply_async ( read_image , ( path , False , True ) ) \n y_batch [ i ] = label \n i += <NUM_LIT:1> \n if i == args . batchsize : \n for j , x in enumerate ( batch_pool ) : \n x_batch [ j ] = x . get ( ) \n data_q . put ( ( x_batch . copy ( ) , y_batch . copy ( ) ) ) \n i = <NUM_LIT:0> \n count += <NUM_LIT:1> \n if count % <NUM_LIT:1000> == <NUM_LIT:0> : \n data_q . put ( '<STR_LIT>' ) \n j = <NUM_LIT:0> \n for path , label in val_list : \n val_batch_pool [ j ] = pool . apply_async ( \n read_image , ( path , True , False ) ) \n val_y_batch [ j ] = label \n j += <NUM_LIT:1> \n if j == args . val_batchsize : \n for k , x in enumerate ( val_batch_pool ) : \n val_x_batch [ k ] = x . get ( ) \n data_q . put ( ( val_x_batch . copy ( ) , val_y_batch . copy ( ) ) ) \n j = <NUM_LIT:0> \n data_q . put ( '<STR_LIT:train>' ) \n optimizer . lr *= <NUM_LIT> \n pool . close ( ) \n pool . join ( ) \n data_q . put ( '<STR_LIT:end>' ) \n def log_result ( ) : \n train_count = <NUM_LIT:0> \n train_cur_loss = <NUM_LIT:0> \n train_cur_accuracy = <NUM_LIT:0> \n begin_at = time . time ( ) \n val_begin_at = None \n while True : \n result = res_q . get ( ) \n if result == '<STR_LIT:end>' : \n print ( file = sys . stderr ) \n break \n elif result == '<STR_LIT:train>' : \n print ( file = sys . stderr ) \n train = True \n if val_begin_at is not None : \n begin_at += time . time ( ) - val_begin_at \n val_begin_at = None \n continue \n elif result == '<STR_LIT>' : \n print ( file = sys . stderr ) \n train = False \n val_count = val_loss = val_accuracy = <NUM_LIT:0> \n val_begin_at = time . time ( ) \n continue \n loss , accuracy = result \n if train : \n train_count += <NUM_LIT:1> \n duration = time . time ( ) - begin_at \n throughput = train_count * args . batchsize / duration \n sys . stderr . write ( \n '<STR_LIT>' \n . format ( train_count , train_count * args . batchsize , \n datetime . timedelta ( seconds = duration ) , throughput ) ) \n train_cur_loss += loss \n train_cur_accuracy += accuracy \n if train_count % <NUM_LIT:1000> == <NUM_LIT:0> : \n mean_loss = train_cur_loss / <NUM_LIT:1000> \n mean_error = <NUM_LIT:1> - train_cur_accuracy / <NUM_LIT:1000> \n print ( file = sys . stderr ) \n print ( json . dumps ( { '<STR_LIT:type>' : '<STR_LIT:train>' , '<STR_LIT>' : train_count , \n '<STR_LIT:error>' : mean_error , '<STR_LIT>' : mean_loss } ) ) \n sys . stdout . flush ( ) \n train_cur_loss = <NUM_LIT:0> \n train_cur_accuracy = <NUM_LIT:0> \n else : \n val_count += args . val_batchsize \n duration = time . time ( ) - val_begin_at \n throughput = val_count / duration \n sys . stderr . write ( \n '<STR_LIT>' \n . format ( val_count / args . val_batchsize , val_count , \n datetime . timedelta ( seconds = duration ) , throughput ) ) \n val_loss += loss \n val_accuracy += accuracy \n if val_count == <NUM_LIT> : \n mean_loss = val_loss * args . val_batchsize / <NUM_LIT> \n mean_error = <NUM_LIT:1> - val_accuracy * args . val_batchsize / <NUM_LIT> \n print ( file = sys . stderr ) \n print ( json . dumps ( { '<STR_LIT:type>' : '<STR_LIT>' , '<STR_LIT>' : train_count , \n '<STR_LIT:error>' : mean_error , '<STR_LIT>' : mean_loss } ) ) \n sys . stdout . flush ( ) \n def train_loop ( ) : \n graph_generated = False \n while True : \n while data_q . empty ( ) : \n time . sleep ( <NUM_LIT:0.1> ) \n inp = data_q . get ( ) \n if inp == '<STR_LIT:end>' : \n res_q . put ( '<STR_LIT:end>' ) \n break \n elif inp == '<STR_LIT:train>' : \n res_q . put ( '<STR_LIT:train>' ) \n model . train = True \n continue \n elif inp == '<STR_LIT>' : \n res_q . put ( '<STR_LIT>' ) \n serializers . save_hdf5 ( args . out , model ) \n serializers . save_hdf5 ( args . outstate , optimizer ) \n model . train = False \n continue \n volatile = '<STR_LIT>' if model . train else '<STR_LIT>' \n x = chainer . Variable ( xp . asarray ( inp [ <NUM_LIT:0> ] ) , volatile = volatile ) \n t = chainer . Variable ( xp . asarray ( inp [ <NUM_LIT:1> ] ) , volatile = volatile ) \n if model . train : \n optimizer . update ( model , x , t ) \n if not graph_generated : \n with open ( '<STR_LIT>' , '<STR_LIT:w>' ) as o : \n o . write ( computational_graph . build_computational_graph ( \n ( model . loss , ) ) . dump ( ) ) \n print ( '<STR_LIT>' , file = sys . stderr ) \n graph_generated = True \n else : \n model ( x , t ) \n res_q . put ( ( float ( model . loss . data ) , float ( model . accuracy . data ) ) ) \n del x , t \n feeder = threading . Thread ( target = feed_data ) \n feeder . daemon = True \n feeder . start ( ) \n logger = threading . Thread ( target = log_result ) \n logger . daemon = True \n logger . start ( ) \n train_loop ( ) \n feeder . join ( ) \n logger . join ( ) \n serializers . save_hdf5 ( args . out , model ) \n serializers . save_hdf5 ( args . outstate , <mask0> ) \n", "gt": "optimizer"}
{"input": "\n import sys , os \n sys . path . insert ( <NUM_LIT:0> , os . path . abspath ( '<STR_LIT:..>' ) ) \n import local_settings \n extensions = [ '<STR_LIT>' , '<STR_LIT>' ] \n templates_path = [ '<STR_LIT>' ] \n source_suffix = '<STR_LIT>' \n master_doc = '<STR_LIT:index>' \n project = u'<STR_LIT>' \n copyright = u'<STR_LIT>' \n html_show_copyright = False \n import djoauth2 \n version = djoauth2 . __version__ \n release = djoauth2 . __version__ \n exclude_patterns = [ '<STR_LIT>' ] \n pygments_style = '<STR_LIT>' \n html_theme = '<STR_LIT:default>' \n html_static_path = [ '<STR_LIT>' ] \n htmlhelp_basename = '<STR_LIT>' \n latex_elements = { \n } \n latex_documents = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n u'<STR_LIT>' , '<STR_LIT>' ) , \n ] \n man_pages = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n [ u'<STR_LIT>' ] , <NUM_LIT:1> ) \n ] \n texinfo_documents = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n u'<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' ) , \n ] \n epub_title = u'<STR_LIT>' \n epub_author = u'<STR_LIT>' \n epub_publisher = u'<STR_LIT>' \n epub_copyright = <mask0> \n", "gt": "u'<STR_LIT>'"}
{"input": "\n import asyncio \n from zeroservices import ZeroMQMedium , ResourceService \n from zeroservices . services import get_http_interface \n from zeroservices . discovery import UdpDiscoveryMedium \n if __name__ == '<STR_LIT:__main__>' : \n loop = asyncio . get_event_loop ( ) \n medium = ZeroMQMedium ( loop , UdpDiscoveryMedium ) \n service = ResourceService ( '<STR_LIT>' , medium ) \n application = get_http_interface ( service , loop , port = <NUM_LIT> , allowed_origins = \"<STR_LIT:*>\" ) \n application = loop . run_until_complete ( application ) \n loop . run_until_complete ( service . start ( ) ) \n loop . <mask0> ( ) \n", "gt": "run_forever"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import argparse \n import sys \n from trello import TrelloClient \n from slugify import slugify \n from matterllo . utils import config \n from matterllo . utils import logger \n SETTINGS = config ( ) \n LOGGING = logger ( ) \n def main ( ) : \n try : \n parser = argparse . ArgumentParser ( description = \"<STR_LIT>\" ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , action = '<STR_LIT:store_true>' , help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , action = '<STR_LIT:store_true>' , help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , dest = '<STR_LIT>' , action = '<STR_LIT:store_true>' , help = '<STR_LIT>' ) \n args = parser . parse_args ( ) \n if not args . cleanup and not args . update and not args . init : \n print parser . print_help ( ) \n sys . exit ( <NUM_LIT:0> ) \n client = TrelloClient ( api_key = SETTINGS [ '<STR_LIT>' ] , token = SETTINGS [ '<STR_LIT>' ] ) \n trello_boards = client . list_boards ( ) \n boards_name = [ slugify ( b [ '<STR_LIT:name>' ] ) for b in SETTINGS . get ( '<STR_LIT>' , { } ) . values ( ) ] \n if args . cleanup or args . init : \n result = [ h . delete ( ) for h in client . list_hooks ( ) ] \n LOGGING . info ( '<STR_LIT>' . format ( len ( result ) ) ) \n if args . update or args . init : \n for board in trello_boards : \n board_name = slugify ( board . name ) \n if board_name not in boards_name : \n continue \n LOGGING . info ( '<STR_LIT>' . format ( board_name ) ) \n url = SETTINGS [ '<STR_LIT>' ] + '<STR_LIT>' \n result = client . create_hook ( url , board . id ) \n LOGGING . info ( '<STR_LIT>' . format ( board_name , result ) ) \n except Exception as e : \n LOGGING . error ( '<STR_LIT>' . format ( e ) ) \n sys . exit ( <NUM_LIT:1> ) \n if __name__ == '<STR_LIT:__main__>' : \n <mask0> ( ) \n", "gt": "main"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from contextlib import contextmanager \n import sys \n import zlib \n try : \n from . import ssl_compat \n except ImportError : \n ssl_compat = None \n _ver = sys . version_info \n is_py2 = _ver [ <NUM_LIT:0> ] == <NUM_LIT:2> \n is_py2_7_9_or_later = _ver [ <NUM_LIT:0> ] >= <NUM_LIT:2> and _ver [ <NUM_LIT:1> ] >= <NUM_LIT:7> and _ver [ <NUM_LIT:2> ] >= <NUM_LIT:9> \n is_py3 = _ver [ <NUM_LIT:0> ] == <NUM_LIT:3> \n is_py3_3 = is_py3 and _ver [ <NUM_LIT:1> ] == <NUM_LIT:3> \n @ contextmanager \n def ignore_missing ( ) : \n try : \n yield \n except ( AttributeError , NotImplementedError ) : \n pass \n if is_py2 : \n if is_py2_7_9_or_later : \n import ssl \n else : \n ssl = ssl_compat \n from urllib import urlencode \n from urlparse import urlparse , urlsplit \n from itertools import imap \n def to_byte ( char ) : \n return ord ( char ) \n def decode_hex ( b ) : \n return b . decode ( '<STR_LIT>' ) \n def write_to_stdout ( data ) : \n sys . stdout . write ( data + '<STR_LIT:\\n>' ) \n sys . stdout . flush ( ) \n def zlib_compressobj ( level = <NUM_LIT:6> , method = zlib . DEFLATED , wbits = <NUM_LIT:15> , memlevel = <NUM_LIT:8> , \n strategy = zlib . Z_DEFAULT_STRATEGY ) : \n return zlib . compressobj ( level , method , wbits , memlevel , strategy ) \n unicode = unicode \n bytes = str \n elif is_py3 : \n from urllib . parse import urlencode , urlparse , urlsplit \n imap = map \n def to_byte ( char ) : \n return char \n def decode_hex ( b ) : \n return bytes . fromhex ( b ) \n def write_to_stdout ( data ) : \n sys . stdout . buffer . write ( data + b'<STR_LIT:\\n>' ) \n sys . stdout . buffer . flush ( ) \n zlib_compressobj = zlib . compressobj \n if is_py3_3 : \n ssl = ssl_compat \n else : \n import ssl \n unicode = str \n bytes = <mask0> \n", "gt": "bytes"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import threading \n import socket \n import sys \n from hyper import HTTP20Connection \n from hyper . compat import ssl \n from hyper . http11 . connection import HTTP11Connection \n from hpack . hpack import Encoder \n from hpack . huffman import HuffmanEncoder \n from hpack . huffman_constants import ( \n REQUEST_CODES , REQUEST_CODES_LENGTH \n ) \n from hyper . tls import NPN_PROTOCOL \n class SocketServerThread ( threading . Thread ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , \n socket_handler , \n host = '<STR_LIT:localhost>' , \n ready_event = None , \n h2 = True , \n secure = True ) : \n threading . Thread . __init__ ( self ) \n self . socket_handler = socket_handler \n self . host = host \n self . secure = secure \n self . ready_event = ready_event \n self . daemon = True \n if self . secure : \n self . cxt = ssl . SSLContext ( ssl . PROTOCOL_SSLv23 ) \n if ssl . HAS_NPN and h2 : \n self . cxt . set_npn_protocols ( [ NPN_PROTOCOL ] ) \n self . cxt . load_cert_chain ( certfile = '<STR_LIT>' , \n keyfile = '<STR_LIT>' ) \n def _start_server ( self ) : \n sock = socket . socket ( ) \n if sys . platform != '<STR_LIT:win32>' : \n sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , <NUM_LIT:1> ) \n if self . secure : \n sock = self . cxt . wrap_socket ( sock , server_side = True ) \n sock . bind ( ( self . host , <NUM_LIT:0> ) ) \n self . port = sock . getsockname ( ) [ <NUM_LIT:1> ] \n sock . listen ( <NUM_LIT:1> ) \n if self . ready_event : \n self . ready_event . set ( ) \n self . socket_handler ( sock ) \n sock . close ( ) \n def _wrap_socket ( self , sock ) : \n raise NotImplementedError ( ) \n def run ( self ) : \n self . server = self . _start_server ( ) \n class SocketLevelTest ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def set_up ( self , secure = True , proxy = False ) : \n self . host = None \n self . port = None \n self . secure = secure if not proxy else False \n self . proxy = proxy \n self . server_thread = None \n def _start_server ( self , socket_handler ) : \n \"\"\"<STR_LIT>\"\"\" \n ready_event = threading . Event ( ) \n self . server_thread = SocketServerThread ( \n socket_handler = socket_handler , \n ready_event = ready_event , \n h2 = self . h2 , \n secure = self . secure \n ) \n self . server_thread . start ( ) \n ready_event . wait ( ) \n self . host = self . server_thread . host \n self . port = self . server_thread . port \n self . secure = self . server_thread . secure \n def get_connection ( self ) : \n if self . h2 : \n if not self . proxy : \n return HTTP20Connection ( self . host , self . port , self . secure ) \n else : \n return HTTP20Connection ( '<STR_LIT>' , secure = self . secure , \n proxy_host = self . host , \n proxy_port = self . port ) \n else : \n if not self . proxy : \n return HTTP11Connection ( self . host , self . port , self . secure ) \n else : \n return HTTP11Connection ( '<STR_LIT>' , secure = self . secure , \n proxy_host = self . host , \n proxy_port = self . port ) \n def get_encoder ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n e = Encoder ( ) \n e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n return e \n def tear_down ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . server_thread . <mask0> ( <NUM_LIT:0.1> ) \n", "gt": "join"}
{"input": "\n import numpy as np \n import lxmls . readers . pos_corpus as pcc \n from os import path \n import pickle \n corpus = pcc . PostagCorpus ( ) \n input_data = path . join ( \n path . dirname ( __file__ ) , \n \"<STR_LIT>\" ) \n train_seq = corpus . read_sequence_list_conll ( input_data , max_sent_len = <NUM_LIT:15> , max_nr_sent = <NUM_LIT:1000> ) \n pickle . dump ( ( corpus . word_dict , corpus . tag_dict ) , open ( '<STR_LIT>' , '<STR_LIT:w>' ) ) \n with open ( '<STR_LIT>' , '<STR_LIT:w>' ) as output : \n for seq in train_seq : \n words = [ corpus . word_dict . get_label_name ( seq . x [ i ] ) for i in xrange ( len ( seq ) ) ] \n tags = [ corpus . tag_dict . get_label_name ( seq . y [ i ] ) for i in xrange ( len ( seq ) ) ] \n s = '<STR_LIT:U+0020>' . join ( [ '<STR_LIT:_>' . join ( [ word , tag ] ) for word , tag in zip ( words , tags ) ] ) \n output . write ( s + <mask0> ) \n", "gt": "'<STR_LIT:\\n>'"}
{"input": "\n import sys \n import numpy as np \n from lxmls . parsing . dependency_reader import * \n from lxmls . parsing . dependency_writer import * \n from lxmls . parsing . dependency_features import * \n from lxmls . parsing . dependency_decoder import * \n from lxmls . util . my_math_utils import * \n class DependencyParser ( ) : \n '''<STR_LIT>''' \n def __init__ ( self ) : \n self . trained = False \n self . projective = False \n self . language = \"<STR_LIT>\" \n self . weights = [ ] \n self . decoder = DependencyDecoder ( ) \n self . reader = DependencyReader ( ) \n self . writer = DependencyWriter ( ) \n self . features = DependencyFeatures ( ) \n def read_data ( self , language ) : \n self . language = language \n self . reader . load ( language ) \n self . features . create_dictionary ( self . reader . train_instances ) \n def train_perceptron ( self , n_epochs ) : \n '''<STR_LIT>''' \n self . weights = np . zeros ( self . features . n_feats ) \n total = np . zeros ( self . features . n_feats ) \n for epoch in range ( n_epochs ) : \n print \"<STR_LIT>\" . format ( epoch + <NUM_LIT:1> ) \n n_mistakes = <NUM_LIT:0> \n n_tokens = <NUM_LIT:0> \n n_instances = <NUM_LIT:0> \n for instance in self . reader . train_instances : \n feats = self . features . create_features ( instance ) \n scores = self . features . compute_scores ( feats , self . weights ) \n if self . projective : \n heads_pred = self . decoder . parse_proj ( scores ) \n else : \n heads_pred = self . decoder . parse_nonproj ( scores ) \n for m in range ( np . size ( heads_pred ) ) : \n if heads_pred [ m ] != instance . heads [ m ] : \n for f in feats [ instance . heads [ m ] ] [ m ] : \n if f < <NUM_LIT:0> : \n continue \n self . weights [ f ] += <NUM_LIT:1.0> \n for f in feats [ heads_pred [ m ] ] [ m ] : \n if f < <NUM_LIT:0> : \n continue \n self . weights [ f ] -= <NUM_LIT:1.0> \n n_mistakes += <NUM_LIT:1> \n n_tokens += <NUM_LIT:1> \n n_instances += <NUM_LIT:1> \n print \"<STR_LIT>\" . format ( np . double ( n_tokens - n_mistakes ) / np . double ( n_tokens ) ) \n total += self . weights \n self . weights = total / np . double ( n_epochs ) \n def train_crf_sgd ( self , n_epochs , sigma , eta0 = <NUM_LIT> ) : \n '''<STR_LIT>''' \n self . weights = np . zeros ( self . features . n_feats ) \n t = <NUM_LIT:0> \n t0 = <NUM_LIT:1.0> / ( sigma * eta0 ) \n for epoch in range ( n_epochs ) : \n print \"<STR_LIT>\" . format ( epoch + <NUM_LIT:1> ) \n n_mistakes = <NUM_LIT:0> \n n_tokens = <NUM_LIT:0> \n n_instances = <NUM_LIT:0> \n objective = <NUM_LIT:0.0> \n for instance in self . reader . train_instances : \n eta = <NUM_LIT:1.0> / ( sigma * ( t + t0 ) ) \n feats = self . features . create_features ( instance ) \n scores = self . features . compute_scores ( feats , self . weights ) \n marginals , logZ = self . decoder . parse_marginals_nonproj ( scores ) \n self . weights -= eta * sigma * self . weights \n for h in range ( np . size ( marginals , <NUM_LIT:0> ) ) : \n for m in range ( <NUM_LIT:1> , np . size ( marginals , <NUM_LIT:1> ) ) : \n if feats [ h ] [ m ] == None : \n continue \n for f in feats [ h ] [ m ] : \n if f < <NUM_LIT:0> : \n continue \n self . weights [ f ] -= eta * marginals [ h , m ] \n score_corr = <NUM_LIT:0.0> \n for m in range ( <NUM_LIT:1> , np . size ( instance . heads ) ) : \n h = instance . heads [ m ] \n score_corr += scores [ h , m ] \n for f in feats [ h ] [ m ] : \n if f < <NUM_LIT:0> : \n continue \n self . weights [ f ] += eta \n objective += <NUM_LIT:0.5> * sigma * np . dot ( self . weights , self . weights ) - score_corr + logZ \n n_instances += <NUM_LIT:1> \n t += <NUM_LIT:1> \n print \"<STR_LIT>\" . format ( objective / n_instances ) \n def test ( self ) : \n n_mistakes = <NUM_LIT:0> \n n_tokens = <NUM_LIT:0> \n n_instances = <NUM_LIT:0> \n arr_heads_pred = [ ] ; \n for instance in self . reader . test_instances : \n feats = self . features . create_features ( instance ) \n scores = self . features . compute_scores ( feats , self . weights ) \n if self . projective : \n heads_pred = self . decoder . parse_proj ( scores ) \n else : \n heads_pred = self . decoder . parse_nonproj ( scores ) \n for m in range ( np . size ( heads_pred ) ) : \n if heads_pred [ m ] != instance . heads [ m ] : \n for f in feats [ instance . heads [ m ] ] [ m ] : \n if f < <NUM_LIT:0> : \n continue \n for f in feats [ heads_pred [ m ] ] [ m ] : \n if f < <NUM_LIT:0> : \n continue \n n_mistakes += <NUM_LIT:1> \n n_tokens += <NUM_LIT:1> \n n_instances += <NUM_LIT:1> \n arr_heads_pred . append ( heads_pred ) \n print \"<STR_LIT>\" . format ( n_instances , np . double ( n_tokens - n_mistakes ) / np . double ( n_tokens ) ) \n self . writer . save ( self . language , <mask0> ) \n", "gt": "arr_heads_pred"}
{"input": "\n from lxmls . sequences . label_dictionary import * \n import pdb \n class IDFeatures : \n '''<STR_LIT>''' \n def __init__ ( self , dataset ) : \n '''<STR_LIT>''' \n self . feature_dict = LabelDictionary ( ) \n self . feature_list = [ ] \n self . add_features = False \n self . dataset = dataset \n self . node_feature_cache = { } \n self . initial_state_feature_cache = { } \n self . final_state_feature_cache = { } \n self . edge_feature_cache = { } \n def get_num_features ( self ) : \n return len ( self . feature_dict ) \n def build_features ( self ) : \n '''<STR_LIT>''' \n self . add_features = True \n for sequence in self . dataset . seq_list : \n initial_features , transition_features , final_features , emission_features = self . get_sequence_features ( sequence ) \n self . feature_list . append ( [ initial_features , transition_features , final_features , emission_features ] ) \n self . add_features = False \n def get_sequence_features ( self , sequence ) : \n '''<STR_LIT>''' \n emission_features = [ ] \n initial_features = [ ] \n transition_features = [ ] \n final_features = [ ] \n features = [ ] \n features = self . add_initial_features ( sequence , sequence . y [ <NUM_LIT:0> ] , features ) \n initial_features . append ( features ) \n for pos , tag in enumerate ( sequence . y ) : \n features = [ ] \n features = self . add_emission_features ( sequence , pos , sequence . y [ pos ] , features ) \n emission_features . append ( features ) \n if pos > <NUM_LIT:0> : \n prev_tag = sequence . y [ pos - <NUM_LIT:1> ] \n features = [ ] \n features = self . add_transition_features ( sequence , pos - <NUM_LIT:1> , tag , prev_tag , features ) \n transition_features . append ( features ) \n features = [ ] \n features = self . add_final_features ( sequence , sequence . y [ - <NUM_LIT:1> ] , features ) \n final_features . append ( features ) \n return initial_features , transition_features , final_features , emission_features \n def get_emission_features ( self , sequence , pos , y ) : \n all_feat = [ ] \n x = sequence . x [ pos ] \n if ( x not in self . node_feature_cache ) : \n self . node_feature_cache [ x ] = { } \n if ( y not in self . node_feature_cache [ x ] ) : \n node_idx = [ ] \n node_idx = self . add_emission_features ( sequence , pos , y , node_idx ) \n self . node_feature_cache [ x ] [ y ] = node_idx \n idx = self . node_feature_cache [ x ] [ y ] \n all_feat = idx [ : ] \n return all_feat \n def get_transition_features ( self , sequence , pos , y , y_prev ) : \n assert ( pos >= <NUM_LIT:0> and pos < len ( sequence . x ) ) , pdb . set_trace ( ) \n if ( y not in self . edge_feature_cache ) : \n self . edge_feature_cache [ y ] = { } \n if ( y_prev not in self . edge_feature_cache [ y ] ) : \n edge_idx = [ ] \n edge_idx = self . add_transition_features ( sequence , pos , y , y_prev , edge_idx ) \n self . edge_feature_cache [ y ] [ y_prev ] = edge_idx \n return self . edge_feature_cache [ y ] [ y_prev ] \n def get_initial_features ( self , sequence , y ) : \n if ( y not in self . initial_state_feature_cache ) : \n edge_idx = [ ] \n edge_idx = self . add_initial_features ( sequence , y , edge_idx ) \n self . initial_state_feature_cache [ y ] = edge_idx \n return self . initial_state_feature_cache [ y ] \n def get_final_features ( self , sequence , y_prev ) : \n if ( y_prev not in self . final_state_feature_cache ) : \n edge_idx = [ ] \n edge_idx = self . add_final_features ( sequence , y_prev , edge_idx ) \n self . final_state_feature_cache [ y_prev ] = edge_idx \n return self . final_state_feature_cache [ y_prev ] \n def add_initial_features ( self , sequence , y , features ) : \n y_name = self . dataset . y_dict . get_label_name ( y ) \n feat_name = \"<STR_LIT>\" % ( y_name ) \n feat_id = self . add_feature ( feat_name ) \n if ( feat_id != - <NUM_LIT:1> ) : \n features . append ( feat_id ) \n return features \n def add_final_features ( self , sequence , y_prev , features ) : \n y_name = self . dataset . y_dict . get_label_name ( y_prev ) \n feat_name = \"<STR_LIT>\" % ( y_name ) \n feat_id = self . add_feature ( feat_name ) \n if ( feat_id != - <NUM_LIT:1> ) : \n features . append ( feat_id ) \n return features \n def add_emission_features ( self , sequence , pos , y , features ) : \n '''<STR_LIT>''' \n x = sequence . x [ pos ] \n y_name = self . dataset . y_dict . get_label_name ( y ) \n x_name = self . dataset . x_dict . get_label_name ( x ) \n feat_name = \"<STR_LIT>\" % ( x_name , y_name ) \n feat_id = self . add_feature ( feat_name ) \n if feat_id != - <NUM_LIT:1> : \n features . append ( feat_id ) \n return features \n def add_transition_features ( self , sequence , pos , y , y_prev , features ) : \n \"\"\"<STR_LIT>\"\"\" \n assert pos < len ( sequence . x ) - <NUM_LIT:1> , pdb . set_trace ( ) \n y_name = self . dataset . y_dict . get_label_name ( y ) \n y_prev_name = self . dataset . y_dict . get_label_name ( y_prev ) \n feat_name = \"<STR_LIT>\" % ( y_prev_name , y_name ) \n feat_id = self . add_feature ( feat_name ) \n if ( feat_id != - <NUM_LIT:1> ) : \n features . append ( feat_id ) \n return features \n def add_feature ( self , feat_name ) : \n \"\"\"<STR_LIT>\"\"\" \n if ( feat_name in self . feature_dict ) : \n return self . feature_dict [ feat_name ] \n if not self . add_features : \n return - <NUM_LIT:1> \n return self . feature_dict . add ( <mask0> ) \n", "gt": "feat_name"}
{"input": "\n '''<STR_LIT>''' \n import os \n try : \n import ogr \n except ImportError : \n from osgeo import ogr \n line_shp_file = \"<STR_LIT>\" \n line_datasource = ogr . Open ( line_shp_file ) \n driver = ogr . GetDriverByName ( '<STR_LIT>' ) \n point_shp_file = '<STR_LIT>' \n layer_name = '<STR_LIT>' \n if os . path . exists ( point_shp_file ) : \n driver . DeleteDataSource ( point_shp_file ) \n point_datasource = driver . CreateDataSource ( point_shp_file ) \n layer_count = line_datasource . GetLayerCount ( ) \n for each_layer in range ( layer_count ) : \n layer = line_datasource . GetLayerByIndex ( each_layer ) \n srs = layer . GetSpatialRef ( ) \n point_shp_layer = point_datasource . CreateLayer ( layer_name , srs , ogr . wkbPoint ) \n feature_count = layer . GetFeatureCount ( ) \n for each_feature in range ( feature_count ) : \n line_feature = layer . GetFeature ( each_feature ) \n feature_geom = line_feature . GetGeometryRef ( ) \n if feature_geom . GetGeometryName ( ) != '<STR_LIT>' : \n points = feature_geom . GetPoints ( ) \n for point in points : \n point_geom = ogr . Geometry ( ogr . wkbPoint ) \n point_geom . AddPoint ( point [ <NUM_LIT:0> ] , point [ <NUM_LIT:1> ] ) \n point_feature = ogr . Feature ( point_shp_layer . GetLayerDefn ( ) ) \n point_feature . SetGeometry ( point_geom ) \n point_shp_layer . CreateFeature ( <mask0> ) \n", "gt": "point_feature"}
{"input": "\n '''<STR_LIT>''' \n try : \n import ogr \n except ImportError : \n from osgeo import ogr \n latitudes = [ <NUM_LIT:50> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n longitudes = [ <NUM_LIT:100> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n elevation = <NUM_LIT:0> \n points = ogr . Geometry ( ogr . wkbMultiPoint ) \n point_1 = ogr . Geometry ( ogr . wkbPoint ) \n point_1 . AddPoint ( longitudes [ <NUM_LIT:0> ] , latitudes [ <NUM_LIT:0> ] , elevation ) \n points . AddGeometry ( point_1 ) \n point_2 = ogr . Geometry ( ogr . wkbPoint ) \n point_2 . AddPoint ( longitudes [ <NUM_LIT:1> ] , latitudes [ <NUM_LIT:1> ] , elevation ) \n points . AddGeometry ( point_2 ) \n point_3 = ogr . Geometry ( ogr . wkbPoint ) \n point_3 . AddPoint ( longitudes [ <NUM_LIT:2> ] , latitudes [ <NUM_LIT:2> ] , elevation ) \n points . AddGeometry ( point_3 ) \n point_4 = ogr . Geometry ( ogr . wkbPoint ) \n point_4 . AddPoint ( longitudes [ <NUM_LIT:3> ] , latitudes [ <NUM_LIT:3> ] , elevation ) \n points . AddGeometry ( point_4 ) \n point_5 = ogr . Geometry ( ogr . wkbPoint ) \n point_5 . AddPoint ( longitudes [ <NUM_LIT:4> ] , latitudes [ <NUM_LIT:4> ] , elevation ) \n points . AddGeometry ( point_5 ) \n points . ExportToWkt ( ) \n print <mask0> \n", "gt": "points"}
{"input": "\n from __future__ import division \n import numpy as np \n from collections import defaultdict \n import json \n import itertools \n from sklearn import cluster , preprocessing , manifold \n from datetime import datetime \n import sys \n class KeplerMapper ( object ) : \n def __init__ ( self , verbose = <NUM_LIT:2> ) : \n self . verbose = verbose \n self . chunk_dist = [ ] \n self . overlap_dist = [ ] \n self . d = [ ] \n self . nr_cubes = <NUM_LIT:0> \n self . overlap_perc = <NUM_LIT:0> \n self . clusterer = False \n def fit_transform ( self , X , projection = \"<STR_LIT>\" , scaler = preprocessing . MinMaxScaler ( ) ) : \n self . scaler = scaler \n self . projection = str ( projection ) \n if str ( type ( projection ) ) [ <NUM_LIT:1> : <NUM_LIT:6> ] == \"<STR_LIT:class>\" : \n reducer = projection \n if self . verbose > <NUM_LIT:0> : \n try : \n projection . set_params ( ** { \"<STR_LIT>\" : self . verbose } ) \n except : \n pass \n print ( \"<STR_LIT>\" % str ( projection ) ) \n X = reducer . fit_transform ( X ) \n if isinstance ( projection , str ) : \n if self . verbose > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % ( projection ) ) \n if projection == \"<STR_LIT>\" : \n X = np . sum ( X , axis = <NUM_LIT:1> ) . reshape ( ( X . shape [ <NUM_LIT:0> ] , <NUM_LIT:1> ) ) \n if projection == \"<STR_LIT>\" : \n X = np . mean ( X , axis = <NUM_LIT:1> ) . reshape ( ( X . shape [ <NUM_LIT:0> ] , <NUM_LIT:1> ) ) \n if projection == \"<STR_LIT>\" : \n X = np . median ( X , axis = <NUM_LIT:1> ) . reshape ( ( X . shape [ <NUM_LIT:0> ] , <NUM_LIT:1> ) ) \n if projection == \"<STR_LIT>\" : \n X = np . max ( X , axis = <NUM_LIT:1> ) . reshape ( ( X . shape [ <NUM_LIT:0> ] , <NUM_LIT:1> ) ) \n if projection == \"<STR_LIT>\" : \n X = np . min ( X , axis = <NUM_LIT:1> ) . reshape ( ( X . shape [ <NUM_LIT:0> ] , <NUM_LIT:1> ) ) \n if projection == \"<STR_LIT>\" : \n X = np . std ( X , axis = <NUM_LIT:1> ) . reshape ( ( X . shape [ <NUM_LIT:0> ] , <NUM_LIT:1> ) ) \n if projection == \"<STR_LIT>\" : \n X_mean = np . mean ( X , axis = <NUM_LIT:0> ) \n X = np . sum ( np . sqrt ( ( X - X_mean ) ** <NUM_LIT:2> ) , axis = <NUM_LIT:1> ) . reshape ( ( X . shape [ <NUM_LIT:0> ] , <NUM_LIT:1> ) ) \n if isinstance ( projection , list ) : \n if self . verbose > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % ( str ( projection ) ) ) \n X = X [ : , np . array ( projection ) ] \n if scaler is not None : \n if self . verbose > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % str ( scaler ) ) \n X = scaler . fit_transform ( X ) \n return X \n def map ( self , projected_X , inverse_X = None , clusterer = cluster . DBSCAN ( eps = <NUM_LIT:0.5> , min_samples = <NUM_LIT:3> ) , nr_cubes = <NUM_LIT:10> , overlap_perc = <NUM_LIT:0.1> ) : \n start = datetime . now ( ) \n def cube_coordinates_all ( nr_cubes , nr_dimensions ) : \n l = [ ] \n for x in range ( nr_cubes ) : \n l += [ x ] * nr_dimensions \n return [ np . array ( list ( f ) ) for f in sorted ( set ( itertools . permutations ( l , nr_dimensions ) ) ) ] \n nodes = defaultdict ( list ) \n links = defaultdict ( list ) \n complex = { } \n self . nr_cubes = nr_cubes \n self . clusterer = clusterer \n self . overlap_perc = overlap_perc \n if self . verbose > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % ( str ( projected_X . shape ) ) ) \n if inverse_X is None : \n inverse_X = projected_X \n self . chunk_dist = ( np . max ( projected_X , axis = <NUM_LIT:0> ) - np . min ( projected_X , axis = <NUM_LIT:0> ) ) / nr_cubes \n self . overlap_dist = self . overlap_perc * self . chunk_dist \n self . d = np . min ( projected_X , axis = <NUM_LIT:0> ) \n di = np . array ( [ x for x in range ( projected_X . shape [ <NUM_LIT:1> ] ) ] ) \n ids = np . array ( [ x for x in range ( projected_X . shape [ <NUM_LIT:0> ] ) ] ) \n projected_X = np . c_ [ ids , projected_X ] \n inverse_X = np . c_ [ ids , inverse_X ] \n if self . verbose > <NUM_LIT:0> : \n total_cubes = len ( cube_coordinates_all ( nr_cubes , projected_X . shape [ <NUM_LIT:1> ] ) ) \n print ( \"<STR_LIT>\" % total_cubes ) \n for i , coor in enumerate ( cube_coordinates_all ( nr_cubes , di . shape [ <NUM_LIT:0> ] ) ) : \n hypercube = projected_X [ np . invert ( np . any ( ( projected_X [ : , di + <NUM_LIT:1> ] >= self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) & \n ( projected_X [ : , di + <NUM_LIT:1> ] < self . d [ di ] + ( coor * self . chunk_dist [ di ] ) + self . chunk_dist [ di ] + self . overlap_dist [ di ] ) == False , axis = <NUM_LIT:1> ) ) ] \n if self . verbose > <NUM_LIT:1> : \n print ( \"<STR_LIT>\" % \n ( hypercube . shape [ <NUM_LIT:0> ] , i , total_cubes , self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) ) \n if hypercube . shape [ <NUM_LIT:0> ] > <NUM_LIT:0> : \n inverse_x = inverse_X [ [ int ( nn ) for nn in hypercube [ : , <NUM_LIT:0> ] ] ] \n clusterer . fit ( inverse_x [ : , <NUM_LIT:1> : ] ) \n if self . verbose > <NUM_LIT:1> : \n print ( \"<STR_LIT>\" % ( np . unique ( clusterer . labels_ [ clusterer . labels_ > - <NUM_LIT:1> ] ) . shape [ <NUM_LIT:0> ] , i ) ) \n for a in np . c_ [ hypercube [ : , <NUM_LIT:0> ] , clusterer . labels_ ] : \n if a [ <NUM_LIT:1> ] != - <NUM_LIT:1> : \n cluster_id = str ( coor [ <NUM_LIT:0> ] ) + \"<STR_LIT:_>\" + str ( i ) + \"<STR_LIT:_>\" + str ( a [ <NUM_LIT:1> ] ) + \"<STR_LIT:_>\" + str ( coor ) + \"<STR_LIT:_>\" + str ( self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) \n nodes [ cluster_id ] . append ( int ( a [ <NUM_LIT:0> ] ) ) \n else : \n if self . verbose > <NUM_LIT:1> : \n print ( \"<STR_LIT>\" % ( i ) ) \n candidates = itertools . combinations ( nodes . keys ( ) , <NUM_LIT:2> ) \n for candidate in candidates : \n if len ( nodes [ candidate [ <NUM_LIT:0> ] ] + nodes [ candidate [ <NUM_LIT:1> ] ] ) != len ( set ( nodes [ candidate [ <NUM_LIT:0> ] ] + nodes [ candidate [ <NUM_LIT:1> ] ] ) ) : \n links [ candidate [ <NUM_LIT:0> ] ] . append ( candidate [ <NUM_LIT:1> ] ) \n if self . verbose > <NUM_LIT:0> : \n nr_links = <NUM_LIT:0> \n for k in links : \n nr_links += len ( links [ k ] ) \n print ( \"<STR_LIT>\" % ( nr_links , len ( nodes ) , str ( datetime . now ( ) - start ) ) ) \n complex [ \"<STR_LIT>\" ] = nodes \n complex [ \"<STR_LIT>\" ] = links \n complex [ \"<STR_LIT>\" ] = self . projection \n return complex \n def visualize ( self , complex , color_function = \"<STR_LIT>\" , path_html = \"<STR_LIT>\" , title = \"<STR_LIT>\" , \n graph_link_distance = <NUM_LIT:30> , graph_gravity = <NUM_LIT:0.1> , graph_charge = - <NUM_LIT> , custom_tooltips = None , width_html = <NUM_LIT:0> , \n height_html = <NUM_LIT:0> , show_tooltips = True , show_title = True , show_meta = True ) : \n json_s = { } \n json_s [ \"<STR_LIT>\" ] = [ ] \n json_s [ \"<STR_LIT>\" ] = [ ] \n k2e = { } \n for e , k in enumerate ( complex [ \"<STR_LIT>\" ] ) : \n if custom_tooltips is not None : \n tooltip_s = \"<STR_LIT>\" % k + \"<STR_LIT:U+0020>\" . join ( [ str ( f ) for f in custom_tooltips [ complex [ \"<STR_LIT>\" ] [ k ] ] ] ) \n if color_function == \"<STR_LIT>\" : \n tooltip_i = int ( ( ( sum ( [ f for f in custom_tooltips [ complex [ \"<STR_LIT>\" ] [ k ] ] ] ) / len ( custom_tooltips [ complex [ \"<STR_LIT>\" ] [ k ] ] ) ) * <NUM_LIT:30> ) ) \n json_s [ \"<STR_LIT>\" ] . append ( { \"<STR_LIT:name>\" : str ( k ) , \"<STR_LIT>\" : tooltip_s , \"<STR_LIT>\" : <NUM_LIT:2> * int ( np . log ( len ( complex [ \"<STR_LIT>\" ] [ k ] ) ) ) , \"<STR_LIT>\" : str ( tooltip_i ) } ) \n else : \n json_s [ \"<STR_LIT>\" ] . append ( { \"<STR_LIT:name>\" : str ( k ) , \"<STR_LIT>\" : tooltip_s , \"<STR_LIT>\" : <NUM_LIT:2> * int ( np . log ( len ( complex [ \"<STR_LIT>\" ] [ k ] ) ) ) , \"<STR_LIT>\" : str ( k . split ( \"<STR_LIT:_>\" ) [ <NUM_LIT:0> ] ) } ) \n else : \n tooltip_s = \"<STR_LIT>\" % ( k , len ( complex [ \"<STR_LIT>\" ] [ k ] ) ) \n json_s [ \"<STR_LIT>\" ] . append ( { \"<STR_LIT:name>\" : str ( k ) , \"<STR_LIT>\" : tooltip_s , \"<STR_LIT>\" : <NUM_LIT:2> * int ( np . log ( len ( complex [ \"<STR_LIT>\" ] [ k ] ) ) ) , \"<STR_LIT>\" : str ( k . split ( \"<STR_LIT:_>\" ) [ <NUM_LIT:0> ] ) } ) \n k2e [ k ] = e \n for k in complex [ \"<STR_LIT>\" ] : \n for link in complex [ \"<STR_LIT>\" ] [ k ] : \n json_s [ \"<STR_LIT>\" ] . append ( { \"<STR_LIT:source>\" : k2e [ k ] , \"<STR_LIT:target>\" : k2e [ link ] , \"<STR_LIT:value>\" : <NUM_LIT:1> } ) \n if width_html == <NUM_LIT:0> : \n width_css = \"<STR_LIT>\" \n width_js = '<STR_LIT>' \n else : \n width_css = \"<STR_LIT>\" % width_html \n width_js = \"<STR_LIT:%s>\" % width_html \n if height_html == <NUM_LIT:0> : \n height_css = \"<STR_LIT>\" \n height_js = '<STR_LIT>' \n else : \n height_css = \"<STR_LIT>\" % height_html \n height_js = \"<STR_LIT:%s>\" % height_html \n if show_tooltips == False : \n tooltips_display = \"<STR_LIT>\" \n else : \n tooltips_display = \"<STR_LIT>\" \n if show_meta == False : \n meta_display = \"<STR_LIT>\" \n else : \n meta_display = \"<STR_LIT>\" \n if show_title == False : \n title_display = \"<STR_LIT>\" \n else : \n title_display = \"<STR_LIT>\" \n with open ( path_html , \"<STR_LIT:wb>\" ) as outfile : \n html = \"\"\"<STR_LIT>\"\"\" % ( title , width_css , height_css , title_display , meta_display , tooltips_display , title , complex [ \"<STR_LIT>\" ] , self . nr_cubes , self . overlap_perc * <NUM_LIT:100> , color_function , complex [ \"<STR_LIT>\" ] , str ( self . clusterer ) , str ( self . scaler ) , width_js , height_js , graph_charge , graph_link_distance , graph_gravity , json . dumps ( json_s ) ) \n outfile . write ( html . encode ( \"<STR_LIT:utf-8>\" ) ) \n if self . verbose > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % <mask0> ) \n", "gt": "path_html"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n import os \n import unittest \n from SystemConfiguration import * \n class SCPreferences ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n proxy_protocols = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n session = None \n def __init__ ( self ) : \n super ( SCPreferences , self ) . __init__ ( ) \n self . session = SCPreferencesCreate ( None , \"<STR_LIT>\" , None ) \n def save ( self ) : \n if not self . session : \n return \n if not SCPreferencesCommitChanges ( self . session ) : \n raise RuntimeError ( \"<STR_LIT>\" ) \n if not SCPreferencesApplyChanges ( self . session ) : \n raise RuntimeError ( \"<STR_LIT>\" ) \n def set_proxy ( self , enable = True , protocol = \"<STR_LIT>\" , server = \"<STR_LIT:localhost>\" , port = <NUM_LIT> ) : \n new_settings = SCPreferencesPathGetValue ( self . session , u'<STR_LIT>' ) \n for interface in new_settings : \n new_settings [ interface ] [ '<STR_LIT>' ] [ \"<STR_LIT>\" % protocol ] = <NUM_LIT:1> if enable else <NUM_LIT:0> \n if enable : \n new_settings [ interface ] [ '<STR_LIT>' ] [ '<STR_LIT>' % protocol ] = int ( port ) \n new_settings [ interface ] [ '<STR_LIT>' ] [ '<STR_LIT>' % protocol ] = server \n SCPreferencesPathSetValue ( self . session , u'<STR_LIT>' , new_settings ) \n class SCPreferencesTests ( unittest . TestCase ) : \n def setUp ( self ) : \n raise RuntimeError ( \"<STR_LIT>\" ) \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import sublime , sublime_plugin , os , re \n class StyleSheetSetup : \n def __init__ ( self , extensions , regex , partials = None , index = None ) : \n if partials is None : \n self . partials = False \n else : \n self . partials = partials \n if index is None : \n self . index = False \n else : \n self . index = index \n self . extensions = extensions \n self . regex = regex \n class ListStylesheetVariables ( sublime_plugin . TextCommand ) : \n def run ( self , edit ) : \n settings = sublime . load_settings ( '<STR_LIT>' ) \n handle_imports = settings . get ( \"<STR_LIT>\" ) \n read_all_views = settings . get ( \"<STR_LIT>\" ) \n less_setup = StyleSheetSetup ( ( b'<STR_LIT>' , b'<STR_LIT>' ) , \"<STR_LIT>\" ) \n sass_setup = StyleSheetSetup ( ( b'<STR_LIT>' , b'<STR_LIT>' ) , \"<STR_LIT>\" , True ) \n stylus_setup = StyleSheetSetup ( ( b'<STR_LIT>' , ) , \"<STR_LIT>\" , False , True ) \n sass_erb_setup = StyleSheetSetup ( ( b'<STR_LIT>' , b'<STR_LIT>' ) , \"<STR_LIT>\" , True ) \n setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n chosen_setup = None \n self . edit = edit \n fn = self . view . file_name ( ) . encode ( \"<STR_LIT>\" ) \n for setup in setups : \n for ext in setup . extensions : \n if fn . endswith ( ext ) : \n chosen_setup = setup \n if chosen_setup == None : \n return \n imports = [ ] \n imported_vars = [ ] \n compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n if handle_imports : \n self . view . find_all ( \"<STR_LIT>\" , <NUM_LIT:0> , \"<STR_LIT>\" , imports ) \n file_dir = os . path . dirname ( fn ) . decode ( \"<STR_LIT:utf-8>\" ) \n for i , filename in enumerate ( imports ) : \n has_extension = False \n for ext in chosen_setup . extensions : \n if filename . endswith ( ext . decode ( \"<STR_LIT:utf-8>\" ) ) : \n has_extension = True \n if has_extension == False : \n for ext in chosen_setup . extensions : \n ext = ext . decode ( \"<STR_LIT:utf-8>\" ) \n if os . path . isfile ( os . path . normpath ( file_dir + '<STR_LIT:/>' + filename + ext ) ) : \n filename += ext \n break \n if chosen_setup . partials : \n fn_split = os . path . split ( filename ) \n partial_filename = fn_split [ <NUM_LIT:0> ] + \"<STR_LIT>\" + fn_split [ <NUM_LIT:1> ] \n if os . path . isfile ( os . path . normpath ( file_dir + partial_filename + ext ) ) : \n filename = \"<STR_LIT:_>\" + filename + ext \n break \n if chosen_setup . index and os . path . isfile ( os . path . normpath ( file_dir + \"<STR_LIT:/>\" + filename + \"<STR_LIT>\" + ext ) ) : \n filename += \"<STR_LIT>\" + ext \n try : \n f = open ( os . path . normpath ( file_dir + '<STR_LIT:/>' + filename ) , '<STR_LIT:r>' ) \n contents = f . read ( ) \n f . close ( ) \n m = re . findall ( compiled_regex , contents ) \n imported_vars = imported_vars + m \n except : \n print ( '<STR_LIT>' + filename ) \n imported_vars = [ list ( item ) for item in imported_vars ] \n self . variables = [ ] \n vars_from_views = [ ] \n if read_all_views : \n for view in self . view . window ( ) . views ( ) : \n viewfn = self . view . file_name ( ) . encode ( \"<STR_LIT:utf-8>\" ) \n compatible_view = False \n for ext in chosen_setup . extensions : \n if viewfn . endswith ( ext ) : \n viewvars = [ ] \n view . find_all ( chosen_setup . regex , <NUM_LIT:0> , \"<STR_LIT>\" , viewvars ) \n vars_from_views += viewvars \n break ; \n else : \n self . view . find_all ( chosen_setup . regex , <NUM_LIT:0> , \"<STR_LIT>\" , self . variables ) \n self . variables += vars_from_views \n self . variables = list ( set ( self . variables ) ) \n for i , val in enumerate ( self . variables ) : \n self . variables [ i ] = val . split ( \"<STR_LIT:|>\" ) \n self . variables = imported_vars + self . variables \n self . variables . sort ( ) \n self . view . window ( ) . show_quick_panel ( self . variables , self . insert_variable , sublime . MONOSPACE_FONT ) \n def insert_variable ( self , choice ) : \n if choice == - <NUM_LIT:1> : \n return \n self . view . run_command ( '<STR_LIT>' , { '<STR_LIT:string>' : self . variables [ choice ] [ <NUM_LIT:0> ] } ) \n class InsertText ( sublime_plugin . TextCommand ) : \n def run ( self , edit , string = '<STR_LIT>' ) : \n for selection in self . view . sel ( ) : \n self . view . insert ( edit , selection . begin ( ) , <mask0> ) \n", "gt": "string"}
{"input": "\n from driver_base import DriverBase \n import socket \n import sys \n import time \n import os \n os . sys . path . insert ( <NUM_LIT:0> , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n import log \n class CMDTYPE : \n SETUP_DATA = <NUM_LIT:1> \n PIXEL_DATA = <NUM_LIT:2> \n BRIGHTNESS = <NUM_LIT:3> \n class RETURN_CODES : \n SUCCESS = <NUM_LIT:255> \n ERROR = <NUM_LIT:0> \n ERROR_SIZE = <NUM_LIT:1> \n ERROR_UNSUPPORTED = <NUM_LIT:2> \n class DriverNetwork ( DriverBase ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , num = <NUM_LIT:0> , width = <NUM_LIT:0> , height = <NUM_LIT:0> , host = \"<STR_LIT:localhost>\" , port = <NUM_LIT> ) : \n super ( DriverNetwork , self ) . __init__ ( num , width , height ) \n self . _host = host \n self . _port = port \n def _generateHeader ( self , cmd , size ) : \n packet = bytearray ( ) \n packet . append ( cmd ) \n packet . append ( size & <NUM_LIT> ) \n packet . append ( size >> <NUM_LIT:8> ) \n return packet \n def _connect ( self ) : \n try : \n s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n s . connect ( ( self . _host , self . _port ) ) \n return s \n except socket . gaierror : \n error = \"<STR_LIT>\" . format ( \n self . _host ) \n log . error ( error ) \n raise IOError ( error ) \n def update ( self , data ) : \n try : \n s = self . _connect ( ) \n count = self . bufByteCount \n packet = self . _generateHeader ( CMDTYPE . PIXEL_DATA , count ) \n packet . extend ( data ) \n s . sendall ( packet ) \n resp = ord ( s . recv ( <NUM_LIT:1> ) ) \n s . close ( ) \n if resp != RETURN_CODES . SUCCESS : \n log . warning ( \"<STR_LIT>\" , resp ) \n except Exception as e : \n log . exception ( e ) \n error = \"<STR_LIT>\" \n log . error ( error ) \n raise IOError ( error ) \n def setMasterBrightness ( self , brightness ) : \n packet = self . _generateHeader ( CMDTYPE . BRIGHTNESS , <NUM_LIT:1> ) \n packet . append ( brightness ) \n s = self . _connect ( ) \n s . sendall ( packet ) \n resp = ord ( s . recv ( <NUM_LIT:1> ) ) \n if resp != RETURN_CODES . SUCCESS : \n return False \n else : \n return True \n MANIFEST = [ \n { \n \"<STR_LIT:id>\" : \"<STR_LIT>\" , \n \"<STR_LIT:class>\" : DriverNetwork , \n \"<STR_LIT:type>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : [ { \n \"<STR_LIT:id>\" : \"<STR_LIT>\" , \n \"<STR_LIT:label>\" : \"<STR_LIT>\" , \n \"<STR_LIT:type>\" : \"<STR_LIT:int>\" , \n \"<STR_LIT:default>\" : <NUM_LIT:0> , \n \"<STR_LIT>\" : <NUM_LIT:0> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , { \n \"<STR_LIT:id>\" : \"<STR_LIT:width>\" , \n \"<STR_LIT:label>\" : \"<STR_LIT>\" , \n \"<STR_LIT:type>\" : \"<STR_LIT:int>\" , \n \"<STR_LIT:default>\" : <NUM_LIT:0> , \n \"<STR_LIT>\" : <NUM_LIT:0> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , { \n \"<STR_LIT:id>\" : \"<STR_LIT>\" , \n \"<STR_LIT:label>\" : \"<STR_LIT>\" , \n \"<STR_LIT:type>\" : \"<STR_LIT:int>\" , \n \"<STR_LIT:default>\" : <NUM_LIT:0> , \n \"<STR_LIT>\" : <NUM_LIT:0> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , { \n \"<STR_LIT:id>\" : \"<STR_LIT:host>\" , \n \"<STR_LIT:label>\" : \"<STR_LIT>\" , \n \"<STR_LIT:type>\" : \"<STR_LIT:str>\" , \n \"<STR_LIT:default>\" : \"<STR_LIT:localhost>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , { \n \"<STR_LIT:id>\" : \"<STR_LIT:port>\" , \n \"<STR_LIT:label>\" : \"<STR_LIT>\" , \n \"<STR_LIT:type>\" : \"<STR_LIT:int>\" , \n <mask0> : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } ] \n } \n ] \n", "gt": "\"<STR_LIT:default>\""}
{"input": "\n _ID = '<STR_LIT>' \n API = '<STR_LIT>' \n API_CALL_TIMEOUT = '<STR_LIT>' \n API_VERSION = '<STR_LIT>' \n API_VERSION_MAXIMUM = '<STR_LIT>' \n API_VERSION_MINIMUM = '<STR_LIT>' \n AREA = '<STR_LIT>' \n AREA_MAX = '<STR_LIT>' \n BBOX = '<STR_LIT>' \n BOUNDS = '<STR_LIT>' \n CFGSLAB = '<STR_LIT>' \n CFGVERSION = <NUM_LIT:1> \n CHANGESET = '<STR_LIT>' \n CHANGESETS = '<STR_LIT>' \n CHANGESETS_INLINE_SIZE = '<STR_LIT>' \n CHANGESETS_PER_SLAB = '<STR_LIT>' \n CHANGESETS_MAX = '<STR_LIT>' \n CONFIGURATION_SCHEMA_VERSION = '<STR_LIT>' \n CONTENT_TYPE = '<STR_LIT:Content-Type>' \n COUCHDB = '<STR_LIT>' \n DATASTORE = '<STR_LIT>' \n DATASTORE_BACKEND = '<STR_LIT>' \n DATASTORE_CONFIG = '<STR_LIT>' \n DATASTORE_ENCODING = '<STR_LIT>' \n DBHOST = '<STR_LIT>' \n DBJOB_ADDELEM = '<STR_LIT>' \n DBJOB_QUIT = '<STR_LIT>' \n DBNAME = '<STR_LIT>' \n DBNAME_SUFFIXES = '<STR_LIT>' \n DBPORT = '<STR_LIT>' \n DBURL = '<STR_LIT>' \n DEFAULT = '<STR_LIT>' \n ELEMENT = '<STR_LIT>' \n FRONT_END = '<STR_LIT>' \n GENERATOR = '<STR_LIT>' \n GEODOC = '<STR_LIT>' \n GEODOC_LRU_SIZE = '<STR_LIT>' \n GEODOC_LRU_THREADS = '<STR_LIT>' \n GEOHASH_LENGTH = '<STR_LIT>' \n ID = '<STR_LIT:id>' \n JSON = '<STR_LIT>' \n K = '<STR_LIT:k>' \n LAT = '<STR_LIT>' \n LAT_MAX = + <NUM_LIT> \n LAT_MIN = - <NUM_LIT> \n LON = '<STR_LIT>' \n LON_MAX = + <NUM_LIT> \n LON_MIN = - <NUM_LIT> \n MAXIMUM = '<STR_LIT>' \n MAXIMUM_ELEMENTS = '<STR_LIT>' \n MAXGHLAT = <NUM_LIT> \n MAXLAT = '<STR_LIT>' \n MAXLON = '<STR_LIT>' \n MEMBASE = '<STR_LIT>' \n MEMBASE_MAX_VALUE_LENGTH = <NUM_LIT:20> * <NUM_LIT> * <NUM_LIT> \n MEMBER = '<STR_LIT>' \n MEMBERS = '<STR_LIT>' \n MINIMUM = '<STR_LIT>' \n MINLAT = '<STR_LIT>' \n MINLON = '<STR_LIT>' \n ND = '<STR_LIT>' \n NODE = '<STR_LIT>' \n NODES = '<STR_LIT>' \n NODES_INLINE_SIZE = '<STR_LIT>' \n NODES_PER_SLAB = '<STR_LIT>' \n OSM = '<STR_LIT>' \n PER_PAGE = '<STR_LIT>' \n PORT = '<STR_LIT:port>' \n PROJECT_DOC = '<STR_LIT>' \n PROTOBUF = '<STR_LIT>' \n REF = '<STR_LIT>' \n REFERENCES = '<STR_LIT>' \n RELATION = '<STR_LIT>' \n RELATIONS = '<STR_LIT>' \n RELATIONS_INLINE_SIZE = '<STR_LIT>' \n RELATIONS_PER_SLAB = '<STR_LIT>' \n ROLE = '<STR_LIT>' \n SCALE_FACTOR = '<STR_LIT>' \n SECONDS = '<STR_LIT>' \n SERVER_NAME = '<STR_LIT>' \n SERVER_VERSION = '<STR_LIT>' \n SLAB_INDIRECT = <NUM_LIT:1> \n SLAB_INLINE = <NUM_LIT:0> \n SLAB_LRU_SIZE = '<STR_LIT>' \n SLAB_LRU_THREADS = '<STR_LIT>' \n SLAB_NOT_PRESENT = <NUM_LIT:2> \n SOURCE_REPOSITORY = '<STR_LIT>' \n STATUS = '<STR_LIT:status>' \n TAG = '<STR_LIT>' \n TAGS = '<STR_LIT>' \n TEXT_XML = '<STR_LIT>' \n TIMEOUT = '<STR_LIT>' \n TRACEPOINTS = '<STR_LIT>' \n TRACEPOINTS_PER_PAGE = '<STR_LIT>' \n TYPE = '<STR_LIT:type>' \n UTF8 = '<STR_LIT:utf-8>' \n V = '<STR_LIT:v>' \n VERSION = <mask0> \n WAY = '<STR_LIT>' \n WAYS = '<STR_LIT>' \n WAYS_INLINE_SIZE = '<STR_LIT>' \n WAYS_PER_SLAB = '<STR_LIT>' \n WAYNODES = '<STR_LIT>' \n WAYNODES_MAX = '<STR_LIT>' \n", "gt": "'<STR_LIT:version>'"}
{"input": "\n import webapp2 \n from urllib import urlencode \n import json , urllib2 \n from secret import client_id , client_secret \n import config \n class AuthRedirector ( webapp2 . RequestHandler ) : \n def get ( self ) : \n args = self . request . GET \n args [ \"<STR_LIT>\" ] = client_id \n args [ \"<STR_LIT>\" ] = config . auth_redir_uri \n url = \"<STR_LIT>\" + urlencode ( args ) \n self . response . location = url \n self . response . status_int = <NUM_LIT> \n def query_json ( url , data ) : \n \"\"\"<STR_LIT>\"\"\" \n if not ( data is str ) : \n data = urlencode ( data ) \n try : \n return json . loads ( urllib2 . urlopen ( url , data ) . read ( ) ) \n except urllib2 . HTTPError as e : \n return json . loads ( e . read ( ) ) \n def json_compactify ( data ) : \n return json . dumps ( data , separators = ( '<STR_LIT:U+002C>' , '<STR_LIT::>' ) ) \n class AuthCallback ( webapp2 . RequestHandler ) : \n \"\"\"<STR_LIT>\"\"\" \n def get ( self ) : \n state = self . request . get ( \"<STR_LIT:state>\" ) \n code = self . request . get ( \"<STR_LIT:code>\" ) \n error = self . request . get ( \"<STR_LIT:error>\" ) \n q = { \n \"<STR_LIT:code>\" : code , \n \"<STR_LIT>\" : client_id , \n \"<STR_LIT>\" : client_secret , \n \"<STR_LIT>\" : config . auth_redir_uri , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n result = query_json ( \"<STR_LIT>\" , q ) \n url = ( config . auth_success_page if \"<STR_LIT>\" in result \n else config . auth_failure_page ) + \"<STR_LIT:#>\" + urlencode ( result ) \n self . response . location = url \n self . response . status_int = <NUM_LIT> \n class AuthRefresh ( webapp2 . RequestHandler ) : \n \"\"\"<STR_LIT>\"\"\" \n def get ( self ) : \n refresh_token = self . request . get ( \"<STR_LIT>\" ) \n if not refresh_token : \n self . response . status_int = <NUM_LIT> \n return \n q = { \n \"<STR_LIT>\" : refresh_token , \n \"<STR_LIT>\" : client_id , \n \"<STR_LIT>\" : client_secret , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n result = query_json ( \"<STR_LIT>\" , q ) \n self . response . headers [ '<STR_LIT:Content-Type>' ] = \"<STR_LIT>\" \n self . response . write ( json_compactify ( result ) ) \n application = webapp2 . WSGIApplication ( [ \n ( '<STR_LIT>' , AuthRedirector ) , \n ( '<STR_LIT>' , AuthCallback ) , \n ( '<STR_LIT>' , AuthRefresh ) , \n ] , debug = <mask0> ) \n", "gt": "True"}
{"input": "\n from __future__ import unicode_literals \n from django . db import migrations , models \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ] \n operations = [ \n migrations . AddField ( \n model_name = '<STR_LIT>' , \n name = '<STR_LIT>' , \n field = models . EmailField ( help_text = '<STR_LIT>' , max_length = <NUM_LIT> , null = True , verbose_name = '<STR_LIT>' , blank = True ) , \n preserve_default = <mask0> , \n ) , \n ] \n", "gt": "True"}
{"input": "\n from __future__ import print_function \n __author__ = \"<STR_LIT>\" \n __email__ = \"<STR_LIT>\" \n \"\"\"<STR_LIT>\"\"\" \n from debug import Debuggable \n import sys \n from difflib import SequenceMatcher \n import locale \n class Interactive ( Debuggable ) : \n def __init__ ( self , debug ) : \n self . debug = debug \n Debuggable . __init__ ( self , '<STR_LIT>' ) \n self . COLOR_ESCAPE = \"<STR_LIT>\" \n self . DARK_COLORS = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n self . LIGHT_COLORS = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n self . RESET_COLOR = self . COLOR_ESCAPE + \"<STR_LIT>\" \n def input_options ( self , options , require = False , prompt = None , fallback_prompt = None , \n numrange = None , default = None , max_width = <NUM_LIT> ) : \n \"\"\"<STR_LIT>\"\"\" \n letters = { } \n display_letters = [ ] \n capitalized = [ ] \n first = True \n for option in options : \n for letter in option : \n if letter . isalpha ( ) and letter . upper ( ) == letter : \n found_letter = letter \n break \n else : \n for letter in option : \n if not letter . isalpha ( ) : \n continue \n if letter not in letters : \n found_letter = letter \n break \n else : \n raise ValueError ( '<STR_LIT>' ) \n letters [ found_letter . lower ( ) ] = option \n index = option . index ( found_letter ) \n if not require and ( ( default is None and not numrange and first ) or \n ( isinstance ( default , basestring ) and \n found_letter . lower ( ) == default . lower ( ) ) ) : \n show_letter = '<STR_LIT>' % found_letter . upper ( ) \n is_default = True \n else : \n show_letter = found_letter . upper ( ) \n is_default = False \n show_letter = self . colorize ( '<STR_LIT>' if is_default else '<STR_LIT>' , \n show_letter ) \n capitalized . append ( \n option [ : index ] + show_letter + option [ index + <NUM_LIT:1> : ] \n ) \n display_letters . append ( found_letter . upper ( ) ) \n first = False \n if require : \n default = None \n elif default is None : \n if numrange : \n default = numrange [ <NUM_LIT:0> ] \n else : \n default = display_letters [ <NUM_LIT:0> ] . lower ( ) \n if not prompt : \n prompt_parts = [ ] \n prompt_part_lengths = [ ] \n if numrange : \n if isinstance ( default , int ) : \n default_name = str ( default ) \n default_name = self . colorize ( '<STR_LIT>' , default_name ) \n tmpl = '<STR_LIT>' \n prompt_parts . append ( tmpl % default_name ) \n prompt_part_lengths . append ( len ( tmpl % str ( default ) ) ) \n else : \n prompt_parts . append ( '<STR_LIT>' ) \n prompt_part_lengths . append ( len ( prompt_parts [ - <NUM_LIT:1> ] ) ) \n prompt_parts += capitalized \n prompt_part_lengths += [ len ( s ) for s in options ] \n prompt = '<STR_LIT>' \n line_length = <NUM_LIT:0> \n for i , ( part , length ) in enumerate ( zip ( prompt_parts , \n prompt_part_lengths ) ) : \n if i == len ( prompt_parts ) - <NUM_LIT:1> : \n part += '<STR_LIT:?>' \n else : \n part += '<STR_LIT:U+002C>' \n length += <NUM_LIT:1> \n if line_length + length + <NUM_LIT:1> > max_width : \n prompt += '<STR_LIT:\\n>' \n line_length = <NUM_LIT:0> \n if line_length != <NUM_LIT:0> : \n part = '<STR_LIT:U+0020>' + part \n length += <NUM_LIT:1> \n prompt += part \n line_length += length \n if not fallback_prompt : \n fallback_prompt = '<STR_LIT>' \n if numrange : \n fallback_prompt += '<STR_LIT>' % numrange \n fallback_prompt += '<STR_LIT:U+002CU+0020>' . join ( display_letters ) + '<STR_LIT::>' \n resp = self . input_ ( prompt ) \n while True : \n resp = resp . strip ( ) . lower ( ) \n if default is not None and not resp : \n resp = default \n if numrange : \n try : \n resp = int ( resp ) \n except ValueError : \n pass \n else : \n low , high = numrange \n if low <= resp <= high : \n return resp \n else : \n resp = None \n if resp : \n resp = resp [ <NUM_LIT:0> ] \n if resp in letters : \n return resp \n resp = self . input_ ( fallback_prompt ) \n def input_ ( self , prompt = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if prompt : \n if isinstance ( prompt , unicode ) : \n prompt = prompt . encode ( self . _encoding ( ) , '<STR_LIT:replace>' ) \n print ( prompt , end = '<STR_LIT:U+0020>' ) \n try : \n resp = raw_input ( ) \n except EOFError : \n self . debug . print_debug ( '<STR_LIT>' ) \n return resp . decode ( sys . stdin . encoding or '<STR_LIT:utf8>' , '<STR_LIT:ignore>' ) \n def _encoding ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n return locale . getdefaultlocale ( ) [ <NUM_LIT:1> ] or '<STR_LIT:utf8>' \n except ValueError : \n return '<STR_LIT:utf8>' \n def _colorize ( self , color , text ) : \n \"\"\"<STR_LIT>\"\"\" \n if color in self . DARK_COLORS : \n escape = self . COLOR_ESCAPE + \"<STR_LIT>\" % ( self . DARK_COLORS . index ( color ) + <NUM_LIT:30> ) \n elif color in self . LIGHT_COLORS : \n escape = self . COLOR_ESCAPE + \"<STR_LIT>\" % ( self . LIGHT_COLORS . index ( color ) + <NUM_LIT:30> ) \n else : \n raise ValueError ( '<STR_LIT>' , color ) \n return escape + text + self . RESET_COLOR \n def colorize ( self , color , text ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _colorize ( color , text ) \n def _colordiff ( self , a , b , highlight = '<STR_LIT>' , minor_highlight = '<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n if not isinstance ( a , basestring ) or not isinstance ( b , basestring ) : \n a = unicode ( a ) \n b = unicode ( b ) \n if a == b : \n return a , b \n else : \n return self . colorize ( highlight , a ) , self . colorize ( highlight , b ) \n if isinstance ( a , bytes ) or isinstance ( b , bytes ) : \n a = self . displayable_path ( a ) \n b = self . displayable_path ( b ) \n a_out = [ ] \n b_out = [ ] \n matcher = SequenceMatcher ( lambda x : False , a , b ) \n for op , a_start , a_end , b_start , b_end in matcher . get_opcodes ( ) : \n if op == '<STR_LIT>' : \n a_out . append ( a [ a_start : a_end ] ) \n b_out . append ( b [ b_start : b_end ] ) \n elif op == '<STR_LIT>' : \n b_out . append ( self . colorize ( highlight , b [ b_start : b_end ] ) ) \n elif op == '<STR_LIT>' : \n a_out . append ( self . colorize ( highlight , a [ a_start : a_end ] ) ) \n elif op == '<STR_LIT:replace>' : \n if a [ a_start : a_end ] . lower ( ) != b [ b_start : b_end ] . lower ( ) : \n color = highlight \n else : \n color = minor_highlight \n a_out . append ( self . colorize ( color , a [ a_start : a_end ] ) ) \n b_out . append ( self . colorize ( color , b [ b_start : b_end ] ) ) \n else : \n assert ( False ) \n return u'<STR_LIT>' . join ( a_out ) , u'<STR_LIT>' . join ( b_out ) \n def displayable_path ( self , path , separator = u'<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( path , ( list , tuple ) ) : \n return separator . join ( self . displayable_path ( p ) for p in path ) \n elif isinstance ( path , unicode ) : \n return path \n elif not isinstance ( path , str ) : \n return unicode ( path ) \n try : \n return path . decode ( self . _fsencoding ( ) , '<STR_LIT:ignore>' ) \n except ( UnicodeError , LookupError ) : \n return path . decode ( '<STR_LIT:utf8>' , '<STR_LIT:ignore>' ) \n def _fsencoding ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n encoding = sys . getfilesystemencoding ( ) or sys . getdefaultencoding ( ) \n if encoding == '<STR_LIT>' : \n encoding = '<STR_LIT:utf8>' \n return encoding \n def colordiff ( self , a , b , highlight = '<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . gv . settings . get_setting ( '<STR_LIT>' , self ) == '<STR_LIT:True>' : \n return self . _colordiff ( a , b , highlight ) \n else : \n return unicode ( a ) , unicode ( b ) \n def print_ ( self , * strings ) : \n \"\"\"<STR_LIT>\"\"\" \n if strings : \n if isinstance ( strings [ <NUM_LIT:0> ] , unicode ) : \n txt = u'<STR_LIT:U+0020>' . join ( strings ) \n else : \n txt = '<STR_LIT:U+0020>' . join ( strings ) \n else : \n txt = u'<STR_LIT>' \n if isinstance ( txt , unicode ) : \n txt = txt . encode ( self . _encoding ( ) , '<STR_LIT:replace>' ) \n print ( txt ) \n def color_diff_suffix ( self , a , b , highlight = '<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n a , b = unicode ( a ) , unicode ( b ) \n if not self . gv . settings . get_setting ( '<STR_LIT>' , self ) == '<STR_LIT:True>' : \n return a , b \n if a == b : \n return a , b \n first_diff = None \n for i in range ( min ( len ( a ) , len ( b ) ) ) : \n if a [ i ] != b [ i ] : \n first_diff = i \n break \n else : \n first_diff = min ( len ( a ) , len ( b ) ) \n return a [ : first_diff ] + self . colorize ( highlight , a [ first_diff : ] ) , b [ : first_diff ] + self . colorize ( highlight , b [ first_diff : ] ) \n def choose_candidate ( self , candidates , manipulate , opts , item = None , itemcount = None ) : \n self . print_ ( u'<STR_LIT>' ) \n for i , match in enumerate ( candidates ) : \n line = [ \n u'<STR_LIT>' . format ( i + <NUM_LIT:1> ) , \n u'<STR_LIT>' . format ( manipulate . get_stripped_text ( match . reference_to_link ) \n ) \n ] \n self . print_ ( '<STR_LIT:U+0020>' . join ( line ) ) \n sel = self . input_options ( opts , numrange = ( <NUM_LIT:1> , len ( candidates ) ) ) \n return <mask0> \n", "gt": "sel"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import re \n import string \n from jsbeautifier . unpackers import UnpackingError \n PRIORITY = <NUM_LIT:1> \n def detect ( source ) : \n \"\"\"<STR_LIT>\"\"\" \n return source . replace ( '<STR_LIT:U+0020>' , '<STR_LIT>' ) . startswith ( '<STR_LIT>' ) \n def unpack ( source ) : \n \"\"\"<STR_LIT>\"\"\" \n payload , symtab , radix , count = _filterargs ( source ) \n if count != len ( symtab ) : \n raise UnpackingError ( '<STR_LIT>' ) \n try : \n unbase = Unbaser ( radix ) \n except TypeError : \n raise UnpackingError ( '<STR_LIT>' ) \n def lookup ( match ) : \n \"\"\"<STR_LIT>\"\"\" \n word = match . group ( <NUM_LIT:0> ) \n return symtab [ unbase ( word ) ] or word \n source = re . sub ( r'<STR_LIT>' , lookup , payload ) \n return _replacestrings ( source ) \n def _filterargs ( source ) : \n \"\"\"<STR_LIT>\"\"\" \n argsregex = ( r\"<STR_LIT>\" \n r\"<STR_LIT>\" ) \n args = re . search ( argsregex , source , re . DOTALL ) . groups ( ) \n try : \n return args [ <NUM_LIT:0> ] , args [ <NUM_LIT:3> ] . split ( '<STR_LIT:|>' ) , int ( args [ <NUM_LIT:1> ] ) , int ( args [ <NUM_LIT:2> ] ) \n except ValueError : \n raise UnpackingError ( '<STR_LIT>' ) \n def _replacestrings ( source ) : \n \"\"\"<STR_LIT>\"\"\" \n match = re . search ( r'<STR_LIT>' , source , re . DOTALL ) \n if match : \n varname , strings = match . groups ( ) \n startpoint = len ( match . group ( <NUM_LIT:0> ) ) \n lookup = strings . split ( '<STR_LIT>' ) \n variable = '<STR_LIT>' % varname \n for index , value in enumerate ( lookup ) : \n source = source . replace ( variable % index , '<STR_LIT>' % value ) \n return source [ startpoint : ] \n return source \n class Unbaser ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n ALPHABET = { \n <NUM_LIT> : '<STR_LIT>' , \n <NUM_LIT> : ( '<STR_LIT>' \n '<STR_LIT>' ) \n } \n def __init__ ( self , base ) : \n self . base = base \n if <NUM_LIT:2> <= base <= <NUM_LIT> : \n self . unbase = lambda string : int ( string , base ) \n else : \n try : \n self . dictionary = dict ( ( cipher , index ) for \n index , cipher in enumerate ( self . ALPHABET [ base ] ) ) \n except KeyError : \n raise TypeError ( '<STR_LIT>' ) \n self . unbase = self . _dictunbaser \n def __call__ ( self , string ) : \n return self . unbase ( string ) \n def _dictunbaser ( self , string ) : \n \"\"\"<STR_LIT>\"\"\" \n ret = <NUM_LIT:0> \n for index , cipher in enumerate ( string [ : : - <NUM_LIT:1> ] ) : \n ret += ( self . base ** index ) * self . dictionary [ cipher ] \n return <mask0> \n", "gt": "ret"}
{"input": "\n import os , sys \n parentdir = os . path . dirname ( __file__ ) \n sys . path . insert ( <NUM_LIT:0> , parentdir ) \n import executemechanize \n class redirection : \n def createarray ( self ) : \n setattr ( self , \"<STR_LIT>\" , [ ] ) \n def appendurl ( self , url ) : \n url = str ( url ) \n if not url . endswith ( \"<STR_LIT>\" ) or url . endswith ( \"<STR_LIT>\" ) : \n self . redirection_list . append ( url ) ; \n self . passarray ( ) \n def passarray ( self ) : \n executemechanize . set_redirection_list ( self . <mask0> ) \n", "gt": "redirection_list"}
{"input": "\n __author__ = '<STR_LIT>' \n __copyright__ = '<STR_LIT>' \n __license__ = '<STR_LIT>' \n __version__ = '<STR_LIT>' \n __maintainer__ = '<STR_LIT>' \n __email__ = '<STR_LIT>' \n <mask0> = '<STR_LIT>' \n", "gt": "__status__"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import absolute_import \n from . . config import PATHS \n from . . entity import Entity \n class StrategyConcept ( Entity ) : \n \"\"\"<STR_LIT>\"\"\" \n collection = '<STR_LIT>' \n resource = '<STR_LIT>' \n _relations = { \n '<STR_LIT>' , \n '<STR_LIT>' , \n } \n _pull = { \n '<STR_LIT>' : int , \n '<STR_LIT>' : Entity . _strpt , \n '<STR_LIT:id>' : int , \n '<STR_LIT:status>' : Entity . _int_to_bool , \n '<STR_LIT>' : int , \n '<STR_LIT>' : Entity . _strpt , \n '<STR_LIT:version>' : int , \n } \n _push = _pull . copy ( ) \n _push . update ( { \n '<STR_LIT:status>' : int , \n } ) \n _readonly = Entity . _readonly | { '<STR_LIT:name>' , } \n def __init__ ( self , session , properties = None , ** kwargs ) : \n super ( StrategyConcept , self ) . __init__ ( session , properties , ** kwargs ) \n def remove ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n url = '<STR_LIT:/>' . join ( [ self . collection , \n str ( self . id ) , \n '<STR_LIT>' ] ) \n self . _post ( PATHS [ '<STR_LIT>' ] , rest = url , data = { '<STR_LIT:version>' : self . version } ) \n for item in list ( self . properties . keys ( ) ) : \n del self . properties [ <mask0> ] \n", "gt": "item"}
{"input": "\n from __future__ import print_function \n from __future__ import absolute_import \n import unittest \n import responses \n import requests \n from . requests_patch import patched_extract_cookies_to_jar \n from terminalone import T1 \n mock_credentials = { \n '<STR_LIT:username>' : '<STR_LIT>' , \n '<STR_LIT:password>' : '<STR_LIT:password>' , \n '<STR_LIT>' : '<STR_LIT>' , \n } \n API_BASE = '<STR_LIT>' \n requests . sessions . extract_cookies_to_jar = patched_extract_cookies_to_jar \n requests . adapters . extract_cookies_to_jar = patched_extract_cookies_to_jar \n class TestPermissions ( unittest . TestCase ) : \n def setup ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n with open ( '<STR_LIT>' ) as f : \n fixture = f . read ( ) \n responses . add ( responses . POST , '<STR_LIT>' , \n body = fixture , \n adding_headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n } , \n content_type = '<STR_LIT>' ) \n self . t1 = T1 ( auth_method = '<STR_LIT>' , \n api_base = API_BASE , \n ** mock_credentials ) \n @ responses . activate \n def test_get_permissions ( self ) : \n self . setup ( ) \n with open ( '<STR_LIT>' ) as f : \n fixture = f . read ( ) \n responses . add ( responses . GET , \n '<STR_LIT>' , \n body = fixture , \n content_type = '<STR_LIT>' , \n match_querystring = True ) \n p = self . t1 . get ( '<STR_LIT>' , <NUM_LIT> , child = '<STR_LIT>' ) \n assert p . _type == '<STR_LIT>' , '<STR_LIT>' . format ( p . _type ) \n assert p . parent_id == <NUM_LIT> , '<STR_LIT>' . format ( p . parent_id ) \n @ responses . activate \n def test_remove_advertiser ( self ) : \n self . setup ( ) \n with open ( '<STR_LIT>' ) as f : \n fixture = f . read ( ) \n responses . add ( responses . GET , \n '<STR_LIT>' , \n body = fixture , \n content_type = '<STR_LIT>' , \n match_querystring = True ) \n p = self . t1 . get ( '<STR_LIT>' , <NUM_LIT> , child = '<STR_LIT>' ) \n remove_id = <NUM_LIT:6> \n assert remove_id in p . advertiser . keys ( ) , '<STR_LIT>' . format ( remove_id ) \n p . remove ( '<STR_LIT>' , <NUM_LIT:6> ) \n assert remove_id not in p . advertiser . keys ( ) , '<STR_LIT>' . format ( remove_id ) \n @ responses . activate \n def test_it_should_remove_child_advertisers_when_removing_agency ( self ) : \n self . setup ( ) \n with open ( '<STR_LIT>' ) as f : \n fixture = f . read ( ) \n responses . add ( responses . GET , \n '<STR_LIT>' , \n body = fixture , \n content_type = '<STR_LIT>' , \n match_querystring = True ) \n p = self . t1 . get ( '<STR_LIT>' , <NUM_LIT> , child = '<STR_LIT>' ) \n remove_ids = [ <NUM_LIT:6> , <NUM_LIT:7> ] \n for ad_id in remove_ids : \n assert ad_id in p . advertiser . keys ( ) , '<STR_LIT>' . format ( ad_id ) \n p . remove ( '<STR_LIT>' , <NUM_LIT:3> ) \n for ad_id in remove_ids : \n assert ad_id not in p . advertiser . keys ( ) , '<STR_LIT>' . format ( ad_id ) \n @ responses . activate \n def test_it_should_remove_child_agencies_and_advertisers_when_removing_organization ( self ) : \n self . setup ( ) \n with open ( '<STR_LIT>' ) as f : \n fixture = f . read ( ) \n responses . add ( responses . GET , \n '<STR_LIT>' , \n body = fixture , \n content_type = '<STR_LIT>' , \n match_querystring = True ) \n p = self . t1 . get ( '<STR_LIT>' , <NUM_LIT> , child = '<STR_LIT>' ) \n remove_advertiser_ids = [ <NUM_LIT:8> , <NUM_LIT:9> , <NUM_LIT:10> ] \n remove_agency_ids = [ <NUM_LIT:4> , <NUM_LIT:5> ] \n for advertiser_id in remove_advertiser_ids : \n assert advertiser_id in p . advertiser . keys ( ) , '<STR_LIT>' . format ( advertiser_id ) \n for agency_id in remove_agency_ids : \n assert agency_id in p . agency . keys ( ) , '<STR_LIT>' . format ( agency_id ) \n p . remove ( '<STR_LIT>' , <NUM_LIT:2> ) \n for advertiser_id in remove_advertiser_ids : \n assert advertiser_id not in p . advertiser . keys ( ) , '<STR_LIT>' . format ( advertiser_id ) \n for agency_id in remove_agency_ids : \n assert agency_id not in p . agency . keys ( ) , '<STR_LIT>' . format ( agency_id ) \n @ responses . activate \n def test_it_should_add_entity_ids_on_save ( self ) : \n self . setup ( ) \n with open ( '<STR_LIT>' ) as f : \n fixture = f . read ( ) \n responses . add ( responses . GET , \n '<STR_LIT>' , \n body = fixture , \n content_type = '<STR_LIT>' , \n match_querystring = True ) \n p = self . t1 . get ( '<STR_LIT>' , <NUM_LIT> , child = '<STR_LIT>' ) \n p . add ( '<STR_LIT>' , <NUM_LIT:10> ) \n data = p . _generate_save_data ( ) \n assert sorted ( data [ '<STR_LIT>' ] ) == [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:10> ] , data [ '<STR_LIT>' ] \n @ responses . activate \n def test_it_should_add_access_to_empty_permissions ( self ) : \n self . setup ( ) \n with open ( '<STR_LIT>' ) as f : \n fixture = f . read ( ) \n responses . add ( responses . GET , \n '<STR_LIT>' , \n body = fixture , \n content_type = '<STR_LIT>' , \n match_querystring = True ) \n p = self . t1 . get ( '<STR_LIT>' , <NUM_LIT> , child = '<STR_LIT>' ) \n p . add ( '<STR_LIT>' , <NUM_LIT:10> ) \n data = p . _generate_save_data ( ) \n assert sorted ( data [ '<STR_LIT>' ] ) == [ <NUM_LIT:10> ] , <mask0> [ '<STR_LIT>' ] \n", "gt": "data"}
{"input": "\n VERSION = ( <NUM_LIT:0> , <NUM_LIT:1> , <NUM_LIT:9> ) \n <mask0> = \"<STR_LIT>\" \n", "gt": "__version__"}
{"input": "\n import sys , os \n sys . path . insert ( <NUM_LIT:0> , os . path . abspath ( '<STR_LIT:.>' ) ) \n sys . path . insert ( <NUM_LIT:0> , os . path . abspath ( '<STR_LIT:..>' ) ) \n needs_sphinx = '<STR_LIT:1.0>' \n extensions = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n templates_path = [ '<STR_LIT>' ] \n source_suffix = '<STR_LIT>' \n master_doc = '<STR_LIT:index>' \n project = u'<STR_LIT>' \n copyright = u'<STR_LIT>' \n import pkg_resources \n try : \n release = pkg_resources . get_distribution ( '<STR_LIT>' ) . version \n except pkg_resources . DistributionNotFound : \n print '<STR_LIT>' \n print '<STR_LIT>' \n print '<STR_LIT>' \n print '<STR_LIT>' \n sys . exit ( <NUM_LIT:1> ) \n del pkg_resources \n version = '<STR_LIT:.>' . join ( release . split ( '<STR_LIT:.>' ) [ : <NUM_LIT:2> ] ) \n exclude_patterns = [ '<STR_LIT>' ] \n pygments_style = '<STR_LIT>' \n html_theme = '<STR_LIT>' \n html_static_path = [ '<STR_LIT>' ] \n html_use_smartypants = True \n htmlhelp_basename = '<STR_LIT>' \n latex_elements = { \n } \n latex_documents = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n u'<STR_LIT>' , '<STR_LIT>' ) , \n ] \n man_pages = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n [ u'<STR_LIT>' ] , <NUM_LIT:1> ) \n ] \n texinfo_documents = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n <mask0> , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' ) , \n ] \n", "gt": "u'<STR_LIT>'"}
{"input": "\n import math \n import glob \n import re \n import os \n import subprocess \n from shutil import rmtree \n import logging \n from mrec import load_sparse_matrix , save_recommender \n class ItemSimilarityRunner ( object ) : \n def run ( self , view , model , input_format , trainfile , num_engines , simsdir , overwrite , max_sims , simsfile , modelfile ) : \n logging . info ( '<STR_LIT>' ) \n dataset = load_sparse_matrix ( input_format , trainfile ) \n num_users , num_items = dataset . shape \n del dataset \n logging . info ( '<STR_LIT>' , num_users , num_items ) \n logging . info ( '<STR_LIT>' . format ( simsdir ) ) \n subprocess . check_call ( [ '<STR_LIT>' , '<STR_LIT>' , simsdir ] ) \n done = [ ] \n if not overwrite : \n logging . info ( '<STR_LIT>' ) \n done . extend ( self . find_done ( simsdir ) ) \n if done : \n logging . info ( '<STR_LIT>' . format ( len ( done ) ) ) \n logging . info ( '<STR_LIT>' ) \n tasks = self . create_tasks ( model , input_format , trainfile , simsdir , num_items , num_engines , max_sims , done ) \n if num_engines > <NUM_LIT:0> : \n logging . info ( '<STR_LIT>' \n '<STR_LIT>' , len ( tasks ) ) \n async_job = view . map_async ( process , tasks , retries = <NUM_LIT:2> ) \n results = async_job . get ( ) \n else : \n logging . info ( '<STR_LIT>' ) \n results = [ process ( task ) for task in tasks ] \n logging . info ( '<STR_LIT>' ) \n done = self . find_done ( simsdir ) \n remaining = len ( tasks ) - len ( done ) \n if remaining == <NUM_LIT:0> : \n logging . info ( '<STR_LIT>' ) \n logging . info ( '<STR_LIT>' . format ( len ( done ) ) ) \n paths = [ os . path . join ( simsdir , '<STR_LIT>' . format ( start , end ) ) for start , end in done ] \n cmd = [ '<STR_LIT>' ] + paths \n subprocess . check_call ( cmd , stdout = open ( simsfile , '<STR_LIT:w>' ) ) \n logging . info ( '<STR_LIT>' ) \n rmtree ( simsdir ) \n logging . info ( '<STR_LIT>' , \n num_items , type ( model ) . __name__ , simsfile ) \n model . load_similarity_matrix ( simsfile , num_items ) \n save_recommender ( model , modelfile ) \n logging . info ( '<STR_LIT>' ) \n else : \n logging . error ( '<STR_LIT>' . format ( remaining , len ( tasks ) ) ) \n logging . error ( '<STR_LIT>' ) \n def find_done ( self , outdir ) : \n success_files = glob . glob ( os . path . join ( outdir , '<STR_LIT>' ) ) \n r = re . compile ( '<STR_LIT>' ) \n done = [ ] \n for path in success_files : \n m = r . match ( path ) \n start = int ( m . group ( <NUM_LIT:1> ) ) \n end = int ( m . group ( <NUM_LIT:2> ) ) \n done . append ( ( start , end ) ) \n return done \n def create_tasks ( self , model , input_format , trainfile , outdir , num_items , num_engines , max_similar_items , done ) : \n if num_engines == <NUM_LIT:0> : \n num_engines = <NUM_LIT:1> \n items_per_engine = int ( math . ceil ( float ( num_items ) / num_engines ) ) \n tasks = [ ] \n for start in xrange ( <NUM_LIT:0> , num_items , items_per_engine ) : \n end = min ( num_items , start + items_per_engine ) \n if ( start , end ) not in done : \n tasks . append ( ( model , input_format , trainfile , outdir , start , end , max_similar_items ) ) \n return tasks \n def process ( task ) : \n \"\"\"<STR_LIT>\"\"\" \n import os \n import subprocess \n from mrec import load_fast_sparse_matrix \n model , input_format , trainfile , outdir , start , end , max_similar_items = task \n dataset = load_fast_sparse_matrix ( input_format , trainfile ) \n if hasattr ( model , '<STR_LIT>' ) : \n model . similarity_matrix = None \n outfile = os . path . join ( outdir , '<STR_LIT>' . format ( start , end ) ) \n out = open ( outfile , '<STR_LIT:w>' ) \n for j in xrange ( start , end ) : \n w = model . get_similar_items ( j , max_similar_items = max_similar_items , dataset = dataset ) \n for k , v in w : \n print >> out , '<STR_LIT>' . format ( j + <NUM_LIT:1> , k + <NUM_LIT:1> , v ) \n out . close ( ) \n cmd = [ '<STR_LIT>' , os . path . join ( outdir , '<STR_LIT>' . format ( start , end ) ) ] \n subprocess . check_call ( cmd ) \n return start , <mask0> \n", "gt": "end"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import inspect \n NO_DEFAULT = object ( ) \n def memoize_default ( default = NO_DEFAULT , evaluator_is_first_arg = False , second_arg_is_evaluator = False ) : \n \"\"\"<STR_LIT>\"\"\" \n def func ( function ) : \n def wrapper ( obj , * args , ** kwargs ) : \n if evaluator_is_first_arg : \n cache = obj . memoize_cache \n elif second_arg_is_evaluator : \n cache = args [ <NUM_LIT:0> ] . memoize_cache \n else : \n cache = obj . _evaluator . memoize_cache \n try : \n memo = cache [ function ] \n except KeyError : \n memo = { } \n cache [ function ] = memo \n key = ( obj , args , frozenset ( kwargs . items ( ) ) ) \n if key in memo : \n return memo [ key ] \n else : \n if default is not NO_DEFAULT : \n memo [ key ] = default \n rv = function ( obj , * args , ** kwargs ) \n if inspect . isgenerator ( rv ) : \n rv = list ( rv ) \n memo [ key ] = rv \n return rv \n return wrapper \n return func \n class CachedMetaClass ( type ) : \n \"\"\"<STR_LIT>\"\"\" \n @ memoize_default ( None , second_arg_is_evaluator = True ) \n def __call__ ( self , * args , ** kwargs ) : \n return super ( CachedMetaClass , self ) . __call__ ( * args , ** <mask0> ) \n", "gt": "kwargs"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import absolute_import \n import __main__ \n from collections import namedtuple \n import re \n import os \n import sys \n from jedi import Interpreter \n from jedi . api . helpers import completion_parts \n from jedi . parser . user_context import UserContext \n def setup_readline ( namespace_module = __main__ ) : \n \"\"\"<STR_LIT>\"\"\" \n class JediRL ( object ) : \n def complete ( self , text , state ) : \n \"\"\"<STR_LIT>\"\"\" \n if state == <NUM_LIT:0> : \n sys . path . insert ( <NUM_LIT:0> , os . getcwd ( ) ) \n try : \n interpreter = Interpreter ( text , [ namespace_module . __dict__ ] ) \n path = UserContext ( text , ( <NUM_LIT:1> , len ( text ) ) ) . get_path_until_cursor ( ) \n path , dot , like = completion_parts ( path ) \n before = text [ : len ( text ) - len ( like ) ] \n completions = interpreter . completions ( ) \n finally : \n sys . path . pop ( <NUM_LIT:0> ) \n self . matches = [ before + c . name_with_symbols for c in completions ] \n try : \n return self . matches [ state ] \n except IndexError : \n return None \n try : \n import readline \n except ImportError : \n print ( \"<STR_LIT>\" ) \n else : \n readline . set_completer ( JediRL ( ) . complete ) \n readline . parse_and_bind ( \"<STR_LIT>\" ) \n readline . parse_and_bind ( \"<STR_LIT>\" ) \n readline . parse_and_bind ( \"<STR_LIT>\" ) \n readline . parse_and_bind ( \"<STR_LIT>\" ) \n readline . parse_and_bind ( \"<STR_LIT>\" ) \n readline . set_completer_delims ( '<STR_LIT>' ) \n def version_info ( ) : \n \"\"\"<STR_LIT>\"\"\" \n Version = namedtuple ( '<STR_LIT>' , '<STR_LIT>' ) \n from jedi import __version__ \n tupl = re . findall ( '<STR_LIT>' , __version__ ) \n return Version ( * [ x if i == <NUM_LIT:3> else int ( x ) for i , x in enumerate ( <mask0> ) ] ) \n", "gt": "tupl"}
{"input": "\n import collections \n import copy \n from . Utils import _write_complex_object \n class ExceptionDetails ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n _defaults = collections . OrderedDict ( [ \n ( '<STR_LIT:id>' , None ) , \n ( '<STR_LIT>' , None ) , \n ( '<STR_LIT>' , None ) , \n ( '<STR_LIT:message>' , None ) , \n ( '<STR_LIT>' , True ) , \n ( '<STR_LIT>' , None ) , \n ( '<STR_LIT>' , [ ] ) \n ] ) \n def __init__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _values = { \n '<STR_LIT>' : None , \n '<STR_LIT:message>' : None , \n '<STR_LIT>' : True , \n } \n self . _initialize ( ) \n @ property \n def id ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if '<STR_LIT:id>' in self . _values : \n return self . _values [ '<STR_LIT:id>' ] \n return self . _defaults [ '<STR_LIT:id>' ] \n @ id . setter \n def id ( self , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if value == self . _defaults [ '<STR_LIT:id>' ] and '<STR_LIT:id>' in self . _values : \n del self . _values [ '<STR_LIT:id>' ] \n else : \n self . _values [ '<STR_LIT:id>' ] = value \n @ property \n def outer_id ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if '<STR_LIT>' in self . _values : \n return self . _values [ '<STR_LIT>' ] \n return self . _defaults [ '<STR_LIT>' ] \n @ outer_id . setter \n def outer_id ( self , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if value == self . _defaults [ '<STR_LIT>' ] and '<STR_LIT>' in self . _values : \n del self . _values [ '<STR_LIT>' ] \n else : \n self . _values [ '<STR_LIT>' ] = value \n @ property \n def type_name ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _values [ '<STR_LIT>' ] \n @ type_name . setter \n def type_name ( self , value ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _values [ '<STR_LIT>' ] = value \n @ property \n def message ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _values [ '<STR_LIT:message>' ] \n @ message . setter \n def message ( self , value ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _values [ '<STR_LIT:message>' ] = value \n @ property \n def has_full_stack ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if '<STR_LIT>' in self . _values : \n return self . _values [ '<STR_LIT>' ] \n return self . _defaults [ '<STR_LIT>' ] \n @ has_full_stack . setter \n def has_full_stack ( self , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if value == self . _defaults [ '<STR_LIT>' ] and '<STR_LIT>' in self . _values : \n del self . _values [ '<STR_LIT>' ] \n else : \n self . _values [ '<STR_LIT>' ] = value \n @ property \n def stack ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if '<STR_LIT>' in self . _values : \n return self . _values [ '<STR_LIT>' ] \n return self . _defaults [ '<STR_LIT>' ] \n @ stack . setter \n def stack ( self , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if value == self . _defaults [ '<STR_LIT>' ] and '<STR_LIT>' in self . _values : \n del self . _values [ '<STR_LIT>' ] \n else : \n self . _values [ '<STR_LIT>' ] = value \n @ property \n def parsed_stack ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if '<STR_LIT>' in self . _values : \n return self . _values [ '<STR_LIT>' ] \n self . _values [ '<STR_LIT>' ] = copy . deepcopy ( self . _defaults [ '<STR_LIT>' ] ) \n return self . _values [ '<STR_LIT>' ] \n @ parsed_stack . setter \n def parsed_stack ( self , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if value == self . _defaults [ '<STR_LIT>' ] and '<STR_LIT>' in self . _values : \n del self . _values [ '<STR_LIT>' ] \n else : \n self . _values [ '<STR_LIT>' ] = value \n def _initialize ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n def write ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return _write_complex_object ( self . _defaults , self . <mask0> ) \n", "gt": "_values"}
{"input": "\n import random \n import unittest \n import time \n import threading \n try : \n import BaseHTTPServer as HTTPServer \n except ImportError : \n import http . server as HTTPServer \n import sys , os , os . path \n rootDirectory = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , '<STR_LIT:..>' , '<STR_LIT:..>' ) \n if rootDirectory not in sys . path : \n sys . path . append ( rootDirectory ) \n from applicationinsights import channel \n class TestSenderBase ( unittest . TestCase ) : \n def test_construct ( self ) : \n actual = channel . SenderBase ( '<STR_LIT>' ) \n self . assertIsNotNone ( actual ) \n self . assertEqual ( '<STR_LIT>' , actual . service_endpoint_uri ) \n self . assertIsNone ( actual . queue ) \n self . assertEqual ( <NUM_LIT:100> , actual . send_buffer_size ) \n def test_service_endpoint_uri_works_as_expected ( self ) : \n actual = channel . SenderBase ( '<STR_LIT>' ) \n self . assertEqual ( '<STR_LIT>' , actual . service_endpoint_uri ) \n actual . service_endpoint_uri = '<STR_LIT:foo>' \n self . assertEqual ( '<STR_LIT:foo>' , actual . service_endpoint_uri ) \n def test_queue_works_as_expected ( self ) : \n actual = channel . SenderBase ( '<STR_LIT>' ) \n self . assertIsNone ( actual . queue ) \n expected = object ( ) \n actual . queue = expected \n self . assertEqual ( expected , actual . queue ) \n def test_send_buffer_size_works_as_expected ( self ) : \n actual = channel . SenderBase ( '<STR_LIT>' ) \n self . assertEqual ( <NUM_LIT:100> , actual . send_buffer_size ) \n actual . send_buffer_size = <NUM_LIT> \n self . assertEqual ( <NUM_LIT> , actual . send_buffer_size ) \n actual . send_buffer_size = - <NUM_LIT:1> \n self . assertEqual ( <NUM_LIT:1> , actual . send_buffer_size ) \n def test_send_works_as_expected ( self ) : \n port = random . randint ( <NUM_LIT> , <NUM_LIT> ) \n actual = channel . SenderBase ( \"<STR_LIT>\" + str ( port ) + \"<STR_LIT>\" ) \n actual . queue = channel . QueueBase ( None ) \n MockHTTPRequestHandler . ExpectedContent = \"<STR_LIT>\" \n MockHTTPRequestHandler . TestCase = self \n thread = WorkerThread ( actual ) \n thread . start ( ) \n runHttpHandlerOnce ( handler = MockHTTPRequestHandler , port = port ) \n thread . join ( ) \n if \"<STR_LIT>\" in dir ( self ) : \n self . fail ( self . failed ) \n self . assertEqual ( None , actual . queue . get ( ) ) \n class WorkerThread ( threading . Thread ) : \n def __init__ ( self , sender ) : \n threading . Thread . __init__ ( self ) \n self . sender = sender \n def run ( self ) : \n time . sleep ( <NUM_LIT:1> ) \n self . sender . send ( [ MockSerializable ( <NUM_LIT> ) , MockSerializable ( <NUM_LIT> ) ] ) \n class MockSerializable ( object ) : \n def __init__ ( self , data ) : \n self . _data = data \n def write ( self ) : \n return self . _data \n class MockHTTPRequestHandler ( HTTPServer . BaseHTTPRequestHandler ) : \n ExpectedContent = None \n TestCase = None \n def do_POST ( self ) : \n contentLength = int ( self . headers [ '<STR_LIT>' ] ) \n content = self . rfile . read ( contentLength ) \n response = \"<STR_LIT>\" \n if isinstance ( content , bytes ) : \n content = content . decode ( \"<STR_LIT:utf-8>\" ) \n response = b\"<STR_LIT>\" \n if \"<STR_LIT:POST>\" != self . command : \n MockHTTPRequestHandler . TestCase . failed = '<STR_LIT>' \n if \"<STR_LIT>\" != self . headers [ \"<STR_LIT:Content-Type>\" ] : \n MockHTTPRequestHandler . TestCase . failed = '<STR_LIT>' \n if MockHTTPRequestHandler . ExpectedContent != content : \n MockHTTPRequestHandler . TestCase . failed = '<STR_LIT:\">' + MockHTTPRequestHandler . ExpectedContent + '<STR_LIT>' \n self . send_response ( <NUM_LIT:200> ) \n self . send_header ( \"<STR_LIT:Content-Type>\" , \"<STR_LIT:application/json>\" ) \n self . send_header ( \"<STR_LIT>\" , \"<STR_LIT:0>\" ) \n self . end_headers ( ) \n self . wfile . write ( response ) \n def runHttpHandlerOnce ( server = HTTPServer . HTTPServer , handler = HTTPServer . BaseHTTPRequestHandler , port = <NUM_LIT> ) : \n serverAddress = ( '<STR_LIT>' , port ) \n httpd = server ( serverAddress , handler ) \n httpd . <mask0> ( ) \n", "gt": "handle_request"}
{"input": "\n from . import <mask0> \n", "gt": "TestEnable"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys as _sys \n from operator import itemgetter as _itemgetter \n from keyword import iskeyword as _iskeyword \n from collections import OrderedDict \n class tagtuple ( tuple ) : \n \"\"\"<STR_LIT>\"\"\" \n __slots__ = ( ) \n def __new__ ( cls , * args ) : \n \"\"\"<STR_LIT>\"\"\" \n return super ( tagtuple , cls ) . __new__ ( cls , args ) \n def __repr__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return type ( self ) . __name__ + super ( tagtuple , self ) . __repr__ ( ) \n def __getnewargs__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return tuple ( self ) \n def __eq__ ( self , other ) : \n return type ( self ) is type ( other ) and super ( tagtuple , self ) . __eq__ ( other ) \n def __ne__ ( self , other ) : \n return not self . __eq__ ( other ) \n def __getslice__ ( self , i , j ) : \n return type ( self ) ( * super ( tagtuple , self ) . __getslice__ ( i , j ) ) \n __add__ = property ( ) \n __contains__ = property ( ) \n __mul__ = property ( ) \n __rmul__ = property ( ) \n count = property ( ) \n index = property ( ) \n _class_template = '''<STR_LIT>''' \n _repr_template = '<STR_LIT>' \n _field_template = '''<STR_LIT>''' \n def rectuple ( typename , field_names , verbose = False , rename = False ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( field_names , basestring ) : \n field_names = field_names . replace ( '<STR_LIT:U+002C>' , '<STR_LIT:U+0020>' ) . split ( ) \n field_names = map ( str , field_names ) \n if rename : \n seen = set ( ) \n for index , name in enumerate ( field_names ) : \n if ( not all ( c . isalnum ( ) or c == '<STR_LIT:_>' for c in name ) \n or _iskeyword ( name ) \n or not name \n or name [ <NUM_LIT:0> ] . isdigit ( ) \n or name . startswith ( '<STR_LIT:_>' ) \n or name in seen ) : \n field_names [ index ] = '<STR_LIT>' % index \n seen . add ( name ) \n for name in [ typename ] + field_names : \n if not all ( c . isalnum ( ) or c == '<STR_LIT:_>' for c in name ) : \n raise ValueError ( '<STR_LIT>' \n '<STR_LIT>' % name ) \n if _iskeyword ( name ) : \n raise ValueError ( '<STR_LIT>' \n '<STR_LIT>' % name ) \n if name [ <NUM_LIT:0> ] . isdigit ( ) : \n raise ValueError ( '<STR_LIT>' \n '<STR_LIT>' % name ) \n seen = set ( ) \n for name in field_names : \n if name . startswith ( '<STR_LIT:_>' ) and not rename : \n raise ValueError ( '<STR_LIT>' \n '<STR_LIT>' % name ) \n if name in seen : \n raise ValueError ( '<STR_LIT>' % name ) \n seen . add ( name ) \n class_definition = _class_template . format ( \n typename = typename , \n field_names = tuple ( field_names ) , \n num_fields = len ( field_names ) , \n arg_list = repr ( tuple ( field_names ) ) . replace ( \"<STR_LIT:'>\" , \"<STR_LIT>\" ) [ <NUM_LIT:1> : - <NUM_LIT:1> ] , \n repr_fmt = '<STR_LIT:U+002CU+0020>' . join ( _repr_template . format ( name = name ) \n for name in field_names ) , \n field_defs = '<STR_LIT:\\n>' . join ( _field_template . format ( index = index , name = name ) \n for index , name in enumerate ( field_names ) ) \n ) \n if verbose : \n print class_definition \n namespace = dict ( _itemgetter = _itemgetter , __name__ = '<STR_LIT>' % typename , \n OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n try : \n exec class_definition in namespace \n except SyntaxError as e : \n raise SyntaxError ( e . message + '<STR_LIT>' + class_definition ) \n result = namespace [ typename ] \n try : \n result . __module__ = _sys . _getframe ( <NUM_LIT:1> ) . f_globals . get ( '<STR_LIT>' , '<STR_LIT:__main__>' ) \n except ( AttributeError , ValueError ) : \n pass \n return result \n if __name__ == '<STR_LIT:__main__>' : \n import pickle \n from itertools import chain , product \n print \"<STR_LIT>\" \n print \n class A ( tagtuple ) : \n __slots__ = ( ) \n class B ( tagtuple ) : \n __slots__ = ( ) \n a = A ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) \n b = B ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) \n t = ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) \n print \"<STR_LIT>\" , a \n print \"<STR_LIT>\" , b \n print \"<STR_LIT>\" , t \n print \n print \"<STR_LIT>\" , a == b \n print \"<STR_LIT>\" , a != b \n print \"<STR_LIT>\" , hash ( a ) == hash ( b ) \n print \"<STR_LIT>\" , a <= b \n print \"<STR_LIT>\" , b <= a \n print \n print \"<STR_LIT>\" , a == t \n print \"<STR_LIT>\" , a != t \n print \"<STR_LIT>\" , hash ( a ) == hash ( t ) \n print \"<STR_LIT>\" , a <= t \n print \"<STR_LIT>\" , t <= a \n print \n d = { } \n d [ a ] = <NUM_LIT:1> \n d [ b ] = <NUM_LIT:2> \n d [ t ] = <NUM_LIT:3> \n print \"<STR_LIT>\" , d \n s = set ( ) \n s . add ( a ) \n s . add ( b ) \n s . add ( t ) \n print \"<STR_LIT>\" , s \n print \n print \"<STR_LIT>\" , tuple ( x for x in a ) \n print \"<STR_LIT>\" , list ( a ) \n print \"<STR_LIT>\" , tuple ( a ) \n print \n a0 = pickle . loads ( pickle . dumps ( a , <NUM_LIT:0> ) ) \n a1 = pickle . loads ( pickle . dumps ( a , <NUM_LIT:1> ) ) \n a2 = pickle . loads ( pickle . dumps ( a , <NUM_LIT:2> ) ) \n print \"<STR_LIT>\" , a0 \n print \"<STR_LIT>\" , a1 \n print \"<STR_LIT>\" , a2 \n print \"<STR_LIT>\" , a0 == a , hash ( a0 ) == hash ( a ) \n print \"<STR_LIT>\" , a1 == a , hash ( a1 ) == hash ( a ) \n print \"<STR_LIT>\" , a2 == a , hash ( a2 ) == hash ( a ) \n print \n print \"<STR_LIT>\" , a [ : ] \n print \"<STR_LIT>\" , a [ <NUM_LIT:1> : - <NUM_LIT:1> ] \n print \"<STR_LIT>\" , a + a \n print \"<STR_LIT>\" , a + b \n print \"<STR_LIT>\" , ( <NUM_LIT:0> , ) + a \n print \"<STR_LIT>\" , a + ( <NUM_LIT:0> , ) \n print \"<STR_LIT>\" , <NUM_LIT:2> * a \n print \"<STR_LIT>\" , a * <NUM_LIT:2> \n print \n print \"<STR_LIT>\" , A ( * chain ( ( x ** <NUM_LIT:2> for x in range ( <NUM_LIT:10> ) ) , a ) ) \n print \"<STR_LIT>\" , A ( * product ( range ( <NUM_LIT:3> ) , repeat = <NUM_LIT:2> ) ) \n print \n print \"<STR_LIT>\" \n print \n A = rectuple ( '<STR_LIT:A>' , '<STR_LIT>' , verbose = True ) \n B = rectuple ( '<STR_LIT:B>' , '<STR_LIT>' , verbose = True ) \n a = A ( <NUM_LIT:1> , <NUM_LIT:2> ) \n b = B ( <NUM_LIT:1> , <NUM_LIT:2> ) \n t = ( <NUM_LIT:1> , <NUM_LIT:2> ) \n print \"<STR_LIT>\" , a \n print \"<STR_LIT>\" , b \n print \"<STR_LIT>\" , t \n print \n print \"<STR_LIT>\" , a == b \n print \"<STR_LIT>\" , a != b \n print \"<STR_LIT>\" , hash ( a ) == hash ( b ) \n print \"<STR_LIT>\" , a <= b \n print \"<STR_LIT>\" , b <= a \n print \n print \"<STR_LIT>\" , a == t \n print \"<STR_LIT>\" , a != t \n print \"<STR_LIT>\" , hash ( a ) == hash ( t ) \n print \"<STR_LIT>\" , a <= t \n print \"<STR_LIT>\" , t <= a \n print \n d = { } \n d [ a ] = <NUM_LIT:1> \n d [ b ] = <NUM_LIT:2> \n d [ t ] = <NUM_LIT:3> \n print \"<STR_LIT>\" , d \n s = set ( ) \n s . add ( a ) \n s . add ( b ) \n s . add ( t ) \n print \"<STR_LIT>\" , s \n print \n print \"<STR_LIT>\" , tuple ( x for x in a ) \n print \"<STR_LIT>\" , list ( a ) \n print \"<STR_LIT>\" , tuple ( a ) \n print \n a0 = pickle . loads ( pickle . dumps ( a , <NUM_LIT:0> ) ) \n a1 = pickle . loads ( pickle . dumps ( a , <NUM_LIT:1> ) ) \n a2 = pickle . loads ( pickle . dumps ( a , <NUM_LIT:2> ) ) \n print \"<STR_LIT>\" , a0 \n print \"<STR_LIT>\" , a1 \n print \"<STR_LIT>\" , a2 \n print \"<STR_LIT>\" , a0 == a , hash ( a0 ) == hash ( a ) \n print \"<STR_LIT>\" , a1 == a , hash ( a1 ) == hash ( a ) \n print \"<STR_LIT>\" , a2 == a , hash ( a2 ) == hash ( <mask0> ) \n", "gt": "a"}
{"input": "\n import pandas \n import util \n import matplotlib . pyplot as plt \n import scipy as sp \n import scipy . stats \n import numpy as np \n import os \n cur_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) \n def from_custom_file ( data_file , learn_options ) : \n print \"<STR_LIT>\" % data_file \n data = pandas . read_csv ( data_file ) \n mandatory_columns = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n for col in mandatory_columns : \n assert col in data . columns , \"<STR_LIT>\" % mandatory_columns \n Xdf = pandas . DataFrame ( data ) \n Xdf [ '<STR_LIT>' ] = Xdf [ '<STR_LIT>' ] \n Xdf = Xdf . set_index ( [ '<STR_LIT>' , '<STR_LIT>' ] ) \n Xdf [ '<STR_LIT>' ] = Xdf [ '<STR_LIT>' ] \n Xdf . index . names = [ '<STR_LIT>' , '<STR_LIT>' ] \n Xdf [ '<STR_LIT>' ] = [ '<STR_LIT>' % i for i in range ( Xdf . shape [ <NUM_LIT:0> ] ) ] \n Xdf = Xdf . set_index ( '<STR_LIT>' , append = True ) \n Y = None \n gene_position = Xdf [ [ '<STR_LIT>' , '<STR_LIT>' ] ] \n target_genes = np . unique ( Xdf . index . levels [ <NUM_LIT:1> ] ) \n learn_options = set_V2_target_names ( learn_options ) \n return Xdf , Y , gene_position , target_genes \n def from_file ( data_file , learn_options , data_file2 = None , data_file3 = None ) : \n if learn_options [ \"<STR_LIT>\" ] == <NUM_LIT:1> : \n print \"<STR_LIT>\" % learn_options [ \"<STR_LIT>\" ] \n assert not learn_options [ \"<STR_LIT>\" ] is not None , \"<STR_LIT>\" \n annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options ) \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n elif learn_options [ \"<STR_LIT>\" ] == <NUM_LIT:2> : \n Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , learn_options ) \n xx = Xdf [ '<STR_LIT>' ] . values \n yy = Y [ '<STR_LIT>' ] . values \n rr , pp = sp . stats . pearsonr ( xx , yy ) \n assert rr > <NUM_LIT:0> , \"<STR_LIT>\" \n learn_options = set_V2_target_names ( learn_options ) \n elif learn_options [ \"<STR_LIT>\" ] == <NUM_LIT:3> : \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n learn_options [ '<STR_LIT>' ] = None \n Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n elif learn_options [ \"<STR_LIT>\" ] == <NUM_LIT:4> : \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n learn_options [ '<STR_LIT>' ] = None \n Xdf , Y , gene_position , target_genes = merge_all ( data_file , data_file2 , data_file3 , learn_options ) \n elif learn_options [ '<STR_LIT>' ] == <NUM_LIT:5> : \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n learn_options [ '<STR_LIT>' ] = None \n gene_position , target_genes , Xdf , Y = read_xu_et_al ( data_file3 ) \n Xdf [ \"<STR_LIT>\" ] = Xdf [ \"<STR_LIT>\" ] . apply ( lambda x : x [ <NUM_LIT:0> : <NUM_LIT:30> ] ) \n return Xdf , Y , gene_position , target_genes \n def set_V2_target_names ( learn_options ) : \n if '<STR_LIT>' not in learn_options . keys ( ) : \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n if '<STR_LIT>' not in learn_options . keys ( ) : \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n learn_options [ '<STR_LIT>' ] = '<STR_LIT>' \n return learn_options \n def combine_organisms ( human_data , mouse_data ) : \n cd13 = human_data . xs ( '<STR_LIT>' , level = '<STR_LIT>' , drop_level = False ) \n X_CD13 , Y_CD13 = util . get_data ( cd13 , y_names = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n cd33 = human_data . xs ( '<STR_LIT>' , level = '<STR_LIT>' , drop_level = False ) \n X_CD33 , Y_CD33 = util . get_data ( cd33 , y_names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n cd15 = human_data . xs ( '<STR_LIT>' , level = '<STR_LIT>' , drop_level = False ) \n X_CD15 , Y_CD15 = util . get_data ( cd15 , y_names = [ '<STR_LIT>' ] ) \n mouse_X = pandas . DataFrame ( ) \n mouse_Y = pandas . DataFrame ( ) \n for k in mouse_data . index . levels [ <NUM_LIT:1> ] : \n X , Y = util . get_data ( mouse_data . xs ( k , level = '<STR_LIT>' , drop_level = False ) , [ \"<STR_LIT>\" ] , target_gene = k , organism = '<STR_LIT>' ) \n mouse_X = pandas . concat ( [ mouse_X , X ] , axis = <NUM_LIT:0> ) \n mouse_Y = pandas . concat ( [ mouse_Y , Y ] , axis = <NUM_LIT:0> ) \n X = pandas . concat ( [ X_CD13 , X_CD15 , X_CD33 , mouse_X ] , axis = <NUM_LIT:0> ) \n Y = pandas . concat ( [ Y_CD13 , Y_CD15 , Y_CD33 , mouse_Y ] , axis = <NUM_LIT:0> ) \n return X , Y \n def read_V1_data ( data_file , learn_options , AML_file = cur_dir + \"<STR_LIT>\" ) : \n if data_file is None : \n data_file = cur_dir + \"<STR_LIT>\" \n human_data = pandas . read_excel ( data_file , sheetname = <NUM_LIT:0> , index_col = [ <NUM_LIT:0> , <NUM_LIT:1> ] ) \n mouse_data = pandas . read_excel ( data_file , sheetname = <NUM_LIT:1> , index_col = [ <NUM_LIT:0> , <NUM_LIT:1> ] ) \n Xdf , Y = combine_organisms ( human_data , mouse_data ) \n annotations = pandas . read_csv ( AML_file , delimiter = '<STR_LIT:\\t>' , index_col = [ <NUM_LIT:0> , <NUM_LIT:4> ] ) \n annotations . index . names = Xdf . index . names \n gene_position = pandas . merge ( Xdf , annotations , how = \"<STR_LIT>\" , left_index = True , right_index = True ) \n gene_position = util . impute_gene_position ( gene_position ) \n gene_position = gene_position [ [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] \n Y = Y . loc [ gene_position . index ] \n Xdf = Xdf . loc [ gene_position . index ] \n Y [ '<STR_LIT:test>' ] = <NUM_LIT:1> \n target_genes = Y [ '<STR_LIT>' ] . unique ( ) \n Y . index . names = [ '<STR_LIT>' , '<STR_LIT>' ] \n assert Xdf . index . equals ( Y . index ) , \"<STR_LIT>\" \n if learn_options is not None and learn_options [ \"<STR_LIT>\" ] : \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n Y [ '<STR_LIT>' ] = Y [ '<STR_LIT>' ] < <NUM_LIT> \n print \"<STR_LIT>\" \n import ipdb \n ipdb . set_trace ( ) \n return annotations , gene_position , target_genes , Xdf , Y \n def rank_transform ( x ) : \n return <NUM_LIT:1.0> - sp . stats . mstats . rankdata ( x ) / sp . stats . mstats . rankdata ( x ) . max ( ) \n def read_xu_et_al ( data_file , learn_options = None , verbose = True , subsetting = '<STR_LIT>' ) : \n if data_file is None : \n data_file = '<STR_LIT>' \n datasets = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n aggregated = None \n for d in datasets : \n data_efficient = pandas . read_excel ( data_file , sheetname = '<STR_LIT>' % d , skiprows = <NUM_LIT:2> ) \n data_inefficient = pandas . read_excel ( data_file , sheetname = '<STR_LIT>' % d , skiprows = <NUM_LIT:2> ) \n data_efficient [ '<STR_LIT>' ] = <NUM_LIT:1.> \n data_inefficient [ '<STR_LIT>' ] = <NUM_LIT:0.> \n exp_data = pandas . concat ( ( data_efficient , data_inefficient ) ) \n exp_data [ '<STR_LIT>' ] = exp_data . groupby ( '<STR_LIT>' ) [ '<STR_LIT>' ] . transform ( rank_transform ) \n exp_data [ '<STR_LIT>' ] = exp_data . groupby ( '<STR_LIT>' ) [ '<STR_LIT>' ] . transform ( rank_transform ) \n if aggregated is None : \n aggregated = exp_data \n else : \n aggregated = pandas . concat ( ( aggregated , exp_data ) ) \n if subsetting == '<STR_LIT>' : \n aggregated [ \"<STR_LIT>\" ] = aggregated [ \"<STR_LIT>\" ] . apply ( lambda x : x [ <NUM_LIT:6> : - <NUM_LIT:4> ] ) \n else : \n aggregated [ \"<STR_LIT>\" ] = aggregated [ \"<STR_LIT>\" ] . apply ( lambda x : x [ <NUM_LIT:10> : ] ) \n aggregated [ \"<STR_LIT>\" ] = aggregated [ \"<STR_LIT>\" ] . apply ( lambda x : x . upper ( ) ) \n aggregated . rename ( columns = { \"<STR_LIT>\" : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , inplace = True ) \n aggregated [ '<STR_LIT>' ] . loc [ aggregated [ '<STR_LIT>' ] == '<STR_LIT:+>' ] = '<STR_LIT>' \n aggregated [ '<STR_LIT>' ] . loc [ aggregated [ '<STR_LIT>' ] == '<STR_LIT:->' ] = '<STR_LIT>' \n aggregated [ '<STR_LIT>' ] = aggregated [ [ '<STR_LIT>' , '<STR_LIT>' ] ] . mean ( axis = <NUM_LIT:1> ) \n df = aggregated \n df = df . rename ( columns = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ) \n df [ '<STR_LIT>' ] = '<STR_LIT>' \n df [ '<STR_LIT:test>' ] = <NUM_LIT:1> \n df = df . set_index ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n df [ '<STR_LIT>' ] = df . index . get_level_values ( <NUM_LIT:0> ) \n df [ '<STR_LIT>' ] = df . index . get_level_values ( <NUM_LIT:1> ) \n df [ '<STR_LIT>' ] = '<STR_LIT>' \n df [ '<STR_LIT>' ] = df [ '<STR_LIT>' ] \n df [ '<STR_LIT>' ] = df [ '<STR_LIT>' ] \n df [ '<STR_LIT>' ] = df [ '<STR_LIT>' ] \n df [ '<STR_LIT>' ] = <NUM_LIT:0> \n df [ '<STR_LIT>' ] = <NUM_LIT:0> \n target_genes = np . unique ( df [ '<STR_LIT>' ] . values ) \n return df [ [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] , target_genes , df [ [ '<STR_LIT>' , '<STR_LIT>' ] ] , df [ [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT:test>' , '<STR_LIT>' ] ] \n def read_V2_data ( data_file , learn_options = None , verbose = True ) : \n if data_file is None : \n data_file = cur_dir + \"<STR_LIT>\" \n data = pandas . read_excel ( data_file , sheetname = \"<STR_LIT>\" , skiprows = range ( <NUM_LIT:0> , <NUM_LIT:6> + <NUM_LIT:1> ) , index_col = [ <NUM_LIT:0> , <NUM_LIT:4> ] ) \n Xdf = pandas . DataFrame ( ) \n known_pairs = { '<STR_LIT>' : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , \n '<STR_LIT>' : [ '<STR_LIT>' ] , \n '<STR_LIT>' : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] } \n drugs_to_genes = { '<STR_LIT>' : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , \n '<STR_LIT>' : [ '<STR_LIT>' ] , \n '<STR_LIT>' : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] } \n if learn_options is not None : \n assert not ( learn_options [ '<STR_LIT>' ] and learn_options [ '<STR_LIT>' ] ) , \"<STR_LIT>\" \n if learn_options [ '<STR_LIT>' ] : \n drugs_to_genes [ '<STR_LIT>' ] . extend ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n elif learn_options [ '<STR_LIT>' ] : \n drugs_to_genes [ '<STR_LIT>' ] . extend ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n drugs_to_genes [ '<STR_LIT>' ] . extend ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n drugs_to_genes [ '<STR_LIT>' ] . extend ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n count = <NUM_LIT:0> \n for drug in drugs_to_genes . keys ( ) : \n genes = drugs_to_genes [ drug ] \n for g in genes : \n Xtmp = data . copy ( ) . xs ( g , level = '<STR_LIT>' , drop_level = False ) \n Xtmp [ '<STR_LIT>' ] = drug \n Xtmp [ '<STR_LIT>' ] = Xtmp [ drug ] . copy ( ) \n if g in known_pairs [ drug ] : \n Xtmp [ '<STR_LIT:test>' ] = <NUM_LIT:1.> \n else : \n Xtmp [ '<STR_LIT:test>' ] = <NUM_LIT:0.> \n count = count + Xtmp . shape [ <NUM_LIT:0> ] \n Xdf = pandas . concat ( [ Xdf , Xtmp ] , axis = <NUM_LIT:0> ) \n if verbose : \n print \"<STR_LIT>\" % ( Xtmp . shape [ <NUM_LIT:0> ] , g , count ) \n Xdf = Xdf . set_index ( '<STR_LIT>' , append = True ) \n Y = pandas . DataFrame ( Xdf . pop ( \"<STR_LIT>\" ) ) \n Y . columns . names = [ \"<STR_LIT>\" ] \n test_gene = pandas . DataFrame ( Xdf . pop ( '<STR_LIT:test>' ) ) \n target = pandas . DataFrame ( Xdf . index . get_level_values ( '<STR_LIT>' ) . values , index = Y . index , columns = [ \"<STR_LIT>\" ] ) \n Y = pandas . concat ( ( Y , target , test_gene ) , axis = <NUM_LIT:1> ) \n target_genes = Y [ '<STR_LIT>' ] . unique ( ) \n gene_position = Xdf [ [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ] . copy ( ) \n y_rank = pandas . DataFrame ( ) \n y_threshold = pandas . DataFrame ( ) \n y_quant = pandas . DataFrame ( ) \n for drug in drugs_to_genes . keys ( ) : \n gene_list = drugs_to_genes [ drug ] \n for gene in gene_list : \n ytmp = pandas . DataFrame ( Y . xs ( ( gene , drug ) , level = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , drop_level = False ) [ '<STR_LIT>' ] ) \n y_ranktmp , y_rank_raw , y_thresholdtmp , y_quanttmp = util . get_ranks ( ytmp , thresh = <NUM_LIT> , prefix = \"<STR_LIT>\" , flip = False ) \n y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = <NUM_LIT:0> ) \n y_threshold = pandas . concat ( ( y_threshold , y_thresholdtmp ) , axis = <NUM_LIT:0> ) \n y_quant = pandas . concat ( ( y_quant , y_quanttmp ) , axis = <NUM_LIT:0> ) \n yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = <NUM_LIT:1> ) \n Y = pandas . merge ( Y , yall , how = '<STR_LIT>' , left_index = True , right_index = True ) \n y_rank = pandas . DataFrame ( ) \n y_threshold = pandas . DataFrame ( ) \n y_quant = pandas . DataFrame ( ) \n for drug in drugs_to_genes . keys ( ) : \n ytmp = pandas . DataFrame ( Y . xs ( drug , level = \"<STR_LIT>\" , drop_level = False ) [ '<STR_LIT>' ] ) \n y_ranktmp , y_rank_raw , y_thresholdtmp , y_quanttmp = util . get_ranks ( ytmp , thresh = <NUM_LIT> , prefix = \"<STR_LIT>\" , flip = False ) \n y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = <NUM_LIT:0> ) \n y_threshold = pandas . concat ( ( y_threshold , y_thresholdtmp ) , axis = <NUM_LIT:0> ) \n y_quant = pandas . concat ( ( y_quant , y_quanttmp ) , axis = <NUM_LIT:0> ) \n yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = <NUM_LIT:1> ) \n Y = pandas . merge ( Y , yall , how = '<STR_LIT>' , left_index = True , right_index = True ) \n PLOT = False \n if PLOT : \n labels = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n for label in labels : \n plt . figure ( ) \n plt . plot ( Xdf [ '<STR_LIT>' ] . values , Y [ label ] . values , '<STR_LIT:.>' ) \n r , pearp = sp . stats . pearsonr ( Xdf [ '<STR_LIT>' ] . values . flatten ( ) , Y [ label ] . values . flatten ( ) ) \n plt . title ( label + '<STR_LIT>' % ( r , pearp ) ) \n plt . xlabel ( \"<STR_LIT>\" ) \n plt . ylabel ( label ) \n gene_position = util . impute_gene_position ( gene_position ) \n if learn_options is not None and learn_options [ \"<STR_LIT>\" ] == \"<STR_LIT>\" : \n print \"<STR_LIT>\" \n data = pandas . read_excel ( data_file , sheetname = \"<STR_LIT>\" , skiprows = range ( <NUM_LIT:0> , <NUM_LIT:6> + <NUM_LIT:1> ) , index_col = [ <NUM_LIT:0> , <NUM_LIT:4> ] ) \n data . index . names = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n experiments = { } \n experiments [ '<STR_LIT>' ] = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n experiments [ '<STR_LIT>' ] = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n experiments [ '<STR_LIT>' ] = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n variance = None \n for drug in drugs_to_genes . keys ( ) : \n data_tmp = data . iloc [ data . index . get_level_values ( '<STR_LIT>' ) . isin ( drugs_to_genes [ drug ] ) ] [ experiments [ drug ] ] \n data_tmp [ \"<STR_LIT>\" ] = drug \n data_tmp = data_tmp . set_index ( '<STR_LIT>' , append = True ) \n data_tmp [ \"<STR_LIT>\" ] = np . var ( data_tmp . values , axis = <NUM_LIT:1> ) \n if variance is None : \n variance = data_tmp [ \"<STR_LIT>\" ] . copy ( ) \n else : \n variance = pandas . concat ( ( variance , data_tmp [ \"<STR_LIT>\" ] ) , axis = <NUM_LIT:0> ) \n orig_index = Y . index . copy ( ) \n Y = pandas . merge ( Y , pandas . DataFrame ( variance ) , how = \"<STR_LIT>\" , left_index = True , right_index = True ) \n Y = Y . ix [ orig_index ] \n print \"<STR_LIT>\" \n assert Xdf . index . equals ( Y . index ) , \"<STR_LIT>\" \n return Xdf , drugs_to_genes , target_genes , Y , gene_position \n def merge_all ( data_file = None , data_file2 = None , data_file3 = None , learn_options = None ) : \n Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n gene_position_xu , target_genes_xu , Xdf_xu , Y_xu = read_xu_et_al ( data_file3 , learn_options ) \n Xdf = pandas . concat ( ( Xdf , Xdf_xu ) ) \n Y = pandas . concat ( ( Y , Y_xu ) ) \n gene_position = pandas . concat ( ( gene_position , gene_position_xu ) ) \n target_genes = np . concatenate ( ( target_genes , target_genes_xu ) ) \n return Xdf , Y , gene_position , target_genes \n def mergeV1_V2 ( data_file , data_file2 , learn_options ) : \n '''<STR_LIT>''' \n assert not learn_options [ '<STR_LIT>' ] , \"<STR_LIT>\" \n annotations , gene_position1 , target_genes1 , Xdf1 , Y1 = read_V1_data ( data_file , learn_options ) \n Xdf2 , drugs_to_genes , target_genes2 , Y2 , gene_position2 = read_V2_data ( data_file2 ) \n Y1 . rename ( columns = { '<STR_LIT>' : learn_options [ \"<STR_LIT>\" ] } , inplace = True ) \n Y1 . rename ( columns = { '<STR_LIT>' : learn_options [ \"<STR_LIT>\" ] } , inplace = True ) \n Y1 [ \"<STR_LIT>\" ] = [ \"<STR_LIT>\" for x in range ( Y1 . shape [ <NUM_LIT:0> ] ) ] \n Y1 = Y1 . set_index ( '<STR_LIT>' , append = True ) \n Y1 . index . names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n Y_cols_to_keep = np . unique ( [ '<STR_LIT>' , '<STR_LIT:test>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n Y1 = Y1 [ Y_cols_to_keep ] \n Y2 = Y2 [ Y_cols_to_keep ] \n Xdf1 [ \"<STR_LIT>\" ] = [ \"<STR_LIT>\" for x in range ( Xdf1 . shape [ <NUM_LIT:0> ] ) ] \n Xdf1 = Xdf1 . set_index ( '<STR_LIT>' , append = True ) \n X_cols_to_keep = [ '<STR_LIT>' , '<STR_LIT>' ] \n Xdf1 = Xdf1 [ X_cols_to_keep ] \n Xdf2 = Xdf2 [ X_cols_to_keep ] \n gene_position1 [ \"<STR_LIT>\" ] = [ \"<STR_LIT>\" for x in range ( gene_position1 . shape [ <NUM_LIT:0> ] ) ] \n gene_position1 = gene_position1 . set_index ( '<STR_LIT>' , append = True ) \n gene_position1 . index . names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n cols_to_keep = [ u'<STR_LIT>' , u'<STR_LIT>' ] \n gene_position1 = gene_position1 [ cols_to_keep ] \n gene_position2 = gene_position2 [ cols_to_keep ] \n Y = pandas . concat ( ( Y1 , Y2 ) , axis = <NUM_LIT:0> ) \n Xdf = pandas . concat ( ( Xdf1 , Xdf2 ) , axis = <NUM_LIT:0> ) \n gene_position = pandas . concat ( ( gene_position1 , gene_position2 ) ) \n target_genes = np . concatenate ( ( target_genes1 , target_genes2 ) ) \n save_to_file = False \n if save_to_file : \n Y . index . names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n assert np . all ( Xdf . index . values == Y . index . values ) , \"<STR_LIT>\" \n onedupind = np . where ( Y . index . duplicated ( ) ) [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] \n alldupind = np . where ( Y . index . get_level_values ( <NUM_LIT:0> ) . values == Y . index [ onedupind ] [ <NUM_LIT:0> ] ) [ <NUM_LIT:0> ] \n assert len ( alldupind ) == <NUM_LIT:2> , \"<STR_LIT>\" \n newindex = Y . index . tolist ( ) \n newindex [ onedupind ] = ( newindex [ onedupind ] [ <NUM_LIT:0> ] , newindex [ onedupind ] [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n Y . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n Xdf . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n XandY = pandas . merge ( Xdf , Y , how = \"<STR_LIT>\" , left_index = True , right_index = True ) \n gene_position_tmp = gene_position . copy ( ) \n gene_position_tmp . index . names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n gene_position_tmp . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n XandY = pandas . merge ( XandY , gene_position_tmp , how = \"<STR_LIT>\" , left_index = True , right_index = True ) \n XandY [ \"<STR_LIT>\" ] = XandY [ \"<STR_LIT>\" ] . apply ( lambda x : x [ <NUM_LIT:0> : <NUM_LIT:30> ] ) \n XandY . to_csv ( r'<STR_LIT>' ) \n return Xdf , Y , gene_position , target_genes \n def get_V1_genes ( data_file = None ) : \n annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options = None ) \n return target_genes \n def get_V2_genes ( data_file = None ) : \n Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , verbose = False ) \n return target_genes \n def get_V3_genes ( data_fileV1 = None , data_fileV2 = None ) : \n target_genes = np . concatenate ( ( get_V1_genes ( data_fileV1 ) , get_V2_genes ( data_fileV2 ) ) ) \n return target_genes \n def get_xu_genes ( data_file = None ) : \n return read_xu_et_al ( data_file ) [ <NUM_LIT:1> ] \n def get_mouse_genes ( data_file = None ) : \n annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options = None ) \n return Xdf [ Xdf [ '<STR_LIT>' ] == '<STR_LIT>' ] [ '<STR_LIT>' ] . unique ( ) \n def get_human_genes ( data_file = None ) : \n annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options = None ) \n mouse_genes = Xdf [ Xdf [ '<STR_LIT>' ] == '<STR_LIT>' ] [ '<STR_LIT>' ] . unique ( ) \n all_genes = get_V3_genes ( None , None ) \n return np . setdiff1d ( all_genes , <mask0> ) \n", "gt": "mouse_genes"}
{"input": "\n import os \n from os import path \n import sys \n try : \n import simplejson as json \n except ImportError : \n import json \n from . detector import Detector \n from . lang_detect_exception import ErrorCode , LangDetectException \n from . utils . lang_profile import LangProfile \n class DetectorFactory ( object ) : \n '''<STR_LIT>''' \n seed = None \n def __init__ ( self ) : \n self . word_lang_prob_map = { } \n self . langlist = [ ] \n def load_profile ( self , profile_directory ) : \n list_files = os . listdir ( profile_directory ) \n if not list_files : \n raise LangDetectException ( ErrorCode . NeedLoadProfileError , '<STR_LIT>' + profile_directory ) \n langsize , index = len ( list_files ) , <NUM_LIT:0> \n for filename in list_files : \n if filename . startswith ( '<STR_LIT:.>' ) : \n continue \n filename = path . join ( profile_directory , filename ) \n if not path . isfile ( filename ) : \n continue \n f = None \n try : \n if sys . version_info [ <NUM_LIT:0> ] < <NUM_LIT:3> : \n f = open ( filename , '<STR_LIT:r>' ) \n else : \n f = open ( filename , '<STR_LIT:r>' , encoding = '<STR_LIT:utf-8>' ) \n json_data = json . load ( f ) \n profile = LangProfile ( ** json_data ) \n self . add_profile ( profile , index , langsize ) \n index += <NUM_LIT:1> \n except IOError : \n raise LangDetectException ( ErrorCode . FileLoadError , '<STR_LIT>' % filename ) \n except : \n raise LangDetectException ( ErrorCode . FormatError , '<STR_LIT>' % filename ) \n finally : \n if f : \n f . close ( ) \n def load_json_profile ( self , json_profiles ) : \n langsize , index = len ( json_profiles ) , <NUM_LIT:0> \n if langsize < <NUM_LIT:2> : \n raise LangDetectException ( ErrorCode . NeedLoadProfileError , '<STR_LIT>' ) \n for json_profile in json_profiles : \n try : \n json_data = json . loads ( json_profile ) \n profile = LangProfile ( ** json_data ) \n self . add_profile ( profile , index , langsize ) \n index += <NUM_LIT:1> \n except : \n raise LangDetectException ( ErrorCode . FormatError , '<STR_LIT>' ) \n def add_profile ( self , profile , index , langsize ) : \n lang = profile . name \n if lang in self . langlist : \n raise LangDetectException ( ErrorCode . DuplicateLangError , '<STR_LIT>' ) \n self . langlist . append ( lang ) \n for word in profile . freq : \n if word not in self . word_lang_prob_map : \n self . word_lang_prob_map [ word ] = [ <NUM_LIT:0.0> ] * langsize \n length = len ( word ) \n if <NUM_LIT:1> <= length <= <NUM_LIT:3> : \n prob = <NUM_LIT:1.0> * profile . freq . get ( word ) / profile . n_words [ length - <NUM_LIT:1> ] \n self . word_lang_prob_map [ word ] [ index ] = prob \n def clear ( self ) : \n self . langlist = [ ] \n self . word_lang_prob_map = { } \n def create ( self , alpha = None ) : \n '''<STR_LIT>''' \n detector = self . _create_detector ( ) \n if alpha is not None : \n detector . set_alpha ( alpha ) \n return detector \n def _create_detector ( self ) : \n if not self . langlist : \n raise LangDetectException ( ErrorCode . NeedLoadProfileError , '<STR_LIT>' ) \n return Detector ( self ) \n def set_seed ( self , seed ) : \n self . seed = seed \n def get_lang_list ( self ) : \n return list ( self . langlist ) \n PROFILES_DIRECTORY = path . join ( path . dirname ( __file__ ) , '<STR_LIT>' ) \n _factory = None \n def init_factory ( ) : \n global _factory \n if _factory is None : \n _factory = DetectorFactory ( ) \n _factory . load_profile ( PROFILES_DIRECTORY ) \n def detect ( text ) : \n init_factory ( ) \n detector = _factory . create ( ) \n detector . append ( text ) \n return detector . detect ( ) \n def detect_langs ( text ) : \n init_factory ( ) \n detector = _factory . create ( ) \n detector . append ( text ) \n return detector . <mask0> ( ) \n", "gt": "get_probabilities"}
{"input": "\n import math \n def distance ( pa , pb ) : \n ax , ay = pa \n bx , by = pb \n return math . sqrt ( ( ax - bx ) ** <NUM_LIT:2> + ( ay - by ) ** <NUM_LIT:2> ) \n def index_of_nearest ( p , hot_points , distance_f = distance ) : \n \"\"\"<STR_LIT>\"\"\" \n min_dist = None \n nearest_hp_i = None \n for i , hp in enumerate ( hot_points ) : \n dist = distance_f ( p , hp ) \n if min_dist is None or dist < min_dist : \n min_dist = dist \n nearest_hp_i = i \n return <mask0> \n", "gt": "nearest_hp_i"}
{"input": "\n from pig_util import outputSchema \n @ outputSchema ( '<STR_LIT>' ) \n def reverse ( word ) : \n \"\"\"<STR_LIT>\"\"\" \n return word [ : : - <NUM_LIT:1> ] \n @ outputSchema ( '<STR_LIT>' ) \n def num_chars ( word ) : \n \"\"\"<STR_LIT>\"\"\" \n return len ( <mask0> ) \n", "gt": "word"}
{"input": "\n from fabric import main as fab_main \n from cloudferry import fabfile \n def main ( ) : \n fab = fabfile . __file__ \n if fab . endswith ( '<STR_LIT>' ) : \n fab = fab [ : - <NUM_LIT:1> ] \n fab_main . main ( [ fab ] ) \n if __name__ == '<STR_LIT:__main__>' : \n <mask0> ( ) \n", "gt": "main"}
{"input": "\n from cloudferry . lib . base . action import action \n DEFAULT = <NUM_LIT:0> \n PATH_ONE = <NUM_LIT:1> \n PATH_TWO = <NUM_LIT:2> \n class IsOption ( action . Action ) : \n def __init__ ( self , init , option_name ) : \n self . option_name = option_name \n super ( IsOption , self ) . __init__ ( init ) \n def run ( self , ** kwargs ) : \n self . set_next_path ( DEFAULT ) \n option_value = self . cfg . migrate [ self . option_name ] \n if option_value : \n self . set_next_path ( PATH_ONE ) \n else : \n self . set_next_path ( PATH_TWO ) \n <mask0> { } \n", "gt": "return"}
{"input": "\n from cloudferry . lib . base . action import action \n from cloudferry . lib . utils import log \n from cloudferry . lib . utils import utils as utl \n LOG = log . getLogger ( __name__ ) \n class CheckConfigQuotaNeutron ( action . Action ) : \n \"\"\"<STR_LIT>\"\"\" \n def run ( self , ** kwargs ) : \n src_cloud = self . src_cloud \n dst_cloud = self . dst_cloud \n network_src = src_cloud . resources [ utl . NETWORK_RESOURCE ] \n identity_dst = dst_cloud . resources [ utl . IDENTITY_RESOURCE ] \n network_dst = dst_cloud . resources [ utl . NETWORK_RESOURCE ] \n search_opts_tenant = kwargs . get ( '<STR_LIT>' , { } ) \n tenants_src = self . get_src_tenants ( search_opts_tenant ) \n list_quotas = network_src . list_quotas ( ) \n tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n list_quotas ) \n if not tenants_without_quotas : \n LOG . info ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n LOG . info ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n LOG . info ( \"<STR_LIT>\" ) \n return \n LOG . info ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n quot = network_src . show_quota ( tenants_without_quotas [ <NUM_LIT:0> ] ) \n dst_temp_tenant = identity_dst . create_tenant ( \"<STR_LIT>\" ) \n quot_default_dst = network_dst . show_quota ( dst_temp_tenant . id ) \n is_configs_different = False \n identity_dst . delete_tenant ( dst_temp_tenant ) \n for item_quot , val_quot in quot . iteritems ( ) : \n if val_quot != quot_default_dst [ item_quot ] : \n is_configs_different = True \n LOG . info ( \"<STR_LIT>\" \n \"<STR_LIT>\" , item_quot , val_quot , \n quot_default_dst [ item_quot ] ) \n if not is_configs_different : \n LOG . info ( \"<STR_LIT>\" ) \n @ staticmethod \n def get_tenants_without_quotas ( tenants_src , list_quotas ) : \n tenants_ids = tenants_src . keys ( ) \n quotas_ids_tenants = [ quota [ \"<STR_LIT>\" ] for quota in list_quotas ] \n return list ( set ( tenants_ids ) - set ( quotas_ids_tenants ) ) \n def get_src_tenants ( self , search_opts ) : \n identity_src = self . src_cloud . resources [ utl . IDENTITY_RESOURCE ] \n if search_opts . get ( '<STR_LIT>' ) : \n filter_tenants_ids_list = search_opts [ '<STR_LIT>' ] \n tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n tnt_id in filter_tenants_ids_list ] \n else : \n tenants = identity_src . get_tenants_list ( ) \n tenants_dict = { tenant . id : tenant . name for tenant in tenants } \n return <mask0> \n", "gt": "tenants_dict"}
{"input": "\n import copy \n import logging \n from oslo_config import cfg \n from cloudferry . lib . base . action import action \n from cloudferry . lib . utils import utils \n CONF = cfg . CONF \n LOG = logging . getLogger ( __name__ ) \n class DetachVolumesCompute ( action . Action ) : \n def run ( self , info , ** kwargs ) : \n info = copy . deepcopy ( info ) \n compute_resource = self . cloud . resources [ utils . COMPUTE_RESOURCE ] \n storage_resource = self . cloud . resources [ utils . STORAGE_RESOURCE ] \n for instance in info [ utils . INSTANCES_TYPE ] . itervalues ( ) : \n LOG . info ( \"<STR_LIT>\" , \n instance [ '<STR_LIT>' ] [ '<STR_LIT:name>' ] , instance [ '<STR_LIT>' ] [ '<STR_LIT:id>' ] ) \n if not instance [ '<STR_LIT>' ] [ utils . VOLUMES_TYPE ] : \n continue \n for vol in instance [ '<STR_LIT>' ] [ utils . VOLUMES_TYPE ] : \n volume_status = storage_resource . get_status ( vol [ '<STR_LIT:id>' ] ) \n LOG . debug ( \"<STR_LIT>\" , \n vol [ '<STR_LIT:id>' ] , volume_status ) \n if volume_status == '<STR_LIT>' : \n compute_resource . detach_volume ( instance [ '<STR_LIT>' ] [ '<STR_LIT:id>' ] , \n vol [ '<STR_LIT:id>' ] ) \n LOG . debug ( \"<STR_LIT>\" , vol [ '<STR_LIT:id>' ] ) \n timeout = CONF . migrate . storage_backend_timeout \n storage_resource . wait_for_status ( \n vol [ '<STR_LIT:id>' ] , storage_resource . get_status , '<STR_LIT>' , \n timeout = timeout ) \n <mask0> { } \n", "gt": "return"}
{"input": "\n from cloudferry . lib . base . action import action \n from cloudferry . lib . utils . ssh_util import SshUtil \n class RemoteExecution ( action . Action ) : \n def __init__ ( self , cloud , host = None , int_host = None , config_migrate = None ) : \n self . cloud = cloud \n self . host = host \n self . int_host = int_host \n self . config_migrate = config_migrate \n self . remote_exec_obj = SshUtil ( self . cloud , \n self . config_migrate , \n self . host ) \n super ( RemoteExecution , self ) . __init__ ( { } ) \n def run ( self , command , ** kwargs ) : \n self . remote_exec_obj . execute ( command , self . int_host ) \n <mask0> { } \n", "gt": "return"}
{"input": "\n import json \n import os \n from xml . etree import ElementTree \n from cloudferry . lib . utils import log \n LOG = log . getLogger ( __name__ ) \n nova_instances_path = \"<STR_LIT>\" \n def instance_path ( instance_id ) : \n return os . path . join ( nova_instances_path , instance_id ) \n def instance_image_path ( instance_id ) : \n return os . path . join ( instance_path ( instance_id ) , \"<STR_LIT>\" ) \n def _qemu_img_rebase ( src , dst ) : \n return \"<STR_LIT>\" . format ( src = src , dst = dst ) \n class QemuBackingFileMover ( object ) : \n def __init__ ( self , runner , src , instance_id ) : \n self . runner = runner \n self . src = src \n self . dst = instance_image_path ( instance_id ) \n def __enter__ ( self ) : \n cmd = _qemu_img_rebase ( self . src , self . dst ) \n self . runner . run ( cmd ) \n return self \n def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n cmd = _qemu_img_rebase ( self . dst , self . src ) \n self . runner . run_ignoring_errors ( cmd ) \n return self \n class DestNovaInstanceDestroyer ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , dest_libvirt , dest_nova , libvirt_name , nova_vm_id ) : \n self . dest_libvirt = dest_libvirt \n self . dest_nova = dest_nova \n self . libvirt_name = libvirt_name \n self . nova_vm_id = nova_vm_id \n def __enter__ ( self ) : \n self . do ( ) \n def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n self . undo ( ) \n def do ( self ) : \n self . dest_libvirt . destroy_vm ( self . libvirt_name ) \n def undo ( self ) : \n try : \n LOG . debug ( \"<STR_LIT>\" , self . nova_vm_id ) \n self . dest_nova . reset_state ( self . nova_vm_id ) \n self . dest_nova . delete_vm_by_id ( self . nova_vm_id ) \n except RuntimeError : \n pass \n class Libvirt ( object ) : \n def __init__ ( self , remote_runner ) : \n \"\"\"<STR_LIT>\"\"\" \n self . runner = remote_runner \n def get_backing_file ( self , instance_id ) : \n cmd = ( \"<STR_LIT>\" . format ( \n image_path = instance_image_path ( instance_id ) ) ) \n try : \n image_info = json . loads ( self . runner . run ( cmd ) ) \n return image_info [ '<STR_LIT>' ] \n except ( ValueError , TypeError ) as e : \n LOG . error ( \"<STR_LIT>\" , e ) \n except KeyError : \n LOG . warning ( \"<STR_LIT>\" , \n instance_id ) \n def get_xml ( self , libvirt_instance_name ) : \n cmd = ( \"<STR_LIT>\" . format ( \n inst_name = libvirt_instance_name ) ) \n return LibvirtXml ( self . runner . run ( cmd ) ) \n def destroy_vm ( self , libvirt_instance_name ) : \n cmds = [ \n \"<STR_LIT>\" . format ( instance = libvirt_instance_name ) , \n \"<STR_LIT>\" . format ( instance = libvirt_instance_name ) \n ] \n for cmd in cmds : \n self . runner . run ( cmd ) \n def move_backing_file ( self , source_file , instance_id ) : \n cmd = _qemu_img_rebase ( src = source_file , \n dst = instance_image_path ( instance_id ) ) \n self . runner . run ( cmd ) \n def live_migrate ( self , libvirt_instance_name , dest_host , migration_xml ) : \n cmd = ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( instance = libvirt_instance_name , \n dst_host = dest_host , \n migration_xml = migration_xml ) ) \n self . runner . run ( cmd ) \n class LibvirtDeviceInterfaceHwAddress ( object ) : \n def __init__ ( self , element ) : \n self . type = element . get ( '<STR_LIT:type>' ) \n self . domain = element . get ( '<STR_LIT>' ) \n self . bus = element . get ( '<STR_LIT>' ) \n self . slot = element . get ( '<STR_LIT>' ) \n self . function = element . get ( '<STR_LIT>' ) \n def __repr__ ( self ) : \n return \"<STR_LIT>\" % ( self . type , self . bus , self . slot ) \n def __eq__ ( self , other ) : \n return ( isinstance ( other , self . __class__ ) and \n self . type == other . type and \n self . domain == other . domain and \n self . bus == other . bus and \n self . slot == other . slot and \n self . function == other . function ) \n class LibvirtDeviceInterface ( object ) : \n def __init__ ( self , interface ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _xml_element = interface \n self . mac = interface . find ( '<STR_LIT>' ) . get ( '<STR_LIT:address>' ) \n self . source_iface = interface . find ( '<STR_LIT:source>' ) . get ( '<STR_LIT>' ) \n self . target_iface = interface . find ( '<STR_LIT:target>' ) . get ( '<STR_LIT>' ) \n self . hw_address = LibvirtDeviceInterfaceHwAddress ( \n interface . find ( '<STR_LIT:address>' ) ) \n def __eq__ ( self , other ) : \n return ( isinstance ( other , self . __class__ ) and \n self . source_iface == other . source_iface and \n self . target_iface == other . target_iface and \n self . hw_address == other . hw_address ) \n def __repr__ ( self ) : \n return \"<STR_LIT>\" . format ( \n mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n @ classmethod \n def _replace_attr ( cls , element , attr , value ) : \n if element . get ( attr ) != value : \n element . clear ( ) \n element . attrib = { attr : value } \n def element ( self ) : \n source = self . _xml_element . find ( '<STR_LIT:source>' ) \n target = self . _xml_element . find ( '<STR_LIT:target>' ) \n self . _replace_attr ( source , '<STR_LIT>' , self . source_iface ) \n self . _replace_attr ( target , '<STR_LIT>' , self . target_iface ) \n return self . _xml_element \n class LibvirtXml ( object ) : \n def __init__ ( self , contents ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _xml = ElementTree . fromstring ( contents ) \n self . _interfaces = [ LibvirtDeviceInterface ( i ) \n for i in self . _xml . findall ( '<STR_LIT>' ) ] \n self . disk_file = self . _get ( '<STR_LIT>' , '<STR_LIT:file>' ) \n self . serial_file = self . _get ( '<STR_LIT>' , '<STR_LIT:path>' ) \n self . console_file = self . _get ( '<STR_LIT>' , '<STR_LIT:path>' ) \n def _get ( self , element , attribute ) : \n el = self . _xml . find ( element ) \n if el is not None : \n return el . get ( attribute ) \n def _set ( self , element , attribute , value ) : \n el = self . _xml . find ( element ) \n if el is not None : \n el . set ( attribute , value ) \n @ property \n def interfaces ( self ) : \n return self . _interfaces \n @ interfaces . setter \n def interfaces ( self , other ) : \n \"\"\"<STR_LIT>\"\"\" \n if len ( self . interfaces ) != len ( other ) : \n raise RuntimeError ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n for other_iface in other : \n for this_iface in self . interfaces : \n identical = ( this_iface . mac == other_iface . mac ) \n if identical : \n this_iface . source_iface = other_iface . source_iface \n this_iface . target_iface = other_iface . target_iface \n break \n def dump ( self ) : \n self . _set ( '<STR_LIT>' , '<STR_LIT:file>' , self . disk_file ) \n self . _set ( '<STR_LIT>' , '<STR_LIT:path>' , self . serial_file ) \n self . _set ( '<STR_LIT>' , '<STR_LIT:path>' , self . console_file ) \n xml_devices = self . _xml . find ( '<STR_LIT>' ) \n xml_interfaces = self . _xml . findall ( '<STR_LIT>' ) \n for iface in xml_interfaces : \n xml_devices . remove ( iface ) \n for iface in self . _interfaces : \n xml_devices . append ( iface . element ( ) ) \n return ElementTree . tostring ( self . <mask0> ) \n", "gt": "_xml"}
{"input": "\n import abc \n from cloudferry . lib . utils import files \n from cloudferry . lib . utils import remote_runner \n from cloudferry . lib . copy_engines import base \n class CopyFailed ( RuntimeError ) : \n pass \n class CopyMechanism ( object ) : \n __metaclass__ = abc . ABCMeta \n @ abc . abstractmethod \n def copy ( self , context , source_object , destination_object ) : \n raise NotImplementedError ( ) \n class CopyObject ( object ) : \n def __init__ ( self , host = None , path = None ) : \n self . host = host \n self . path = path \n def __repr__ ( self ) : \n return \"<STR_LIT>\" . format ( host = self . host , path = self . path ) \n class RemoteFileCopy ( CopyMechanism ) : \n \"\"\"<STR_LIT>\"\"\" \n def copy ( self , context , source_object , destination_object ) : \n data = { \n '<STR_LIT>' : source_object . host , \n '<STR_LIT>' : source_object . path , \n '<STR_LIT>' : destination_object . host , \n '<STR_LIT>' : destination_object . path \n } \n try : \n copier = base . get_copier ( context . src_cloud , \n context . dst_cloud , \n data ) \n copier . transfer ( data ) \n except ( base . FileCopyError , \n base . CopierCannotBeUsed , \n base . CopierNotFound ) as e : \n msg = ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) . format ( \n src_host = source_object . host , \n src_file = source_object . path , \n dst_host = destination_object . host , \n dst_file = destination_object . path , \n err = e . message ) \n raise CopyFailed ( msg ) \n class CopyRegularFileToBlockDevice ( CopyMechanism ) : \n \"\"\"<STR_LIT>\"\"\" \n def copy ( self , context , source_object , destination_object ) : \n src_user = context . cfg . src . ssh_user \n dst_user = context . cfg . dst . ssh_user \n src_host = source_object . host \n dst_host = destination_object . host \n rr = remote_runner . RemoteRunner ( src_host , src_user ) \n ssh_opts = ( '<STR_LIT>' \n '<STR_LIT>' ) \n try : \n progress_view = \"<STR_LIT>\" \n if files . is_installed ( rr , \"<STR_LIT>\" ) : \n src_file_size = files . remote_file_size ( rr , source_object . path ) \n progress_view = \"<STR_LIT>\" . format ( \n size = src_file_size ) \n copy = ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n rr . run ( copy . format ( src_file = source_object . path , \n dst_user = dst_user , \n dst_host = dst_host , \n ssh_opts = ssh_opts , \n dst_device = destination_object . path , \n progress_view = progress_view ) ) \n except remote_runner . RemoteExecutionError as e : \n msg = \"<STR_LIT>\" \n msg = msg . format ( src_object = source_object , \n dst_object = destination_object , \n error = e . message ) \n raise CopyFailed ( <mask0> ) \n", "gt": "msg"}
{"input": "\n import datetime \n import logging \n from logging import config \n from logging import handlers \n import os \n import sys \n from fabric import api \n from oslo_config import cfg \n import yaml \n from cloudferry . lib . utils import sizeof_format \n getLogger = logging . getLogger \n CONF = cfg . CONF \n class StdoutLogger ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , name = None ) : \n self . log = logging . getLogger ( name or '<STR_LIT>' ) \n def write ( self , message ) : \n message = message . strip ( ) \n if message : \n self . log . info ( message ) \n def flush ( self ) : \n pass \n def configure_logging ( log_config = None , debug = None , forward_stdout = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if log_config is None : \n log_config = CONF . migrate . log_config \n if debug is None : \n debug = CONF . migrate . debug \n if forward_stdout is None : \n forward_stdout = CONF . migrate . forward_stdout \n with open ( log_config , '<STR_LIT:r>' ) as f : \n config . dictConfig ( yaml . load ( f ) ) \n if debug : \n logger = logging . getLogger ( '<STR_LIT>' ) \n for handler in logger . handlers : \n if handler . name == '<STR_LIT>' : \n handler . setLevel ( logging . DEBUG ) \n if forward_stdout : \n sys . stdout = StdoutLogger ( ) \n class RunRotatingFileHandler ( handlers . RotatingFileHandler ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , \n filename = '<STR_LIT>' , \n date_format = '<STR_LIT>' , \n ** kwargs ) : \n self . date_format = date_format \n max_bytes = sizeof_format . parse_size ( kwargs . pop ( '<STR_LIT>' , <NUM_LIT:0> ) ) \n super ( RunRotatingFileHandler , self ) . __init__ ( \n filename = self . get_filename ( filename ) , \n maxBytes = max_bytes , \n ** kwargs ) \n def get_filename ( self , filename ) : \n \"\"\"<STR_LIT>\"\"\" \n if hasattr ( CONF , '<STR_LIT>' ) and hasattr ( CONF . migrate , '<STR_LIT>' ) : \n scenario_filename = os . path . basename ( CONF . migrate . scenario ) \n scenario = os . path . splitext ( scenario_filename ) [ <NUM_LIT:0> ] \n else : \n scenario = '<STR_LIT:none>' \n dt = datetime . datetime . now ( ) . strftime ( self . date_format ) \n return filename % { \n '<STR_LIT>' : scenario , \n '<STR_LIT:date>' : dt \n } \n class CurrentTaskFilter ( logging . Filter ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , name_format = '<STR_LIT>' , ** kwargs ) : \n super ( CurrentTaskFilter , self ) . __init__ ( ** kwargs ) \n self . name_format = name_format \n def filter ( self , record ) : \n current_task = self . name_format % { \n '<STR_LIT:name>' : api . env . current_task or '<STR_LIT>' , \n } \n record . current_task = current_task \n return <mask0> \n", "gt": "True"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import os \n import yaml \n import cloudferry_devlab . tests . config as config \n from cloudferry_devlab . tests . data_collector import DataCollector \n from cloudferry_devlab . tests import functional_test \n import cloudferry_devlab . tests . utils as utils \n class RollbackVerification ( functional_test . FunctionalTest ) : \n def setUp ( self ) : \n data_collector = DataCollector ( config = config ) \n self . data_after = utils . convert ( data_collector . data_collector ( ) ) \n file_name = config . rollback_params [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n with open ( pre_file_path , \"<STR_LIT:r>\" ) as f : \n self . pre_data = yaml . load ( f ) \n def test_verify_rollback ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . maxDiff = None \n msg = '<STR_LIT>' \n for cloud in self . data_after : \n for service in self . data_after [ cloud ] : \n for resource in self . data_after [ cloud ] [ service ] : \n print ( msg . format ( service . lower ( ) , resource . lower ( ) ) ) \n self . assertEqual ( self . data_after [ cloud ] [ service ] [ resource ] , \n self . pre_data [ cloud ] [ service ] [ <mask0> ] ) \n", "gt": "resource"}
{"input": "\n import mock \n from cloudferry . lib . os . actions import convert_volume_to_image \n from cloudferry . lib . utils import utils \n from tests import test \n class ConverterVolumeToImageTest ( test . TestCase ) : \n def setUp ( self ) : \n super ( ConverterVolumeToImageTest , self ) . setUp ( ) \n self . fake_src_cloud = mock . Mock ( ) \n self . fake_storage = mock . Mock ( ) \n self . fake_storage . deploy = mock . Mock ( ) \n self . fake_storage . upload_volume_to_image . return_value = ( \n '<STR_LIT>' , '<STR_LIT>' ) \n self . fake_storage . get_backend . return_value = '<STR_LIT>' \n self . fake_image = mock . Mock ( ) \n self . fake_image . wait_for_status = mock . Mock ( ) \n self . fake_image . get_image_by_id_converted = mock . Mock ( ) \n self . fake_image . get_image_by_id_converted . return_value = { \n '<STR_LIT>' : { \n '<STR_LIT>' : { '<STR_LIT:image>' : '<STR_LIT>' , '<STR_LIT>' : { } } } } \n self . fake_image . patch_image = mock . Mock ( ) \n self . fake_src_cloud . resources = { '<STR_LIT>' : self . fake_storage , \n '<STR_LIT:image>' : self . fake_image } \n self . fake_volumes_info = { \n '<STR_LIT>' : { \n '<STR_LIT>' : { \n '<STR_LIT>' : { \n '<STR_LIT:id>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n } , \n '<STR_LIT>' : { \n '<STR_LIT:image>' : '<STR_LIT:image>' , \n } , \n } } , \n } \n self . fake_dst_cloud = mock . Mock ( ) \n self . fake_config = utils . ext_dict ( migrate = utils . ext_dict ( \n { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' } ) ) \n self . fake_init = { \n '<STR_LIT>' : self . fake_src_cloud , \n '<STR_LIT>' : self . fake_dst_cloud , \n '<STR_LIT>' : self . fake_config \n } \n def test_action ( self ) : \n fake_action = convert_volume_to_image . ConvertVolumeToImage ( \n self . fake_init , \n cloud = '<STR_LIT>' ) \n res = fake_action . run ( self . fake_volumes_info ) \n self . assertEqual ( '<STR_LIT>' , \n res [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT:image>' ] ) \n self . assertEqual ( '<STR_LIT>' , \n <mask0> [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ \n '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n", "gt": "res"}
{"input": "\n from cloudferry . lib . utils . cache import Memoized , Cached \n from tests import test \n class MemoizationTestCase ( test . TestCase ) : \n def test_treats_self_as_separate_objects ( self ) : \n class C ( object ) : \n def __init__ ( self , i ) : \n self . i = i \n @ Memoized \n def get_i ( self ) : \n return self . i \n o1 = C ( <NUM_LIT:1> ) \n o2 = C ( <NUM_LIT:2> ) \n self . assertNotEqual ( o1 . get_i ( ) , o2 . get_i ( ) ) \n self . assertEqual ( o1 . get_i ( ) , <NUM_LIT:1> ) \n self . assertEqual ( o2 . get_i ( ) , <NUM_LIT:2> ) \n def test_takes_value_from_cache ( self ) : \n class C ( object ) : \n def __init__ ( self , i ) : \n self . i = i \n @ Memoized \n def get_i ( self ) : \n return self . i \n def set_i ( self , i ) : \n self . i = i \n original = <NUM_LIT:1> \n o = C ( original ) \n self . assertEqual ( o . get_i ( ) , original ) \n o . set_i ( <NUM_LIT:10> ) \n self . assertEqual ( o . get_i ( ) , original ) \n class CacheTestCase ( test . TestCase ) : \n def test_resets_cache_when_modifier_called ( self ) : \n @ Cached ( getter = '<STR_LIT>' , modifier = '<STR_LIT>' ) \n class C ( object ) : \n def __init__ ( self , i ) : \n self . i = i \n def get_i ( self ) : \n return self . i \n def set_i ( self , i ) : \n self . i = i \n o = C ( <NUM_LIT:1> ) \n self . assertEqual ( o . get_i ( ) , <NUM_LIT:1> ) \n o . set_i ( <NUM_LIT:100> ) \n self . assertEqual ( o . <mask0> ( ) , <NUM_LIT:100> ) \n", "gt": "get_i"}
{"input": "\n from django . http import HttpResponse , HttpResponseRedirect , HttpResponseNotFound \n from django . template import Context , loader \n from django . core . urlresolvers import reverse \n from django . template import RequestContext \n from django . shortcuts import get_object_or_404 , render_to_response \n from django . core . exceptions import ObjectDoesNotExist \n from datetime import datetime \n from tagging . models import Tag , TaggedItem \n from django . views . decorators . csrf import csrf_exempt \n from django . contrib . auth . models import User \n from django . contrib . auth . decorators import login_required \n from django . contrib . auth import authenticate , login \n from django . core . mail import send_mail \n from django . conf import settings \n from django . http import Http404 \n from django . db . models import Q \n import json \n from openwatch . recordings . models import Recording \n from openwatch import recording_tags \n @ login_required \n def moderate ( request ) : \n '''<STR_LIT>''' \n response_values = { } \n org_tag = request . user . get_profile ( ) . org_tag \n if not request . user . is_superuser and ( not request . user . get_profile ( ) . can_moderate or org_tag == '<STR_LIT>' ) : \n raise Http404 \n if recording_tags . ACLU_NJ in org_tag : \n location = { } \n location [ '<STR_LIT>' ] = <NUM_LIT> \n location [ '<STR_LIT>' ] = - <NUM_LIT> \n response_values [ '<STR_LIT:location>' ] = location \n response_values [ '<STR_LIT>' ] = '<STR_LIT>' \n return render_to_response ( '<STR_LIT>' , response_values , context_instance = RequestContext ( request ) ) \n def map ( request ) : \n total = \"<STR_LIT>\" \n return render_to_response ( '<STR_LIT>' , { '<STR_LIT>' : total } , context_instance = RequestContext ( request ) ) \n def size ( request ) : \n featureset = Recording . objects . filter ( ~ Q ( lat = None ) , ~ Q ( lon = None ) , ~ Q ( jtype = '<STR_LIT>' ) ) . exclude ( location__exact = '<STR_LIT>' ) . exclude ( location__exact = '<STR_LIT>' ) . order_by ( '<STR_LIT>' ) \n total = len ( featureset ) \n return render_to_response ( '<STR_LIT>' , { '<STR_LIT>' : total } , context_instance = RequestContext ( request ) ) \n def redir ( self ) : \n return HttpResponseRedirect ( '<STR_LIT:/>' ) \n def map_json ( request ) : \n featureset = Recording . objects . all ( ) . order_by ( '<STR_LIT>' ) . filter ( ~ Q ( location = '<STR_LIT>' ) ) . exclude ( location__isnull = True ) . exclude ( location__exact = '<STR_LIT>' ) [ : <NUM_LIT> ] \n resp = encode_queryset ( featureset ) \n return HttpResponse ( resp , mimetype = \"<STR_LIT:application/json>\" ) \n @ login_required \n def map_json_moderate ( request ) : \n org_tag = request . user . get_profile ( ) . org_tag \n if org_tag != '<STR_LIT>' : \n featureset = Recording . objects . filter ( org_approved = False , org_flagged = False , tags__contains = org_tag ) \n else : \n featureset = Recording . objects . all ( ) \n featureset = featureset . order_by ( '<STR_LIT>' ) . filter ( ~ Q ( location = '<STR_LIT>' ) ) . exclude ( location__isnull = True ) . exclude ( location__exact = '<STR_LIT>' ) \n resp = encode_queryset ( featureset ) \n return HttpResponse ( resp , mimetype = \"<STR_LIT:application/json>\" ) \n def map_location_json ( request , ne_lat = <NUM_LIT:0> , ne_lon = <NUM_LIT:0> , sw_lat = <NUM_LIT:0> , sw_lon = <NUM_LIT:0> ) : \n ne_lat = float ( ne_lat ) \n ne_lon = float ( ne_lon ) \n sw_lat = float ( sw_lat ) \n sw_lon = float ( sw_lon ) \n featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon ) . order_by ( '<STR_LIT>' ) . exclude ( location__isnull = True ) . exclude ( location__exact = '<STR_LIT>' ) . exclude ( location__exact = '<STR_LIT>' ) . exclude ( location__exact = '<STR_LIT>' ) [ : <NUM_LIT> ] \n if len ( featureset ) < <NUM_LIT:1> : \n return HttpResponse ( \"<STR_LIT>\" , mimetype = \"<STR_LIT:application/json>\" ) \n resp = encode_queryset ( featureset ) \n return HttpResponse ( resp , mimetype = \"<STR_LIT:application/json>\" ) \n def encode_queryset ( featureset ) : \n resp = '<STR_LIT>' \n for obj in featureset : \n resp = resp + json . dumps ( obj . to_dict ( ) ) + '<STR_LIT:U+002C>' \n resp = resp [ : - <NUM_LIT:1> ] + '<STR_LIT>' \n return <mask0> \n", "gt": "resp"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from django . template import loader , RequestContext \n from django . http import HttpResponse , Http404 \n from django . http import HttpResponseRedirect , HttpResponsePermanentRedirect \n from django . db . models . base import ModelBase \n from django . db . models . manager import Manager \n from django . db . models . query import QuerySet \n from django . core import urlresolvers \n from django . utils import six \n import datetime \n try : \n import json \n except Exception , e : \n import simplejson as json \n from . dumper import DataDumper \n from dicttoxml import dicttoxml as dict2xml \n from xml . dom . minidom import parseString \n import yaml \n def render_to_easy_api_response ( * args , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n httpresponse_kwargs = { '<STR_LIT>' : kwargs . pop ( '<STR_LIT>' , None ) } \n context = kwargs . pop ( '<STR_LIT>' ) \n processors = context . context_processors \n request = processors [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n if request . GET . has_key ( '<STR_LIT>' ) : \n api_type = request . GET [ '<STR_LIT>' ] \n for arg in args : \n passed = arg \n dump_me = { } \n for key in passed . keys ( ) : \n value = passed [ key ] \n dump_me [ key ] = dump_object ( value ) \n if api_type == '<STR_LIT>' : \n def replace_spaces ( dump_me ) : \n new = { } \n for k , v in dump_me . iteritems ( ) : \n if isinstance ( v , dict ) : \n v = replace_spaces ( v ) \n new [ k . replace ( '<STR_LIT:U+0020>' , '<STR_LIT:_>' ) ] = v \n return new \n new = replace_spaces ( dump_me ) \n dump_me = dict2xml ( new ) \n dom = parseString ( dump_me ) \n pretty = dom . toprettyxml ( ) \n return HttpResponse ( pretty , content_type = '<STR_LIT>' ) \n if api_type == '<STR_LIT>' : \n yml = yaml . safe_dump ( dump_me ) \n return HttpResponse ( yml , content_type = '<STR_LIT>' ) \n else : \n dump_me = json . dumps ( dump_me , indent = <NUM_LIT:2> ) \n return HttpResponse ( dump_me , content_type = '<STR_LIT:application/json>' ) \n return HttpResponse ( loader . render_to_string ( * args , ** kwargs ) , ** httpresponse_kwargs ) \n def render_to_response ( * args , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n return render_to_easy_api_response ( * args , ** kwargs ) \n def dump_object ( queryset ) : \n if str ( type ( queryset ) ) != \"<STR_LIT>\" : \n d = DataDumper ( ) \n ret = d . dump ( queryset ) \n return ret \n try : \n modelName = queryset [ <NUM_LIT:0> ] . __class__ . __name__ \n modelNameData = [ ] \n fields = get_fields ( queryset [ <NUM_LIT:0> ] ) \n for obj in queryset : \n temp_dict = dict ( ) \n for field in fields : \n try : \n attribute = getattr ( obj , str ( field ) ) \n temp_dict [ field ] = attribute \n except Exception , e : \n continue \n modelNameData . append ( temp_dict ) \n dthandler = lambda obj : obj . isoformat ( ) if isinstance ( obj , datetime . datetime ) or isinstance ( obj , datetime . date ) else None \n return json . loads ( json . dumps ( modelNameData , default = dthandler ) ) \n except Exception , e : \n return '<STR_LIT>' \n def get_fields ( model ) : \n try : \n if hasattr ( model , \"<STR_LIT>\" ) : \n fields = model . easy_api_fields ( ) \n else : \n try : \n fields = model . to_dict ( ) . keys ( ) \n except Exception , e : \n fields = model . _meta . get_all_field_names ( ) \n return fields \n except Exception , e : \n <mask0> [ ] \n", "gt": "return"}
{"input": "\n class SimpleEngagementCalculator ( object ) : \n def calculate_user_engagement_score ( self , user , start_date , end_date ) : \n return <NUM_LIT:0> \n ROOT_URLCONF = None \n DATABASE_ENGINE = '<STR_LIT>' \n DATABASE_NAME = '<STR_LIT>' \n DATABASE_SUPPORTS_TRANSACTIONS = <mask0> \n INSTALLED_APPS = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' ] \n TEMPLATE_CONTEXT_PROCESSORS = ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ) \n", "gt": "False"}
{"input": "\n import os \n import sys \n if __name__ == \"<STR_LIT:__main__>\" : \n os . environ . setdefault ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n from django . core . management import execute_from_command_line \n is_testing = '<STR_LIT:test>' in sys . argv \n if is_testing : \n import coverage \n cov = coverage . coverage ( include = \"<STR_LIT>\" , omit = [ '<STR_LIT>' ] ) \n cov . erase ( ) \n cov . start ( ) \n execute_from_command_line ( sys . argv ) \n if is_testing : \n cov . stop ( ) \n cov . save ( ) \n cov . <mask0> ( ) \n", "gt": "report"}
{"input": "\n import datetime \n import time \n class CountdownManager ( object ) : \n def __init__ ( self , root_tk_app ) : \n self . start_time = time . time ( ) \n self . minutes = <NUM_LIT:0> \n self . seconds = <NUM_LIT:0> \n self . time_change_callbacks = [ ] \n self . count_down_total = datetime . timedelta ( days = - <NUM_LIT:1> , minutes = <NUM_LIT:0> , seconds = <NUM_LIT:0> ) \n self . root_tk_app = root_tk_app \n self . refresh_timer ( ) \n def set_countdown_duration ( self , minutes , seconds ) : \n self . start_time = time . time ( ) \n self . minutes = minutes \n self . seconds = seconds \n self . count_down_total = datetime . timedelta ( minutes = minutes , seconds = seconds ) \n self . fire_time_change_callbacks ( ) \n def subscribe_to_time_changes ( self , time_change_callback ) : \n self . time_change_callbacks . append ( time_change_callback ) \n def fire_time_change_callbacks ( self ) : \n end_time = time . time ( ) \n up_time = end_time - self . start_time \n remaining_time = self . count_down_total - datetime . timedelta ( seconds = ( int ( up_time ) ) ) \n for callback in self . time_change_callbacks : \n if callback : \n callback ( remaining_time . days , ( remaining_time . seconds // <NUM_LIT> ) % <NUM_LIT> , remaining_time . seconds % <NUM_LIT> ) \n def refresh_timer ( self ) : \n self . fire_time_change_callbacks ( ) \n if self . root_tk_app : \n self . root_tk_app . after ( <NUM_LIT> , self . <mask0> ) \n", "gt": "refresh_timer"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import simplexml , time , sys \n from protocol import * \n from client import PlugIn \n DefaultTimeout = <NUM_LIT> \n ID = <NUM_LIT:0> \n class Dispatcher ( PlugIn ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n PlugIn . __init__ ( self ) \n DBG_LINE = '<STR_LIT>' \n self . handlers = { } \n self . _expected = { } \n self . _defaultHandler = None \n self . _pendingExceptions = [ ] \n self . _eventHandler = None \n self . _cycleHandlers = [ ] \n self . _exported_methods = [ self . Process , self . RegisterHandler , self . RegisterDefaultHandler , self . RegisterEventHandler , self . UnregisterCycleHandler , self . RegisterCycleHandler , self . RegisterHandlerOnce , self . UnregisterHandler , self . RegisterProtocol , self . WaitForResponse , self . SendAndWaitForResponse , self . send , self . disconnect , self . SendAndCallForResponse , ] \n def dumpHandlers ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . handlers \n def restoreHandlers ( self , handlers ) : \n \"\"\"<STR_LIT>\"\"\" \n self . handlers = handlers \n def _init ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . RegisterNamespace ( '<STR_LIT>' ) \n self . RegisterNamespace ( NS_STREAMS ) \n self . RegisterNamespace ( self . _owner . defaultNamespace ) \n self . RegisterProtocol ( '<STR_LIT>' , Iq ) \n self . RegisterProtocol ( '<STR_LIT>' , Presence ) \n self . RegisterProtocol ( '<STR_LIT:message>' , Message ) \n self . RegisterDefaultHandler ( self . returnStanzaHandler ) \n self . RegisterHandler ( '<STR_LIT:error>' , self . streamErrorHandler , xmlns = NS_STREAMS ) \n def plugin ( self , owner ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _init ( ) \n for method in self . _old_owners_methods : \n if method . __name__ == '<STR_LIT>' : self . _owner_send = method ; break \n self . _owner . lastErrNode = None \n self . _owner . lastErr = None \n self . _owner . lastErrCode = None \n self . StreamInit ( ) \n def plugout ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . Stream . dispatch = None \n self . Stream . DEBUG = None \n self . Stream . features = None \n self . Stream . destroy ( ) \n def StreamInit ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . Stream = simplexml . NodeBuilder ( ) \n self . Stream . _dispatch_depth = <NUM_LIT:2> \n self . Stream . dispatch = self . dispatch \n self . Stream . stream_header_received = self . _check_stream_start \n self . _owner . debug_flags . append ( simplexml . DBG_NODEBUILDER ) \n self . Stream . DEBUG = self . _owner . DEBUG \n self . Stream . features = None \n self . _metastream = Node ( '<STR_LIT>' ) \n self . _metastream . setNamespace ( self . _owner . Namespace ) \n self . _metastream . setAttr ( '<STR_LIT:version>' , '<STR_LIT:1.0>' ) \n self . _metastream . setAttr ( '<STR_LIT>' , NS_STREAMS ) \n self . _metastream . setAttr ( '<STR_LIT:to>' , self . _owner . Server ) \n self . _owner . send ( \"<STR_LIT>\" % str ( self . _metastream ) [ : - <NUM_LIT:2> ] ) \n def _check_stream_start ( self , ns , tag , attrs ) : \n if ns < > NS_STREAMS or tag < > '<STR_LIT>' : \n raise ValueError ( '<STR_LIT>' % ( tag , ns ) ) \n def Process ( self , timeout = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n for handler in self . _cycleHandlers : handler ( self ) \n if len ( self . _pendingExceptions ) > <NUM_LIT:0> : \n _pendingException = self . _pendingExceptions . pop ( ) \n raise _pendingException [ <NUM_LIT:0> ] , _pendingException [ <NUM_LIT:1> ] , _pendingException [ <NUM_LIT:2> ] \n if self . _owner . Connection . pending_data ( timeout ) : \n try : data = self . _owner . Connection . receive ( ) \n except IOError : return \n self . Stream . Parse ( data ) \n if len ( self . _pendingExceptions ) > <NUM_LIT:0> : \n _pendingException = self . _pendingExceptions . pop ( ) \n raise _pendingException [ <NUM_LIT:0> ] , _pendingException [ <NUM_LIT:1> ] , _pendingException [ <NUM_LIT:2> ] \n if data : return len ( data ) \n return '<STR_LIT:0>' \n def RegisterNamespace ( self , xmlns , order = '<STR_LIT:info>' ) : \n \"\"\"<STR_LIT>\"\"\" \n self . DEBUG ( '<STR_LIT>' % xmlns , order ) \n self . handlers [ xmlns ] = { } \n self . RegisterProtocol ( '<STR_LIT>' , Protocol , xmlns = xmlns ) \n self . RegisterProtocol ( '<STR_LIT:default>' , Protocol , xmlns = xmlns ) \n def RegisterProtocol ( self , tag_name , Proto , xmlns = None , order = '<STR_LIT:info>' ) : \n \"\"\"<STR_LIT>\"\"\" \n if not xmlns : xmlns = self . _owner . defaultNamespace \n self . DEBUG ( '<STR_LIT>' % ( tag_name , Proto , xmlns ) , order ) \n self . handlers [ xmlns ] [ tag_name ] = { type : Proto , '<STR_LIT:default>' : [ ] } \n def RegisterNamespaceHandler ( self , xmlns , handler , typ = '<STR_LIT>' , ns = '<STR_LIT>' , makefirst = <NUM_LIT:0> , system = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n self . RegisterHandler ( '<STR_LIT:default>' , handler , typ , ns , xmlns , makefirst , system ) \n def RegisterHandler ( self , name , handler , typ = '<STR_LIT>' , ns = '<STR_LIT>' , xmlns = None , makefirst = <NUM_LIT:0> , system = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n if not xmlns : xmlns = self . _owner . defaultNamespace \n self . DEBUG ( '<STR_LIT>' % ( handler , name , typ , ns , xmlns ) , '<STR_LIT:info>' ) \n if not typ and not ns : typ = '<STR_LIT:default>' \n if not self . handlers . has_key ( xmlns ) : self . RegisterNamespace ( xmlns , '<STR_LIT>' ) \n if not self . handlers [ xmlns ] . has_key ( name ) : self . RegisterProtocol ( name , Protocol , xmlns , '<STR_LIT>' ) \n if not self . handlers [ xmlns ] [ name ] . has_key ( typ + ns ) : self . handlers [ xmlns ] [ name ] [ typ + ns ] = [ ] \n if makefirst : self . handlers [ xmlns ] [ name ] [ typ + ns ] . insert ( <NUM_LIT:0> , { '<STR_LIT>' : handler , '<STR_LIT>' : system } ) \n else : self . handlers [ xmlns ] [ name ] [ typ + ns ] . append ( { '<STR_LIT>' : handler , '<STR_LIT>' : system } ) \n def RegisterHandlerOnce ( self , name , handler , typ = '<STR_LIT>' , ns = '<STR_LIT>' , xmlns = None , makefirst = <NUM_LIT:0> , system = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n if not xmlns : xmlns = self . _owner . defaultNamespace \n self . RegisterHandler ( name , handler , typ , ns , xmlns , makefirst , system ) \n def UnregisterHandler ( self , name , handler , typ = '<STR_LIT>' , ns = '<STR_LIT>' , xmlns = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if not xmlns : xmlns = self . _owner . defaultNamespace \n if not self . handlers . has_key ( xmlns ) : return \n if not typ and not ns : typ = '<STR_LIT:default>' \n for pack in self . handlers [ xmlns ] [ name ] [ typ + ns ] : \n if handler == pack [ '<STR_LIT>' ] : break \n else : pack = None \n try : self . handlers [ xmlns ] [ name ] [ typ + ns ] . remove ( pack ) \n except ValueError : pass \n def RegisterDefaultHandler ( self , handler ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _defaultHandler = handler \n def RegisterEventHandler ( self , handler ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _eventHandler = handler \n def returnStanzaHandler ( self , conn , stanza ) : \n \"\"\"<STR_LIT>\"\"\" \n if stanza . getType ( ) in [ '<STR_LIT>' , '<STR_LIT>' ] : \n conn . send ( Error ( stanza , ERR_FEATURE_NOT_IMPLEMENTED ) ) \n def streamErrorHandler ( self , conn , error ) : \n name , text = '<STR_LIT:error>' , error . getData ( ) \n for tag in error . getChildren ( ) : \n if tag . getNamespace ( ) == NS_XMPP_STREAMS : \n if tag . getName ( ) == '<STR_LIT:text>' : text = tag . getData ( ) \n else : name = tag . getName ( ) \n if name in stream_exceptions . keys ( ) : exc = stream_exceptions [ name ] \n else : exc = StreamError \n raise exc ( ( name , text ) ) \n def RegisterCycleHandler ( self , handler ) : \n \"\"\"<STR_LIT>\"\"\" \n if handler not in self . _cycleHandlers : self . _cycleHandlers . append ( handler ) \n def UnregisterCycleHandler ( self , handler ) : \n \"\"\"<STR_LIT>\"\"\" \n if handler in self . _cycleHandlers : self . _cycleHandlers . remove ( handler ) \n def Event ( self , realm , event , data ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _eventHandler : self . _eventHandler ( realm , event , data ) \n def dispatch ( self , stanza , session = None , direct = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n if not session : session = self \n session . Stream . _mini_dom = None \n name = stanza . getName ( ) \n if not direct and self . _owner . _route : \n if name == '<STR_LIT>' : \n if stanza . getAttr ( '<STR_LIT:error>' ) == None : \n if len ( stanza . getChildren ( ) ) == <NUM_LIT:1> : \n stanza = stanza . getChildren ( ) [ <NUM_LIT:0> ] \n name = stanza . getName ( ) \n else : \n for each in stanza . getChildren ( ) : \n self . dispatch ( each , session , direct = <NUM_LIT:1> ) \n return \n elif name == '<STR_LIT>' : \n return \n elif name in ( '<STR_LIT>' , '<STR_LIT>' ) : \n pass \n else : \n raise UnsupportedStanzaType ( name ) \n if name == '<STR_LIT>' : session . Stream . features = stanza \n xmlns = stanza . getNamespace ( ) \n if not self . handlers . has_key ( xmlns ) : \n self . DEBUG ( \"<STR_LIT>\" + xmlns , '<STR_LIT>' ) \n xmlns = '<STR_LIT>' \n if not self . handlers [ xmlns ] . has_key ( name ) : \n self . DEBUG ( \"<STR_LIT>\" + name , '<STR_LIT>' ) \n name = '<STR_LIT>' \n else : \n self . DEBUG ( \"<STR_LIT>\" % ( xmlns , name ) , '<STR_LIT>' ) \n if stanza . __class__ . __name__ == '<STR_LIT>' : stanza = self . handlers [ xmlns ] [ name ] [ type ] ( node = stanza ) \n typ = stanza . getType ( ) \n if not typ : typ = '<STR_LIT>' \n stanza . props = stanza . getProperties ( ) \n ID = stanza . getID ( ) \n session . DEBUG ( \"<STR_LIT>\" % ( name , typ , stanza . props , ID ) , '<STR_LIT>' ) \n list = [ '<STR_LIT:default>' ] \n if self . handlers [ xmlns ] [ name ] . has_key ( typ ) : list . append ( typ ) \n for prop in stanza . props : \n if self . handlers [ xmlns ] [ name ] . has_key ( prop ) : list . append ( prop ) \n if typ and self . handlers [ xmlns ] [ name ] . has_key ( typ + prop ) : list . append ( typ + prop ) \n chain = self . handlers [ xmlns ] [ '<STR_LIT:default>' ] [ '<STR_LIT:default>' ] \n for key in list : \n if key : chain = chain + self . handlers [ xmlns ] [ name ] [ key ] \n output = '<STR_LIT>' \n if session . _expected . has_key ( ID ) : \n user = <NUM_LIT:0> \n if type ( session . _expected [ ID ] ) == type ( ( ) ) : \n cb , args = session . _expected [ ID ] \n session . DEBUG ( \"<STR_LIT>\" % ( cb , args ) , '<STR_LIT>' ) \n try : cb ( session , stanza , ** args ) \n except Exception , typ : \n if typ . __class__ . __name__ < > '<STR_LIT>' : raise \n else : \n session . DEBUG ( \"<STR_LIT>\" , '<STR_LIT>' ) \n session . _expected [ ID ] = stanza \n else : user = <NUM_LIT:1> \n for handler in chain : \n if user or handler [ '<STR_LIT>' ] : \n try : \n handler [ '<STR_LIT>' ] ( session , stanza ) \n except Exception , typ : \n if typ . __class__ . __name__ < > '<STR_LIT>' : \n self . _pendingExceptions . insert ( <NUM_LIT:0> , sys . exc_info ( ) ) \n return \n user = <NUM_LIT:0> \n if user and self . _defaultHandler : self . _defaultHandler ( session , stanza ) \n def WaitForResponse ( self , ID , timeout = DefaultTimeout ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _expected [ ID ] = None \n has_timed_out = <NUM_LIT:0> \n abort_time = time . time ( ) + timeout \n self . DEBUG ( \"<STR_LIT>\" % ( ID , timeout ) , '<STR_LIT>' ) \n while not self . _expected [ ID ] : \n if not self . Process ( <NUM_LIT> ) : \n self . _owner . lastErr = \"<STR_LIT>\" \n return None \n if time . time ( ) > abort_time : \n self . _owner . lastErr = \"<STR_LIT>\" \n return None \n response = self . _expected [ ID ] \n del self . _expected [ ID ] \n if response . getErrorCode ( ) : \n self . _owner . lastErrNode = response \n self . _owner . lastErr = response . getError ( ) \n self . _owner . lastErrCode = response . getErrorCode ( ) \n return response \n def SendAndWaitForResponse ( self , stanza , timeout = DefaultTimeout ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . WaitForResponse ( self . send ( stanza ) , timeout ) \n def SendAndCallForResponse ( self , stanza , func , args = { } ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _expected [ self . send ( stanza ) ] = ( func , args ) \n def send ( self , stanza ) : \n \"\"\"<STR_LIT>\"\"\" \n if type ( stanza ) in [ type ( '<STR_LIT>' ) , type ( u'<STR_LIT>' ) ] : return self . _owner_send ( stanza ) \n if not isinstance ( stanza , Protocol ) : _ID = None \n elif not stanza . getID ( ) : \n global ID \n ID += <NUM_LIT:1> \n _ID = ` ID ` \n stanza . setID ( _ID ) \n else : _ID = stanza . getID ( ) \n if self . _owner . _registered_name and not stanza . getAttr ( '<STR_LIT>' ) : stanza . setAttr ( '<STR_LIT>' , self . _owner . _registered_name ) \n if self . _owner . _route and stanza . getName ( ) != '<STR_LIT>' : \n to = self . _owner . Server \n if stanza . getTo ( ) and stanza . getTo ( ) . getDomain ( ) : \n to = stanza . getTo ( ) . getDomain ( ) \n frm = stanza . getFrom ( ) \n if frm . getDomain ( ) : \n frm = frm . getDomain ( ) \n route = Protocol ( '<STR_LIT>' , to = to , frm = frm , payload = [ stanza ] ) \n stanza = route \n stanza . setNamespace ( self . _owner . Namespace ) \n stanza . setParent ( self . _metastream ) \n self . _owner_send ( stanza ) \n return _ID \n def disconnect ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _owner_send ( '<STR_LIT>' ) \n while self . Process ( <NUM_LIT:1> ) : <mask0> \n", "gt": "pass"}
{"input": "\n from . gl_utils import * \n from . texture import VideoTexture \n from . widget import Widget , BGUI_DEFAULT , WeakMethod \n from . image import Image \n class Video ( Image ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , parent , vid , name = None , play_audio = False , repeat = <NUM_LIT:0> , aspect = None , size = [ <NUM_LIT:1> , <NUM_LIT:1> ] , pos = [ <NUM_LIT:0> , <NUM_LIT:0> ] , \n sub_theme = '<STR_LIT>' , options = BGUI_DEFAULT ) : \n \"\"\"<STR_LIT>\"\"\" \n Image . __init__ ( self , parent , name , None , aspect , size , pos , sub_theme = sub_theme , options = options ) \n self . _texture = VideoTexture ( vid , GL_LINEAR , repeat , play_audio ) \n self . _on_finish = None \n self . _on_finish_called = False \n def play ( self , start , end , use_frames = True , fps = None ) : \n self . _texture . play ( start , end , use_frames , fps ) \n self . _on_finish_called = False \n @ property \n def on_finish ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _on_finish \n @ on_finish . setter \n def on_finish ( self , value ) : \n self . _on_finish = WeakMethod ( value ) \n def _draw ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _texture . update ( ) \n Image . _draw ( self ) \n if self . _texture . video . status == <NUM_LIT:3> : \n if self . _on_finish and not self . _on_finish_called : \n self . on_finish ( self ) \n self . _on_finish_called = <mask0> \n", "gt": "True"}
{"input": "\n from django import template \n from django . conf import settings \n register = template . Library ( ) \n class CheckGrappelli ( template . Node ) : \n def __init__ ( self , var_name ) : \n self . var_name = var_name \n def render ( self , context ) : \n context [ self . var_name ] = '<STR_LIT>' in settings . INSTALLED_APPS \n return '<STR_LIT>' \n def check_grappelli ( parser , token ) : \n \"\"\"<STR_LIT>\"\"\" \n bits = token . contents . split ( ) \n if len ( bits ) != <NUM_LIT:3> : \n raise template . TemplateSyntaxError ( \"<STR_LIT>\" ) \n if bits [ <NUM_LIT:1> ] != '<STR_LIT>' : \n raise template . TemplateSyntaxError ( \"<STR_LIT>\" ) \n varname = bits [ <NUM_LIT:2> ] \n return CheckGrappelli ( varname ) \n register . tag ( <mask0> ) \n", "gt": "check_grappelli"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from setuptools import setup , find_packages \n import sys , os \n __version__ = '<STR_LIT>' \n __description__ = '<STR_LIT>' , \n __license__ = '<STR_LIT>' \n __author__ = '<STR_LIT>' , \n __email__ = '<STR_LIT>' , \n sys . path . insert ( <NUM_LIT:0> , os . path . dirname ( __file__ ) ) \n REQUIRES = [ i . strip ( ) for i in open ( \"<STR_LIT>\" ) . readlines ( ) ] \n setup ( \n name = '<STR_LIT>' , \n version = __version__ , \n url = '<STR_LIT>' , \n download_url = '<STR_LIT>' , \n license = __license__ , \n author = __author__ , \n author_email = __email__ , \n description = __description__ , \n long_description = __doc__ , \n test_suite = '<STR_LIT>' , \n zip_safe = False , \n platforms = '<STR_LIT>' , \n install_requires = REQUIRES , \n packages = find_packages ( exclude = ( '<STR_LIT>' , '<STR_LIT>' , ) ) , \n include_package_data = True , \n setup_requires = [ '<STR_LIT>' , '<STR_LIT>' ] , \n <mask0> = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] \n ) \n", "gt": "classifiers"}
{"input": "\n from mongoengine . base import BaseField \n __all__ = ( '<STR_LIT>' ) \n class WtfBaseField ( BaseField ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , validators = None , filters = None , ** kwargs ) : \n self . validators = self . _ensure_callable_or_list ( validators , '<STR_LIT>' ) \n self . filters = self . _ensure_callable_or_list ( filters , '<STR_LIT>' ) \n BaseField . __init__ ( self , ** kwargs ) \n def _ensure_callable_or_list ( self , field , msg_flag ) : \n \"\"\"<STR_LIT>\"\"\" \n if field is not None : \n if callable ( field ) : \n field = [ field ] \n else : \n msg = \"<STR_LIT>\" % msg_flag \n if not isinstance ( field , list ) : \n raise TypeError ( msg ) \n return <mask0> \n", "gt": "field"}
{"input": "\n from bson import DBRef , SON \n from mongoengine . python_support import txt_type \n from base import ( \n BaseDict , BaseList , EmbeddedDocumentList , \n TopLevelDocumentMetaclass , get_document \n ) \n from fields import ( ReferenceField , ListField , DictField , MapField ) \n from connection import get_db \n from queryset import QuerySet \n from document import Document , EmbeddedDocument \n class DeReference ( object ) : \n def __call__ ( self , items , max_depth = <NUM_LIT:1> , instance = None , name = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if items is None or isinstance ( items , basestring ) : \n return items \n if isinstance ( items , QuerySet ) : \n items = [ i for i in items ] \n self . max_depth = max_depth \n doc_type = None \n if instance and isinstance ( instance , ( Document , EmbeddedDocument , \n TopLevelDocumentMetaclass ) ) : \n doc_type = instance . _fields . get ( name ) \n while hasattr ( doc_type , '<STR_LIT>' ) : \n doc_type = doc_type . field \n if isinstance ( doc_type , ReferenceField ) : \n field = doc_type \n doc_type = doc_type . document_type \n is_list = not hasattr ( items , '<STR_LIT>' ) \n if is_list and all ( [ i . __class__ == doc_type for i in items ] ) : \n return items \n elif not is_list and all ( \n [ i . __class__ == doc_type for i in items . values ( ) ] ) : \n return items \n elif not field . dbref : \n if not hasattr ( items , '<STR_LIT>' ) : \n def _get_items ( items ) : \n new_items = [ ] \n for v in items : \n if isinstance ( v , list ) : \n new_items . append ( _get_items ( v ) ) \n elif not isinstance ( v , ( DBRef , Document ) ) : \n new_items . append ( field . to_python ( v ) ) \n else : \n new_items . append ( v ) \n return new_items \n items = _get_items ( items ) \n else : \n items = dict ( [ \n ( k , field . to_python ( v ) ) \n if not isinstance ( v , ( DBRef , Document ) ) else ( k , v ) \n for k , v in items . iteritems ( ) ] \n ) \n self . reference_map = self . _find_references ( items ) \n self . object_map = self . _fetch_objects ( doc_type = doc_type ) \n return self . _attach_objects ( items , <NUM_LIT:0> , instance , name ) \n def _find_references ( self , items , depth = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n reference_map = { } \n if not items or depth >= self . max_depth : \n return reference_map \n if not hasattr ( items , '<STR_LIT>' ) : \n iterator = enumerate ( items ) \n else : \n iterator = items . iteritems ( ) \n depth += <NUM_LIT:1> \n for k , item in iterator : \n if isinstance ( item , ( Document , EmbeddedDocument ) ) : \n for field_name , field in item . _fields . iteritems ( ) : \n v = item . _data . get ( field_name , None ) \n if isinstance ( v , DBRef ) : \n reference_map . setdefault ( field . document_type , set ( ) ) . add ( v . id ) \n elif isinstance ( v , ( dict , SON ) ) and '<STR_LIT>' in v : \n reference_map . setdefault ( get_document ( v [ '<STR_LIT>' ] ) , set ( ) ) . add ( v [ '<STR_LIT>' ] . id ) \n elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n field_cls = getattr ( getattr ( field , '<STR_LIT>' , None ) , '<STR_LIT>' , None ) \n references = self . _find_references ( v , depth ) \n for key , refs in references . iteritems ( ) : \n if isinstance ( field_cls , ( Document , TopLevelDocumentMetaclass ) ) : \n key = field_cls \n reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n elif isinstance ( item , DBRef ) : \n reference_map . setdefault ( item . collection , set ( ) ) . add ( item . id ) \n elif isinstance ( item , ( dict , SON ) ) and '<STR_LIT>' in item : \n reference_map . setdefault ( get_document ( item [ '<STR_LIT>' ] ) , set ( ) ) . add ( item [ '<STR_LIT>' ] . id ) \n elif isinstance ( item , ( dict , list , tuple ) ) and depth - <NUM_LIT:1> <= self . max_depth : \n references = self . _find_references ( item , depth - <NUM_LIT:1> ) \n for key , refs in references . iteritems ( ) : \n reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n return reference_map \n def _fetch_objects ( self , doc_type = None ) : \n \"\"\"<STR_LIT>\"\"\" \n object_map = { } \n for collection , dbrefs in self . reference_map . iteritems ( ) : \n if hasattr ( collection , '<STR_LIT>' ) : \n col_name = collection . _get_collection_name ( ) \n refs = [ dbref for dbref in dbrefs \n if ( col_name , dbref ) not in object_map ] \n references = collection . objects . in_bulk ( refs ) \n for key , doc in references . iteritems ( ) : \n object_map [ ( col_name , key ) ] = doc \n else : \n if isinstance ( doc_type , ( ListField , DictField , MapField , ) ) : \n continue \n refs = [ dbref for dbref in dbrefs \n if ( collection , dbref ) not in object_map ] \n if doc_type : \n references = doc_type . _get_db ( ) [ collection ] . find ( { '<STR_LIT>' : { '<STR_LIT>' : refs } } ) \n for ref in references : \n doc = doc_type . _from_son ( ref ) \n object_map [ ( collection , doc . id ) ] = doc \n else : \n references = get_db ( ) [ collection ] . find ( { '<STR_LIT>' : { '<STR_LIT>' : refs } } ) \n for ref in references : \n if '<STR_LIT>' in ref : \n doc = get_document ( ref [ \"<STR_LIT>\" ] ) . _from_son ( ref ) \n elif doc_type is None : \n doc = get_document ( \n '<STR_LIT>' . join ( x . capitalize ( ) \n for x in collection . split ( '<STR_LIT:_>' ) ) ) . _from_son ( ref ) \n else : \n doc = doc_type . _from_son ( ref ) \n object_map [ ( collection , doc . id ) ] = doc \n return object_map \n def _attach_objects ( self , items , depth = <NUM_LIT:0> , instance = None , name = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if not items : \n if isinstance ( items , ( BaseDict , BaseList ) ) : \n return items \n if instance : \n if isinstance ( items , dict ) : \n return BaseDict ( items , instance , name ) \n else : \n return BaseList ( items , instance , name ) \n if isinstance ( items , ( dict , SON ) ) : \n if '<STR_LIT>' in items : \n return self . object_map . get ( \n ( items [ '<STR_LIT>' ] . collection , items [ '<STR_LIT>' ] . id ) , items ) \n elif '<STR_LIT>' in items : \n doc = get_document ( items [ '<STR_LIT>' ] ) . _from_son ( items ) \n _cls = doc . _data . pop ( '<STR_LIT>' , None ) \n del items [ '<STR_LIT>' ] \n doc . _data = self . _attach_objects ( doc . _data , depth , doc , None ) \n if _cls is not None : \n doc . _data [ '<STR_LIT>' ] = _cls \n return doc \n if not hasattr ( items , '<STR_LIT>' ) : \n is_list = True \n list_type = BaseList \n if isinstance ( items , EmbeddedDocumentList ) : \n list_type = EmbeddedDocumentList \n as_tuple = isinstance ( items , tuple ) \n iterator = enumerate ( items ) \n data = [ ] \n else : \n is_list = False \n iterator = items . iteritems ( ) \n data = { } \n depth += <NUM_LIT:1> \n for k , v in iterator : \n if is_list : \n data . append ( v ) \n else : \n data [ k ] = v \n if k in self . object_map and not is_list : \n data [ k ] = self . object_map [ k ] \n elif isinstance ( v , ( Document , EmbeddedDocument ) ) : \n for field_name , field in v . _fields . iteritems ( ) : \n v = data [ k ] . _data . get ( field_name , None ) \n if isinstance ( v , DBRef ) : \n data [ k ] . _data [ field_name ] = self . object_map . get ( \n ( v . collection , v . id ) , v ) \n elif isinstance ( v , ( dict , SON ) ) and '<STR_LIT>' in v : \n data [ k ] . _data [ field_name ] = self . object_map . get ( \n ( v [ '<STR_LIT>' ] . collection , v [ '<STR_LIT>' ] . id ) , v ) \n elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n item_name = txt_type ( \"<STR_LIT>\" ) . format ( name , k , field_name ) \n data [ k ] . _data [ field_name ] = self . _attach_objects ( v , depth , instance = instance , name = item_name ) \n elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n item_name = '<STR_LIT>' % ( name , k ) if name else name \n data [ k ] = self . _attach_objects ( v , depth - <NUM_LIT:1> , instance = instance , name = item_name ) \n elif hasattr ( v , '<STR_LIT:id>' ) : \n data [ k ] = self . object_map . get ( ( v . collection , v . id ) , v ) \n if instance and name : \n if is_list : \n return tuple ( data ) if as_tuple else list_type ( data , instance , name ) \n return BaseDict ( data , instance , name ) \n depth += <NUM_LIT:1> \n return <mask0> \n", "gt": "data"}
{"input": "\n import sys \n sys . path [ <NUM_LIT:0> : <NUM_LIT:0> ] = [ \"<STR_LIT>\" ] \n import unittest \n from mongoengine import * \n from mongoengine . connection import get_db \n __all__ = ( \"<STR_LIT>\" , ) \n class GeoFieldTest ( unittest . TestCase ) : \n def setUp ( self ) : \n connect ( db = '<STR_LIT>' ) \n self . db = get_db ( ) \n def _test_for_expected_error ( self , Cls , loc , expected ) : \n try : \n Cls ( loc = loc ) . validate ( ) \n self . fail ( '<STR_LIT>' . format ( loc ) ) \n except ValidationError as e : \n self . assertEqual ( expected , e . to_dict ( ) [ '<STR_LIT>' ] ) \n def test_geopoint_validation ( self ) : \n class Location ( Document ) : \n loc = GeoPointField ( ) \n invalid_coords = [ { \"<STR_LIT:x>\" : <NUM_LIT:1> , \"<STR_LIT:y>\" : <NUM_LIT:2> } , <NUM_LIT:5> , \"<STR_LIT:a>\" ] \n expected = '<STR_LIT>' \n for coord in invalid_coords : \n self . _test_for_expected_error ( Location , coord , expected ) \n invalid_coords = [ [ ] , [ <NUM_LIT:1> ] , [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] \n for coord in invalid_coords : \n expected = \"<STR_LIT>\" % repr ( coord ) \n self . _test_for_expected_error ( Location , coord , expected ) \n invalid_coords = [ [ { } , { } ] , ( \"<STR_LIT:a>\" , \"<STR_LIT:b>\" ) ] \n for coord in invalid_coords : \n expected = \"<STR_LIT>\" % repr ( coord ) \n self . _test_for_expected_error ( Location , coord , expected ) \n def test_point_validation ( self ) : \n class Location ( Document ) : \n loc = PointField ( ) \n invalid_coords = { \"<STR_LIT:x>\" : <NUM_LIT:1> , \"<STR_LIT:y>\" : <NUM_LIT:2> } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ ] } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] } \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ <NUM_LIT:5> , \"<STR_LIT:a>\" ] \n expected = \"<STR_LIT>\" \n for coord in invalid_coords : \n self . _test_for_expected_error ( Location , coord , expected ) \n invalid_coords = [ [ ] , [ <NUM_LIT:1> ] , [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] \n for coord in invalid_coords : \n expected = \"<STR_LIT>\" % repr ( coord ) \n self . _test_for_expected_error ( Location , coord , expected ) \n invalid_coords = [ [ { } , { } ] , ( \"<STR_LIT:a>\" , \"<STR_LIT:b>\" ) ] \n for coord in invalid_coords : \n expected = \"<STR_LIT>\" % repr ( coord ) \n self . _test_for_expected_error ( Location , coord , expected ) \n Location ( loc = [ <NUM_LIT:1> , <NUM_LIT:2> ] ) . validate ( ) \n Location ( loc = { \n \"<STR_LIT:type>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : [ \n <NUM_LIT> , \n <NUM_LIT> \n ] } ) . validate ( ) \n def test_linestring_validation ( self ) : \n class Location ( Document ) : \n loc = LineStringField ( ) \n invalid_coords = { \"<STR_LIT:x>\" : <NUM_LIT:1> , \"<STR_LIT:y>\" : <NUM_LIT:2> } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ [ ] ] } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] } \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ <NUM_LIT:5> , \"<STR_LIT:a>\" ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ <NUM_LIT:1> ] ] \n expected = \"<STR_LIT>\" % repr ( invalid_coords [ <NUM_LIT:0> ] ) \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] \n expected = \"<STR_LIT>\" % repr ( invalid_coords [ <NUM_LIT:0> ] ) \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ { } , { } ] ] , [ ( \"<STR_LIT:a>\" , \"<STR_LIT:b>\" ) ] ] \n for coord in invalid_coords : \n expected = \"<STR_LIT>\" % repr ( coord [ <NUM_LIT:0> ] ) \n self . _test_for_expected_error ( Location , coord , expected ) \n Location ( loc = [ [ <NUM_LIT:1> , <NUM_LIT:2> ] , [ <NUM_LIT:3> , <NUM_LIT:4> ] , [ <NUM_LIT:5> , <NUM_LIT:6> ] , [ <NUM_LIT:1> , <NUM_LIT:2> ] ] ) . validate ( ) \n def test_polygon_validation ( self ) : \n class Location ( Document ) : \n loc = PolygonField ( ) \n invalid_coords = { \"<STR_LIT:x>\" : <NUM_LIT:1> , \"<STR_LIT:y>\" : <NUM_LIT:2> } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ [ ] ] } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ [ [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] ] } \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ <NUM_LIT:5> , \"<STR_LIT:a>\" ] ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ ] ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ { } , { } ] ] , [ ( \"<STR_LIT:a>\" , \"<STR_LIT:b>\" ) ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ <NUM_LIT:1> , <NUM_LIT:2> ] , [ <NUM_LIT:3> , <NUM_LIT:4> ] ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n Location ( loc = [ [ [ <NUM_LIT:1> , <NUM_LIT:2> ] , [ <NUM_LIT:3> , <NUM_LIT:4> ] , [ <NUM_LIT:5> , <NUM_LIT:6> ] , [ <NUM_LIT:1> , <NUM_LIT:2> ] ] ] ) . validate ( ) \n def test_multipoint_validation ( self ) : \n class Location ( Document ) : \n loc = MultiPointField ( ) \n invalid_coords = { \"<STR_LIT:x>\" : <NUM_LIT:1> , \"<STR_LIT:y>\" : <NUM_LIT:2> } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ [ ] ] } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] } \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ <NUM_LIT:1> ] ] , [ [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] ] \n for coord in invalid_coords : \n expected = \"<STR_LIT>\" % repr ( coord [ <NUM_LIT:0> ] ) \n self . _test_for_expected_error ( Location , coord , expected ) \n invalid_coords = [ [ [ { } , { } ] ] , [ ( \"<STR_LIT:a>\" , \"<STR_LIT:b>\" ) ] ] \n for coord in invalid_coords : \n expected = \"<STR_LIT>\" % repr ( coord [ <NUM_LIT:0> ] ) \n self . _test_for_expected_error ( Location , coord , expected ) \n Location ( loc = [ [ <NUM_LIT:1> , <NUM_LIT:2> ] ] ) . validate ( ) \n Location ( loc = { \n \"<STR_LIT:type>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : [ \n [ <NUM_LIT:1> , <NUM_LIT:2> ] , \n [ <NUM_LIT> , <NUM_LIT> ] \n ] } ) . validate ( ) \n def test_multilinestring_validation ( self ) : \n class Location ( Document ) : \n loc = MultiLineStringField ( ) \n invalid_coords = { \"<STR_LIT:x>\" : <NUM_LIT:1> , \"<STR_LIT:y>\" : <NUM_LIT:2> } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ [ ] ] } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ [ [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] ] } \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ <NUM_LIT:5> , \"<STR_LIT:a>\" ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ <NUM_LIT:1> ] ] ] \n expected = \"<STR_LIT>\" % repr ( invalid_coords [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] ) \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] ] \n expected = \"<STR_LIT>\" % repr ( invalid_coords [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] ) \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ [ { } , { } ] ] ] , [ [ ( \"<STR_LIT:a>\" , \"<STR_LIT:b>\" ) ] ] ] \n for coord in invalid_coords : \n expected = \"<STR_LIT>\" % repr ( coord [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] ) \n self . _test_for_expected_error ( Location , coord , expected ) \n Location ( loc = [ [ [ <NUM_LIT:1> , <NUM_LIT:2> ] , [ <NUM_LIT:3> , <NUM_LIT:4> ] , [ <NUM_LIT:5> , <NUM_LIT:6> ] , [ <NUM_LIT:1> , <NUM_LIT:2> ] ] ] ) . validate ( ) \n def test_multipolygon_validation ( self ) : \n class Location ( Document ) : \n loc = MultiPolygonField ( ) \n invalid_coords = { \"<STR_LIT:x>\" : <NUM_LIT:1> , \"<STR_LIT:y>\" : <NUM_LIT:2> } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ [ ] ] } \n expected = '<STR_LIT>' \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = { \"<STR_LIT:type>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : [ [ [ [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] ] ] } \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ [ <NUM_LIT:5> , \"<STR_LIT:a>\" ] ] ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ [ ] ] ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] ] ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ [ { } , { } ] ] ] , [ [ ( \"<STR_LIT:a>\" , \"<STR_LIT:b>\" ) ] ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n invalid_coords = [ [ [ [ <NUM_LIT:1> , <NUM_LIT:2> ] , [ <NUM_LIT:3> , <NUM_LIT:4> ] ] ] ] \n expected = \"<STR_LIT>\" \n self . _test_for_expected_error ( Location , invalid_coords , expected ) \n Location ( loc = [ [ [ [ <NUM_LIT:1> , <NUM_LIT:2> ] , [ <NUM_LIT:3> , <NUM_LIT:4> ] , [ <NUM_LIT:5> , <NUM_LIT:6> ] , [ <NUM_LIT:1> , <NUM_LIT:2> ] ] ] ] ) . validate ( ) \n def test_indexes_geopoint ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n class Event ( Document ) : \n title = StringField ( ) \n location = GeoPointField ( ) \n geo_indicies = Event . _geo_indices ( ) \n self . assertEqual ( geo_indicies , [ { '<STR_LIT>' : [ ( '<STR_LIT:location>' , '<STR_LIT>' ) ] } ] ) \n def test_geopoint_embedded_indexes ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n class Venue ( EmbeddedDocument ) : \n location = GeoPointField ( ) \n name = StringField ( ) \n class Event ( Document ) : \n title = StringField ( ) \n venue = EmbeddedDocumentField ( Venue ) \n geo_indicies = Event . _geo_indices ( ) \n self . assertEqual ( geo_indicies , [ { '<STR_LIT>' : [ ( '<STR_LIT>' , '<STR_LIT>' ) ] } ] ) \n def test_indexes_2dsphere ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n class Event ( Document ) : \n title = StringField ( ) \n point = PointField ( ) \n line = LineStringField ( ) \n polygon = PolygonField ( ) \n geo_indicies = Event . _geo_indices ( ) \n self . assertTrue ( { '<STR_LIT>' : [ ( '<STR_LIT>' , '<STR_LIT>' ) ] } in geo_indicies ) \n self . assertTrue ( { '<STR_LIT>' : [ ( '<STR_LIT>' , '<STR_LIT>' ) ] } in geo_indicies ) \n self . assertTrue ( { '<STR_LIT>' : [ ( '<STR_LIT>' , '<STR_LIT>' ) ] } in geo_indicies ) \n def test_indexes_2dsphere_embedded ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n class Venue ( EmbeddedDocument ) : \n name = StringField ( ) \n point = PointField ( ) \n line = LineStringField ( ) \n polygon = PolygonField ( ) \n class Event ( Document ) : \n title = StringField ( ) \n venue = EmbeddedDocumentField ( Venue ) \n geo_indicies = Event . _geo_indices ( ) \n self . assertTrue ( { '<STR_LIT>' : [ ( '<STR_LIT>' , '<STR_LIT>' ) ] } in geo_indicies ) \n self . assertTrue ( { '<STR_LIT>' : [ ( '<STR_LIT>' , '<STR_LIT>' ) ] } in geo_indicies ) \n self . assertTrue ( { '<STR_LIT>' : [ ( '<STR_LIT>' , '<STR_LIT>' ) ] } in geo_indicies ) \n def test_geo_indexes_recursion ( self ) : \n class Location ( Document ) : \n name = StringField ( ) \n location = GeoPointField ( ) \n class Parent ( Document ) : \n name = StringField ( ) \n location = ReferenceField ( Location ) \n Location . drop_collection ( ) \n Parent . drop_collection ( ) \n Parent ( name = '<STR_LIT>' ) . save ( ) \n info = Parent . _get_collection ( ) . index_information ( ) \n self . assertFalse ( '<STR_LIT>' in info ) \n info = Location . _get_collection ( ) . index_information ( ) \n self . assertTrue ( '<STR_LIT>' in info ) \n self . assertEqual ( len ( Parent . _geo_indices ( ) ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( Location . _geo_indices ( ) ) , <NUM_LIT:1> ) \n def test_geo_indexes_auto_index ( self ) : \n class Log ( Document ) : \n location = PointField ( auto_index = False ) \n datetime = DateTimeField ( ) \n meta = { \n '<STR_LIT>' : [ [ ( \"<STR_LIT:location>\" , \"<STR_LIT>\" ) , ( \"<STR_LIT>\" , <NUM_LIT:1> ) ] ] \n } \n self . assertEqual ( [ ] , Log . _geo_indices ( ) ) \n Log . drop_collection ( ) \n Log . ensure_indexes ( ) \n info = Log . _get_collection ( ) . index_information ( ) \n self . assertEqual ( info [ \"<STR_LIT>\" ] [ \"<STR_LIT:key>\" ] , \n [ ( '<STR_LIT:location>' , '<STR_LIT>' ) , ( '<STR_LIT>' , <NUM_LIT:1> ) ] ) \n class Log ( Document ) : \n location = PointField ( auto_index = False ) \n datetime = DateTimeField ( ) \n meta = { \n '<STR_LIT>' : [ \n { '<STR_LIT>' : [ ( \"<STR_LIT:location>\" , \"<STR_LIT>\" ) , ( \"<STR_LIT>\" , <NUM_LIT:1> ) ] } \n ] \n } \n self . assertEqual ( [ ] , Log . _geo_indices ( ) ) \n Log . drop_collection ( ) \n Log . ensure_indexes ( ) \n info = Log . _get_collection ( ) . index_information ( ) \n self . assertEqual ( info [ \"<STR_LIT>\" ] [ \"<STR_LIT:key>\" ] , \n [ ( '<STR_LIT:location>' , '<STR_LIT>' ) , ( '<STR_LIT>' , <NUM_LIT:1> ) ] ) \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n from south . db import db \n from django . db import models \n from django_lean . experiments . models import * \n class Migration : \n def forwards ( self , orm ) : \n db . create_table ( '<STR_LIT>' , ( \n ( '<STR_LIT:id>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT:name>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT:state>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT>' , orm [ '<STR_LIT>' ] ) , \n ) ) \n db . send_create_signal ( '<STR_LIT>' , [ '<STR_LIT>' ] ) \n db . create_table ( '<STR_LIT>' , ( \n ( '<STR_LIT:id>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT:user>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT>' , orm [ '<STR_LIT>' ] ) , \n ) ) \n db . send_create_signal ( '<STR_LIT>' , [ '<STR_LIT>' ] ) \n db . create_table ( '<STR_LIT>' , ( \n ( '<STR_LIT:id>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT:date>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT>' , orm [ '<STR_LIT>' ] ) , \n ( '<STR_LIT>' , orm [ '<STR_LIT>' ] ) , \n ) ) \n db . send_create_signal ( '<STR_LIT>' , [ '<STR_LIT>' ] ) \n db . create_unique ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def backwards ( self , orm ) : \n db . delete_table ( '<STR_LIT>' ) \n db . delete_table ( '<STR_LIT>' ) \n db . delete_table ( '<STR_LIT>' ) \n db . delete_unique ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] ) \n models = { \n '<STR_LIT>' : { \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:name>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT>' : \"<STR_LIT>\" } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:name>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT>' } ) , \n '<STR_LIT:email>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:password>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:username>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT:True>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT>' : \"<STR_LIT>\" , '<STR_LIT>' : \"<STR_LIT>\" } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' } ) , \n '<STR_LIT:name>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { } ) , \n '<STR_LIT:date>' : ( '<STR_LIT>' , [ ] , { } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:name>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:state>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:0>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT>' : \"<STR_LIT>\" } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:user>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" } ) \n } \n } \n <mask0> = [ '<STR_LIT>' ] \n", "gt": "complete_apps"}
{"input": "\n class SimpleEngagementCalculator ( object ) : \n def calculate_user_engagement_score ( self , user , start_date , end_date ) : \n return <NUM_LIT:0> \n ROOT_URLCONF = None \n DATABASE_ENGINE = '<STR_LIT>' \n DATABASE_NAME = '<STR_LIT>' \n DATABASE_SUPPORTS_TRANSACTIONS = <mask0> \n INSTALLED_APPS = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' ] \n TEMPLATE_CONTEXT_PROCESSORS = ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ) \n", "gt": "False"}
{"input": "\n from contextlib import contextmanager \n from django . contrib . sites . models import Site \n from django . core import mail \n from django . db import transaction \n from django . utils . functional import LazyObject \n def get_current_site ( ) : \n if Site . _meta . installed : \n return Site . objects . get_current ( ) \n return None \n def in_transaction ( test_ignore = True ) : \n result = transaction . is_managed ( ) \n if test_ignore : \n result = result and not hasattr ( mail , '<STR_LIT>' ) \n return result \n @ contextmanager \n def patch ( namespace , name , function ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( namespace , LazyObject ) : \n if namespace . _wrapped is None : \n namespace . _setup ( ) \n namespace = namespace . _wrapped \n try : \n original = getattr ( namespace , name ) \n except AttributeError : \n original = NotImplemented \n try : \n setattr ( namespace , name , function ) \n yield \n finally : \n if original is NotImplemented : \n delattr ( namespace , name ) \n else : \n setattr ( namespace , name , <mask0> ) \n", "gt": "original"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from construct import * \n from ipv4 import IpAddress \n echo_payload = Struct ( \"<STR_LIT>\" , \n UBInt16 ( \"<STR_LIT>\" ) , \n UBInt16 ( \"<STR_LIT>\" ) , \n Bytes ( \"<STR_LIT:data>\" , <NUM_LIT:32> ) , \n ) \n dest_unreachable_payload = Struct ( \"<STR_LIT>\" , \n Padding ( <NUM_LIT:2> ) , \n UBInt16 ( \"<STR_LIT>\" ) , \n IpAddress ( \"<STR_LIT:host>\" ) , \n Bytes ( \"<STR_LIT>\" , <NUM_LIT:8> ) , \n ) \n dest_unreachable_code = Enum ( Byte ( \"<STR_LIT:code>\" ) , \n Network_unreachable_error = <NUM_LIT:0> , \n Host_unreachable_error = <NUM_LIT:1> , \n Protocol_unreachable_error = <NUM_LIT:2> , \n Port_unreachable_error = <NUM_LIT:3> , \n The_datagram_is_too_big = <NUM_LIT:4> , \n Source_route_failed_error = <NUM_LIT:5> , \n Destination_network_unknown_error = <NUM_LIT:6> , \n Destination_host_unknown_error = <NUM_LIT:7> , \n Source_host_isolated_error = <NUM_LIT:8> , \n Desination_administratively_prohibited = <NUM_LIT:9> , \n Host_administratively_prohibited2 = <NUM_LIT:10> , \n Network_TOS_unreachable = <NUM_LIT:11> , \n Host_TOS_unreachable = <NUM_LIT:12> , \n ) \n icmp_header = Struct ( \"<STR_LIT>\" , \n Enum ( Byte ( \"<STR_LIT:type>\" ) , \n Echo_reply = <NUM_LIT:0> , \n Destination_unreachable = <NUM_LIT:3> , \n Source_quench = <NUM_LIT:4> , \n Redirect = <NUM_LIT:5> , \n Alternate_host_address = <NUM_LIT:6> , \n Echo_request = <NUM_LIT:8> , \n Router_advertisement = <NUM_LIT:9> , \n Router_solicitation = <NUM_LIT:10> , \n Time_exceeded = <NUM_LIT:11> , \n Parameter_problem = <NUM_LIT:12> , \n Timestamp_request = <NUM_LIT> , \n Timestamp_reply = <NUM_LIT> , \n Information_request = <NUM_LIT:15> , \n Information_reply = <NUM_LIT:16> , \n Address_mask_request = <NUM_LIT> , \n Address_mask_reply = <NUM_LIT> , \n _default_ = Pass , \n ) , \n Switch ( \"<STR_LIT:code>\" , lambda ctx : ctx . type , \n { \n \"<STR_LIT>\" : dest_unreachable_code , \n } , \n default = Byte ( \"<STR_LIT:code>\" ) , \n ) , \n UBInt16 ( \"<STR_LIT>\" ) , \n Switch ( \"<STR_LIT>\" , lambda ctx : ctx . type , \n { \n \"<STR_LIT>\" : echo_payload , \n \"<STR_LIT>\" : echo_payload , \n \"<STR_LIT>\" : dest_unreachable_payload , \n } , \n default = Pass \n ) \n ) \n if __name__ == \"<STR_LIT:__main__>\" : \n cap1 = ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) . decode ( \"<STR_LIT>\" ) \n cap2 = ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) . decode ( \"<STR_LIT>\" ) \n cap3 = ( \"<STR_LIT>\" ) . decode ( \"<STR_LIT>\" ) \n print icmp_header . parse ( cap1 ) \n print icmp_header . parse ( cap2 ) \n print icmp_header . parse ( <mask0> ) \n", "gt": "cap3"}
{"input": "\n from construct . core import Container \n from construct . adapters import Adapter \n class AstNode ( Container ) : \n def __init__ ( self , nodetype , ** kw ) : \n Container . __init__ ( self ) \n self . nodetype = nodetype \n for k , v in sorted ( kw . iteritems ( ) ) : \n setattr ( self , k , v ) \n def accept ( self , visitor ) : \n return getattr ( visitor , \"<STR_LIT>\" % ( self . nodetype , ) ) ( self ) \n class AstTransformator ( Adapter ) : \n def _decode ( self , obj , context ) : \n return self . to_ast ( obj , context ) \n def _encode ( self , obj , context ) : \n return self . to_cst ( obj , <mask0> ) \n", "gt": "context"}
{"input": "\n def pytest_funcarg__setupopts ( request ) : \n return OptsSetup ( request ) \n def pytest_addoption ( parser ) : \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n type = str , default = None , \n help = \"<STR_LIT>\" ) \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n action = \"<STR_LIT:store_true>\" , \n help = \"<STR_LIT>\" ) \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n action = \"<STR_LIT:store_true>\" , \n help = \"<STR_LIT>\" ) \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n type = int , default = <NUM_LIT:0> , \n help = \"<STR_LIT>\" ) \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT:time>\" , \n type = str , default = \"<STR_LIT>\" , \n help = \"<STR_LIT>\" ) \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n type = int , default = <NUM_LIT:8> , \n help = \"<STR_LIT>\" ) \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n type = float , default = <NUM_LIT:16> , \n help = \"<STR_LIT>\" ) \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n type = int , default = <NUM_LIT:8> , \n help = \"<STR_LIT>\" ) \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n type = str , default = None , \n help = \"<STR_LIT>\" ) \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n action = \"<STR_LIT:store_true>\" , \n help = \"<STR_LIT>\" ) \n parser . addoption ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n type = str , default = \"<STR_LIT>\" , \n help = \"<STR_LIT>\" ) \n class OptsSetup ( ) : \n def __init__ ( self , request ) : \n self . config = request . config \n def returnAllOptions ( self ) : \n return self . config . option \n def getNumExecutors ( self ) : \n return self . config . option . num_exec \n def getTime ( self ) : \n return self . config . option . time \n def getProc ( self ) : \n return self . config . option . proc \n def getMem ( self ) : \n return self . config . option . mem \n def getQueue ( self ) : \n return self . config . option . queue \n def getPpn ( self ) : \n return self . config . option . ppn \n def getRestart ( self ) : \n return self . config . option . restart \n def getBackupDir ( self ) : \n return self . config . option . backup_directory \n def returnSampleArgs ( self ) : \n sampleArgArray = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n return <mask0> \n", "gt": "sampleArgArray"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import base64 \n import random \n from twisted . python import log \n from twisted . web . error import Error as WebError \n from opennsa import constants as cnt , config \n from opennsa . backends . common import genericbackend \n from opennsa . protocols . shared import httpclient \n NCS_TIMEOUT = <NUM_LIT> \n NO_OUT_OF_SYNC_CHECK = '<STR_LIT>' \n ETHERNET_VPN_PAYLOAD_BASE = \"\"\"<STR_LIT>\"\"\" \n ETHERNET_VLAN_VPN_PAYLOAD_BASE = \"\"\"<STR_LIT>\"\"\" \n ETHERNET_VLAN_REWRITE_VPN_PAYLOAD_BASE = \"\"\"<STR_LIT>\"\"\" \n LOG_SYSTEM = '<STR_LIT>' \n class NCSVPNTarget ( object ) : \n def __init__ ( self , router , interface , vlan = None ) : \n self . router = router \n self . interface = interface \n self . vlan = vlan \n def __str__ ( self ) : \n if self . vlan : \n return '<STR_LIT>' % ( self . router , self . interface , self . vlan ) \n else : \n return '<STR_LIT>' % ( self . router , self . interface ) \n def createVPNPayload ( service_name , source_target , dest_target ) : \n intps = { \n '<STR_LIT>' : service_name , \n '<STR_LIT>' : service_name , \n '<STR_LIT>' : source_target . router , \n '<STR_LIT>' : source_target . interface , \n '<STR_LIT>' : dest_target . router , \n '<STR_LIT>' : dest_target . interface \n } \n if source_target . vlan and dest_target . vlan : \n if source_target . vlan == dest_target . vlan : \n intps [ '<STR_LIT>' ] = source_target . vlan \n payload = ETHERNET_VLAN_VPN_PAYLOAD_BASE % intps \n else : \n intps [ '<STR_LIT>' ] = source_target . vlan \n intps [ '<STR_LIT>' ] = dest_target . vlan \n payload = ETHERNET_VLAN_REWRITE_VPN_PAYLOAD_BASE % intps \n else : \n payload = ETHERNET_VPN_PAYLOAD_BASE % intps \n return payload \n def _extractErrorMessage ( failure ) : \n if isinstance ( failure . value , WebError ) : \n return failure . value . response \n else : \n return failure . getErrorMessage ( ) \n class NCSVPNConnectionManager : \n def __init__ ( self , ncs_services_url , user , password , port_map , log_system ) : \n self . ncs_services_url = ncs_services_url \n self . user = user \n self . password = password \n self . port_map = port_map \n self . log_system = log_system \n def getResource ( self , port , label_type , label_value ) : \n assert label_type in ( None , cnt . ETHERNET_VLAN ) , '<STR_LIT>' \n return port + '<STR_LIT::>' + str ( label_value ) \n def getTarget ( self , port , label_type , label_value ) : \n assert label_type in ( None , cnt . ETHERNET_VLAN ) , '<STR_LIT>' \n if label_type == cnt . ETHERNET_VLAN : \n vlan = int ( label_value ) \n assert <NUM_LIT:1> <= vlan <= <NUM_LIT> , '<STR_LIT>' % label_value \n ri = self . port_map [ port ] \n router , interface = ri . split ( '<STR_LIT::>' ) \n return NCSVPNTarget ( router , interface , vlan ) \n def createConnectionId ( self , source_target , dest_target ) : \n return '<STR_LIT>' + str ( random . randint ( <NUM_LIT> , <NUM_LIT> ) ) \n def canSwapLabel ( self , label_type ) : \n return label_type == cnt . ETHERNET_VLAN \n def _createAuthzHeader ( self ) : \n return '<STR_LIT>' + base64 . b64encode ( self . user + '<STR_LIT::>' + self . password ) \n def _createHeaders ( self ) : \n headers = { } \n headers [ '<STR_LIT:Content-Type>' ] = '<STR_LIT>' \n headers [ '<STR_LIT>' ] = self . _createAuthzHeader ( ) \n return headers \n def setupLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n service_url = self . ncs_services_url + '<STR_LIT:?>' + NO_OUT_OF_SYNC_CHECK \n payload = createVPNPayload ( connection_id , source_target , dest_target ) \n headers = self . _createHeaders ( ) \n def linkUp ( _ ) : \n log . msg ( '<STR_LIT>' % ( source_target , dest_target ) , system = self . log_system ) \n def error ( failure ) : \n log . msg ( '<STR_LIT>' % ( source_target , dest_target ) , system = self . log_system ) \n log . msg ( '<STR_LIT>' % _extractErrorMessage ( failure ) , system = self . log_system ) \n return failure \n d = httpclient . httpRequest ( service_url , payload , headers , method = '<STR_LIT:POST>' , timeout = NCS_TIMEOUT ) \n d . addCallbacks ( linkUp , error ) \n return d \n def teardownLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n service_url = self . ncs_services_url + '<STR_LIT>' + connection_id + '<STR_LIT:?>' + NO_OUT_OF_SYNC_CHECK \n headers = self . _createHeaders ( ) \n def linkDown ( _ ) : \n log . msg ( '<STR_LIT>' % ( source_target , dest_target ) , system = self . log_system ) \n def error ( failure ) : \n log . msg ( '<STR_LIT>' % ( source_target , dest_target ) , system = self . log_system ) \n log . msg ( '<STR_LIT>' % _extractErrorMessage ( failure ) , system = self . log_system ) \n return failure \n d = httpclient . httpRequest ( service_url , None , headers , method = '<STR_LIT>' , timeout = NCS_TIMEOUT ) \n d . addCallbacks ( linkDown , error ) \n return d \n def NCSVPNBackend ( network_name , nrm_ports , parent_requester , cfg ) : \n name = '<STR_LIT>' % network_name \n nrm_map = dict ( [ ( p . name , p ) for p in nrm_ports ] ) \n port_map = dict ( [ ( p . name , p . interface ) for p in nrm_ports ] ) \n ncs_services_url = str ( cfg [ config . NCS_SERVICES_URL ] ) \n user = cfg [ config . NCS_USER ] \n password = cfg [ config . NCS_PASSWORD ] \n cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n return genericbackend . GenericBackend ( network_name , nrm_map , cm , parent_requester , <mask0> ) \n", "gt": "name"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import time \n from twisted . python import log , failure \n from opennsa import nsa , error \n from opennsa . shared import xmlhelper \n from opennsa . protocols . shared import minisoap , soapresource \n from opennsa . protocols . nsi2 import helper , queryhelper \n from opennsa . protocols . nsi2 . bindings import actions , nsiconnection , p2pservices \n LOG_SYSTEM = '<STR_LIT>' \n class ProviderService : \n def __init__ ( self , soap_resource , provider ) : \n self . provider = provider \n soap_resource . registerDecoder ( actions . RESERVE , self . reserve ) \n soap_resource . registerDecoder ( actions . RESERVE_COMMIT , self . reserveCommit ) \n soap_resource . registerDecoder ( actions . RESERVE_ABORT , self . reserveAbort ) \n soap_resource . registerDecoder ( actions . PROVISION , self . provision ) \n soap_resource . registerDecoder ( actions . RELEASE , self . release ) \n soap_resource . registerDecoder ( actions . TERMINATE , self . terminate ) \n soap_resource . registerDecoder ( actions . QUERY_SUMMARY , self . querySummary ) \n soap_resource . registerDecoder ( actions . QUERY_SUMMARY_SYNC , self . querySummarySync ) \n soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n def _createSOAPFault ( self , err , provider_nsa , connection_id = None , service_type = None ) : \n log . msg ( '<STR_LIT>' % err . getErrorMessage ( ) , system = LOG_SYSTEM ) \n se = helper . createServiceException ( err , provider_nsa , connection_id ) \n ex_element = se . xml ( nsiconnection . serviceException ) \n soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n return soap_fault \n def reserve ( self , soap_data , request_info ) : \n t_start = time . time ( ) \n header , reservation = helper . parseRequest ( soap_data ) \n criteria = reservation . criteria \n service_type = criteria . serviceType \n p2ps = criteria . serviceDefinition \n if type ( p2ps ) is not p2pservices . P2PServiceBaseType : \n err = failure . Failure ( error . PayloadError ( '<STR_LIT>' ) ) \n return self . _createSOAPFault ( err , header . provider_nsa , service_type = service_type ) \n if p2ps . directionality in ( None , '<STR_LIT>' ) : \n err = failure . Failure ( error . MissingParameterError ( '<STR_LIT>' ) ) \n return self . _createSOAPFault ( err , header . provider_nsa ) \n start_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . startTime ) if criteria . schedule . startTime is not None else None \n end_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . endTime ) if criteria . schedule . endTime is not None else None \n schedule = nsa . Schedule ( start_time , end_time ) \n src_stp = helper . createSTP ( p2ps . sourceSTP ) \n dst_stp = helper . createSTP ( p2ps . destSTP ) \n if p2ps . ero : \n err = failure . Failure ( error . PayloadError ( '<STR_LIT>' ) ) \n return self . _createSOAPFault ( err , header . provider_nsa ) \n params = [ ( p . type_ , p . value ) for p in p2ps . parameter ] if p2ps . parameter else None \n symmetric = p2ps . symmetricPath or False \n sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , None , params ) \n crt = nsa . Criteria ( criteria . version , schedule , sd ) \n t_delta = time . time ( ) - t_start \n log . msg ( '<STR_LIT>' % round ( t_delta , <NUM_LIT:3> ) , profile = True , system = LOG_SYSTEM ) \n d = self . provider . reserve ( header , reservation . connectionId , reservation . globalReservationId , reservation . description , crt , request_info ) \n def createReserveAcknowledgement ( connection_id ) : \n soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa , None , header . correlation_id ) \n reserve_response = nsiconnection . ReserveResponseType ( connection_id ) \n reserve_response_element = reserve_response . xml ( nsiconnection . reserveResponse ) \n payload = minisoap . createSoapPayload ( reserve_response_element , soap_header_element ) \n return payload \n d . addCallbacks ( createReserveAcknowledgement , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n return d \n def reserveCommit ( self , soap_data , request_info ) : \n header , confirm = helper . parseRequest ( soap_data ) \n d = self . provider . reserveCommit ( header , confirm . connectionId , request_info ) \n d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , confirm . connectionId ) ) \n return d \n def reserveAbort ( self , soap_data , request_info ) : \n header , request = helper . parseRequest ( soap_data ) \n d = self . provider . reserveAbort ( header , request . connectionId , request_info ) \n d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n return d \n def provision ( self , soap_data , request_info ) : \n header , request = helper . parseRequest ( soap_data ) \n d = self . provider . provision ( header , request . connectionId , request_info ) \n d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n return d \n def release ( self , soap_data , request_info ) : \n header , request = helper . parseRequest ( soap_data ) \n d = self . provider . release ( header , request . connectionId , request_info ) \n d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n return d \n def terminate ( self , soap_data , request_info ) : \n header , request = helper . parseRequest ( soap_data ) \n d = self . provider . terminate ( header , request . connectionId , request_info ) \n d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n return d \n def querySummary ( self , soap_data , request_info ) : \n header , query = helper . parseRequest ( soap_data ) \n d = self . provider . querySummary ( header , query . connectionId , query . globalReservationId , request_info ) \n d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n return d \n def querySummarySync ( self , soap_data , request_info ) : \n def gotReservations ( reservations , header ) : \n soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa , correlation_id = header . correlation_id ) \n qs_reservations = queryhelper . buildQuerySummaryResultType ( reservations ) \n qsct = nsiconnection . QuerySummaryConfirmedType ( qs_reservations ) \n payload = minisoap . createSoapPayload ( qsct . xml ( nsiconnection . querySummarySyncConfirmed ) , soap_header_element ) \n return payload \n header , query = helper . parseRequest ( soap_data ) \n d = self . provider . querySummarySync ( header , query . connectionId , query . globalReservationId , request_info ) \n d . addCallbacks ( gotReservations , self . _createSOAPFault , callbackArgs = ( header , ) , errbackArgs = ( header . provider_nsa , ) ) \n return d \n def queryRecursive ( self , soap_data , request_info ) : \n header , query = helper . parseRequest ( soap_data ) \n d = self . provider . queryRecursive ( header , query . connectionId , query . globalReservationId , request_info ) \n d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n return <mask0> \n", "gt": "d"}
{"input": "\n import os , datetime , json \n from twisted . trial import unittest \n from twisted . internet import defer , task \n from opennsa import config , nsa , database \n from opennsa . topology import nml \n from opennsa . backends import ncsvpn \n from . import common \n class NCSVPNBackendTest ( unittest . TestCase ) : \n def setUp ( self ) : \n self . clock = task . Clock ( ) \n tcf = os . path . expanduser ( '<STR_LIT>' ) \n tc = json . load ( open ( tcf ) ) \n ncs_config = { \n config . NCS_SERVICES_URL : tc [ '<STR_LIT>' ] , \n config . NCS_USER : tc [ '<STR_LIT>' ] , \n config . NCS_PASSWORD : tc [ '<STR_LIT>' ] \n } \n self . requester = common . DUDRequester ( ) \n self . backend = ncsvpn . NCSVPNBackend ( '<STR_LIT>' , self . sr , self . requester , ncs_config ) \n self . backend . scheduler . clock = self . clock \n self . backend . startService ( ) \n database . setupDatabase ( tc [ '<STR_LIT>' ] , tc [ '<STR_LIT>' ] , tc [ '<STR_LIT>' ] ) \n self . requester_nsa = nsa . NetworkServiceAgent ( '<STR_LIT>' , '<STR_LIT>' ) \n self . provider_nsa = nsa . NetworkServiceAgent ( '<STR_LIT>' , '<STR_LIT>' ) \n source_stp = nsa . STP ( '<STR_LIT>' , '<STR_LIT>' , labels = [ nsa . Label ( nml . ETHERNET_VLAN , '<STR_LIT>' ) ] ) \n dest_stp = nsa . STP ( '<STR_LIT>' , '<STR_LIT>' , labels = [ nsa . Label ( nml . ETHERNET_VLAN , '<STR_LIT>' ) ] ) \n start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = <NUM_LIT:2> ) \n end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = <NUM_LIT:30> ) \n bandwidth = <NUM_LIT:200> \n self . service_params = nsa . ServiceParameters ( start_time , end_time , source_stp , dest_stp , bandwidth ) \n @ defer . inlineCallbacks \n def tearDown ( self ) : \n from opennsa . backends . common import simplebackend \n yield simplebackend . Simplebackendconnection . deleteAll ( ) \n yield self . backend . stopService ( ) \n @ defer . inlineCallbacks \n def testActivation ( self ) : \n _ , _ , cid , sp = yield self . reserve ( self . requester_nsa , self . provider_nsa , None , None , None , None , self . service_params ) \n yield self . backend . reserveCommit ( self . requester_nsa , self . provider_nsa , None , cid ) \n yield self . backend . provision ( self . requester_nsa , self . provider_nsa , None , cid ) \n self . clock . advance ( <NUM_LIT:3> ) \n connection_id , active , version_consistent , version , timestamp = yield d_up \n self . failUnlessEqual ( cid , connection_id ) \n self . failUnlessEqual ( active , True ) \n self . failUnlessEqual ( version_consistent , True ) \n yield self . backend . terminate ( self . requester_nsa , self . provider_nsa , None , cid ) \n connection_id , active , version_consistent , version , timestamp = yield d_down \n self . failUnlessEqual ( cid , connection_id ) \n self . failUnlessEqual ( active , False ) \n self . failUnlessEqual ( version_consistent , True ) \n testActivation . <mask0> = '<STR_LIT>' \n", "gt": "skip"}
{"input": "\n from __future__ import absolute_import \n from . classification import * \n from . generic import * \n from . job import <mask0> \n", "gt": "ImageDatasetJob"}
{"input": "\n from __future__ import absolute_import \n from . images import * \n from . job import <mask0> \n", "gt": "InferenceJob"}
{"input": "\n from __future__ import absolute_import \n from . caffe_train import CaffeTrainTask \n from . torch_train import TorchTrainTask \n from . train import <mask0> \n", "gt": "TrainTask"}
{"input": "\n from __future__ import absolute_import \n import flask \n from flask . ext . socketio import SocketIO \n from gevent import monkey ; monkey . patch_all ( ) \n from . config import config_value \n from digits import utils \n import digits . scheduler \n app = flask . Flask ( __name__ ) \n app . config [ '<STR_LIT>' ] = True \n app . config [ '<STR_LIT>' ] = False \n app . config [ '<STR_LIT>' ] = config_value ( '<STR_LIT>' ) \n app . url_map . redirect_defaults = False \n socketio = SocketIO ( app ) \n scheduler = digits . scheduler . Scheduler ( config_value ( '<STR_LIT>' ) , True ) \n app . jinja_env . globals [ '<STR_LIT>' ] = config_value ( '<STR_LIT>' ) \n app . jinja_env . globals [ '<STR_LIT>' ] = digits . __version__ \n app . jinja_env . filters [ '<STR_LIT>' ] = utils . time_filters . print_time \n app . jinja_env . filters [ '<STR_LIT>' ] = utils . time_filters . print_time_diff \n app . jinja_env . filters [ '<STR_LIT>' ] = utils . time_filters . print_time_since \n app . jinja_env . filters [ '<STR_LIT>' ] = utils . sizeof_fmt \n app . jinja_env . filters [ '<STR_LIT>' ] = utils . auth . has_permission \n app . jinja_env . trim_blocks = True \n app . jinja_env . lstrip_blocks = True \n import digits . views \n app . register_blueprint ( digits . views . blueprint ) \n import digits . dataset . views \n app . register_blueprint ( digits . dataset . views . blueprint , url_prefix = '<STR_LIT>' ) \n import digits . dataset . images . views \n app . register_blueprint ( digits . dataset . images . views . blueprint , url_prefix = '<STR_LIT>' ) \n import digits . dataset . images . classification . views \n app . register_blueprint ( digits . dataset . images . classification . views . blueprint , url_prefix = '<STR_LIT>' ) \n import digits . dataset . images . generic . views \n app . register_blueprint ( digits . dataset . images . generic . views . blueprint , url_prefix = '<STR_LIT>' ) \n import digits . model . views \n app . register_blueprint ( digits . model . views . blueprint , url_prefix = '<STR_LIT>' ) \n import digits . model . images . views \n app . register_blueprint ( digits . model . images . views . blueprint , url_prefix = '<STR_LIT>' ) \n import digits . model . images . classification . views \n app . register_blueprint ( digits . model . images . classification . views . blueprint , url_prefix = '<STR_LIT>' ) \n import digits . model . images . generic . views \n app . register_blueprint ( digits . model . images . generic . views . blueprint , url_prefix = '<STR_LIT>' ) \n def username_decorator ( f ) : \n from functools import wraps \n @ wraps ( f ) \n def decorated ( * args , ** kwargs ) : \n this_username = flask . request . cookies . get ( '<STR_LIT:username>' , None ) \n app . jinja_env . globals [ '<STR_LIT:username>' ] = this_username \n return f ( * args , ** kwargs ) \n return decorated \n for endpoint , function in app . view_functions . iteritems ( ) : \n app . view_functions [ endpoint ] = username_decorator ( function ) \n scheduler . <mask0> ( ) \n", "gt": "load_past_jobs"}
{"input": "\n import os \n import unittest \n import requests \n import requests_cache \n from nytcampfin import NytCampfin , NytCampfinError , NytNotFoundError \n CURRENT_CYCLE = <NUM_LIT> \n try : \n API_KEY = os . environ [ '<STR_LIT>' ] \n except : \n print \"<STR_LIT>\" \n class APITest ( unittest . TestCase ) : \n def check_response ( self , result , url , parse = lambda r : r [ '<STR_LIT>' ] ) : \n with requests_cache . disabled ( ) : \n response = requests . get ( url ) \n if parse and callable ( parse ) : \n response = parse ( response . json ) \n self . assertEqual ( result , response ) \n def setUp ( self ) : \n self . finance = NytCampfin ( API_KEY ) \n class FilingTest ( APITest ) : \n def test_todays_filings ( self ) : \n today = self . finance . filings . today ( offset = <NUM_LIT:20> ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( today , url ) \n def test_filings_for_date ( self ) : \n july4th = self . finance . filings . date ( <NUM_LIT> , <NUM_LIT:0> <NUM_LIT:7> , <NUM_LIT:0> <NUM_LIT:4> ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( july4th , url ) \n def test_form_types ( self ) : \n form_types = self . finance . filings . form_types ( ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( form_types , url ) \n def test_filings_by_form_type ( self ) : \n f2s = self . finance . filings . by_type ( '<STR_LIT>' ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( f2s , url ) \n def test_amended_filings ( self ) : \n amendments = self . finance . filings . amendments ( ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( amendments , url ) \n class IndependentExpenditureTest ( APITest ) : \n def test_latest ( self ) : \n latest = self . finance . indexp . latest ( ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( latest , url ) \n def test_ies_for_date ( self ) : \n july3rd = self . finance . indexp . date ( <NUM_LIT> , <NUM_LIT:0> <NUM_LIT:7> , <NUM_LIT:0> <NUM_LIT:3> ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( july3rd , url ) \n def test_committee_ies ( self ) : \n ies = self . finance . indexp . committee ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( ies , url ) \n def test_race_totals ( self ) : \n races = self . finance . indexp . race_totals ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( races , url ) \n def test_candidate_ies ( self ) : \n ies = self . finance . indexp . candidate ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( ies , url ) \n def test_president_ies ( self ) : \n ies = self . finance . indexp . president ( ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( ies , url ) \n def test_superpacs ( self ) : \n superpacs = self . finance . indexp . superpacs ( ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( superpacs , url ) \n class CandidateTest ( APITest ) : \n def test_latest ( self ) : \n latest = self . finance . candidates . latest ( ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( latest , url ) \n def test_detail ( self ) : \n detail = self . finance . candidates . get ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n response = requests . get ( url ) \n self . check_response ( detail , url , parse = lambda r : r [ '<STR_LIT>' ] [ <NUM_LIT:0> ] ) \n def test_filter ( self ) : \n wilson = self . finance . candidates . filter ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( wilson , url ) \n def test_leaders ( self ) : \n loans = self . finance . candidates . leaders ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( loans , url ) \n def test_candidates_for_state ( self ) : \n candidates = self . finance . candidates . seats ( '<STR_LIT>' ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( candidates , url ) \n def test_candidates_for_state_and_chamber ( self ) : \n candidates = self . finance . candidates . seats ( '<STR_LIT>' , '<STR_LIT>' ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( candidates , url ) \n def test_candidates_for_state_and_chamber_and_district ( self ) : \n candidates = self . finance . candidates . seats ( '<STR_LIT>' , '<STR_LIT>' , <NUM_LIT:6> ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( candidates , url ) \n def test_late_contributions ( self ) : \n late_contribs = self . finance . candidates . late_contributions ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( late_contribs , url ) \n class CommitteeTest ( APITest ) : \n def test_latest ( self ) : \n latest = self . finance . committees . latest ( ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( latest , url ) \n def test_detail ( self ) : \n detail = self . finance . committees . get ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n response = requests . get ( url ) \n self . check_response ( detail , url , parse = lambda r : r [ '<STR_LIT>' ] [ <NUM_LIT:0> ] ) \n def test_filter ( self ) : \n hallmark = self . finance . committees . filter ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( hallmark , url ) \n def test_contributions ( self ) : \n contributions = self . finance . committees . contributions ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( contributions , url ) \n def test_contributions_to_candidate ( self ) : \n contributions = self . finance . committees . contributions_to_candidate ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( contributions , url ) \n def test_late_contributions ( self ) : \n late_contribs = self . finance . committees . late_contributions ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( late_contribs , url ) \n def test_filings ( self ) : \n filings = self . finance . committees . filings ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( filings , url ) \n def test_ie_totals ( self ) : \n ie_totals = self . finance . committees . ie_totals ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( ie_totals , url ) \n def test_leadership ( self ) : \n leadership = self . finance . committees . leadership ( ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( leadership , url ) \n class LateContributionTest ( APITest ) : \n def test_latest ( self ) : \n late_contribs = self . finance . late_contribs . latest ( ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( late_contribs , url ) \n def test_date ( self ) : \n late_contribs = self . finance . late_contribs . date ( <NUM_LIT> , <NUM_LIT:3> , <NUM_LIT> ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( late_contribs , url ) \n class PresidentTest ( APITest ) : \n def test_candidates ( self ) : \n candidates = self . finance . president . candidates ( ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( candidates , url ) \n def test_detail_using_id ( self ) : \n candidate = self . finance . president . detail ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( candidate , url , parse = lambda r : r [ '<STR_LIT>' ] [ <NUM_LIT:0> ] ) \n def test_detail_using_name ( self ) : \n candidate = self . finance . president . detail ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( candidate , url , parse = lambda r : r [ '<STR_LIT>' ] [ <NUM_LIT:0> ] ) \n def test_state_total ( self ) : \n state = self . finance . president . state ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( state , url ) \n def test_zip_total ( self ) : \n zipcode = self . finance . president . zipcode ( \"<STR_LIT>\" ) \n url = \"<STR_LIT>\" % API_KEY \n self . check_response ( zipcode , url ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n from O365 . attachment import Attachment \n from O365 . contact import Contact \n from O365 . group import Group \n import logging \n import json \n import requests \n logging . basicConfig ( filename = '<STR_LIT>' , level = logging . DEBUG ) \n log = logging . getLogger ( __name__ ) \n class Message ( object ) : \n '''<STR_LIT>''' \n att_url = '<STR_LIT>' \n send_url = '<STR_LIT>' \n draft_url = '<STR_LIT>' \n update_url = '<STR_LIT>' \n def __init__ ( self , json = None , auth = None ) : \n '''<STR_LIT>''' \n if json : \n self . json = json \n self . hasAttachments = json [ '<STR_LIT>' ] \n else : \n self . json = { '<STR_LIT>' : { '<STR_LIT>' : { } } , '<STR_LIT>' : { } } \n self . hasAttachments = False \n self . auth = auth \n self . attachments = [ ] \n self . reciever = None \n def fetchAttachments ( self ) : \n '''<STR_LIT>''' \n if not self . hasAttachments : \n log . debug ( '<STR_LIT>' ) \n return False \n response = requests . get ( self . att_url . format ( self . json [ '<STR_LIT>' ] ) , auth = self . auth ) \n log . info ( '<STR_LIT>' , str ( response ) ) \n json = response . json ( ) \n for att in json [ '<STR_LIT:value>' ] : \n try : \n self . attachments . append ( Attachment ( att ) ) \n log . debug ( '<STR_LIT>' , self . auth [ <NUM_LIT:0> ] ) \n except Exception as e : \n log . info ( '<STR_LIT>' , self . auth [ <NUM_LIT:0> ] ) \n return len ( self . attachments ) \n def sendMessage ( self ) : \n '''<STR_LIT>''' \n headers = { '<STR_LIT>' : '<STR_LIT:application/json>' , '<STR_LIT>' : '<STR_LIT>' } \n try : \n data = { '<STR_LIT>' : { '<STR_LIT>' : { } } } \n data [ '<STR_LIT>' ] [ '<STR_LIT>' ] = self . json [ '<STR_LIT>' ] \n data [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] = self . json [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n data [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] = self . json [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n data [ '<STR_LIT>' ] [ '<STR_LIT>' ] = self . json [ '<STR_LIT>' ] \n data [ '<STR_LIT>' ] [ '<STR_LIT>' ] = [ att . json for att in self . attachments ] \n data [ '<STR_LIT>' ] = \"<STR_LIT:false>\" \n data = json . dumps ( data ) \n log . debug ( str ( data ) ) \n except Exception as e : \n log . error ( str ( e ) ) \n return False \n response = requests . post ( self . send_url , data , headers = headers , auth = self . auth ) \n log . debug ( '<STR_LIT>' + str ( response ) ) \n if response . status_code != <NUM_LIT> : \n return False \n return True \n def markAsRead ( self ) : \n '''<STR_LIT>''' \n read = '<STR_LIT>' \n headers = { '<STR_LIT>' : '<STR_LIT:application/json>' , '<STR_LIT>' : '<STR_LIT:application/json>' } \n try : \n response = requests . patch ( self . update_url . format ( self . json [ '<STR_LIT>' ] ) , read , headers = headers , auth = self . auth ) \n except : \n return False \n return True \n def getSender ( self ) : \n '''<STR_LIT>''' \n return self . json [ '<STR_LIT>' ] \n def getSenderEmail ( self ) : \n '''<STR_LIT>''' \n return self . json [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n def getSenderName ( self ) : \n '''<STR_LIT>''' \n try : \n return self . json [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT:Name>' ] \n except : \n return '<STR_LIT>' \n def getSubject ( self ) : \n '''<STR_LIT>''' \n return self . json [ '<STR_LIT>' ] \n def getBody ( self ) : \n '''<STR_LIT>''' \n return self . json [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n def setRecipients ( self , val ) : \n '''<STR_LIT>''' \n self . json [ '<STR_LIT>' ] = [ ] \n if isinstance ( val , list ) : \n for con in val : \n if isinstance ( con , Contact ) : \n self . addRecipient ( con ) \n elif isinstance ( con , str ) : \n if '<STR_LIT:@>' in con : \n self . addRecipient ( con ) \n elif isinstance ( con , dict ) : \n self . json [ '<STR_LIT>' ] . append ( con ) \n elif isinstance ( val , dict ) : \n self . json [ '<STR_LIT>' ] = [ val ] \n elif isinstance ( val , str ) : \n if '<STR_LIT:@>' in val : \n self . addRecipient ( val ) \n elif isinstance ( val , Contact ) : \n self . addRecipient ( val ) \n elif isinstance ( val , Group ) : \n for person in val : \n self . addRecipient ( person ) \n else : \n return False \n return True \n def addRecipient ( self , address , name = None ) : \n '''<STR_LIT>''' \n if isinstance ( address , Contact ) : \n self . json [ '<STR_LIT>' ] . append ( address . getFirstEmailAddress ( ) ) \n elif isinstance ( address , Group ) : \n for con in address . contacts : \n self . json [ '<STR_LIT>' ] . append ( address . getFirstEmailAddress ( ) ) \n else : \n if name is None : \n name = address [ : address . index ( '<STR_LIT:@>' ) ] \n self . json [ '<STR_LIT>' ] . append ( { '<STR_LIT>' : { '<STR_LIT>' : address , '<STR_LIT:Name>' : name } } ) \n def setSubject ( self , val ) : \n '''<STR_LIT>''' \n self . json [ '<STR_LIT>' ] = val \n def setBody ( self , val ) : \n '''<STR_LIT>''' \n cont = False \n while not cont : \n try : \n self . json [ '<STR_LIT>' ] [ '<STR_LIT>' ] = val \n self . json [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' \n cont = True \n except : \n self . json [ '<STR_LIT>' ] = { } \n def setBodyHTML ( self , val = None ) : \n '''<STR_LIT>''' \n self . json [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' \n if val : \n self . json [ '<STR_LIT>' ] [ '<STR_LIT>' ] = <mask0> \n", "gt": "val"}
{"input": "\n from . main import <mask0> \n", "gt": "Dora"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import eventlet \n from eventlet import wsgi \n from eventlet . green import time \n from urlparse import parse_qs \n from random import random , choice \n datas = [ '<STR_LIT:hello>' , '<STR_LIT>' ] \n comparators = [ '<STR_LIT:<>' , '<STR_LIT:=>' , '<STR_LIT:>>' , '<STR_LIT:false>' ] \n def parse_response ( env , start_response ) : \n '''<STR_LIT>''' \n delay = random ( ) \n time . sleep ( delay / <NUM_LIT:10> ) \n try : \n params = parse_qs ( env [ '<STR_LIT>' ] ) \n row_index = int ( params [ '<STR_LIT>' ] [ <NUM_LIT:0> ] ) \n char_index = int ( params [ '<STR_LIT>' ] [ <NUM_LIT:0> ] ) - <NUM_LIT:1> \n test_char = int ( params [ '<STR_LIT>' ] [ <NUM_LIT:0> ] ) \n comparator = comparators . index ( params [ '<STR_LIT>' ] [ <NUM_LIT:0> ] ) - <NUM_LIT:1> \n try : \n sleep_int = float ( params [ '<STR_LIT>' ] . pop ( <NUM_LIT:0> ) ) \n except KeyError : \n sleep_int = <NUM_LIT:1> \n current_character = datas [ row_index ] [ char_index ] \n truth = ( cmp ( ord ( current_character ) , test_char ) == comparator ) \n response = types [ env [ '<STR_LIT>' ] ] ( test_char , current_character , comparator , sleep_int , start_response , truth ) \n return response \n except : \n start_response ( '<STR_LIT>' , [ ( '<STR_LIT:Content-Type>' , '<STR_LIT>' ) ] ) \n return [ '<STR_LIT>' ] \n def time_based_blind ( test_char , current_character , comparator , sleep_int , start_response , truth ) : \n sleep_time = sleep_int * truth \n time . sleep ( sleep_time ) \n start_response ( '<STR_LIT>' , [ ( '<STR_LIT:Content-Type>' , '<STR_LIT>' ) ] ) \n return [ '<STR_LIT>' ] \n def boolean_based_error ( test_char , current_character , comparator , env , start_response , truth ) : \n if truth : \n start_response ( '<STR_LIT>' , [ ( '<STR_LIT:Content-Type>' , '<STR_LIT>' ) ] ) \n return [ '<STR_LIT>' ] \n else : \n start_response ( '<STR_LIT>' , [ ( '<STR_LIT:Content-Type>' , '<STR_LIT>' ) ] ) \n return [ '<STR_LIT>' ] \n def boolean_based_size ( test_char , current_character , comparator , env , start_response , truth ) : \n if truth : \n start_response ( '<STR_LIT>' , [ ( '<STR_LIT:Content-Type>' , '<STR_LIT>' ) ] ) \n return [ '<STR_LIT>' ] \n else : \n start_response ( '<STR_LIT>' , [ ( '<STR_LIT:Content-Type>' , '<STR_LIT>' ) ] ) \n return [ '<STR_LIT>' ] \n types = { '<STR_LIT>' : time_based_blind , '<STR_LIT>' : boolean_based_error , '<STR_LIT>' : boolean_based_size } \n if __name__ == \"<STR_LIT:__main__>\" : \n print \"<STR_LIT:\\n>\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n print \"<STR_LIT>\" \n print \"<STR_LIT:\\n>\" \n from sys import argv \n import re \n CHARSET = [ chr ( x ) for x in xrange ( <NUM_LIT:32> , <NUM_LIT> ) ] \n rre = re . compile ( u'<STR_LIT>' ) \n cre = re . compile ( u'<STR_LIT>' ) \n rows = filter ( rre . match , argv ) \n cols = filter ( cre . match , argv ) \n if rows and cols : \n rows = rows [ <NUM_LIT:0> ] \n cols = cols [ <NUM_LIT:0> ] \n CHARSET = [ chr ( x ) for x in xrange ( <NUM_LIT:32> , <NUM_LIT> ) ] \n datas = [ ] \n for asdf in range ( <NUM_LIT:5> ) : \n datas . append ( \"<STR_LIT>\" ) \n for fdsa in range ( <NUM_LIT:100> ) : \n datas [ - <NUM_LIT:1> ] += choice ( CHARSET ) \n wsgi . server ( eventlet . listen ( ( '<STR_LIT>' , <NUM_LIT> ) ) , <mask0> ) \n", "gt": "parse_response"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import xml . etree . ElementTree as ET \n import os \n import cPickle \n import numpy as np \n def parse_rec ( filename ) : \n \"\"\"<STR_LIT>\"\"\" \n tree = ET . parse ( filename ) \n objects = [ ] \n for obj in tree . findall ( '<STR_LIT:object>' ) : \n obj_struct = { } \n obj_struct [ '<STR_LIT:name>' ] = obj . find ( '<STR_LIT:name>' ) . text \n obj_struct [ '<STR_LIT>' ] = obj . find ( '<STR_LIT>' ) . text \n obj_struct [ '<STR_LIT>' ] = int ( obj . find ( '<STR_LIT>' ) . text ) \n obj_struct [ '<STR_LIT>' ] = int ( obj . find ( '<STR_LIT>' ) . text ) \n bbox = obj . find ( '<STR_LIT>' ) \n obj_struct [ '<STR_LIT>' ] = [ int ( bbox . find ( '<STR_LIT>' ) . text ) , \n int ( bbox . find ( '<STR_LIT>' ) . text ) , \n int ( bbox . find ( '<STR_LIT>' ) . text ) , \n int ( bbox . find ( '<STR_LIT>' ) . text ) ] \n objects . append ( obj_struct ) \n return objects \n def voc_ap ( rec , prec , use_07_metric = False ) : \n \"\"\"<STR_LIT>\"\"\" \n if use_07_metric : \n ap = <NUM_LIT:0.> \n for t in np . arange ( <NUM_LIT:0.> , <NUM_LIT> , <NUM_LIT:0.1> ) : \n if np . sum ( rec >= t ) == <NUM_LIT:0> : \n p = <NUM_LIT:0> \n else : \n p = np . max ( prec [ rec >= t ] ) \n ap = ap + p / <NUM_LIT> \n else : \n mrec = np . concatenate ( ( [ <NUM_LIT:0.> ] , rec , [ <NUM_LIT:1.> ] ) ) \n mpre = np . concatenate ( ( [ <NUM_LIT:0.> ] , prec , [ <NUM_LIT:0.> ] ) ) \n for i in range ( mpre . size - <NUM_LIT:1> , <NUM_LIT:0> , - <NUM_LIT:1> ) : \n mpre [ i - <NUM_LIT:1> ] = np . maximum ( mpre [ i - <NUM_LIT:1> ] , mpre [ i ] ) \n i = np . where ( mrec [ <NUM_LIT:1> : ] != mrec [ : - <NUM_LIT:1> ] ) [ <NUM_LIT:0> ] \n ap = np . sum ( ( mrec [ i + <NUM_LIT:1> ] - mrec [ i ] ) * mpre [ i + <NUM_LIT:1> ] ) \n return ap \n def voc_eval ( detpath , \n annopath , \n imagesetfile , \n classname , \n cachedir , \n ovthresh = <NUM_LIT:0.5> , \n use_07_metric = False ) : \n \"\"\"<STR_LIT>\"\"\" \n if not os . path . isdir ( cachedir ) : \n os . mkdir ( cachedir ) \n cachefile = os . path . join ( cachedir , '<STR_LIT>' ) \n with open ( imagesetfile , '<STR_LIT:r>' ) as f : \n lines = f . readlines ( ) \n imagenames = [ x . strip ( ) for x in lines ] \n if not os . path . isfile ( cachefile ) : \n recs = { } \n for i , imagename in enumerate ( imagenames ) : \n recs [ imagename ] = parse_rec ( annopath . format ( imagename ) ) \n if i % <NUM_LIT:100> == <NUM_LIT:0> : \n print '<STR_LIT>' . format ( \n i + <NUM_LIT:1> , len ( imagenames ) ) \n print '<STR_LIT>' . format ( cachefile ) \n with open ( cachefile , '<STR_LIT:w>' ) as f : \n cPickle . dump ( recs , f ) \n else : \n with open ( cachefile , '<STR_LIT:r>' ) as f : \n recs = cPickle . load ( f ) \n class_recs = { } \n npos = <NUM_LIT:0> \n for imagename in imagenames : \n R = [ obj for obj in recs [ imagename ] if obj [ '<STR_LIT:name>' ] == classname ] \n bbox = np . array ( [ x [ '<STR_LIT>' ] for x in R ] ) \n difficult = np . array ( [ x [ '<STR_LIT>' ] for x in R ] ) . astype ( np . bool ) \n det = [ False ] * len ( R ) \n npos = npos + sum ( ~ difficult ) \n class_recs [ imagename ] = { '<STR_LIT>' : bbox , \n '<STR_LIT>' : difficult , \n '<STR_LIT>' : det } \n detfile = detpath . format ( classname ) \n with open ( detfile , '<STR_LIT:r>' ) as f : \n lines = f . readlines ( ) \n splitlines = [ x . strip ( ) . split ( '<STR_LIT:U+0020>' ) for x in lines ] \n image_ids = [ x [ <NUM_LIT:0> ] for x in splitlines ] \n confidence = np . array ( [ float ( x [ <NUM_LIT:1> ] ) for x in splitlines ] ) \n BB = np . array ( [ [ float ( z ) for z in x [ <NUM_LIT:2> : ] ] for x in splitlines ] ) \n sorted_ind = np . argsort ( - confidence ) \n BB = BB [ sorted_ind , : ] \n image_ids = [ image_ids [ x ] for x in sorted_ind ] \n nd = len ( image_ids ) \n tp = np . zeros ( nd ) \n fp = np . zeros ( nd ) \n for d in range ( nd ) : \n R = class_recs [ image_ids [ d ] ] \n bb = BB [ d , : ] . astype ( float ) \n ovmax = - np . inf \n BBGT = R [ '<STR_LIT>' ] . astype ( float ) \n if BBGT . size > <NUM_LIT:0> : \n ixmin = np . maximum ( BBGT [ : , <NUM_LIT:0> ] , bb [ <NUM_LIT:0> ] ) \n iymin = np . maximum ( BBGT [ : , <NUM_LIT:1> ] , bb [ <NUM_LIT:1> ] ) \n ixmax = np . minimum ( BBGT [ : , <NUM_LIT:2> ] , bb [ <NUM_LIT:2> ] ) \n iymax = np . minimum ( BBGT [ : , <NUM_LIT:3> ] , bb [ <NUM_LIT:3> ] ) \n iw = np . maximum ( ixmax - ixmin + <NUM_LIT:1.> , <NUM_LIT:0.> ) \n ih = np . maximum ( iymax - iymin + <NUM_LIT:1.> , <NUM_LIT:0.> ) \n inters = iw * ih \n uni = ( ( bb [ <NUM_LIT:2> ] - bb [ <NUM_LIT:0> ] + <NUM_LIT:1.> ) * ( bb [ <NUM_LIT:3> ] - bb [ <NUM_LIT:1> ] + <NUM_LIT:1.> ) + \n ( BBGT [ : , <NUM_LIT:2> ] - BBGT [ : , <NUM_LIT:0> ] + <NUM_LIT:1.> ) * \n ( BBGT [ : , <NUM_LIT:3> ] - BBGT [ : , <NUM_LIT:1> ] + <NUM_LIT:1.> ) - inters ) \n overlaps = inters / uni \n ovmax = np . max ( overlaps ) \n jmax = np . argmax ( overlaps ) \n if ovmax > ovthresh : \n if not R [ '<STR_LIT>' ] [ jmax ] : \n if not R [ '<STR_LIT>' ] [ jmax ] : \n tp [ d ] = <NUM_LIT:1.> \n R [ '<STR_LIT>' ] [ jmax ] = <NUM_LIT:1> \n else : \n fp [ d ] = <NUM_LIT:1.> \n else : \n fp [ d ] = <NUM_LIT:1.> \n fp = np . cumsum ( fp ) \n tp = np . cumsum ( tp ) \n rec = tp / float ( npos ) \n prec = tp / ( tp + fp + <NUM_LIT> ) \n ap = voc_ap ( rec , prec , use_07_metric ) \n return rec , prec , <mask0> \n", "gt": "ap"}
{"input": "\n import numpy as np \n from ipdb import set_trace \n from struct import pack , unpack \n def ceil_div ( x , y ) : \n return - ( - x // y ) \n def out_dim ( S , X , padding , strides ) : \n return ceil_div ( X - S + <NUM_LIT:1> + <NUM_LIT:2> * padding , strides ) \n def strip_mantissa ( val ) : \n i = unpack ( '<STR_LIT:I>' , pack ( '<STR_LIT:f>' , val ) ) [ <NUM_LIT:0> ] & <NUM_LIT> \n f = unpack ( '<STR_LIT:f>' , pack ( '<STR_LIT:I>' , i ) ) [ <NUM_LIT:0> ] \n return f \n def quantize ( ary , bits , sign = <NUM_LIT:1> ) : \n maxval = float ( np . max ( np . absolute ( ary ) ) ) \n scale = strip_mantissa ( maxval ) / float ( <NUM_LIT:1> << ( bits - sign - <NUM_LIT:1> ) ) \n ary = np . around ( ary * ( <NUM_LIT:1.0> / scale ) ) . astype ( np . int64 ) \n return ary , np . float64 ( scale ) \n def fconv_slice ( q , S , X , padding , strides ) : \n f1 = <NUM_LIT:0> \n f2 = S - <NUM_LIT:1> \n x1 = q * strides - padding \n x2 = x1 + f2 \n if x1 < <NUM_LIT:0> : \n f1 = - x1 \n x1 = <NUM_LIT:0> \n if x2 >= X : \n dif = x2 - X + <NUM_LIT:1> \n f2 -= dif \n x2 -= dif \n return ( slice ( f1 , f2 + <NUM_LIT:1> ) , slice ( x1 , x2 + <NUM_LIT:1> ) , f2 - f1 + <NUM_LIT:1> ) \n def bconv_slice ( x , S , Q , padding , strides ) : \n qs = x - ( S - padding - <NUM_LIT:1> ) \n firstF = None \n for s in range ( S ) : \n q = qs + s \n if q % strides == <NUM_LIT:0> : \n q //= strides \n if q >= <NUM_LIT:0> and q < Q : \n if firstF is None : \n firstF = s \n firstE = q \n lastF = s \n lastE = q \n return ( slice ( firstF , lastF + <NUM_LIT:1> , strides ) , slice ( firstE , lastE + <NUM_LIT:1> , strides ) , <NUM_LIT:0> ) \n def xprop_direct ( I , F , O , padding , strides , backward = False ) : \n if all ( x == <NUM_LIT:1> for x in F . shape [ <NUM_LIT:1> : <NUM_LIT:3> ] ) : \n C = F . shape [ <NUM_LIT:0> ] \n K = F . shape [ <NUM_LIT:4> ] \n if backward : \n O [ : ] = np . dot ( F . reshape ( ( C , - <NUM_LIT:1> ) ) , I . reshape ( ( K , - <NUM_LIT:1> ) ) ) . reshape ( ( O . shape ) ) \n else : \n O [ : ] = np . dot ( F . reshape ( ( C , - <NUM_LIT:1> ) ) . T , I . reshape ( ( C , - <NUM_LIT:1> ) ) ) . reshape ( ( O . shape ) ) \n return \n if backward : \n F = np . transpose ( F [ : , : : - <NUM_LIT:1> , : : - <NUM_LIT:1> , : ] , ( <NUM_LIT:3> , <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:0> ) ) . copy ( ) \n xconv_slice = bconv_slice \n else : \n xconv_slice = fconv_slice \n C , Y , X , N = I . shape \n C , R , S , K = F . shape \n K , P , Q , N = O . shape \n qSlice = [ xconv_slice ( q , S , X , padding [ <NUM_LIT:0> ] , strides [ <NUM_LIT:0> ] ) for q in range ( Q ) ] \n for p in range ( P ) : \n sliceR , sliceY , _ = xconv_slice ( p , R , Y , padding [ <NUM_LIT:1> ] , strides [ <NUM_LIT:1> ] ) \n for q in range ( Q ) : \n sliceS , sliceX , _ = qSlice [ q ] \n slicedF = F [ : , sliceR , sliceS , : ] . reshape ( ( - <NUM_LIT:1> , K ) ) \n slicedI = I [ : , sliceY , sliceX , : ] . reshape ( ( - <NUM_LIT:1> , N ) ) \n O [ : , p , q , : ] = np . dot ( slicedF . T , slicedI ) \n def updat_direct ( I , E , U , padding , strides ) : \n C , Y , X , N = I . shape \n K , P , Q , N = E . shape \n C , R , S , K = U . shape \n if all ( x == <NUM_LIT:1> for x in ( R , S ) ) : \n U [ : ] = np . dot ( I . reshape ( ( C , - <NUM_LIT:1> ) ) , E . reshape ( ( K , - <NUM_LIT:1> ) ) . T ) . reshape ( ( U . shape ) ) \n return \n U . fill ( <NUM_LIT:0.0> ) \n qSlice = [ fconv_slice ( q , S , X , padding [ <NUM_LIT:0> ] , strides [ <NUM_LIT:0> ] ) for q in range ( Q ) ] \n for p in range ( P ) : \n sliceR , sliceY , rlen = fconv_slice ( p , R , Y , padding [ <NUM_LIT:1> ] , strides [ <NUM_LIT:1> ] ) \n for q in range ( Q ) : \n sliceS , sliceX , slen = qSlice [ q ] \n slicedI = I [ : , sliceY , sliceX , : ] . reshape ( ( - <NUM_LIT:1> , N ) ) \n slicedE = E [ : , p , q , : ] \n U [ : , sliceR , sliceS , : ] += np . dot ( slicedI , slicedE . T ) . reshape ( ( C , rlen , slen , K ) ) \n I_4x4_3x3 = ( \n np . array ( [ \n [ <NUM_LIT> , <NUM_LIT:0.0> , - <NUM_LIT> , <NUM_LIT:0.0> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , - <NUM_LIT> , - <NUM_LIT> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , - <NUM_LIT> , - <NUM_LIT:1.0> , <NUM_LIT> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT> , - <NUM_LIT:1.0> , - <NUM_LIT> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT> , <NUM_LIT:0.0> , - <NUM_LIT> , <NUM_LIT:0.0> , <NUM_LIT:1.0> ] ] ) , \n np . array ( [ \n [ <NUM_LIT> , <NUM_LIT:0.0> , - <NUM_LIT> , <NUM_LIT:0.0> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , - <NUM_LIT> , - <NUM_LIT> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , - <NUM_LIT> , - <NUM_LIT:1.0> , <NUM_LIT> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT> , - <NUM_LIT:1.0> , - <NUM_LIT> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT> , <NUM_LIT:0.0> , - <NUM_LIT> , <NUM_LIT:0.0> , <NUM_LIT:1.0> ] ] ) , \n np . array ( [ \n [ <NUM_LIT:1.0> , <NUM_LIT:0.0> , - <NUM_LIT> / <NUM_LIT> , <NUM_LIT:0.0> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT> / <NUM_LIT> , <NUM_LIT> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , - <NUM_LIT> / <NUM_LIT> , <NUM_LIT> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , - <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT> , <NUM_LIT:0.0> , - <NUM_LIT> , <NUM_LIT:0.0> , <NUM_LIT:1.0> ] ] ) , \n ) \n F_4x4_3x3 = ( \n np . array ( [ \n [ <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:0.0> , <NUM_LIT:0.0> ] , \n [ - <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> ] , \n [ - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> ] , \n [ <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> ] , \n [ <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT:0.0> , <NUM_LIT:1.0> ] ] ) , \n np . array ( [ \n [ <NUM_LIT:1.0> , <NUM_LIT:0.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> ] , \n [ <NUM_LIT:1.0> , - <NUM_LIT:1.0> , <NUM_LIT:1.0> ] , \n [ <NUM_LIT:1.0> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT:1.0> , - <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT:0.0> , <NUM_LIT:1.0> ] ] ) , \n np . array ( [ \n [ <NUM_LIT:1.0> , <NUM_LIT:0.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> ] , \n [ <NUM_LIT:1.0> , - <NUM_LIT:1.0> , <NUM_LIT:1.0> ] , \n [ <NUM_LIT:1.0> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT:1.0> , - <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT:0.0> , <NUM_LIT:1.0> ] ] ) , \n ) \n O_4x4_3x3 = ( \n np . array ( [ \n [ <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT:1.0> , - <NUM_LIT:1.0> , <NUM_LIT> , - <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT:1.0> , - <NUM_LIT:1.0> , <NUM_LIT> , - <NUM_LIT> , <NUM_LIT:1.0> ] ] ) , \n np . array ( [ \n [ <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , - <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> / <NUM_LIT> , - <NUM_LIT:1.0> / <NUM_LIT> , <NUM_LIT:1.0> ] ] ) , \n np . array ( [ \n [ <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT:1.0> , - <NUM_LIT:1.0> , <NUM_LIT> , - <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT:1.0> , <NUM_LIT:1.0> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT:0.0> ] , \n [ <NUM_LIT:0.0> , <NUM_LIT:1.0> , - <NUM_LIT:1.0> , <NUM_LIT> , - <NUM_LIT> , <NUM_LIT:1.0> ] ] ) , \n ) \n rcp3 = <NUM_LIT:1.0> / <NUM_LIT> \n rcp4 = <NUM_LIT:1.0> / <NUM_LIT> \n rcp6 = <NUM_LIT:1.0> / <NUM_LIT> \n rcp12 = <NUM_LIT:1.0> / <NUM_LIT> \n rcp24 = <NUM_LIT:1.0> / <NUM_LIT> \n def trans_I_4x4_3x3 ( Iw , I , minimal = False , trans = False ) : \n if minimal : \n T0 = np . empty ( ( <NUM_LIT:6> , <NUM_LIT:6> ) ) \n T1 = np . empty ( ( <NUM_LIT:6> , <NUM_LIT:6> ) ) \n for O , I in ( ( T0 , I ) , ( T1 , T0 . T ) ) : \n t0 = ( I [ <NUM_LIT:2> , : ] * <NUM_LIT> - I [ <NUM_LIT:4> , : ] ) * rcp6 \n t1 = ( I [ <NUM_LIT:1> , : ] * <NUM_LIT> - I [ <NUM_LIT:3> , : ] ) * rcp6 \n t2 = ( I [ <NUM_LIT:4> , : ] - I [ <NUM_LIT:2> , : ] ) * rcp24 \n t3 = ( I [ <NUM_LIT:3> , : ] - I [ <NUM_LIT:1> , : ] ) * rcp12 \n O [ <NUM_LIT:0> , : ] = I [ <NUM_LIT:0> , : ] + ( I [ <NUM_LIT:2> , : ] * - <NUM_LIT> + I [ <NUM_LIT:4> , : ] ) * rcp4 \n O [ <NUM_LIT:1> , : ] = t0 + t1 \n O [ <NUM_LIT:2> , : ] = t0 - t1 \n O [ <NUM_LIT:3> , : ] = t2 + t3 \n O [ <NUM_LIT:4> , : ] = t2 - t3 \n O [ <NUM_LIT:5> , : ] = I [ <NUM_LIT:1> , : ] * <NUM_LIT> - I [ <NUM_LIT:3> , : ] * <NUM_LIT> + I [ <NUM_LIT:5> , : ] \n Iw [ : ] = T1 . T \n else : \n Iw [ : ] = np . dot ( np . dot ( I_4x4_3x3 [ trans [ <NUM_LIT:0> ] ] , I ) , I_4x4_3x3 [ trans [ <NUM_LIT:1> ] ] . T ) \n def trans_F_4x4_3x3 ( Fw , F , minimal = False , trans = False ) : \n if minimal : \n T0 = np . empty ( ( <NUM_LIT:6> , <NUM_LIT:3> ) ) \n T1 = np . empty ( ( <NUM_LIT:6> , <NUM_LIT:6> ) ) \n for O , I in ( ( T0 , F ) , ( T1 , T0 . T ) ) : \n t0 = I [ <NUM_LIT:0> , : ] + I [ <NUM_LIT:2> , : ] \n t1 = I [ <NUM_LIT:0> , : ] + I [ <NUM_LIT:2> , : ] * <NUM_LIT> \n O [ <NUM_LIT:0> , : ] = I [ <NUM_LIT:0> , : ] \n O [ <NUM_LIT:1> , : ] = t0 + I [ <NUM_LIT:1> , : ] \n O [ <NUM_LIT:2> , : ] = t0 - I [ <NUM_LIT:1> , : ] \n O [ <NUM_LIT:3> , : ] = t1 + I [ <NUM_LIT:1> , : ] * <NUM_LIT> \n O [ <NUM_LIT:4> , : ] = t1 - I [ <NUM_LIT:1> , : ] * <NUM_LIT> \n O [ <NUM_LIT:5> , : ] = I [ <NUM_LIT:2> , : ] \n Fw [ : ] = T1 . T \n else : \n Fw [ : ] = np . dot ( np . dot ( F_4x4_3x3 [ trans [ <NUM_LIT:0> ] ] , F ) , F_4x4_3x3 [ trans [ <NUM_LIT:1> ] ] . T ) \n def trans_O_4x4_3x3 ( Mw , minimal = False , trans = False ) : \n if minimal : \n T0 = np . empty ( ( <NUM_LIT:4> , <NUM_LIT:6> ) ) \n T1 = np . empty ( ( <NUM_LIT:4> , <NUM_LIT:4> ) ) \n for O , I in ( ( T0 , Mw ) , ( T1 , T0 . T ) ) : \n t0 = I [ <NUM_LIT:1> , : ] + I [ <NUM_LIT:2> , : ] \n t1 = I [ <NUM_LIT:3> , : ] + I [ <NUM_LIT:4> , : ] \n t2 = I [ <NUM_LIT:1> , : ] - I [ <NUM_LIT:2> , : ] \n t3 = I [ <NUM_LIT:3> , : ] - I [ <NUM_LIT:4> , : ] \n O [ <NUM_LIT:0> , : ] = t0 + t1 + I [ <NUM_LIT:0> , : ] \n O [ <NUM_LIT:1> , : ] = t2 + t3 * <NUM_LIT> \n O [ <NUM_LIT:2> , : ] = t0 + t1 * <NUM_LIT> \n O [ <NUM_LIT:3> , : ] = t2 + t3 * <NUM_LIT> + I [ <NUM_LIT:5> , : ] \n return T1 . T \n else : \n return np . dot ( np . dot ( O_4x4_3x3 [ trans [ <NUM_LIT:0> ] ] , Mw ) , O_4x4_3x3 [ trans [ <NUM_LIT:1> ] ] . T ) \n def trans_F_3x3_4x4 ( Fw , F , minimal = False , trans = False ) : \n if minimal : \n T0 = np . empty ( ( <NUM_LIT:6> , <NUM_LIT:4> ) ) \n T1 = np . empty ( ( <NUM_LIT:6> , <NUM_LIT:6> ) ) \n for O , I in ( ( T0 , F ) , ( T1 , T0 . T ) ) : \n t0 = I [ <NUM_LIT:0> , : ] + I [ <NUM_LIT:2> , : ] \n t1 = I [ <NUM_LIT:0> , : ] + I [ <NUM_LIT:2> , : ] * <NUM_LIT> \n t2 = I [ <NUM_LIT:1> , : ] + I [ <NUM_LIT:3> , : ] \n t3 = I [ <NUM_LIT:1> , : ] + I [ <NUM_LIT:3> , : ] * <NUM_LIT> \n O [ <NUM_LIT:0> , : ] = I [ <NUM_LIT:0> , : ] \n O [ <NUM_LIT:1> , : ] = t0 + t2 \n O [ <NUM_LIT:2> , : ] = t0 - t2 \n O [ <NUM_LIT:3> , : ] = t1 + t3 * <NUM_LIT> \n O [ <NUM_LIT:4> , : ] = t1 - t3 * <NUM_LIT> \n O [ <NUM_LIT:5> , : ] = I [ <NUM_LIT:3> , : ] \n Fw [ : ] = T1 . T \n else : \n Fw [ : ] = np . dot ( np . dot ( O_4x4_3x3 [ trans [ <NUM_LIT:0> ] ] . T , F ) , O_4x4_3x3 [ trans [ <NUM_LIT:1> ] ] ) \n def trans_O_3x3_4x4 ( Mw , minimal = False , trans = False ) : \n if minimal : \n T0 = np . empty ( ( <NUM_LIT:3> , <NUM_LIT:6> ) ) \n T1 = np . empty ( ( <NUM_LIT:3> , <NUM_LIT:3> ) ) \n for O , I in ( ( T0 , Mw ) , ( T1 , T0 . T ) ) : \n t0 = I [ <NUM_LIT:1> , : ] + I [ <NUM_LIT:2> , : ] \n t1 = I [ <NUM_LIT:3> , : ] + I [ <NUM_LIT:4> , : ] \n O [ <NUM_LIT:0> , : ] = I [ <NUM_LIT:0> , : ] + t0 + t1 \n O [ <NUM_LIT:1> , : ] = I [ <NUM_LIT:1> , : ] - I [ <NUM_LIT:2> , : ] + <NUM_LIT:2> * ( I [ <NUM_LIT:3> , : ] - I [ <NUM_LIT:4> , : ] ) \n O [ <NUM_LIT:2> , : ] = t0 + <NUM_LIT:4> * t1 + I [ <NUM_LIT:5> , : ] \n return T1 . T \n else : \n return np . dot ( np . dot ( F_4x4_3x3 [ trans [ <NUM_LIT:0> ] ] . T , Mw ) , F_4x4_3x3 [ trans [ <NUM_LIT:1> ] ] ) \n def image_slice ( x , X , B , D , pad = <NUM_LIT:0> ) : \n start = x * B - pad \n stop = start + D \n pad = [ <NUM_LIT:0> , <NUM_LIT:0> ] \n if start < <NUM_LIT:0> : \n pad [ <NUM_LIT:0> ] = - start \n start = <NUM_LIT:0> \n if stop - <NUM_LIT:1> >= X : \n pad [ <NUM_LIT:1> ] = stop - X \n return start , stop , pad \n def output_slice ( p , P , B ) : \n p0 = p * B \n p1 = p0 + B \n if p1 > P : \n p1 = P \n return p0 , p1 , p1 - p0 \n def xprop_winograd ( I , F , O , padding , minimal = False , trans = False , backward = False ) : \n if backward : \n F = np . transpose ( F [ : , : : - <NUM_LIT:1> , : : - <NUM_LIT:1> , : ] , ( <NUM_LIT:3> , <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:0> ) ) . copy ( ) \n padding = [ <NUM_LIT:2> - p for p in padding ] \n C , Y , X , N = I . shape \n K , P , Q , N = O . shape \n B = <NUM_LIT:4> \n D = B + <NUM_LIT:2> \n Yw = ceil_div ( P , B ) \n Xw = ceil_div ( Q , B ) \n Fw = np . empty ( ( D , D , C , K ) ) \n Iw = np . empty ( ( D , D , C , Yw , Xw , N ) ) \n Mw = np . empty ( ( D , D , K , Yw , Xw , N ) ) \n for c in range ( C ) : \n for k in range ( K ) : \n trans_F_4x4_3x3 ( Fw [ : , : , c , k ] , F [ c , : , : , k ] , minimal , trans ) \n for y in range ( Yw ) : \n start_y , stop_y , pad_y = image_slice ( y , Y , B , D , padding [ <NUM_LIT:0> ] ) \n for x in range ( Xw ) : \n start_x , stop_x , pad_x = image_slice ( x , X , B , D , padding [ <NUM_LIT:1> ] ) \n sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n if any ( pad_y ) or any ( pad_x ) : \n sliceI = np . pad ( sliceI , ( ( <NUM_LIT:0> , <NUM_LIT:0> ) , pad_y , pad_x , ( <NUM_LIT:0> , <NUM_LIT:0> ) ) , '<STR_LIT>' ) \n for c in range ( C ) : \n for n in range ( N ) : \n trans_I_4x4_3x3 ( Iw [ : , : , c , y , x , n ] , sliceI [ c , : , : , n ] , minimal , trans ) \n for s in range ( D ) : \n for t in range ( D ) : \n Mw [ s , t ] = np . dot ( Fw [ s , t ] . T , Iw [ s , t ] . reshape ( C , - <NUM_LIT:1> ) ) . reshape ( ( K , Yw , Xw , N ) ) \n for y in range ( Yw ) : \n p0 , p1 , plen = output_slice ( y , P , B ) \n for x in range ( Xw ) : \n q0 , q1 , qlen = output_slice ( x , Q , B ) \n for k in range ( K ) : \n for n in range ( N ) : \n Out = trans_O_4x4_3x3 ( Mw [ : , : , k , y , x , n ] , minimal , trans ) \n O [ k , p0 : p1 , q0 : q1 , n ] = Out [ <NUM_LIT:0> : plen , <NUM_LIT:0> : qlen ] \n def updat_winograd ( I , E , U , padding , minimal = False , trans = False , inner = True ) : \n C , Y , X , N = I . shape \n K , P , Q , N = E . shape \n B = <NUM_LIT:4> \n D = B + <NUM_LIT:2> \n Yw = ceil_div ( P , B ) \n Xw = ceil_div ( Q , B ) \n Iw = np . empty ( ( D , D , N , C ) ) \n Ew = np . empty ( ( D , D , N , K ) ) \n if inner : \n Mw = np . empty ( ( D , D , C , K ) ) \n U . fill ( <NUM_LIT:0.0> ) \n else : \n Mw = np . zeros ( ( D , D , C , K ) ) \n for y in range ( Yw ) : \n start_y , stop_y , pad_y = image_slice ( y , Y , B , D , padding [ <NUM_LIT:0> ] ) \n start_p , stop_p , pad_p = image_slice ( y , P , B , B ) \n for x in range ( Xw ) : \n start_x , stop_x , pad_x = image_slice ( x , X , B , D , padding [ <NUM_LIT:1> ] ) \n start_q , stop_q , pad_q = image_slice ( x , Q , B , B ) \n sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n sliceE = E [ : , start_p : stop_p , start_q : stop_q , : ] \n if any ( pad_y ) or any ( pad_x ) : \n sliceI = np . pad ( sliceI , ( ( <NUM_LIT:0> , <NUM_LIT:0> ) , pad_y , pad_x , ( <NUM_LIT:0> , <NUM_LIT:0> ) ) , '<STR_LIT>' ) \n if any ( pad_p ) or any ( pad_q ) : \n sliceE = np . pad ( sliceE , ( ( <NUM_LIT:0> , <NUM_LIT:0> ) , pad_p , pad_q , ( <NUM_LIT:0> , <NUM_LIT:0> ) ) , '<STR_LIT>' ) \n for c in range ( C ) : \n for n in range ( N ) : \n trans_I_4x4_3x3 ( Iw [ : , : , n , c ] , sliceI [ c , : , : , n ] , minimal , trans ) \n for k in range ( K ) : \n for n in range ( N ) : \n trans_F_3x3_4x4 ( Ew [ : , : , n , k ] , sliceE [ k , : , : , n ] , minimal , trans ) \n for s in range ( D ) : \n for t in range ( D ) : \n if inner : \n Mw [ s , t ] = np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n else : \n Mw [ s , t ] += np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n if inner : \n for c in range ( C ) : \n for k in range ( K ) : \n U [ c , : , : , k ] += trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n if not inner : \n for c in range ( C ) : \n for k in range ( K ) : \n U [ c , : , : , k ] = trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n np . set_printoptions ( threshold = <NUM_LIT> * <NUM_LIT:4> , linewidth = <NUM_LIT> , formatter = { '<STR_LIT:float>' : lambda x : \"<STR_LIT>\" % x } ) \n minimal = <NUM_LIT:1> \n trans = ( <NUM_LIT:2> , <NUM_LIT:2> ) \n ones = <NUM_LIT:0> \n N = <NUM_LIT:32> \n C , K = <NUM_LIT:32> , <NUM_LIT:32> \n Y , X = <NUM_LIT:6> , <NUM_LIT:6> \n R , S = <NUM_LIT:3> , <NUM_LIT:3> \n strides = <NUM_LIT:1> , <NUM_LIT:1> \n padding = <NUM_LIT:0> , <NUM_LIT:0> \n P = out_dim ( R , Y , padding [ <NUM_LIT:0> ] , strides [ <NUM_LIT:0> ] ) \n Q = out_dim ( S , X , padding [ <NUM_LIT:1> ] , strides [ <NUM_LIT:1> ] ) \n print P , Q \n dimI = ( C , Y , X , N ) \n dimF = ( C , R , S , K ) \n dimO = ( K , P , Q , N ) \n if ones : \n I = np . zeros ( dimI ) \n F = np . ones ( dimF ) \n E = np . zeros ( dimO ) \n for p , q in np . ndindex ( ( Y , X ) ) : \n I [ : , p , q , : ] = np . identity ( N ) \n for p , q in np . ndindex ( ( P , Q ) ) : \n for n in range ( N ) : \n E [ : , p , q , n ] = range ( K ) \n else : \n I = np . random . uniform ( - <NUM_LIT:1.0> , <NUM_LIT:1.0> , dimI ) \n F = np . random . uniform ( - <NUM_LIT:1.0> , <NUM_LIT:1.0> , dimF ) \n E = np . random . uniform ( - <NUM_LIT:1.0> , <NUM_LIT:1.0> , dimO ) \n Od = np . empty ( dimO ) \n Ow = np . empty ( dimO ) \n Bd = np . empty ( dimI ) \n Bw = np . empty ( dimI ) \n Ud = np . empty ( dimF ) \n Uw = np . empty ( dimF ) \n xprop_direct ( I , F , Od , padding , strides ) \n xprop_winograd ( I , F , Ow , padding , minimal = minimal , trans = trans ) \n xprop_direct ( E , F , Bd , padding , strides , backward = True ) \n xprop_winograd ( E , F , Bw , padding , minimal = minimal , trans = trans , backward = True ) \n updat_direct ( I , E , Ud , padding , strides ) \n updat_winograd ( I , E , Uw , padding , minimal = minimal , trans = trans ) \n difO = Od - Ow \n difB = Bd - Bw \n difU = Ud - Uw \n print abs ( difO ) . max ( ) / Od . max ( ) \n print abs ( difB ) . max ( ) / Bd . max ( ) \n print abs ( difU ) . max ( ) / Ud . <mask0> ( ) \n", "gt": "max"}
{"input": "\n from neon . layers . layer import ( Linear , Bias , Affine , Conv , Convolution , GeneralizedCost , Dropout , \n Pooling , Activation , DataTransform , BatchNorm , BatchNormAutodiff , \n Deconv , Deconvolution , GeneralizedCostMask , LookupTable , \n BranchNode , SkipNode , LRN , ColorNoise ) \n from neon . layers . recurrent import ( Recurrent , LSTM , GRU , RecurrentSum , RecurrentMean , RecurrentLast , \n BiRNN , BiLSTM , DeepBiRNN , DeepBiLSTM ) \n from neon . layers . container import ( Tree , Sequential , MergeMultistream , MergeBroadcast , Multicost , \n RoiPooling , MergeSum , <mask0> ) \n", "gt": "SingleOutputTree"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from neon . util . argparser import NeonArgparser \n from neon . initializers import Constant , Gaussian \n from neon . layers import Conv , Dropout , Pooling , GeneralizedCost , Affine \n from neon . optimizers import GradientDescentMomentum , MultiOptimizer , Schedule \n from neon . transforms import Rectlin , Softmax , CrossEntropyMulti , TopKMisclassification \n from neon . models import Model \n from neon . data import ImageLoader \n from neon . callbacks . callbacks import Callbacks \n parser = NeonArgparser ( __doc__ ) \n args = parser . parse_args ( ) \n img_set_options = dict ( repo_dir = args . data_dir , \n inner_size = <NUM_LIT> , \n subset_pct = <NUM_LIT> ) \n train = ImageLoader ( set_name = '<STR_LIT:train>' , scale_range = ( <NUM_LIT> , <NUM_LIT> ) , shuffle = False , \n do_transforms = False , ** img_set_options ) \n test = ImageLoader ( set_name = '<STR_LIT>' , scale_range = ( <NUM_LIT> , <NUM_LIT> ) , shuffle = False , \n do_transforms = False , ** img_set_options ) \n layers = [ Conv ( ( <NUM_LIT:11> , <NUM_LIT:11> , <NUM_LIT:64> ) , init = Gaussian ( scale = <NUM_LIT> ) , bias = Constant ( <NUM_LIT:0> ) , \n activation = Rectlin ( ) , padding = <NUM_LIT:3> , strides = <NUM_LIT:4> ) , \n Pooling ( <NUM_LIT:3> , strides = <NUM_LIT:2> ) , \n Conv ( ( <NUM_LIT:5> , <NUM_LIT:5> , <NUM_LIT> ) , init = Gaussian ( scale = <NUM_LIT> ) , bias = Constant ( <NUM_LIT:1> ) , \n activation = Rectlin ( ) , padding = <NUM_LIT:2> ) , \n Pooling ( <NUM_LIT:3> , strides = <NUM_LIT:2> ) , \n Conv ( ( <NUM_LIT:3> , <NUM_LIT:3> , <NUM_LIT> ) , init = Gaussian ( scale = <NUM_LIT> ) , bias = Constant ( <NUM_LIT:0> ) , \n activation = Rectlin ( ) , padding = <NUM_LIT:1> ) , \n Conv ( ( <NUM_LIT:3> , <NUM_LIT:3> , <NUM_LIT> ) , init = Gaussian ( scale = <NUM_LIT> ) , bias = Constant ( <NUM_LIT:1> ) , \n activation = Rectlin ( ) , padding = <NUM_LIT:1> ) , \n Conv ( ( <NUM_LIT:3> , <NUM_LIT:3> , <NUM_LIT> ) , init = Gaussian ( scale = <NUM_LIT> ) , bias = Constant ( <NUM_LIT:1> ) , \n activation = Rectlin ( ) , padding = <NUM_LIT:1> ) , \n Pooling ( <NUM_LIT:3> , strides = <NUM_LIT:2> ) , \n Affine ( nout = <NUM_LIT> , init = Gaussian ( scale = <NUM_LIT> ) , bias = Constant ( <NUM_LIT:1> ) , activation = Rectlin ( ) ) , \n Dropout ( keep = <NUM_LIT:1.0> ) , \n Affine ( nout = <NUM_LIT> , init = Gaussian ( scale = <NUM_LIT> ) , bias = Constant ( <NUM_LIT:1> ) , activation = Rectlin ( ) ) , \n Dropout ( keep = <NUM_LIT:1.0> ) , \n Affine ( nout = <NUM_LIT:1000> , init = Gaussian ( scale = <NUM_LIT> ) , bias = Constant ( - <NUM_LIT:7> ) , activation = Softmax ( ) ) ] \n model = Model ( layers = layers ) \n weight_sched = Schedule ( [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , ( <NUM_LIT:1> / <NUM_LIT> ) ** ( <NUM_LIT:1> / <NUM_LIT> ) ) \n opt_gdm = GradientDescentMomentum ( <NUM_LIT> / <NUM_LIT:10> , <NUM_LIT> , wdecay = <NUM_LIT> , schedule = weight_sched , \n stochastic_round = args . rounding ) \n opt_biases = GradientDescentMomentum ( <NUM_LIT> / <NUM_LIT:10> , <NUM_LIT> , schedule = Schedule ( [ <NUM_LIT> ] , <NUM_LIT:0.1> ) , \n stochastic_round = args . rounding ) \n opt = MultiOptimizer ( { '<STR_LIT:default>' : opt_gdm , '<STR_LIT>' : opt_biases } ) \n valmetric = TopKMisclassification ( k = <NUM_LIT:5> ) \n callbacks = Callbacks ( model , eval_set = test , metric = valmetric , ** args . callback_args ) \n cost = GeneralizedCost ( costfunc = CrossEntropyMulti ( ) ) \n model . fit ( train , optimizer = opt , num_epochs = args . epochs , cost = cost , callbacks = <mask0> ) \n", "gt": "callbacks"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import itertools as itt \n import numpy as np \n from neon import NervanaObject \n from neon . layers . layer import Pooling \n from tests . utils import allclose_with_out \n def pytest_generate_tests ( metafunc ) : \n np . random . seed ( <NUM_LIT:1> ) \n if metafunc . config . option . all : \n bsz_rng = [ <NUM_LIT:32> , <NUM_LIT:64> ] \n else : \n bsz_rng = [ <NUM_LIT> ] \n if '<STR_LIT>' in metafunc . fixturenames : \n fargs = [ ] \n if metafunc . config . option . all : \n fs_rng = [ <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:5> ] \n pad_rng = [ <NUM_LIT:0> , <NUM_LIT:1> ] \n nifm_rng = [ <NUM_LIT:16> , <NUM_LIT:32> ] \n in_sz_rng = [ <NUM_LIT:8> , <NUM_LIT:16> ] \n else : \n fs_rng = [ <NUM_LIT:2> , <NUM_LIT:4> ] \n pad_rng = [ <NUM_LIT:0> , <NUM_LIT:1> ] \n nifm_rng = [ <NUM_LIT:8> ] \n in_sz_rng = [ <NUM_LIT:8> ] \n fargs_ = [ ] \n for fs in fs_rng : \n stride_rng = set ( [ <NUM_LIT:1> , fs / <NUM_LIT:2> , fs ] ) \n fargs_ . append ( itt . product ( fs_rng , nifm_rng , pad_rng , stride_rng , in_sz_rng , bsz_rng ) ) \n fargs = itt . chain ( * fargs_ ) \n metafunc . parametrize ( '<STR_LIT>' , fargs ) \n def ref_pooling ( inp , inp_shape , fshape , padding , strides , be , ncheck = None ) : \n inp_lshape = list ( inp_shape ) \n bsz = inp . shape [ - <NUM_LIT:1> ] \n if ncheck is None : \n check_inds = np . arange ( bsz ) \n elif type ( ncheck ) is int : \n check_inds = np . random . permutation ( bsz ) \n check_inds = check_inds [ <NUM_LIT:0> : ncheck ] \n else : \n check_inds = ncheck \n check_inds = np . sort ( check_inds ) \n inp_lshape . append ( bsz ) \n inpa = inp . get ( ) . reshape ( inp_lshape ) \n outshape = ( inp_lshape [ <NUM_LIT:0> ] , \n be . output_dim ( inp_lshape [ <NUM_LIT:1> ] , fshape [ <NUM_LIT:0> ] , padding , strides [ <NUM_LIT:0> ] , pooling = True ) , \n be . output_dim ( inp_lshape [ <NUM_LIT:2> ] , fshape [ <NUM_LIT:1> ] , padding , strides [ <NUM_LIT:1> ] , pooling = True ) , \n len ( check_inds ) ) \n if padding > <NUM_LIT:0> : \n padded_shape = ( inp_lshape [ <NUM_LIT:0> ] , \n inp_lshape [ <NUM_LIT:1> ] + <NUM_LIT:2> * padding , \n inp_lshape [ <NUM_LIT:2> ] + <NUM_LIT:2> * padding , \n inp_lshape [ - <NUM_LIT:1> ] ) \n inp_pad = np . zeros ( padded_shape ) \n inp_pad [ : , padding : - padding , padding : - padding , : ] = inpa [ : , <NUM_LIT:0> : , <NUM_LIT:0> : , : ] \n else : \n inp_pad = inpa \n out_exp = np . zeros ( outshape ) \n for indC in range ( outshape [ <NUM_LIT:0> ] ) : \n for indh in range ( outshape [ <NUM_LIT:1> ] ) : \n hrng = ( indh * strides [ <NUM_LIT:0> ] , indh * strides [ <NUM_LIT:0> ] + fshape [ <NUM_LIT:0> ] ) \n for indw in range ( outshape [ <NUM_LIT:2> ] ) : \n wrng = ( indw * strides [ <NUM_LIT:1> ] , indw * strides [ <NUM_LIT:1> ] + fshape [ <NUM_LIT:1> ] ) \n for cnt , indb in enumerate ( check_inds ) : \n inp_check = inp_pad [ indC , hrng [ <NUM_LIT:0> ] : hrng [ <NUM_LIT:1> ] , wrng [ <NUM_LIT:0> ] : wrng [ <NUM_LIT:1> ] , indb ] \n out_exp [ indC , indh , indw , cnt ] = np . max ( inp_check ) \n return ( out_exp , check_inds ) \n def test_padding ( backend_default , poolargs ) : \n fshape , nifm , padding , stride , in_sz , batch_size = poolargs \n NervanaObject . be . bsz = batch_size \n inshape = ( nifm , in_sz , in_sz ) \n insize = np . prod ( inshape ) \n neon_layer = Pooling ( fshape = fshape , strides = stride , padding = padding ) \n inp = neon_layer . be . array ( np . random . random ( ( insize , batch_size ) ) ) \n inp . lshape = inshape \n neon_layer . configure ( inshape ) \n neon_layer . prev_layer = True \n neon_layer . allocate ( ) \n neon_layer . set_deltas ( [ neon_layer . be . iobuf ( inshape ) ] ) \n out = neon_layer . fprop ( inp ) . get ( ) \n ncheck = [ <NUM_LIT:0> , batch_size / <NUM_LIT:2> , batch_size - <NUM_LIT:1> ] \n ( out_exp , check_inds ) = ref_pooling ( inp , inp . lshape , \n ( fshape , fshape ) , \n padding , \n ( stride , stride ) , \n neon_layer . be , \n ncheck = ncheck ) \n out_shape = list ( out_exp . shape [ <NUM_LIT:0> : <NUM_LIT:3> ] ) \n out_shape . append ( batch_size ) \n outa = out . reshape ( out_shape ) \n assert allclose_with_out ( out_exp , outa [ : , : , : , check_inds ] , atol = <NUM_LIT:0.0> , <mask0> = <NUM_LIT:0.0> ) \n", "gt": "rtol"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n revision = '<STR_LIT>' \n down_revision = '<STR_LIT>' \n from alembic import op \n import sqlalchemy as sa \n def upgrade ( ) : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ) ) \n def downgrade ( ) : \n op . <mask0> ( '<STR_LIT>' , '<STR_LIT>' ) \n", "gt": "drop_column"}
{"input": "\n from . import view \n from . comp import Card , <mask0> \n", "gt": "NewCard"}
{"input": "\n from nagare . i18n import _ \n from nagare import presentation , ajax , security , component \n from . comp import Gallery , Asset , AssetCropper \n def render_image ( self , h , comp , size , randomize = False , ** kw ) : \n metadata = self . assets_manager . get_metadata ( self . filename ) \n src = self . assets_manager . get_image_url ( self . filename , size ) \n if randomize : \n src += '<STR_LIT>' + h . generate_id ( ) \n return h . img ( title = metadata [ '<STR_LIT:filename>' ] , alt = metadata [ '<STR_LIT:filename>' ] , \n src = src , ** kw ) \n def render_file ( self , h , comp , size , ** kw ) : \n kw [ '<STR_LIT:class>' ] += '<STR_LIT>' \n metadata = self . assets_manager . get_metadata ( self . filename ) \n res = [ h . img ( title = metadata [ '<STR_LIT:filename>' ] , alt = metadata [ '<STR_LIT:filename>' ] , \n src = \"<STR_LIT>\" , ** kw ) ] \n if size == '<STR_LIT>' : \n res . append ( h . span ( metadata [ '<STR_LIT:filename>' ] ) ) \n return res \n CONTENT_TYPES = { '<STR_LIT>' : render_image , \n '<STR_LIT>' : render_image , \n '<STR_LIT>' : render_image , \n '<STR_LIT>' : render_image } \n @ presentation . render_for ( Gallery ) \n def render ( self , h , comp , * args ) : \n with h . div ( id = '<STR_LIT>' + self . comp_id ) : \n with h . div ( class_ = '<STR_LIT>' ) : \n h << comp . render ( h , model = '<STR_LIT>' ) \n with h . div ( id = \"<STR_LIT>\" ) : \n h << comp . render ( h , self . model ) \n return h . root \n @ presentation . render_for ( Gallery , '<STR_LIT>' ) \n def render_Gallery_view ( self , h , comp , model ) : \n model = '<STR_LIT>' if security . has_permissions ( '<STR_LIT>' , self ) else '<STR_LIT>' \n for asset in self . assets : \n h << asset . render ( h , model ) \n return h . root \n @ presentation . render_for ( Gallery , '<STR_LIT>' ) \n def render_Gallery_crop ( self , h , comp , model ) : \n return self . cropper . on_answer ( self . action ) \n @ presentation . render_for ( Gallery , '<STR_LIT>' ) \n def render_cover ( self , h , comp , model ) : \n cover = self . get_cover ( ) \n if cover : \n h << h . p ( component . Component ( self . get_cover ( ) , model = '<STR_LIT>' ) , class_ = '<STR_LIT>' ) \n return h . root \n @ presentation . render_for ( Gallery , \"<STR_LIT:action>\" ) \n def render_download ( self , h , comp , * args ) : \n if security . has_permissions ( '<STR_LIT>' , self ) : \n submit_id = h . generate_id ( \"<STR_LIT>\" ) \n input_id = h . generate_id ( \"<STR_LIT>\" ) \n h << h . label ( ( h . i ( class_ = '<STR_LIT>' ) , \n _ ( \"<STR_LIT>\" ) ) , class_ = '<STR_LIT>' , for_ = input_id ) \n with h . form : \n h << h . script ( \n u'''<STR_LIT>''' % \n { \n '<STR_LIT>' : ajax . py2js ( self . assets_manager . max_size ) , \n '<STR_LIT>' : ajax . py2js ( input_id ) , \n '<STR_LIT>' : ajax . py2js ( submit_id ) , \n '<STR_LIT:error>' : ajax . py2js ( \n _ ( u'<STR_LIT>' ) \n ) . decode ( '<STR_LIT>' ) \n } \n ) \n submit_action = ajax . Update ( \n render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( '<STR_LIT>' ) ) , \n component_to_update = '<STR_LIT>' + self . comp_id , \n ) \n h << h . input ( id = input_id , class_ = '<STR_LIT>' , type = \"<STR_LIT:file>\" , name = \"<STR_LIT:file>\" , multiple = \"<STR_LIT>\" , maxlength = \"<STR_LIT:100>\" , ) . action ( self . add_assets ) \n h << h . input ( class_ = '<STR_LIT>' , id = submit_id , type = \"<STR_LIT>\" ) . action ( submit_action ) \n return h . root \n @ presentation . render_for ( Gallery , model = '<STR_LIT>' ) \n def render_gallery_badge ( self , h , * args ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . assets : \n with h . span ( class_ = '<STR_LIT>' ) : \n h << h . span ( h . i ( class_ = '<STR_LIT>' ) , '<STR_LIT:U+0020>' , len ( self . assets ) , class_ = '<STR_LIT:label>' ) \n return h . root \n @ presentation . render_for ( Asset ) \n @ presentation . render_for ( Asset , model = '<STR_LIT>' ) \n @ presentation . render_for ( Asset , model = '<STR_LIT>' ) \n @ presentation . render_for ( Asset , model = '<STR_LIT>' ) \n def render_asset ( self , h , comp , model , * args ) : \n res = [ ] \n metadata = self . assets_manager . get_metadata ( self . filename ) \n kw = { '<STR_LIT>' : True } if model == '<STR_LIT>' else { } \n kw [ '<STR_LIT:class>' ] = model \n if self . is_cover : \n res . append ( h . span ( class_ = '<STR_LIT>' ) ) \n meth = CONTENT_TYPES . get ( metadata [ '<STR_LIT>' ] , render_file ) \n res . append ( meth ( self , h , comp , model , ** kw ) ) \n return res \n @ presentation . render_for ( Asset , model = '<STR_LIT>' ) \n def render_Asset_thumb ( self , h , comp , model , * args ) : \n with h . div ( class_ = '<STR_LIT>' ) : \n action = h . a . action ( lambda : comp . answer ( ( '<STR_LIT>' , self ) ) ) . get ( '<STR_LIT>' ) \n onclick = _ ( u'<STR_LIT>' ) \n onclick = u'<STR_LIT>' % ( onclick , action ) \n with h . a ( class_ = '<STR_LIT>' , title = _ ( u'<STR_LIT>' ) , href = '<STR_LIT:#>' , onclick = onclick ) : \n h << h . i ( class_ = '<STR_LIT>' ) \n if self . is_image ( ) : \n with h . a ( class_ = '<STR_LIT>' , title = _ ( u'<STR_LIT>' ) ) . action ( lambda : comp . answer ( ( '<STR_LIT>' , self ) ) ) : \n if self . is_cover : \n h << { '<STR_LIT>' : '<STR_LIT>' } \n h << h . i ( class_ = '<STR_LIT>' ) \n with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = '<STR_LIT>' ) : \n h << comp . render ( h , '<STR_LIT>' ) \n return h . root \n @ presentation . render_for ( Asset , model = \"<STR_LIT>\" ) \n def render_asset_anonymous ( self , h , comp , model , * args ) : \n with h . div ( class_ = '<STR_LIT>' ) : \n with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = '<STR_LIT>' ) : \n h << comp . render ( h , model = \"<STR_LIT>\" ) \n return h . root \n @ presentation . render_for ( AssetCropper ) \n def render_gallery_cropper ( self , h , comp , * args ) : \n h << h . p ( _ ( '<STR_LIT>' ) ) \n form_id = h . generate_id ( ) \n img_id = h . generate_id ( ) \n with h . form : \n for crop_name in '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' : \n h << h . input ( type = '<STR_LIT>' , id = form_id + '<STR_LIT:_>' + crop_name ) . action ( getattr ( self , crop_name ) ) \n h << h . p ( render_image ( self . asset , h , comp , '<STR_LIT>' , id = img_id ) ) \n h << h . script ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" % ( \n ajax . py2js ( img_id ) , \n ajax . py2js ( img_id ) , \n ajax . py2js ( form_id ) , \n ajax . py2js ( self . crop_width ( ) ) , \n ajax . py2js ( self . crop_height ( ) ) \n ) \n ) \n with h . div ( class_ = '<STR_LIT>' ) : \n h << h . button ( _ ( '<STR_LIT>' ) , class_ = '<STR_LIT>' ) . action ( self . commit , comp ) \n if self . asset . is_cover : \n h << '<STR_LIT:U+0020>' \n h << h . button ( _ ( '<STR_LIT>' ) , class_ = '<STR_LIT>' ) . action ( self . remove_cover , comp ) \n h << '<STR_LIT:U+0020>' \n h << h . button ( _ ( '<STR_LIT>' ) , class_ = '<STR_LIT>' ) . action ( self . cancel , comp ) \n return h . <mask0> \n", "gt": "root"}
{"input": "\n class EventHandlerMixIn ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def emit_event ( self , comp , kind , data = None ) : \n event = kind ( data , source = [ self ] ) \n return comp . answer ( event ) \n def handle_event ( self , comp , event ) : \n local_res = None \n local_handler = getattr ( self , '<STR_LIT>' , None ) \n if local_handler : \n local_res = local_handler ( comp , event ) \n event . append ( self ) \n upper_res = comp . answer ( event ) \n return local_res or upper_res \n class Event ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , data , source = [ ] ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _source = source \n self . data = data \n @ property \n def source ( self ) : \n return self . _source . copy ( ) \n @ property \n def emitter ( self ) : \n return self . _source [ <NUM_LIT:0> ] \n @ property \n def last_relay ( self ) : \n return self . _source [ - <NUM_LIT:1> ] \n def is_ ( self , kind ) : \n return type ( self ) is kind \n def is_kind_of ( self , kind ) : \n return isinstance ( self , kind ) \n def append ( self , relay ) : \n self . _source . append ( relay ) \n def cast_as ( self , sub_kind ) : \n return sub_kind ( self . data , self . _source ) \n class ColumnDeleted ( Event ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class CardClicked ( Event ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class PopinClosed ( Event ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class CardEditorClosed ( PopinClosed ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class CardArchived ( Event ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class SearchIndexUpdated ( Event ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class CardDisplayed ( Event ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class BoardAccessChanged ( Event ) : \n \"\"\"<STR_LIT>\"\"\" \n class BoardDeleted ( BoardAccessChanged ) : \n \"\"\"<STR_LIT>\"\"\" \n class BoardArchived ( BoardAccessChanged ) : \n \"\"\"<STR_LIT>\"\"\" \n class BoardRestored ( BoardAccessChanged ) : \n \"\"\"<STR_LIT>\"\"\" \n class BoardLeft ( BoardAccessChanged ) : \n \"\"\"<STR_LIT>\"\"\" \n class ParentTitleNeeded ( Event ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class NewTemplateRequested ( Event ) : \n \"\"\"<STR_LIT>\"\"\" \n <mask0> \n", "gt": "pass"}
{"input": "\n from . comp import EditableTitle \n from . import <mask0> \n", "gt": "view"}
{"input": "\n import genie2 . client . wrapper \n import genie2 . model . ClusterCriteria \n import genie2 . model . Job \n import genie2 . model . FileAttachment \n import time \n genie = genie2 . client . wrapper . Genie2 ( \"<STR_LIT>\" , \n genie2 . client . wrapper . RetryPolicy ( \n tries = <NUM_LIT:8> , none_on_404 = True , no_retry_http_codes = range ( <NUM_LIT> , <NUM_LIT> ) \n ) ) \n job = genie2 . model . Job . Job ( ) \n job . name = \"<STR_LIT>\" \n job . user = \"<STR_LIT:root>\" \n job . version = \"<STR_LIT>\" \n job . clusterCriterias = list ( ) \n cluster_criteria = genie2 . model . ClusterCriteria . ClusterCriteria ( ) \n criteria = set ( ) \n criteria . add ( \"<STR_LIT>\" ) \n criteria . add ( \"<STR_LIT>\" ) \n cluster_criteria . tags = criteria \n job . clusterCriterias . append ( cluster_criteria ) \n command_criteria = set ( ) \n command_criteria . add ( \"<STR_LIT>\" ) \n job . commandCriteria = command_criteria \n job . fileDependencies = \"<STR_LIT>\" \n job . commandArgs = \"<STR_LIT>\" \n job = genie . submitJob ( job ) \n while job . status != \"<STR_LIT>\" and job . status != \"<STR_LIT>\" and job . status != \"<STR_LIT>\" : \n print \"<STR_LIT>\" + job . id + \"<STR_LIT>\" + job . status \n time . sleep ( <NUM_LIT:10> ) \n job = genie . getJob ( job . id ) \n print \"<STR_LIT>\" + job . id + \"<STR_LIT>\" + job . <mask0> \n", "gt": "status"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import logging \n from os import environ \n from aminator . config import conf_action \n from aminator . plugins . finalizer . tagging_base import TaggingBaseFinalizerPlugin \n from aminator . util . linux import sanitize_metadata \n __all__ = ( '<STR_LIT>' , ) \n log = logging . getLogger ( __name__ ) \n class TaggingEBSFinalizerPlugin ( TaggingBaseFinalizerPlugin ) : \n _name = '<STR_LIT>' \n def add_plugin_args ( self ) : \n tagging = super ( TaggingEBSFinalizerPlugin , self ) . add_plugin_args ( ) \n context = self . _config . context \n tagging . add_argument ( '<STR_LIT>' , '<STR_LIT>' , dest = '<STR_LIT:name>' , action = conf_action ( context . ami ) , help = '<STR_LIT>' ) \n def _set_metadata ( self ) : \n super ( TaggingEBSFinalizerPlugin , self ) . _set_metadata ( ) \n context = self . _config . context \n config = self . _config . plugins [ self . full_name ] \n metadata = context . package . attributes \n ami_name = context . ami . get ( '<STR_LIT:name>' , None ) \n if not ami_name : \n ami_name = config . name_format . format ( ** metadata ) \n context . ami . name = sanitize_metadata ( '<STR_LIT>' . format ( ami_name ) ) \n def _snapshot_volume ( self ) : \n log . info ( '<STR_LIT>' ) \n if not self . _cloud . snapshot_volume ( ) : \n return False \n log . info ( '<STR_LIT>' ) \n return True \n def _register_image ( self , block_device_map = None , root_device = None ) : \n log . info ( '<STR_LIT>' ) \n config = self . _config . plugins [ self . full_name ] \n if block_device_map is None : \n block_device_map = config . default_block_device_map \n if root_device is None : \n root_device = config . default_root_device \n if not self . _cloud . register_image ( block_device_map , root_device ) : \n return False \n log . info ( '<STR_LIT>' ) \n return True \n def finalize ( self ) : \n log . info ( '<STR_LIT>' ) \n self . _set_metadata ( ) \n if not self . _snapshot_volume ( ) : \n log . critical ( '<STR_LIT>' ) \n return False \n if not self . _register_image ( ) : \n log . critical ( '<STR_LIT>' ) \n return False \n if not self . _add_tags ( [ '<STR_LIT>' , '<STR_LIT>' ] ) : \n log . critical ( '<STR_LIT>' ) \n return False \n log . info ( '<STR_LIT>' ) \n self . _log_ami_metadata ( ) \n return True \n def __enter__ ( self ) : \n context = self . _config . context \n environ [ \"<STR_LIT>\" ] = \"<STR_LIT>\" \n if context . ami . get ( \"<STR_LIT:name>\" , None ) : \n environ [ \"<STR_LIT>\" ] = context . ami . name \n return super ( TaggingEBSFinalizerPlugin , self ) . <mask0> ( ) \n", "gt": "__enter__"}
{"input": "\n LOG_LEVEL = \"<STR_LIT>\" \n LOG_FILE = \"<STR_LIT>\" \n SQLALCHEMY_DATABASE_URI = '<STR_LIT>' \n SQLALCHEMY_POOL_SIZE = <NUM_LIT:50> \n SQLALCHEMY_MAX_OVERFLOW = <NUM_LIT:15> \n ENVIRONMENT = '<STR_LIT>' \n USE_ROUTE53 = False \n FQDN = '<STR_LIT>' \n API_PORT = '<STR_LIT>' \n WEB_PORT = '<STR_LIT>' \n WEB_PATH = '<STR_LIT>' \n FRONTED_BY_NGINX = True \n NGINX_PORT = '<STR_LIT>' \n BASE_URL = '<STR_LIT>' . format ( FQDN ) \n SECRET_KEY = '<STR_LIT>' \n MAIL_DEFAULT_SENDER = '<STR_LIT>' \n SECURITY_REGISTERABLE = True \n SECURITY_CONFIRMABLE = False \n SECURITY_RECOVERABLE = False \n SECURITY_PASSWORD_HASH = '<STR_LIT>' \n SECURITY_PASSWORD_SALT = '<STR_LIT>' \n SECURITY_TRACKABLE = True \n SECURITY_POST_LOGIN_VIEW = BASE_URL \n SECURITY_POST_REGISTER_VIEW = BASE_URL \n SECURITY_POST_CONFIRM_VIEW = BASE_URL \n SECURITY_POST_RESET_VIEW = BASE_URL \n SECURITY_POST_CHANGE_VIEW = BASE_URL \n SECURITY_TEAM_EMAIL = [ ] \n EMAILS_USE_SMTP = False \n SES_REGION = '<STR_LIT>' \n MAIL_SERVER = '<STR_LIT>' \n MAIL_PORT = <NUM_LIT> \n MAIL_USE_SSL = True \n MAIL_USERNAME = '<STR_LIT:username>' \n MAIL_PASSWORD = '<STR_LIT:password>' \n WTF_CSRF_ENABLED = True \n WTF_CSRF_SSL_STRICT = <mask0> \n WTF_CSRF_METHODS = [ '<STR_LIT>' , '<STR_LIT:POST>' , '<STR_LIT>' , '<STR_LIT>' ] \n SECURITYGROUP_INSTANCE_DETAIL = '<STR_LIT>' \n CORE_THREADS = <NUM_LIT> \n MAX_THREADS = <NUM_LIT:30> \n", "gt": "True"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from security_monkey . auditor import Auditor \n from security_monkey . watchers . rds_security_group import RDSSecurityGroup \n from security_monkey . datastore import NetworkWhitelistEntry \n from security_monkey . auditors . security_group import _check_rfc_1918 \n import ipaddr \n class RDSSecurityGroupAuditor ( Auditor ) : \n index = RDSSecurityGroup . index \n i_am_singular = RDSSecurityGroup . i_am_singular \n i_am_plural = RDSSecurityGroup . i_am_plural \n network_whitelist = [ ] \n def __init__ ( self , accounts = None , debug = False ) : \n super ( RDSSecurityGroupAuditor , self ) . __init__ ( accounts = accounts , debug = debug ) \n def prep_for_audit ( self ) : \n self . network_whitelist = NetworkWhitelistEntry . query . all ( ) \n def _check_inclusion_in_network_whitelist ( self , cidr ) : \n for entry in self . network_whitelist : \n if ipaddr . IPNetwork ( cidr ) in ipaddr . IPNetwork ( str ( entry . cidr ) ) : \n return True \n return False \n def check_rds_ec2_rfc1918 ( self , sg_item ) : \n \"\"\"<STR_LIT>\"\"\" \n tag = \"<STR_LIT>\" \n severity = <NUM_LIT:8> \n if sg_item . config . get ( \"<STR_LIT>\" , None ) : \n return \n for ipr in sg_item . config . get ( \"<STR_LIT>\" , [ ] ) : \n cidr = ipr . get ( \"<STR_LIT>\" , None ) \n if cidr and _check_rfc_1918 ( cidr ) : \n self . add_issue ( severity , tag , sg_item , notes = cidr ) \n def check_securitygroup_large_subnet ( self , sg_item ) : \n \"\"\"<STR_LIT>\"\"\" \n tag = \"<STR_LIT>\" \n severity = <NUM_LIT:3> \n for ipr in sg_item . config . get ( \"<STR_LIT>\" , [ ] ) : \n cidr = ipr . get ( \"<STR_LIT>\" , None ) \n if cidr and not self . _check_inclusion_in_network_whitelist ( cidr ) : \n if '<STR_LIT:/>' in cidr and not cidr == \"<STR_LIT>\" and not cidr == \"<STR_LIT>\" : \n mask = int ( cidr . split ( '<STR_LIT:/>' ) [ <NUM_LIT:1> ] ) \n if mask < <NUM_LIT> and mask > <NUM_LIT:0> : \n self . add_issue ( severity , tag , sg_item , notes = cidr ) \n def check_securitygroup_zero_subnet ( self , sg_item ) : \n \"\"\"<STR_LIT>\"\"\" \n tag = \"<STR_LIT>\" \n severity = <NUM_LIT:10> \n for ipr in sg_item . config . get ( \"<STR_LIT>\" , [ ] ) : \n cidr = ipr . get ( \"<STR_LIT>\" , None ) \n if cidr and '<STR_LIT:/>' in cidr and not cidr == \"<STR_LIT>\" and not cidr == \"<STR_LIT>\" : \n mask = int ( cidr . split ( '<STR_LIT:/>' ) [ <NUM_LIT:1> ] ) \n if mask == <NUM_LIT:0> : \n self . add_issue ( severity , tag , sg_item , notes = cidr ) \n def check_securitygroup_any ( self , sg_item ) : \n \"\"\"<STR_LIT>\"\"\" \n tag = \"<STR_LIT>\" \n severity = <NUM_LIT:5> \n for ipr in sg_item . config . get ( \"<STR_LIT>\" , [ ] ) : \n cidr = ipr . get ( \"<STR_LIT>\" ) \n if \"<STR_LIT>\" == cidr : \n self . add_issue ( severity , tag , sg_item , notes = cidr ) \n return \n def check_securitygroup_10net ( self , sg_item ) : \n \"\"\"<STR_LIT>\"\"\" \n tag = \"<STR_LIT>\" \n severity = <NUM_LIT:5> \n for ipr in sg_item . config . get ( \"<STR_LIT>\" , [ ] ) : \n cidr = ipr . get ( \"<STR_LIT>\" ) \n if \"<STR_LIT>\" == cidr : \n self . add_issue ( severity , tag , sg_item , notes = cidr ) \n <mask0> \n", "gt": "return"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import json \n from security_monkey . datastore import NetworkWhitelistEntry , Account \n from security_monkey . tests import SecurityMonkeyTestCase \n from security_monkey import db \n from security_monkey . watchers . elasticsearch_service import ElasticSearchServiceItem \n CONFIG_ONE = { \n \"<STR_LIT:name>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . loads ( b\"\"\"<STR_LIT>\"\"\" ) \n } \n CONFIG_TWO = { \n \"<STR_LIT:name>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . loads ( b\"\"\"<STR_LIT>\"\"\" ) \n } \n CONFIG_THREE = { \n \"<STR_LIT:name>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . loads ( b\"\"\"<STR_LIT>\"\"\" ) \n } \n CONFIG_FOUR = { \n \"<STR_LIT:name>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . loads ( b\"\"\"<STR_LIT>\"\"\" ) \n } \n CONFIG_FIVE = { \n \"<STR_LIT:name>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . loads ( b\"\"\"<STR_LIT>\"\"\" ) \n } \n CONFIG_SIX = { \n \"<STR_LIT:name>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . loads ( b\"\"\"<STR_LIT>\"\"\" ) \n } \n CONFIG_SEVEN = { \n \"<STR_LIT:name>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . loads ( b\"\"\"<STR_LIT>\"\"\" ) \n } \n CONFIG_EIGHT = { \n \"<STR_LIT:name>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . loads ( b\"\"\"<STR_LIT>\"\"\" ) \n } \n CONFIG_NINE = { \n \"<STR_LIT:name>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . loads ( b\"\"\"<STR_LIT>\"\"\" ) \n } \n WHITELIST_CIDRS = [ \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ] \n class ElasticSearchServiceTestCase ( SecurityMonkeyTestCase ) : \n def setUp ( self ) : \n self . es_items = [ \n ElasticSearchServiceItem ( region = \"<STR_LIT>\" , account = \"<STR_LIT>\" , name = \"<STR_LIT>\" , config = CONFIG_ONE ) , \n ElasticSearchServiceItem ( region = \"<STR_LIT>\" , account = \"<STR_LIT>\" , name = \"<STR_LIT>\" , config = CONFIG_TWO ) , \n ElasticSearchServiceItem ( region = \"<STR_LIT>\" , account = \"<STR_LIT>\" , name = \"<STR_LIT>\" , config = CONFIG_THREE ) , \n ElasticSearchServiceItem ( region = \"<STR_LIT>\" , account = \"<STR_LIT>\" , name = \"<STR_LIT>\" , config = CONFIG_FOUR ) , \n ElasticSearchServiceItem ( region = \"<STR_LIT>\" , account = \"<STR_LIT>\" , name = \"<STR_LIT>\" , config = CONFIG_FIVE ) , \n ElasticSearchServiceItem ( region = \"<STR_LIT>\" , account = \"<STR_LIT>\" , name = \"<STR_LIT>\" , config = CONFIG_SIX ) , \n ElasticSearchServiceItem ( region = \"<STR_LIT>\" , account = \"<STR_LIT>\" , name = \"<STR_LIT>\" , config = CONFIG_SEVEN ) , \n ElasticSearchServiceItem ( region = \"<STR_LIT>\" , account = \"<STR_LIT>\" , name = \"<STR_LIT>\" , config = CONFIG_EIGHT ) , \n ElasticSearchServiceItem ( region = \"<STR_LIT>\" , account = \"<STR_LIT>\" , name = \"<STR_LIT>\" , config = CONFIG_NINE ) , \n ] \n test_account = Account ( ) \n test_account . name = \"<STR_LIT>\" \n test_account . notes = \"<STR_LIT>\" \n test_account . s3_name = \"<STR_LIT>\" \n test_account . number = \"<STR_LIT>\" \n test_account . role_name = \"<STR_LIT>\" \n db . session . add ( test_account ) \n db . session . commit ( ) \n def tearDown ( self ) : \n test_account = Account . query . filter ( Account . number == \"<STR_LIT>\" ) . first ( ) \n if test_account is not None : \n db . session . delete ( test_account ) \n db . session . commit ( ) \n def test_es_auditor ( self ) : \n from security_monkey . auditors . elasticsearch_service import ElasticSearchServiceAuditor \n es_auditor = ElasticSearchServiceAuditor ( accounts = [ \"<STR_LIT>\" ] ) \n es_auditor . network_whitelist = [ ] \n for cidr in WHITELIST_CIDRS : \n whitelist_cidr = NetworkWhitelistEntry ( ) \n whitelist_cidr . cidr = cidr [ <NUM_LIT:1> ] \n whitelist_cidr . name = cidr [ <NUM_LIT:0> ] \n es_auditor . network_whitelist . append ( whitelist_cidr ) \n for es_domain in self . es_items : \n es_auditor . check_es_access_policy ( es_domain ) \n self . assertEquals ( len ( self . es_items [ <NUM_LIT:0> ] . audit_issues ) , <NUM_LIT:1> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:0> ] . audit_issues [ <NUM_LIT:0> ] . score , <NUM_LIT:20> ) \n self . assertEquals ( len ( self . es_items [ <NUM_LIT:1> ] . audit_issues ) , <NUM_LIT:1> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:1> ] . audit_issues [ <NUM_LIT:0> ] . score , <NUM_LIT:20> ) \n self . assertEquals ( len ( self . es_items [ <NUM_LIT:2> ] . audit_issues ) , <NUM_LIT:2> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:2> ] . audit_issues [ <NUM_LIT:0> ] . score , <NUM_LIT:5> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:2> ] . audit_issues [ <NUM_LIT:1> ] . score , <NUM_LIT:7> ) \n self . assertEquals ( len ( self . es_items [ <NUM_LIT:3> ] . audit_issues ) , <NUM_LIT:1> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:3> ] . audit_issues [ <NUM_LIT:0> ] . score , <NUM_LIT:20> ) \n self . assertEquals ( len ( self . es_items [ <NUM_LIT:4> ] . audit_issues ) , <NUM_LIT:0> ) \n self . assertEquals ( len ( self . es_items [ <NUM_LIT:5> ] . audit_issues ) , <NUM_LIT:0> ) \n self . assertEquals ( len ( self . es_items [ <NUM_LIT:6> ] . audit_issues ) , <NUM_LIT:3> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:6> ] . audit_issues [ <NUM_LIT:0> ] . score , <NUM_LIT:5> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:6> ] . audit_issues [ <NUM_LIT:1> ] . score , <NUM_LIT:5> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:6> ] . audit_issues [ <NUM_LIT:2> ] . score , <NUM_LIT:7> ) \n self . assertEquals ( len ( self . es_items [ <NUM_LIT:7> ] . audit_issues ) , <NUM_LIT:1> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:7> ] . audit_issues [ <NUM_LIT:0> ] . score , <NUM_LIT:20> ) \n self . assertEquals ( len ( self . es_items [ <NUM_LIT:8> ] . audit_issues ) , <NUM_LIT:2> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:8> ] . audit_issues [ <NUM_LIT:0> ] . score , <NUM_LIT:6> ) \n self . assertEquals ( self . es_items [ <NUM_LIT:8> ] . audit_issues [ <NUM_LIT:1> ] . <mask0> , <NUM_LIT:10> ) \n", "gt": "score"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from security_monkey . watcher import Watcher \n from security_monkey . watcher import ChangeItem \n from security_monkey . constants import TROUBLE_REGIONS \n from security_monkey . exceptions import BotoConnectionIssue \n from security_monkey import app \n from boto . redshift import regions \n class Redshift ( Watcher ) : \n index = '<STR_LIT>' \n i_am_singular = '<STR_LIT>' \n i_am_plural = '<STR_LIT>' \n def __init__ ( self , accounts = None , debug = False ) : \n super ( Redshift , self ) . __init__ ( accounts = accounts , debug = debug ) \n def slurp ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . prep_for_slurp ( ) \n from security_monkey . common . sts_connect import connect \n item_list = [ ] \n exception_map = { } \n for account in self . accounts : \n for region in regions ( ) : \n app . logger . debug ( \"<STR_LIT>\" . format ( self . index , account , region . name ) ) \n try : \n redshift = connect ( account , '<STR_LIT>' , region = region ) \n all_clusters = [ ] \n marker = None \n while True : \n response = self . wrap_aws_rate_limited_call ( \n redshift . describe_clusters , \n marker = marker \n ) \n all_clusters . extend ( response [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n if response [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] is not None : \n marker = response [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n else : \n break \n except Exception as e : \n if region . name not in TROUBLE_REGIONS : \n exc = BotoConnectionIssue ( str ( e ) , '<STR_LIT>' , account , region . name ) \n self . slurp_exception ( ( self . index , account , region . name ) , exc , exception_map ) \n continue \n app . logger . debug ( \"<STR_LIT>\" . format ( len ( all_clusters ) , Redshift . i_am_plural ) ) \n for cluster in all_clusters : \n cluster_id = cluster [ '<STR_LIT>' ] \n if self . check_ignore_list ( cluster_id ) : \n continue \n item = RedshiftCluster ( region = region . name , account = account , name = cluster_id , config = dict ( cluster ) ) \n item_list . append ( item ) \n return item_list , exception_map \n class RedshiftCluster ( ChangeItem ) : \n def __init__ ( self , region = None , account = None , name = None , config = { } ) : \n super ( RedshiftCluster , self ) . __init__ ( \n index = Redshift . index , \n region = region , \n account = account , \n name = name , \n new_config = <mask0> ) \n", "gt": "config"}
{"input": "\n '''<STR_LIT>''' \n from __future__ import absolute_import , division , print_function \n import numpy as np \n from neo . core . container import Container \n class RecordingChannelGroup ( Container ) : \n '''<STR_LIT>''' \n _container_child_objects = ( '<STR_LIT>' , ) \n _data_child_objects = ( '<STR_LIT>' , ) \n _multi_child_objects = ( '<STR_LIT>' , ) \n _single_parent_objects = ( '<STR_LIT>' , ) \n _recommended_attrs = ( ( ( '<STR_LIT>' , np . ndarray , <NUM_LIT:1> , np . dtype ( '<STR_LIT:i>' ) ) , \n ( '<STR_LIT>' , np . ndarray , <NUM_LIT:1> , np . dtype ( '<STR_LIT:S>' ) ) ) + \n Container . _recommended_attrs ) \n def __init__ ( self , channel_names = None , channel_indexes = None , name = None , \n description = None , file_origin = None , ** annotations ) : \n '''<STR_LIT>''' \n super ( RecordingChannelGroup , self ) . __init__ ( name = name , \n description = description , \n file_origin = file_origin , \n ** annotations ) \n if channel_indexes is None : \n channel_indexes = np . array ( [ ] , dtype = np . int ) \n if channel_names is None : \n channel_names = np . array ( [ ] , dtype = '<STR_LIT:S>' ) \n self . channel_names = channel_names \n self . channel_indexes = <mask0> \n", "gt": "channel_indexes"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n import ctypes \n import os \n try : \n file \n except NameError : \n import io \n file = io . BufferedReader \n import numpy as np \n import quantities as pq \n from neo . io . baseio import BaseIO \n from neo . core import Segment , AnalogSignal , SpikeTrain , EventArray \n ns_OK = <NUM_LIT:0> \n ns_LIBERROR = - <NUM_LIT:1> \n ns_TYPEERROR = - <NUM_LIT:2> \n ns_FILEERROR = - <NUM_LIT:3> \n ns_BADFILE = - <NUM_LIT:4> \n ns_BADENTITY = - <NUM_LIT:5> \n ns_BADSOURCE = - <NUM_LIT:6> \n ns_BADINDEX = - <NUM_LIT:7> \n class NeuroshareError ( Exception ) : \n def __init__ ( self , lib , errno ) : \n self . lib = lib \n self . errno = errno \n pszMsgBuffer = ctypes . create_string_buffer ( <NUM_LIT> ) \n self . lib . ns_GetLastErrorMsg ( pszMsgBuffer , ctypes . c_uint32 ( <NUM_LIT> ) ) \n errstr = '<STR_LIT>' . format ( errno , pszMsgBuffer . value ) \n Exception . __init__ ( self , errstr ) \n class DllWithError ( ) : \n def __init__ ( self , lib ) : \n self . lib = lib \n def __getattr__ ( self , attr ) : \n f = getattr ( self . lib , attr ) \n return self . decorate_with_error ( f ) \n def decorate_with_error ( self , f ) : \n def func_with_error ( * args ) : \n errno = f ( * args ) \n if errno != ns_OK : \n raise NeuroshareError ( self . lib , errno ) \n return errno \n return func_with_error \n class NeurosharectypesIO ( BaseIO ) : \n \"\"\"<STR_LIT>\"\"\" \n is_readable = True \n is_writable = False \n supported_objects = [ Segment , AnalogSignal , EventArray , SpikeTrain ] \n readable_objects = [ Segment ] \n writeable_objects = [ ] \n has_header = False \n is_streameable = False \n read_params = { Segment : [ ] } \n write_params = None \n name = '<STR_LIT>' \n extensions = [ ] \n mode = '<STR_LIT:file>' \n def __init__ ( self , filename = '<STR_LIT>' , dllname = '<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n BaseIO . __init__ ( self ) \n self . dllname = dllname \n self . filename = filename \n def read_segment ( self , import_neuroshare_segment = True , \n lazy = False , cascade = True ) : \n \"\"\"<STR_LIT>\"\"\" \n seg = Segment ( file_origin = os . path . basename ( self . filename ) , ) \n if sys . platform . startswith ( '<STR_LIT>' ) : \n neuroshare = ctypes . windll . LoadLibrary ( self . dllname ) \n elif sys . platform . startswith ( '<STR_LIT>' ) : \n neuroshare = ctypes . cdll . LoadLibrary ( self . dllname ) \n neuroshare = DllWithError ( neuroshare ) \n info = ns_LIBRARYINFO ( ) \n neuroshare . ns_GetLibraryInfo ( ctypes . byref ( info ) , ctypes . sizeof ( info ) ) \n seg . annotate ( neuroshare_version = str ( info . dwAPIVersionMaj ) + '<STR_LIT:.>' + str ( info . dwAPIVersionMin ) ) \n if not cascade : \n return seg \n hFile = ctypes . c_uint32 ( <NUM_LIT:0> ) \n neuroshare . ns_OpenFile ( ctypes . c_char_p ( self . filename ) , ctypes . byref ( hFile ) ) \n fileinfo = ns_FILEINFO ( ) \n neuroshare . ns_GetFileInfo ( hFile , ctypes . byref ( fileinfo ) , ctypes . sizeof ( fileinfo ) ) \n for dwEntityID in range ( fileinfo . dwEntityCount ) : \n entityInfo = ns_ENTITYINFO ( ) \n neuroshare . ns_GetEntityInfo ( hFile , dwEntityID , ctypes . byref ( entityInfo ) , ctypes . sizeof ( entityInfo ) ) \n if entity_types [ entityInfo . dwEntityType ] == '<STR_LIT>' : \n pEventInfo = ns_EVENTINFO ( ) \n neuroshare . ns_GetEventInfo ( hFile , dwEntityID , ctypes . byref ( pEventInfo ) , ctypes . sizeof ( pEventInfo ) ) \n if pEventInfo . dwEventType == <NUM_LIT:0> : \n pData = ctypes . create_string_buffer ( pEventInfo . dwMaxDataLength ) \n elif pEventInfo . dwEventType == <NUM_LIT:1> : \n pData = ctypes . create_string_buffer ( pEventInfo . dwMaxDataLength ) \n elif pEventInfo . dwEventType == <NUM_LIT:2> : \n pData = ctypes . c_byte ( <NUM_LIT:0> ) \n elif pEventInfo . dwEventType == <NUM_LIT:3> : \n pData = ctypes . c_int16 ( <NUM_LIT:0> ) \n elif pEventInfo . dwEventType == <NUM_LIT:4> : \n pData = ctypes . c_int32 ( <NUM_LIT:0> ) \n pdTimeStamp = ctypes . c_double ( <NUM_LIT:0.> ) \n pdwDataRetSize = ctypes . c_uint32 ( <NUM_LIT:0> ) \n ea = EventArray ( name = str ( entityInfo . szEntityLabel ) , ) \n if not lazy : \n times = [ ] \n labels = [ ] \n for dwIndex in range ( entityInfo . dwItemCount ) : \n neuroshare . ns_GetEventData ( hFile , dwEntityID , dwIndex , \n ctypes . byref ( pdTimeStamp ) , ctypes . byref ( pData ) , \n ctypes . sizeof ( pData ) , ctypes . byref ( pdwDataRetSize ) ) \n times . append ( pdTimeStamp . value ) \n labels . append ( str ( pData . value ) ) \n ea . times = times * pq . s \n ea . labels = np . array ( labels , dtype = '<STR_LIT:S>' ) \n else : \n ea . lazy_shape = entityInfo . dwItemCount \n seg . eventarrays . append ( ea ) \n if entity_types [ entityInfo . dwEntityType ] == '<STR_LIT>' : \n pAnalogInfo = ns_ANALOGINFO ( ) \n neuroshare . ns_GetAnalogInfo ( hFile , dwEntityID , ctypes . byref ( pAnalogInfo ) , ctypes . sizeof ( pAnalogInfo ) ) \n dwIndexCount = entityInfo . dwItemCount \n if lazy : \n signal = [ ] * pq . Quantity ( <NUM_LIT:1> , pAnalogInfo . szUnits ) \n else : \n pdwContCount = ctypes . c_uint32 ( <NUM_LIT:0> ) \n pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = '<STR_LIT>' ) \n total_read = <NUM_LIT:0> \n while total_read < entityInfo . dwItemCount : \n dwStartIndex = ctypes . c_uint32 ( total_read ) \n dwStopIndex = ctypes . c_uint32 ( entityInfo . dwItemCount - total_read ) \n neuroshare . ns_GetAnalogData ( hFile , dwEntityID , dwStartIndex , \n dwStopIndex , ctypes . byref ( pdwContCount ) , pData [ total_read : ] . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n total_read += pdwContCount . value \n signal = pq . Quantity ( pData , units = pAnalogInfo . szUnits , copy = False ) \n dwIndex = <NUM_LIT:0> \n pdTime = ctypes . c_double ( <NUM_LIT:0> ) \n neuroshare . ns_GetTimeByIndex ( hFile , dwEntityID , dwIndex , ctypes . byref ( pdTime ) ) \n anaSig = AnalogSignal ( signal , \n sampling_rate = pAnalogInfo . dSampleRate * pq . Hz , \n t_start = pdTime . value * pq . s , \n name = str ( entityInfo . szEntityLabel ) , \n ) \n anaSig . annotate ( probe_info = str ( pAnalogInfo . szProbeInfo ) ) \n if lazy : \n anaSig . lazy_shape = entityInfo . dwItemCount \n seg . analogsignals . append ( anaSig ) \n if entity_types [ entityInfo . dwEntityType ] == '<STR_LIT>' and import_neuroshare_segment : \n pdwSegmentInfo = ns_SEGMENTINFO ( ) \n if not str ( entityInfo . szEntityLabel ) . startswith ( '<STR_LIT>' ) : \n continue \n neuroshare . ns_GetSegmentInfo ( hFile , dwEntityID , \n ctypes . byref ( pdwSegmentInfo ) , ctypes . sizeof ( pdwSegmentInfo ) ) \n nsource = pdwSegmentInfo . dwSourceCount \n pszMsgBuffer = ctypes . create_string_buffer ( \"<STR_LIT:U+0020>\" * <NUM_LIT> ) \n neuroshare . ns_GetLastErrorMsg ( ctypes . byref ( pszMsgBuffer ) , <NUM_LIT> ) \n for dwSourceID in range ( pdwSegmentInfo . dwSourceCount ) : \n pSourceInfo = ns_SEGSOURCEINFO ( ) \n neuroshare . ns_GetSegmentSourceInfo ( hFile , dwEntityID , dwSourceID , \n ctypes . byref ( pSourceInfo ) , ctypes . sizeof ( pSourceInfo ) ) \n if lazy : \n sptr = SpikeTrain ( times , name = str ( entityInfo . szEntityLabel ) , t_stop = <NUM_LIT:0.> * pq . s ) \n sptr . lazy_shape = entityInfo . dwItemCount \n else : \n pdTimeStamp = ctypes . c_double ( <NUM_LIT:0.> ) \n dwDataBufferSize = pdwSegmentInfo . dwMaxSampleCount * pdwSegmentInfo . dwSourceCount \n pData = np . zeros ( ( dwDataBufferSize ) , dtype = '<STR_LIT>' ) \n pdwSampleCount = ctypes . c_uint32 ( <NUM_LIT:0> ) \n pdwUnitID = ctypes . c_uint32 ( <NUM_LIT:0> ) \n nsample = int ( dwDataBufferSize ) \n times = np . empty ( ( entityInfo . dwItemCount ) , dtype = '<STR_LIT:f>' ) \n waveforms = np . empty ( ( entityInfo . dwItemCount , nsource , nsample ) , dtype = '<STR_LIT:f>' ) \n for dwIndex in range ( entityInfo . dwItemCount ) : \n neuroshare . ns_GetSegmentData ( hFile , dwEntityID , dwIndex , \n ctypes . byref ( pdTimeStamp ) , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) , \n dwDataBufferSize * <NUM_LIT:8> , ctypes . byref ( pdwSampleCount ) , \n ctypes . byref ( pdwUnitID ) ) \n times [ dwIndex ] = pdTimeStamp . value \n waveforms [ dwIndex , : , : ] = pData [ : nsample * nsource ] . reshape ( nsample , nsource ) . transpose ( ) \n sptr = SpikeTrain ( times = pq . Quantity ( times , units = '<STR_LIT:s>' , copy = False ) , \n t_stop = times . max ( ) , \n waveforms = pq . Quantity ( waveforms , units = str ( pdwSegmentInfo . szUnits ) , copy = False ) , \n left_sweep = nsample / <NUM_LIT> / float ( pdwSegmentInfo . dSampleRate ) * pq . s , \n sampling_rate = float ( pdwSegmentInfo . dSampleRate ) * pq . Hz , \n name = str ( entityInfo . szEntityLabel ) , \n ) \n seg . spiketrains . append ( sptr ) \n if entity_types [ entityInfo . dwEntityType ] == '<STR_LIT>' : \n pNeuralInfo = ns_NEURALINFO ( ) \n neuroshare . ns_GetNeuralInfo ( hFile , dwEntityID , \n ctypes . byref ( pNeuralInfo ) , ctypes . sizeof ( pNeuralInfo ) ) \n if lazy : \n times = [ ] * pq . s \n t_stop = <NUM_LIT:0> * pq . s \n else : \n pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = '<STR_LIT>' ) \n dwStartIndex = <NUM_LIT:0> \n dwIndexCount = entityInfo . dwItemCount \n neuroshare . ns_GetNeuralData ( hFile , dwEntityID , dwStartIndex , \n dwIndexCount , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n times = pData * pq . s \n t_stop = times . max ( ) \n sptr = SpikeTrain ( times , t_stop = t_stop , \n name = str ( entityInfo . szEntityLabel ) , ) \n if lazy : \n sptr . lazy_shape = entityInfo . dwItemCount \n seg . spiketrains . append ( sptr ) \n neuroshare . ns_CloseFile ( hFile ) \n seg . create_many_to_one_relationship ( ) \n return seg \n class ns_FILEDESC ( ctypes . Structure ) : \n _fields_ = [ ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:32> ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:8> ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:8> ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:16> ) , \n ] \n class ns_LIBRARYINFO ( ctypes . Structure ) : \n _fields_ = [ ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:64> ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:64> ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ns_FILEDESC * <NUM_LIT:16> ) , \n ] \n class ns_FILEINFO ( ctypes . Structure ) : \n _fields_ = [ ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:32> ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:64> ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT> ) , \n ] \n class ns_ENTITYINFO ( ctypes . Structure ) : \n _fields_ = [ ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:32> ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ] \n entity_types = { <NUM_LIT:0> : '<STR_LIT>' , \n <NUM_LIT:1> : '<STR_LIT>' , \n <NUM_LIT:2> : '<STR_LIT>' , \n <NUM_LIT:3> : '<STR_LIT>' , \n <NUM_LIT:4> : '<STR_LIT>' , \n } \n class ns_EVENTINFO ( ctypes . Structure ) : \n _fields_ = [ \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT> ) , \n ] \n class ns_ANALOGINFO ( ctypes . Structure ) : \n _fields_ = [ \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:16> ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:16> ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:16> ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT> ) , \n ] \n class ns_SEGMENTINFO ( ctypes . Structure ) : \n _fields_ = [ \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:32> ) , \n ] \n class ns_SEGSOURCEINFO ( ctypes . Structure ) : \n _fields_ = [ \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:16> ) , \n ( '<STR_LIT>' , ctypes . c_double ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT:16> ) , \n ( '<STR_LIT>' , ctypes . c_char * <NUM_LIT> ) , \n ] \n class ns_NEURALINFO ( ctypes . Structure ) : \n _fields_ = [ \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . c_uint32 ) , \n ( '<STR_LIT>' , ctypes . <mask0> * <NUM_LIT> ) , \n ] \n", "gt": "c_char"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import absolute_import , division , print_function \n from datetime import datetime \n try : \n import unittest2 as unittest \n except ImportError : \n import unittest \n import numpy as np \n import quantities as pq \n try : \n from IPython . lib . pretty import pretty \n except ImportError as err : \n HAVE_IPYTHON = False \n else : \n HAVE_IPYTHON = True \n from neo . core . segment import Segment \n from neo . core import ( AnalogSignalArray , Block , \n Epoch , EpochArray , \n RecordingChannelGroup , SpikeTrain , Unit ) \n from neo . core . container import filterdata \n from neo . test . tools import ( assert_neo_object_is_compliant , \n assert_same_sub_schema ) \n from neo . test . generate_datasets import ( fake_neo , get_fake_value , \n get_fake_values , get_annotations , \n clone_object , TEST_ANNOTATIONS ) \n class Test__generate_datasets ( unittest . TestCase ) : \n def setUp ( self ) : \n np . random . seed ( <NUM_LIT:0> ) \n self . annotations = dict ( [ ( str ( x ) , TEST_ANNOTATIONS [ x ] ) for x in \n range ( len ( TEST_ANNOTATIONS ) ) ] ) \n def test__get_fake_values ( self ) : \n self . annotations [ '<STR_LIT>' ] = <NUM_LIT:0> \n file_datetime = get_fake_value ( '<STR_LIT>' , datetime , seed = <NUM_LIT:0> ) \n rec_datetime = get_fake_value ( '<STR_LIT>' , datetime , seed = <NUM_LIT:1> ) \n index = get_fake_value ( '<STR_LIT:index>' , int , seed = <NUM_LIT:2> ) \n name = get_fake_value ( '<STR_LIT:name>' , str , seed = <NUM_LIT:3> , obj = Segment ) \n description = get_fake_value ( '<STR_LIT:description>' , str , seed = <NUM_LIT:4> , obj = '<STR_LIT>' ) \n file_origin = get_fake_value ( '<STR_LIT>' , str ) \n attrs1 = { '<STR_LIT>' : file_datetime , \n '<STR_LIT>' : rec_datetime , \n '<STR_LIT:index>' : index , \n '<STR_LIT:name>' : name , \n '<STR_LIT:description>' : description , \n '<STR_LIT>' : file_origin } \n attrs2 = attrs1 . copy ( ) \n attrs2 . update ( self . annotations ) \n res11 = get_fake_values ( Segment , annotate = False , seed = <NUM_LIT:0> ) \n res12 = get_fake_values ( '<STR_LIT>' , annotate = False , seed = <NUM_LIT:0> ) \n res21 = get_fake_values ( Segment , annotate = True , seed = <NUM_LIT:0> ) \n res22 = get_fake_values ( '<STR_LIT>' , annotate = True , seed = <NUM_LIT:0> ) \n self . assertEqual ( res11 , attrs1 ) \n self . assertEqual ( res12 , attrs1 ) \n self . assertEqual ( res21 , attrs2 ) \n self . assertEqual ( res22 , attrs2 ) \n def test__fake_neo__cascade ( self ) : \n self . annotations [ '<STR_LIT>' ] = None \n obj_type = Segment \n cascade = True \n res = fake_neo ( obj_type = obj_type , cascade = cascade ) \n self . assertTrue ( isinstance ( res , Segment ) ) \n assert_neo_object_is_compliant ( res ) \n self . assertEqual ( res . annotations , self . annotations ) \n self . assertEqual ( len ( res . analogsignalarrays ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( res . analogsignals ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( res . irregularlysampledsignals ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( res . spiketrains ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( res . spikes ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( res . events ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( res . epochs ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( res . eventarrays ) , <NUM_LIT:1> ) \n self . assertEqual ( len ( res . epocharrays ) , <NUM_LIT:1> ) \n for child in res . children : \n del child . annotations [ '<STR_LIT:i>' ] \n del child . annotations [ '<STR_LIT>' ] \n self . assertEqual ( res . analogsignalarrays [ <NUM_LIT:0> ] . annotations , \n self . annotations ) \n self . assertEqual ( res . analogsignals [ <NUM_LIT:0> ] . annotations , \n self . annotations ) \n self . assertEqual ( res . irregularlysampledsignals [ <NUM_LIT:0> ] . annotations , \n self . annotations ) \n self . assertEqual ( res . spiketrains [ <NUM_LIT:0> ] . annotations , \n self . annotations ) \n self . assertEqual ( res . spikes [ <NUM_LIT:0> ] . annotations , \n self . annotations ) \n self . assertEqual ( res . events [ <NUM_LIT:0> ] . annotations , \n self . annotations ) \n self . assertEqual ( res . epochs [ <NUM_LIT:0> ] . annotations , \n self . annotations ) \n self . assertEqual ( res . eventarrays [ <NUM_LIT:0> ] . annotations , \n self . annotations ) \n self . assertEqual ( res . epocharrays [ <NUM_LIT:0> ] . annotations , \n self . annotations ) \n def test__fake_neo__nocascade ( self ) : \n self . annotations [ '<STR_LIT>' ] = None \n obj_type = '<STR_LIT>' \n cascade = False \n res = fake_neo ( obj_type = obj_type , cascade = cascade ) \n self . assertTrue ( isinstance ( res , Segment ) ) \n assert_neo_object_is_compliant ( res ) \n self . assertEqual ( res . annotations , self . annotations ) \n self . assertEqual ( len ( res . analogsignalarrays ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( res . analogsignals ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( res . irregularlysampledsignals ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( res . spiketrains ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( res . spikes ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( res . events ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( res . epochs ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( res . eventarrays ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( res . epocharrays ) , <NUM_LIT:0> ) \n class TestSegment ( unittest . TestCase ) : \n def setUp ( self ) : \n self . nchildren = <NUM_LIT:2> \n blk = fake_neo ( Block , seed = <NUM_LIT:0> , n = self . nchildren ) \n self . unit1 , self . unit2 , self . unit3 , self . unit4 = blk . list_units \n self . seg1 , self . seg2 = blk . segments \n self . targobj = self . seg1 \n self . seed1 = self . seg1 . annotations [ '<STR_LIT>' ] \n self . seed2 = self . seg2 . annotations [ '<STR_LIT>' ] \n del self . seg1 . annotations [ '<STR_LIT:i>' ] \n del self . seg2 . annotations [ '<STR_LIT:i>' ] \n del self . seg1 . annotations [ '<STR_LIT>' ] \n del self . seg2 . annotations [ '<STR_LIT>' ] \n self . sigs1 = self . seg1 . analogsignals \n self . sigs2 = self . seg2 . analogsignals \n self . sigarrs1 = self . seg1 . analogsignalarrays \n self . sigarrs2 = self . seg2 . analogsignalarrays \n self . irsigs1 = self . seg1 . irregularlysampledsignals \n self . irsigs2 = self . seg2 . irregularlysampledsignals \n self . spikes1 = self . seg1 . spikes \n self . spikes2 = self . seg2 . spikes \n self . trains1 = self . seg1 . spiketrains \n self . trains2 = self . seg2 . spiketrains \n self . epcs1 = self . seg1 . epochs \n self . epcs2 = self . seg2 . epochs \n self . epcas1 = self . seg1 . epocharrays \n self . epcas2 = self . seg2 . epocharrays \n self . evts1 = self . seg1 . events \n self . evts2 = self . seg2 . events \n self . evtas1 = self . seg1 . eventarrays \n self . evtas2 = self . seg2 . eventarrays \n self . sigs1a = clone_object ( self . sigs1 ) \n self . sigarrs1a = clone_object ( self . sigarrs1 , n = <NUM_LIT:2> ) \n self . irsigs1a = clone_object ( self . irsigs1 ) \n self . spikes1a = clone_object ( self . spikes1 ) \n self . trains1a = clone_object ( self . trains1 ) \n self . epcs1a = clone_object ( self . epcs1 ) \n self . epcas1a = clone_object ( self . epcas1 ) \n self . evts1a = clone_object ( self . evts1 ) \n self . evtas1a = clone_object ( self . evtas1 ) \n for obj , obja in zip ( self . sigs1 + self . sigarrs1 , \n self . sigs1a + self . sigarrs1a ) : \n obja . channel_index = obj . channel_index \n def test_init ( self ) : \n seg = Segment ( name = '<STR_LIT>' , index = <NUM_LIT:3> ) \n assert_neo_object_is_compliant ( seg ) \n self . assertEqual ( seg . name , '<STR_LIT>' ) \n self . assertEqual ( seg . file_origin , None ) \n self . assertEqual ( seg . index , <NUM_LIT:3> ) \n def check_creation ( self , seg ) : \n assert_neo_object_is_compliant ( seg ) \n seed = seg . annotations [ '<STR_LIT>' ] \n targ0 = get_fake_value ( '<STR_LIT>' , datetime , seed = seed + <NUM_LIT:0> ) \n self . assertEqual ( seg . file_datetime , targ0 ) \n targ1 = get_fake_value ( '<STR_LIT>' , datetime , seed = seed + <NUM_LIT:1> ) \n self . assertEqual ( seg . rec_datetime , targ1 ) \n targ2 = get_fake_value ( '<STR_LIT:index>' , int , seed = seed + <NUM_LIT:2> ) \n self . assertEqual ( seg . index , targ2 ) \n targ3 = get_fake_value ( '<STR_LIT:name>' , str , seed = seed + <NUM_LIT:3> , obj = Segment ) \n self . assertEqual ( seg . name , targ3 ) \n targ4 = get_fake_value ( '<STR_LIT:description>' , str , \n seed = seed + <NUM_LIT:4> , obj = Segment ) \n self . assertEqual ( seg . description , targ4 ) \n targ5 = get_fake_value ( '<STR_LIT>' , str ) \n self . assertEqual ( seg . file_origin , targ5 ) \n targ6 = get_annotations ( ) \n targ6 [ '<STR_LIT>' ] = seed \n self . assertEqual ( seg . annotations , targ6 ) \n self . assertTrue ( hasattr ( seg , '<STR_LIT>' ) ) \n self . assertTrue ( hasattr ( seg , '<STR_LIT>' ) ) \n self . assertTrue ( hasattr ( seg , '<STR_LIT>' ) ) \n self . assertTrue ( hasattr ( seg , '<STR_LIT>' ) ) \n self . assertTrue ( hasattr ( seg , '<STR_LIT>' ) ) \n self . assertTrue ( hasattr ( seg , '<STR_LIT>' ) ) \n self . assertTrue ( hasattr ( seg , '<STR_LIT>' ) ) \n self . assertTrue ( hasattr ( seg , '<STR_LIT>' ) ) \n self . assertTrue ( hasattr ( seg , '<STR_LIT>' ) ) \n self . assertEqual ( len ( seg . analogsignals ) , self . nchildren ** <NUM_LIT:2> ) \n self . assertEqual ( len ( seg . analogsignalarrays ) , self . nchildren ) \n self . assertEqual ( len ( seg . irregularlysampledsignals ) , self . nchildren ** <NUM_LIT:2> ) \n self . assertEqual ( len ( seg . epochs ) , self . nchildren ) \n self . assertEqual ( len ( seg . epocharrays ) , self . nchildren ) \n self . assertEqual ( len ( seg . events ) , self . nchildren ) \n self . assertEqual ( len ( seg . eventarrays ) , self . nchildren ) \n self . assertEqual ( len ( seg . spikes ) , self . nchildren ** <NUM_LIT:2> ) \n self . assertEqual ( len ( seg . spiketrains ) , self . nchildren ** <NUM_LIT:2> ) \n def test__creation ( self ) : \n self . check_creation ( self . seg1 ) \n self . check_creation ( self . seg2 ) \n def test__merge ( self ) : \n seg1a = fake_neo ( Block , seed = self . seed1 , n = self . nchildren ) . segments [ <NUM_LIT:0> ] \n assert_same_sub_schema ( self . seg1 , seg1a ) \n seg1a . spikes . append ( self . spikes2 [ <NUM_LIT:0> ] ) \n seg1a . epocharrays . append ( self . epcas2 [ <NUM_LIT:0> ] ) \n seg1a . annotate ( seed = self . seed2 ) \n seg1a . merge ( self . seg2 ) \n self . check_creation ( self . seg2 ) \n assert_same_sub_schema ( self . sigs1a + self . sigs2 , seg1a . analogsignals ) \n assert_same_sub_schema ( self . sigarrs1a + self . sigarrs2 , \n seg1a . analogsignalarrays ) \n assert_same_sub_schema ( self . irsigs1a + self . irsigs2 , \n seg1a . irregularlysampledsignals ) \n assert_same_sub_schema ( self . epcs1 + self . epcs2 , seg1a . epochs ) \n assert_same_sub_schema ( self . epcas1 + self . epcas2 , seg1a . epocharrays ) \n assert_same_sub_schema ( self . evts1 + self . evts2 , seg1a . events ) \n assert_same_sub_schema ( self . evtas1 + self . evtas2 , seg1a . eventarrays ) \n assert_same_sub_schema ( self . spikes1 + self . spikes2 , seg1a . spikes ) \n assert_same_sub_schema ( self . trains1 + self . trains2 , seg1a . spiketrains ) \n def test__children ( self ) : \n blk = Block ( name = '<STR_LIT>' ) \n blk . segments = [ self . seg1 ] \n blk . create_many_to_one_relationship ( force = True ) \n assert_neo_object_is_compliant ( self . seg1 ) \n assert_neo_object_is_compliant ( blk ) \n childobjs = ( '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' ) \n childconts = ( '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' ) \n self . assertEqual ( self . seg1 . _container_child_objects , ( ) ) \n self . assertEqual ( self . seg1 . _data_child_objects , childobjs ) \n self . assertEqual ( self . seg1 . _single_parent_objects , ( '<STR_LIT>' , ) ) \n self . assertEqual ( self . seg1 . _multi_child_objects , ( ) ) \n self . assertEqual ( self . seg1 . _multi_parent_objects , ( ) ) \n self . assertEqual ( self . seg1 . _child_properties , ( ) ) \n self . assertEqual ( self . seg1 . _single_child_objects , childobjs ) \n self . assertEqual ( self . seg1 . _container_child_containers , ( ) ) \n self . assertEqual ( self . seg1 . _data_child_containers , childconts ) \n self . assertEqual ( self . seg1 . _single_child_containers , childconts ) \n self . assertEqual ( self . seg1 . _single_parent_containers , ( '<STR_LIT>' , ) ) \n self . assertEqual ( self . seg1 . _multi_child_containers , ( ) ) \n self . assertEqual ( self . seg1 . _multi_parent_containers , ( ) ) \n self . assertEqual ( self . seg1 . _child_objects , childobjs ) \n self . assertEqual ( self . seg1 . _child_containers , childconts ) \n self . assertEqual ( self . seg1 . _parent_objects , ( '<STR_LIT>' , ) ) \n self . assertEqual ( self . seg1 . _parent_containers , ( '<STR_LIT>' , ) ) \n totchildren = ( self . nchildren * <NUM_LIT:2> * <NUM_LIT:2> + \n self . nchildren + \n <NUM_LIT:2> * ( self . nchildren ** <NUM_LIT:2> ) + \n <NUM_LIT:2> * ( self . nchildren ** <NUM_LIT:2> ) ) \n self . assertEqual ( len ( self . seg1 . _single_children ) , totchildren ) \n self . assertEqual ( len ( self . seg1 . data_children ) , totchildren ) \n self . assertEqual ( len ( self . seg1 . children ) , totchildren ) \n self . assertEqual ( len ( self . seg1 . data_children_recur ) , totchildren ) \n self . assertEqual ( len ( self . seg1 . children_recur ) , totchildren ) \n self . assertEqual ( len ( self . seg1 . _multi_children ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( self . seg1 . container_children ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( self . seg1 . container_children_recur ) , <NUM_LIT:0> ) \n children = ( self . sigs1a + self . sigarrs1a + \n self . epcs1a + self . epcas1a + \n self . evts1a + self . evtas1a + \n self . irsigs1a + \n self . spikes1a + self . trains1a ) \n assert_same_sub_schema ( list ( self . seg1 . _single_children ) , children ) \n assert_same_sub_schema ( list ( self . seg1 . data_children ) , children ) \n assert_same_sub_schema ( list ( self . seg1 . data_children_recur ) , children ) \n assert_same_sub_schema ( list ( self . seg1 . children ) , children ) \n assert_same_sub_schema ( list ( self . seg1 . children_recur ) , children ) \n self . assertEqual ( len ( self . seg1 . parents ) , <NUM_LIT:1> ) \n self . assertEqual ( self . seg1 . parents [ <NUM_LIT:0> ] . name , '<STR_LIT>' ) \n def test__size ( self ) : \n targ1 = { \"<STR_LIT>\" : self . nchildren , \"<STR_LIT>\" : self . nchildren , \n \"<STR_LIT>\" : self . nchildren ** <NUM_LIT:2> , \n \"<STR_LIT>\" : self . nchildren ** <NUM_LIT:2> , \n \"<STR_LIT>\" : self . nchildren ** <NUM_LIT:2> , \n \"<STR_LIT>\" : self . nchildren ** <NUM_LIT:2> , \n \"<STR_LIT>\" : self . nchildren , \"<STR_LIT>\" : self . nchildren , \n \"<STR_LIT>\" : self . nchildren } \n self . assertEqual ( self . targobj . size , targ1 ) \n def test__filter_none ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( ) \n res1 = self . targobj . filter ( { } ) \n res2 = self . targobj . filter ( [ ] ) \n res3 = self . targobj . filter ( [ { } ] ) \n res4 = self . targobj . filter ( [ { } , { } ] ) \n res5 = self . targobj . filter ( [ { } , { } ] ) \n res6 = self . targobj . filter ( targdict = { } ) \n res7 = self . targobj . filter ( targdict = [ ] ) \n res8 = self . targobj . filter ( targdict = [ { } ] ) \n res9 = self . targobj . filter ( targdict = [ { } , { } ] ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n assert_same_sub_schema ( res3 , targ ) \n assert_same_sub_schema ( res4 , targ ) \n assert_same_sub_schema ( res5 , targ ) \n assert_same_sub_schema ( res6 , targ ) \n assert_same_sub_schema ( res7 , targ ) \n assert_same_sub_schema ( res8 , targ ) \n assert_same_sub_schema ( res9 , targ ) \n def test__filter_annotation_single ( self ) : \n targ = ( self . sigs1a + self . sigarrs1a + \n [ self . epcs1a [ <NUM_LIT:0> ] , self . epcas1a [ <NUM_LIT:0> ] ] + \n [ self . evts1a [ <NUM_LIT:0> ] , self . evtas1a [ <NUM_LIT:0> ] ] + \n self . irsigs1a + \n self . spikes1a + self . trains1a ) \n res0 = self . targobj . filter ( j = <NUM_LIT:0> ) \n res1 = self . targobj . filter ( { '<STR_LIT>' : <NUM_LIT:0> } ) \n res2 = self . targobj . filter ( targdict = { '<STR_LIT>' : <NUM_LIT:0> } ) \n res3 = self . targobj . filter ( [ { '<STR_LIT>' : <NUM_LIT:0> } ] ) \n res4 = self . targobj . filter ( targdict = [ { '<STR_LIT>' : <NUM_LIT:0> } ] ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n assert_same_sub_schema ( res3 , targ ) \n assert_same_sub_schema ( res4 , targ ) \n def test__filter_single_annotation_nores ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( j = <NUM_LIT:5> ) \n res1 = self . targobj . filter ( { '<STR_LIT>' : <NUM_LIT:5> } ) \n res2 = self . targobj . filter ( targdict = { '<STR_LIT>' : <NUM_LIT:5> } ) \n res3 = self . targobj . filter ( [ { '<STR_LIT>' : <NUM_LIT:5> } ] ) \n res4 = self . targobj . filter ( targdict = [ { '<STR_LIT>' : <NUM_LIT:5> } ] ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n assert_same_sub_schema ( res3 , targ ) \n assert_same_sub_schema ( res4 , targ ) \n def test__filter_attribute_single ( self ) : \n targ = [ self . epcs1a [ <NUM_LIT:1> ] ] \n res0 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name ) \n res1 = self . targobj . filter ( { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name } ) \n res2 = self . targobj . filter ( targdict = { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name } ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n def test__filter_attribute_single_nores ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( name = self . epcs2 [ <NUM_LIT:0> ] . name ) \n res1 = self . targobj . filter ( { '<STR_LIT:name>' : self . epcs2 [ <NUM_LIT:0> ] . name } ) \n res2 = self . targobj . filter ( targdict = { '<STR_LIT:name>' : self . epcs2 [ <NUM_LIT:0> ] . name } ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n def test__filter_multi ( self ) : \n targ = ( self . sigs1a + self . sigarrs1a + \n [ self . epcs1a [ <NUM_LIT:0> ] , self . epcas1a [ <NUM_LIT:0> ] ] + \n [ self . evts1a [ <NUM_LIT:0> ] , self . evtas1a [ <NUM_LIT:0> ] ] + \n self . irsigs1a + \n self . spikes1a + self . trains1a + \n [ self . epcs1a [ <NUM_LIT:1> ] ] ) \n res0 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , j = <NUM_LIT:0> ) \n res1 = self . targobj . filter ( { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name , '<STR_LIT>' : <NUM_LIT:0> } ) \n res2 = self . targobj . filter ( targdict = { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name , \n '<STR_LIT>' : <NUM_LIT:0> } ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n def test__filter_multi_nores ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( [ { '<STR_LIT>' : <NUM_LIT:0> } , { } ] ) \n res1 = self . targobj . filter ( { } , ttype = <NUM_LIT:0> ) \n res2 = self . targobj . filter ( [ { } ] , ttype = <NUM_LIT:0> ) \n res3 = self . targobj . filter ( { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name } , j = <NUM_LIT:0> ) \n res4 = self . targobj . filter ( targdict = { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name } , \n j = <NUM_LIT:0> ) \n res5 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , \n targdict = { '<STR_LIT>' : <NUM_LIT:0> } ) \n res6 = self . targobj . filter ( name = self . epcs2 [ <NUM_LIT:0> ] . name , j = <NUM_LIT:5> ) \n res7 = self . targobj . filter ( { '<STR_LIT:name>' : self . epcs2 [ <NUM_LIT:1> ] . name , '<STR_LIT>' : <NUM_LIT:5> } ) \n res8 = self . targobj . filter ( targdict = { '<STR_LIT:name>' : self . epcs2 [ <NUM_LIT:1> ] . name , \n '<STR_LIT>' : <NUM_LIT:5> } ) \n res9 = self . targobj . filter ( { '<STR_LIT:name>' : self . epcs2 [ <NUM_LIT:1> ] . name } , j = <NUM_LIT:5> ) \n res10 = self . targobj . filter ( targdict = { '<STR_LIT:name>' : self . epcs2 [ <NUM_LIT:1> ] . name } , \n j = <NUM_LIT:5> ) \n res11 = self . targobj . filter ( name = self . epcs2 [ <NUM_LIT:1> ] . name , \n targdict = { '<STR_LIT>' : <NUM_LIT:5> } ) \n res12 = self . targobj . filter ( { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name } , j = <NUM_LIT:5> ) \n res13 = self . targobj . filter ( targdict = { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name } , \n j = <NUM_LIT:5> ) \n res14 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , \n targdict = { '<STR_LIT>' : <NUM_LIT:5> } ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n assert_same_sub_schema ( res3 , targ ) \n assert_same_sub_schema ( res4 , targ ) \n assert_same_sub_schema ( res5 , targ ) \n assert_same_sub_schema ( res6 , targ ) \n assert_same_sub_schema ( res7 , targ ) \n assert_same_sub_schema ( res8 , targ ) \n assert_same_sub_schema ( res9 , targ ) \n assert_same_sub_schema ( res10 , targ ) \n assert_same_sub_schema ( res11 , targ ) \n assert_same_sub_schema ( res12 , targ ) \n assert_same_sub_schema ( res13 , targ ) \n assert_same_sub_schema ( res14 , targ ) \n def test__filter_multi_partres ( self ) : \n targ = [ self . epcs1a [ <NUM_LIT:1> ] ] \n res0 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , j = <NUM_LIT:5> ) \n res1 = self . targobj . filter ( { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name , '<STR_LIT>' : <NUM_LIT:5> } ) \n res2 = self . targobj . filter ( targdict = { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name , \n '<STR_LIT>' : <NUM_LIT:5> } ) \n res3 = self . targobj . filter ( [ { '<STR_LIT>' : <NUM_LIT:1> } , { '<STR_LIT:i>' : <NUM_LIT:2> } ] ) \n res4 = self . targobj . filter ( { '<STR_LIT>' : <NUM_LIT:1> } , i = <NUM_LIT:2> ) \n res5 = self . targobj . filter ( [ { '<STR_LIT>' : <NUM_LIT:1> } ] , i = <NUM_LIT:2> ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n assert_same_sub_schema ( res3 , targ ) \n assert_same_sub_schema ( res4 , targ ) \n assert_same_sub_schema ( res5 , targ ) \n def test__filter_single_annotation_obj_single ( self ) : \n targ = [ self . epcs1a [ <NUM_LIT:1> ] ] \n res0 = self . targobj . filter ( j = <NUM_LIT:1> , objects = '<STR_LIT>' ) \n res1 = self . targobj . filter ( j = <NUM_LIT:1> , objects = Epoch ) \n res2 = self . targobj . filter ( j = <NUM_LIT:1> , objects = [ '<STR_LIT>' ] ) \n res3 = self . targobj . filter ( j = <NUM_LIT:1> , objects = [ Epoch ] ) \n res4 = self . targobj . filter ( j = <NUM_LIT:1> , objects = [ Epoch , \n RecordingChannelGroup ] ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n assert_same_sub_schema ( res3 , targ ) \n assert_same_sub_schema ( res4 , targ ) \n def test__filter_single_annotation_obj_multi ( self ) : \n targ = [ self . epcs1a [ <NUM_LIT:1> ] , self . epcas1a [ <NUM_LIT:1> ] ] \n res0 = self . targobj . filter ( j = <NUM_LIT:1> , objects = [ '<STR_LIT>' , EpochArray ] ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_annotation_obj_none ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( j = <NUM_LIT:1> , objects = RecordingChannelGroup ) \n res1 = self . targobj . filter ( j = <NUM_LIT:1> , objects = '<STR_LIT>' ) \n res2 = self . targobj . filter ( j = <NUM_LIT:1> , objects = [ ] ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n def test__filter_single_annotation_norecur ( self ) : \n targ = [ self . epcs1a [ <NUM_LIT:1> ] , self . epcas1a [ <NUM_LIT:1> ] , \n self . evts1a [ <NUM_LIT:1> ] , self . evtas1a [ <NUM_LIT:1> ] ] \n res0 = self . targobj . filter ( j = <NUM_LIT:1> , \n recursive = False ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_attribute_norecur ( self ) : \n targ = [ self . epcs1a [ <NUM_LIT:1> ] ] \n res0 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , \n recursive = False ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_annotation_nodata ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( j = <NUM_LIT:0> , \n data = False ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_attribute_nodata ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , \n data = False ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_annotation_nodata_norecur ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( j = <NUM_LIT:0> , \n data = False , recursive = False ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_attribute_nodata_norecur ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , \n data = False , recursive = False ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_annotation_container ( self ) : \n targ = [ self . epcs1a [ <NUM_LIT:1> ] , self . epcas1a [ <NUM_LIT:1> ] , \n self . evts1a [ <NUM_LIT:1> ] , self . evtas1a [ <NUM_LIT:1> ] ] \n res0 = self . targobj . filter ( j = <NUM_LIT:1> , \n container = True ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_attribute_container ( self ) : \n targ = [ self . epcs1a [ <NUM_LIT:1> ] ] \n res0 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , \n container = True ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_annotation_container_norecur ( self ) : \n targ = [ self . epcs1a [ <NUM_LIT:1> ] , self . epcas1a [ <NUM_LIT:1> ] , \n self . evts1a [ <NUM_LIT:1> ] , self . evtas1a [ <NUM_LIT:1> ] ] \n res0 = self . targobj . filter ( j = <NUM_LIT:1> , \n container = True , recursive = False ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_attribute_container_norecur ( self ) : \n targ = [ self . epcs1a [ <NUM_LIT:1> ] ] \n res0 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , \n container = True , recursive = False ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_annotation_nodata_container ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( j = <NUM_LIT:0> , \n data = False , container = True ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_attribute_nodata_container ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , \n data = False , container = True ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_annotation_nodata_container_norecur ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( j = <NUM_LIT:0> , \n data = False , container = True , \n recursive = False ) \n assert_same_sub_schema ( res0 , targ ) \n def test__filter_single_attribute_nodata_container_norecur ( self ) : \n targ = [ ] \n res0 = self . targobj . filter ( name = self . epcs1a [ <NUM_LIT:1> ] . name , \n data = False , container = True , \n recursive = False ) \n assert_same_sub_schema ( res0 , targ ) \n data = self . targobj . children_recur \n targ = ( self . sigs1a + self . sigarrs1a + \n [ self . epcs1a [ <NUM_LIT:0> ] , self . epcas1a [ <NUM_LIT:0> ] ] + \n [ self . evts1a [ <NUM_LIT:0> ] , self . evtas1a [ <NUM_LIT:0> ] ] + \n self . irsigs1a + \n self . spikes1a + self . trains1a + \n [ self . epcs1a [ <NUM_LIT:1> ] ] ) \n res0 = filterdata ( data , name = self . epcs1a [ <NUM_LIT:1> ] . name , j = <NUM_LIT:0> ) \n res1 = filterdata ( data , { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name , '<STR_LIT>' : <NUM_LIT:0> } ) \n res2 = filterdata ( data , targdict = { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name , '<STR_LIT>' : <NUM_LIT:0> } ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n def test__filterdata_multi_nores ( self ) : \n data = self . targobj . children_recur \n targ = [ ] \n res0 = filterdata ( data , [ { '<STR_LIT>' : <NUM_LIT:0> } , { } ] ) \n res1 = filterdata ( data , { } , ttype = <NUM_LIT:0> ) \n res2 = filterdata ( data , [ { } ] , ttype = <NUM_LIT:0> ) \n res3 = filterdata ( data , { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name } , j = <NUM_LIT:0> ) \n res4 = filterdata ( data , targdict = { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name } , j = <NUM_LIT:0> ) \n res5 = filterdata ( data , name = self . epcs1a [ <NUM_LIT:1> ] . name , targdict = { '<STR_LIT>' : <NUM_LIT:0> } ) \n res6 = filterdata ( data , name = self . epcs2 [ <NUM_LIT:0> ] . name , j = <NUM_LIT:5> ) \n res7 = filterdata ( data , { '<STR_LIT:name>' : self . epcs2 [ <NUM_LIT:1> ] . name , '<STR_LIT>' : <NUM_LIT:5> } ) \n res8 = filterdata ( data , targdict = { '<STR_LIT:name>' : self . epcs2 [ <NUM_LIT:1> ] . name , '<STR_LIT>' : <NUM_LIT:5> } ) \n res9 = filterdata ( data , { '<STR_LIT:name>' : self . epcs2 [ <NUM_LIT:1> ] . name } , j = <NUM_LIT:5> ) \n res10 = filterdata ( data , targdict = { '<STR_LIT:name>' : self . epcs2 [ <NUM_LIT:1> ] . name } , j = <NUM_LIT:5> ) \n res11 = filterdata ( data , name = self . epcs2 [ <NUM_LIT:1> ] . name , targdict = { '<STR_LIT>' : <NUM_LIT:5> } ) \n res12 = filterdata ( data , { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name } , j = <NUM_LIT:5> ) \n res13 = filterdata ( data , targdict = { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name } , j = <NUM_LIT:5> ) \n res14 = filterdata ( data , name = self . epcs1a [ <NUM_LIT:1> ] . name , targdict = { '<STR_LIT>' : <NUM_LIT:5> } ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n assert_same_sub_schema ( res3 , targ ) \n assert_same_sub_schema ( res4 , targ ) \n assert_same_sub_schema ( res5 , targ ) \n assert_same_sub_schema ( res6 , targ ) \n assert_same_sub_schema ( res7 , targ ) \n assert_same_sub_schema ( res8 , targ ) \n assert_same_sub_schema ( res9 , targ ) \n assert_same_sub_schema ( res10 , targ ) \n assert_same_sub_schema ( res11 , targ ) \n assert_same_sub_schema ( res12 , targ ) \n assert_same_sub_schema ( res13 , targ ) \n assert_same_sub_schema ( res14 , targ ) \n def test__filterdata_multi_partres ( self ) : \n data = self . targobj . children_recur \n targ = [ self . epcs1a [ <NUM_LIT:1> ] ] \n res0 = filterdata ( data , name = self . epcs1a [ <NUM_LIT:1> ] . name , j = <NUM_LIT:5> ) \n res1 = filterdata ( data , { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name , '<STR_LIT>' : <NUM_LIT:5> } ) \n res2 = filterdata ( data , targdict = { '<STR_LIT:name>' : self . epcs1a [ <NUM_LIT:1> ] . name , '<STR_LIT>' : <NUM_LIT:5> } ) \n res3 = filterdata ( data , [ { '<STR_LIT>' : <NUM_LIT:1> } , { '<STR_LIT:i>' : <NUM_LIT:2> } ] ) \n res4 = filterdata ( data , { '<STR_LIT>' : <NUM_LIT:1> } , i = <NUM_LIT:2> ) \n res5 = filterdata ( data , [ { '<STR_LIT>' : <NUM_LIT:1> } ] , i = <NUM_LIT:2> ) \n assert_same_sub_schema ( res0 , targ ) \n assert_same_sub_schema ( res1 , targ ) \n assert_same_sub_schema ( res2 , targ ) \n assert_same_sub_schema ( res3 , targ ) \n assert_same_sub_schema ( res4 , targ ) \n assert_same_sub_schema ( res5 , targ ) \n @ unittest . skipUnless ( HAVE_IPYTHON , \"<STR_LIT>\" ) \n def test__pretty ( self ) : \n ann = get_annotations ( ) \n ann [ '<STR_LIT>' ] = self . seed1 \n ann = pretty ( ann ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n res = pretty ( self . seg1 ) \n sig0 = pretty ( self . sigs1 [ <NUM_LIT:0> ] ) \n sig1 = pretty ( self . sigs1 [ <NUM_LIT:1> ] ) \n sig2 = pretty ( self . sigs1 [ <NUM_LIT:2> ] ) \n sig3 = pretty ( self . sigs1 [ <NUM_LIT:3> ] ) \n sig0 = sig0 . replace ( '<STR_LIT:\\n>' , '<STR_LIT>' ) \n sig1 = sig1 . replace ( '<STR_LIT:\\n>' , '<STR_LIT>' ) \n sig2 = sig2 . replace ( '<STR_LIT:\\n>' , '<STR_LIT>' ) \n sig3 = sig3 . replace ( '<STR_LIT:\\n>' , '<STR_LIT>' ) \n sigarr0 = pretty ( self . sigarrs1 [ <NUM_LIT:0> ] ) \n sigarr1 = pretty ( self . sigarrs1 [ <NUM_LIT:1> ] ) \n sigarr0 = sigarr0 . replace ( '<STR_LIT:\\n>' , '<STR_LIT>' ) \n sigarr1 = sigarr1 . replace ( '<STR_LIT:\\n>' , '<STR_LIT>' ) \n targ = ( \"<STR_LIT>\" + \n ( \"<STR_LIT>\" % \n ( len ( self . sigs1a ) , len ( self . sigarrs1a ) ) ) + \n ( \"<STR_LIT>\" % \n ( len ( self . epcs1a ) , len ( self . epcas1a ) ) ) + \n ( \"<STR_LIT>\" % \n ( len ( self . evts1a ) , len ( self . evtas1a ) ) ) + \n ( \"<STR_LIT>\" % \n len ( self . irsigs1a ) ) + \n ( \"<STR_LIT>\" % \n ( len ( self . spikes1a ) , len ( self . trains1a ) ) ) + \n ( \"<STR_LIT>\" % \n ( self . seg1 . name , self . seg1 . description ) \n ) + \n ( \"<STR_LIT>\" % ann ) + \n ( \"<STR_LIT>\" % len ( self . sigs1a ) ) + \n ( '<STR_LIT>' % ( <NUM_LIT:0> , sig0 ) ) + \n ( '<STR_LIT>' % ( <NUM_LIT:1> , sig1 ) ) + \n ( '<STR_LIT>' % ( <NUM_LIT:2> , sig2 ) ) + \n ( '<STR_LIT>' % ( <NUM_LIT:3> , sig3 ) ) + \n ( \"<STR_LIT>\" % len ( self . sigarrs1a ) ) + \n ( '<STR_LIT>' % ( <NUM_LIT:0> , sigarr0 ) ) + \n ( '<STR_LIT>' % ( <NUM_LIT:1> , sigarr1 ) ) ) \n self . assertEqual ( res , targ ) \n def test__construct_subsegment_by_unit ( self ) : \n nb_seg = <NUM_LIT:3> \n nb_unit = <NUM_LIT:7> \n unit_with_sig = np . array ( [ <NUM_LIT:0> , <NUM_LIT:2> , <NUM_LIT:5> ] ) \n signal_types = [ '<STR_LIT>' , '<STR_LIT>' ] \n sig_len = <NUM_LIT:100> \n rcgs = [ RecordingChannelGroup ( name = '<STR_LIT>' , \n channel_indexes = unit_with_sig ) , \n RecordingChannelGroup ( name = '<STR_LIT>' , \n channel_indexes = unit_with_sig ) ] \n all_unit = [ ] \n for u in range ( nb_unit ) : \n un = Unit ( name = '<STR_LIT>' % u , channel_indexes = np . array ( [ u ] ) ) \n assert_neo_object_is_compliant ( un ) \n all_unit . append ( un ) \n blk = Block ( ) \n blk . recordingchannelgroups = rcgs \n for s in range ( nb_seg ) : \n seg = Segment ( name = '<STR_LIT>' % s ) \n for j in range ( nb_unit ) : \n st = SpikeTrain ( [ <NUM_LIT:1> , <NUM_LIT:2> ] , units = '<STR_LIT>' , \n t_start = <NUM_LIT:0.> , t_stop = <NUM_LIT:10> ) \n st . unit = all_unit [ j ] \n for t in signal_types : \n anasigarr = AnalogSignalArray ( np . zeros ( ( sig_len , \n len ( unit_with_sig ) ) ) , \n units = '<STR_LIT>' , \n sampling_rate = <NUM_LIT> * pq . Hz , \n channel_indexes = unit_with_sig ) \n seg . analogsignalarrays . append ( anasigarr ) \n blk . create_many_to_one_relationship ( ) \n for unit in all_unit : \n assert_neo_object_is_compliant ( unit ) \n for rcg in rcgs : \n assert_neo_object_is_compliant ( rcg ) \n assert_neo_object_is_compliant ( blk ) \n newseg = seg . construct_subsegment_by_unit ( all_unit [ : <NUM_LIT:4> ] ) \n assert_neo_object_is_compliant ( newseg ) \n def test_segment_take_spikes_by_unit ( self ) : \n result1 = self . seg1 . take_spikes_by_unit ( ) \n result21 = self . seg1 . take_spikes_by_unit ( [ self . unit1 ] ) \n result22 = self . seg1 . take_spikes_by_unit ( [ self . unit2 ] ) \n self . assertEqual ( result1 , [ ] ) \n assert_same_sub_schema ( result21 , [ self . spikes1a [ <NUM_LIT:0> ] ] ) \n assert_same_sub_schema ( result22 , [ self . spikes1a [ <NUM_LIT:1> ] ] ) \n def test_segment_take_spiketrains_by_unit ( self ) : \n result1 = self . seg1 . take_spiketrains_by_unit ( ) \n result21 = self . seg1 . take_spiketrains_by_unit ( [ self . unit1 ] ) \n result22 = self . seg1 . take_spiketrains_by_unit ( [ self . unit2 ] ) \n self . assertEqual ( result1 , [ ] ) \n assert_same_sub_schema ( result21 , [ self . trains1a [ <NUM_LIT:0> ] ] ) \n assert_same_sub_schema ( result22 , [ self . trains1a [ <NUM_LIT:1> ] ] ) \n def test_segment_take_analogsignal_by_unit ( self ) : \n result1 = self . seg1 . take_analogsignal_by_unit ( ) \n result21 = self . seg1 . take_analogsignal_by_unit ( [ self . unit1 ] ) \n result22 = self . seg1 . take_analogsignal_by_unit ( [ self . unit2 ] ) \n self . assertEqual ( result1 , [ ] ) \n assert_same_sub_schema ( result21 , [ self . sigs1a [ <NUM_LIT:0> ] ] ) \n assert_same_sub_schema ( result22 , [ self . sigs1a [ <NUM_LIT:1> ] ] ) \n def test_segment_take_analogsignal_by_channelindex ( self ) : \n ind1 = self . unit1 . channel_indexes [ <NUM_LIT:0> ] \n ind2 = self . unit2 . channel_indexes [ <NUM_LIT:0> ] \n result1 = self . seg1 . take_analogsignal_by_channelindex ( ) \n result21 = self . seg1 . take_analogsignal_by_channelindex ( [ ind1 ] ) \n result22 = self . seg1 . take_analogsignal_by_channelindex ( [ ind2 ] ) \n self . assertEqual ( result1 , [ ] ) \n assert_same_sub_schema ( result21 , [ self . sigs1a [ <NUM_LIT:0> ] ] ) \n assert_same_sub_schema ( result22 , [ self . sigs1a [ <NUM_LIT:1> ] ] ) \n def test_seg_take_slice_of_analogsignalarray_by_unit ( self ) : \n seg = self . seg1 \n result1 = seg . take_slice_of_analogsignalarray_by_unit ( ) \n result21 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit1 ] ) \n result23 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit3 ] ) \n self . assertEqual ( result1 , [ ] ) \n targ1 = [ self . sigarrs1a [ <NUM_LIT:0> ] [ : , np . array ( [ True ] ) ] , \n self . sigarrs1a [ <NUM_LIT:1> ] [ : , np . array ( [ False ] ) ] ] \n targ3 = [ self . sigarrs1a [ <NUM_LIT:0> ] [ : , np . array ( [ False ] ) ] , \n self . sigarrs1a [ <NUM_LIT:1> ] [ : , np . array ( [ True ] ) ] ] \n assert_same_sub_schema ( result21 , targ1 ) \n assert_same_sub_schema ( result23 , targ3 ) \n def test_seg_take_slice_of_analogsignalarray_by_channelindex ( self ) : \n seg = self . seg1 \n ind1 = self . unit1 . channel_indexes [ <NUM_LIT:0> ] \n ind3 = self . unit3 . channel_indexes [ <NUM_LIT:0> ] \n result1 = seg . take_slice_of_analogsignalarray_by_channelindex ( ) \n result21 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind1 ] ) \n result23 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind3 ] ) \n self . assertEqual ( result1 , [ ] ) \n targ1 = [ self . sigarrs1a [ <NUM_LIT:0> ] [ : , np . array ( [ True ] ) ] , \n self . sigarrs1a [ <NUM_LIT:1> ] [ : , np . array ( [ False ] ) ] ] \n targ3 = [ self . sigarrs1a [ <NUM_LIT:0> ] [ : , np . array ( [ False ] ) ] , \n self . sigarrs1a [ <NUM_LIT:1> ] [ : , np . array ( [ True ] ) ] ] \n assert_same_sub_schema ( result21 , targ1 ) \n assert_same_sub_schema ( result23 , targ3 ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import absolute_import , division \n try : \n import unittest2 as unittest \n except ImportError : \n import unittest \n from neo . io import NeuroScopeIO \n from neo . test . iotest . common_io_test import BaseTestIO \n class TestNeuroScopeIO ( BaseTestIO , unittest . TestCase , ) : \n ioclass = NeuroScopeIO \n files_to_test = [ '<STR_LIT>' ] \n files_to_download = [ '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n from django . http import HttpResponseRedirect \n from neurovault . apps . statmaps . utils import HttpRedirectException \n class CollectionRedirectMiddleware : \n def process_exception ( self , request , exception ) : \n if isinstance ( exception , HttpRedirectException ) : \n return HttpResponseRedirect ( exception . <mask0> [ <NUM_LIT:0> ] ) \n", "gt": "args"}
{"input": "\n from __future__ import unicode_literals \n from django . db import models , migrations \n import json , os \n dir = os . path . abspath ( os . path . dirname ( __file__ ) ) \n def populate_cogatlas ( apps , schema_editor ) : \n CognitiveAtlasTask = apps . get_model ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n CognitiveAtlasContrast = apps . get_model ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n json_content = open ( os . path . join ( dir , \"<STR_LIT>\" ) ) . read ( ) \n json_content = json_content . decode ( \"<STR_LIT:utf-8>\" ) . replace ( '<STR_LIT:\\t>' , '<STR_LIT>' ) \n data = json . loads ( json_content ) \n for item in data : \n task = CognitiveAtlasTask ( name = item [ \"<STR_LIT:name>\" ] , cog_atlas_id = item [ \"<STR_LIT:id>\" ] ) \n task . save ( ) \n for contrast in item [ \"<STR_LIT>\" ] : \n contrast = CognitiveAtlasContrast ( name = contrast [ \"<STR_LIT>\" ] , cog_atlas_id = contrast [ \"<STR_LIT>\" ] , task = task ) \n contrast . save ( ) \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ] \n operations = [ \n migrations . RunPython ( <mask0> ) , \n ] \n", "gt": "populate_cogatlas"}
{"input": "\n from __future__ import unicode_literals \n from django . db import models , migrations \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) \n ] \n <mask0> = [ \n ] \n", "gt": "operations"}
{"input": "\n import os . path \n from django . contrib . auth . models import User \n from django . core . files . uploadedfile import SimpleUploadedFile \n from django . test import TestCase , Client \n from neurovault . apps . statmaps . forms import NIDMResultsForm \n from neurovault . apps . statmaps . models import Collection , StatisticMap , Comparison \n from neurovault . apps . statmaps . utils import count_processing_comparisons , count_existing_comparisons \n from . utils import clearDB \n class Test_Counter ( TestCase ) : \n def setUp ( self ) : \n print \"<STR_LIT>\" \n self . test_path = os . path . abspath ( os . path . dirname ( __file__ ) ) \n self . user = User . objects . create ( username = '<STR_LIT>' ) \n self . client = Client ( ) \n self . client . login ( username = self . user ) \n self . Collection1 = Collection ( name = '<STR_LIT>' , owner = self . user , \n DOI = '<STR_LIT>' ) \n self . Collection1 . save ( ) \n self . Collection2 = Collection ( name = '<STR_LIT>' , owner = self . user , \n DOI = '<STR_LIT>' ) \n self . Collection2 . save ( ) \n self . Collection3 = Collection ( name = '<STR_LIT>' , owner = self . user , \n DOI = '<STR_LIT>' ) \n self . Collection3 . save ( ) \n def tearDown ( self ) : \n clearDB ( ) \n def test_statmaps_processing ( self ) : \n print \"<STR_LIT>\" \n Image1 = StatisticMap ( name = '<STR_LIT>' , collection = self . Collection1 , file = '<STR_LIT>' , map_type = \"<STR_LIT>\" ) \n Image1 . file = SimpleUploadedFile ( '<STR_LIT>' , file ( os . path . join ( self . test_path , '<STR_LIT>' ) ) . read ( ) ) \n Image1 . save ( ) \n images_processing = count_processing_comparisons ( Image1 . pk ) \n print \"<STR_LIT>\" % ( images_processing ) \n self . assertEqual ( images_processing , <NUM_LIT:0> ) \n Image2 = StatisticMap ( name = '<STR_LIT>' , collection = self . Collection2 , file = '<STR_LIT>' , map_type = \"<STR_LIT>\" ) \n Image2 . file = SimpleUploadedFile ( '<STR_LIT>' , file ( os . path . join ( self . test_path , '<STR_LIT>' ) ) . read ( ) ) \n Image2 . save ( ) \n images_processing = count_processing_comparisons ( Image1 . pk ) \n print \"<STR_LIT>\" % ( images_processing ) \n self . assertEqual ( images_processing , <NUM_LIT:0> ) \n total_comparisons = count_existing_comparisons ( Image1 . pk ) \n self . assertEqual ( total_comparisons , <NUM_LIT:1> ) \n def test_adding_nidm ( self ) : \n Image2 = StatisticMap ( name = '<STR_LIT>' , collection = self . Collection1 , file = '<STR_LIT>' , map_type = \"<STR_LIT>\" ) \n Image2 . file = SimpleUploadedFile ( '<STR_LIT>' , file ( os . path . join ( self . test_path , '<STR_LIT>' ) ) . read ( ) ) \n Image2 . save ( ) \n zip_file = open ( os . path . join ( self . test_path , '<STR_LIT>' ) , '<STR_LIT:rb>' ) \n post_dict = { \n '<STR_LIT:name>' : '<STR_LIT>' , \n '<STR_LIT:description>' : '<STR_LIT>' . format ( '<STR_LIT>' ) , \n '<STR_LIT>' : self . Collection2 . pk } \n fname = os . path . basename ( os . path . join ( self . test_path , '<STR_LIT>' ) ) \n file_dict = { '<STR_LIT>' : SimpleUploadedFile ( fname , zip_file . read ( ) ) } \n zip_file . close ( ) \n form = NIDMResultsForm ( post_dict , file_dict ) \n nidm = form . save ( ) \n print \"<STR_LIT>\" \n total_comparisons = count_existing_comparisons ( Image2 . pk ) \n self . assertEqual ( total_comparisons , <NUM_LIT:1> ) \n Image2ss = StatisticMap ( name = '<STR_LIT>' , collection = self . Collection3 , file = '<STR_LIT>' , map_type = \"<STR_LIT>\" , analysis_level = '<STR_LIT:S>' ) \n Image2ss . file = SimpleUploadedFile ( '<STR_LIT>' , file ( os . path . join ( self . test_path , '<STR_LIT>' ) ) . read ( ) ) \n Image2ss . save ( ) \n total_comparisons = count_existing_comparisons ( Image2ss . pk ) \n self . assertEqual ( total_comparisons , <NUM_LIT:0> ) \n number_comparisons = len ( Comparison . objects . all ( ) ) \n print \"<STR_LIT>\" % ( number_comparisons ) \n self . assertEqual ( number_comparisons > <NUM_LIT:0> , <mask0> ) \n", "gt": "True"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from neurovault . settings import PRIVATE_MEDIA_ROOT \n import os \n import os . path \n from neurovault . apps . statmaps . models import * \n def delOldCollDir ( ) : \n collDir = os . path . join ( PRIVATE_MEDIA_ROOT , '<STR_LIT>' ) \n for folder in os . listdir ( collDir ) : \n if not Collection . objects . filter ( pk = folder ) : \n os . rmdir ( os . path . join ( collDir , folder ) ) \n <mask0> ( ) \n", "gt": "delOldCollDir"}
{"input": "\n import theano \n from theano import tensor as T \n from theano . sandbox . rng_mrg import MRG_RandomStreams as RandomStreams \n import numpy as np \n from load import mnist \n srng = RandomStreams ( ) \n def floatX ( X ) : \n return np . asarray ( X , dtype = theano . config . floatX ) \n def init_weights ( shape ) : \n return theano . shared ( floatX ( np . random . randn ( * shape ) * <NUM_LIT> ) ) \n def rectify ( X ) : \n return T . maximum ( X , <NUM_LIT:0.> ) \n def softmax ( X ) : \n e_x = T . exp ( X - X . max ( axis = <NUM_LIT:1> ) . dimshuffle ( <NUM_LIT:0> , '<STR_LIT:x>' ) ) \n return e_x / e_x . sum ( axis = <NUM_LIT:1> ) . dimshuffle ( <NUM_LIT:0> , '<STR_LIT:x>' ) \n def RMSprop ( cost , params , lr = <NUM_LIT> , rho = <NUM_LIT> , epsilon = <NUM_LIT> ) : \n grads = T . grad ( cost = cost , wrt = params ) \n updates = [ ] \n for p , g in zip ( params , grads ) : \n acc = theano . shared ( p . get_value ( ) * <NUM_LIT:0.> ) \n acc_new = rho * acc + ( <NUM_LIT:1> - rho ) * g ** <NUM_LIT:2> \n gradient_scaling = T . sqrt ( acc_new + epsilon ) \n g = g / gradient_scaling \n updates . append ( ( acc , acc_new ) ) \n updates . append ( ( p , p - lr * g ) ) \n return updates \n def dropout ( X , p = <NUM_LIT:0.> ) : \n if p > <NUM_LIT:0> : \n retain_prob = <NUM_LIT:1> - p \n X *= srng . binomial ( X . shape , p = retain_prob , dtype = theano . config . floatX ) \n X /= retain_prob \n return X \n def model ( X , w_h , w_h2 , w_o , p_drop_input , p_drop_hidden ) : \n X = dropout ( X , p_drop_input ) \n h = rectify ( T . dot ( X , w_h ) ) \n h = dropout ( h , p_drop_hidden ) \n h2 = rectify ( T . dot ( h , w_h2 ) ) \n h2 = dropout ( h2 , p_drop_hidden ) \n py_x = softmax ( T . dot ( h2 , w_o ) ) \n return h , h2 , py_x \n trX , teX , trY , teY = mnist ( onehot = True ) \n X = T . fmatrix ( ) \n Y = T . fmatrix ( ) \n w_h = init_weights ( ( <NUM_LIT> , <NUM_LIT> ) ) \n w_h2 = init_weights ( ( <NUM_LIT> , <NUM_LIT> ) ) \n w_o = init_weights ( ( <NUM_LIT> , <NUM_LIT:10> ) ) \n noise_h , noise_h2 , noise_py_x = model ( X , w_h , w_h2 , w_o , <NUM_LIT> , <NUM_LIT:0.5> ) \n h , h2 , py_x = model ( X , w_h , w_h2 , w_o , <NUM_LIT:0.> , <NUM_LIT:0.> ) \n y_x = T . argmax ( py_x , axis = <NUM_LIT:1> ) \n cost = T . mean ( T . nnet . categorical_crossentropy ( noise_py_x , Y ) ) \n params = [ w_h , w_h2 , w_o ] \n updates = RMSprop ( cost , params , lr = <NUM_LIT> ) \n train = theano . function ( inputs = [ X , Y ] , outputs = cost , updates = updates , allow_input_downcast = True ) \n predict = theano . function ( inputs = [ X ] , outputs = y_x , allow_input_downcast = True ) \n for i in range ( <NUM_LIT:100> ) : \n for start , end in zip ( range ( <NUM_LIT:0> , len ( trX ) , <NUM_LIT> ) , range ( <NUM_LIT> , len ( trX ) , <NUM_LIT> ) ) : \n cost = train ( trX [ start : end ] , trY [ start : end ] ) \n print np . mean ( np . argmax ( teY , axis = <NUM_LIT:1> ) == predict ( <mask0> ) ) \n", "gt": "teX"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from ndscheduler import settings \n from ndscheduler . core . datastore . providers import base \n class DatastorePostgresql ( base . DatastoreBase ) : \n @ classmethod \n def get_db_url ( cls ) : \n \"\"\"<STR_LIT>\"\"\" \n return '<STR_LIT>' % ( \n settings . DATABASE_CONFIG_DICT [ '<STR_LIT:user>' ] , \n settings . DATABASE_CONFIG_DICT [ '<STR_LIT:password>' ] , \n settings . DATABASE_CONFIG_DICT [ '<STR_LIT>' ] , \n settings . DATABASE_CONFIG_DICT [ '<STR_LIT:port>' ] , \n settings . DATABASE_CONFIG_DICT [ '<STR_LIT>' ] , \n <mask0> . DATABASE_CONFIG_DICT [ '<STR_LIT>' ] ) \n", "gt": "settings"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import logging \n import requests \n from ndscheduler import job \n logger = logging . getLogger ( __name__ ) \n class CurlJob ( job . JobBase ) : \n TIMEOUT = <NUM_LIT:10> \n @ classmethod \n def meta_info ( cls ) : \n return { \n '<STR_LIT>' : '<STR_LIT>' % ( cls . __module__ , cls . __name__ ) , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : [ \n { '<STR_LIT:type>' : '<STR_LIT:string>' , '<STR_LIT:description>' : '<STR_LIT>' } , \n { '<STR_LIT:type>' : '<STR_LIT:string>' , '<STR_LIT:description>' : '<STR_LIT>' \n '<STR_LIT>' } , \n ] , \n '<STR_LIT>' : ( '<STR_LIT>' \n '<STR_LIT>' ) \n } \n def run ( self , url , request_type , * args , ** kwargs ) : \n print ( '<STR_LIT>' % ( url ) ) \n session = requests . Session ( ) \n result = session . request ( request_type , \n url , \n timeout = self . TIMEOUT , \n headers = None , \n data = None ) \n print ( result . text ) \n if __name__ == \"<STR_LIT:__main__>\" : \n job = CurlJob . create_test_instance ( ) \n job . <mask0> ( '<STR_LIT>' ) \n", "gt": "run"}
{"input": "\n import unittest \n from . mock import MagicMock , Mock \n from . util import TrelloElementMock , CommandMock , OperationMock \n from operations import * \n class BaseOperationTests ( unittest . TestCase ) : \n def setUp ( self ) : \n self . base_operation , self . trello_element = OperationMock . create ( BaseOperation ) \n self . class_mock , self . instance_mock = OperationMock . instance ( self . base_operation ) \n self . collection = TrelloElementMock . collection ( ) \n self . base_operation . collection = TrelloCollection ( self . collection ) \n def test_items_sets_the_collection ( self ) : \n self . base_operation . set_collection = MagicMock ( ) \n self . base_operation . items ( ) \n self . base_operation . set_collection . assert_called_with ( ) \n def test_items_returns_every_name_from_the_collection_with_the_added_options ( self ) : \n self . base_operation . set_collection = MagicMock ( ) \n self . assertEqual ( self . base_operation . items ( ) , [ \"<STR_LIT:..>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def test_callback_uses_find_to_instantiate_the_operation_if_the_index_is_in_the_collection ( self ) : \n self . base_operation . callback ( <NUM_LIT:3> ) \n self . class_mock . assert_called_with ( self . collection [ <NUM_LIT:0> ] , self . base_operation ) \n def test_callback_calls_execute_on_the_operation ( self ) : \n self . base_operation . callback ( <NUM_LIT:3> ) \n self . instance_mock . execute . assert_called_with ( self . base_operation . command ) \n def test_callback_doesnt_call_find_if_the_index_is_bigger_than_the_collection_length ( self ) : \n big_index = <NUM_LIT> \n self . base_operation . callback ( big_index ) \n assert not self . class_mock . called \n def test_callback_calls_execute_on_the_previous_operation_if_index_is_0 ( self ) : \n self . base_operation . callback ( <NUM_LIT:0> ) \n self . base_operation . previous_operation . execute . assert_called_with ( ) \n def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ( self ) : \n self . base_operation . command . input = MagicMock ( ) \n self . base_operation . callback ( <NUM_LIT:2> ) \n self . base_operation . command . input . assert_called_with ( \"<STR_LIT:Name>\" , self . base_operation . deferred_add ) \n def test_base_add_calls_add_with_the_text_and_cleans_the_cache_for_the_element ( self ) : \n text = \"<STR_LIT>\" \n self . base_operation . add = MagicMock ( ) \n self . base_operation . trello_element . reload = MagicMock ( ) \n self . base_operation . base_add ( text ) \n self . base_operation . add . assert_called_with ( text ) \n self . trello_element . reload . assert_called_with ( ) \n def test_base_add_calls_add_and_execute_if_renavigate_is_true ( self ) : \n text = \"<STR_LIT>\" \n self . base_operation . command . renavigate = True \n self . base_operation . execute = MagicMock ( ) \n self . base_operation . base_add ( text ) \n self . base_operation . execute . assert_called_with ( ) \n class BoardOperationTests ( unittest . TestCase ) : \n def setUp ( self ) : \n self . operation , self . trello_element = OperationMock . create ( BoardOperation ) \n self . operation . collection = TrelloCollection ( TrelloElementMock . collection ( ) ) \n def test_items_returns_every_name_from_the_collection_without_goback ( self ) : \n self . operation . set_collection = MagicMock ( ) \n self . assertEqual ( self . operation . items ( ) , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def test_trello_element_property ( self ) : \n self . assertEqual ( self . operation . trello_element_property ( ) , \"<STR_LIT>\" ) \n def test_callback_calls_execute_command_with_the_index ( self ) : \n self . operation . execute_command = MagicMock ( ) \n self . operation . callback ( <NUM_LIT:5> ) \n self . operation . execute_command . assert_called_with ( <NUM_LIT:3> ) \n def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ( self ) : \n self . operation . command . input = MagicMock ( ) \n self . operation . callback ( <NUM_LIT:1> ) \n self . operation . command . input . assert_called_with ( \"<STR_LIT:Name>\" , self . operation . deferred_add ) \n def test_next_operation_class ( self ) : \n self . assertEqual ( self . operation . next_operation_class ( ) , ListOperation ) \n def test_add_creates_a_board_with_the_text ( self ) : \n text = \"<STR_LIT>\" \n self . trello_element . add_board = MagicMock ( ) \n self . operation . add ( text ) \n self . trello_element . add_board . assert_called_with ( text ) \n class ListOperationTests ( unittest . TestCase ) : \n def setUp ( self ) : \n self . operation , self . trello_element = OperationMock . create ( ListOperation ) \n def test_trello_element_property ( self ) : \n self . assertEqual ( self . operation . trello_element_property ( ) , \"<STR_LIT>\" ) \n def test_next_operation_class ( self ) : \n self . assertEqual ( self . operation . next_operation_class ( ) , CardOperation ) \n def test_add_creates_a_list_with_the_text ( self ) : \n text = \"<STR_LIT>\" \n self . trello_element . add_list = MagicMock ( ) \n self . operation . add ( text ) \n self . trello_element . add_list . assert_called_with ( text ) \n class CardOperationTests ( unittest . TestCase ) : \n def setUp ( self ) : \n self . operation , self . trello_element = OperationMock . create ( CardOperation ) \n def test_items_returns_every_name_from_the_collection_with_custom_actions ( self ) : \n self . operation . set_collection = MagicMock ( ) \n self . operation . collection = TrelloCollection ( TrelloElementMock . collection ( ) ) \n self . assertEqual ( self . operation . items ( ) , [ '<STR_LIT:..>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n def test_trello_element_property ( self ) : \n self . assertEqual ( self . operation . trello_element_property ( ) , \"<STR_LIT>\" ) \n def test_next_operation_class ( self ) : \n self . assertEqual ( self . operation . next_operation_class ( ) , CardOptions ) \n def test_add_creates_a_card_with_the_text_and_description ( self ) : \n name = \"<STR_LIT>\" \n desc = \"<STR_LIT>\" \n self . trello_element . add_card = MagicMock ( ) \n self . operation . add ( name , desc ) \n self . trello_element . add_card . assert_called_with ( name , desc ) \n def test_split_card_contents_returns_the_name_and_description_splitted_by_new_lines ( self ) : \n content = \"<STR_LIT>\" \n name , desc = self . operation . split_card_contents ( content ) \n self . assertEqual ( name , \"<STR_LIT>\" ) \n self . assertEqual ( desc , \"<STR_LIT>\" ) \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n '''<STR_LIT>''' \n from . common import * \n DEBUG = env . bool ( '<STR_LIT>' , default = True ) \n TEMPLATES [ <NUM_LIT:0> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] = DEBUG \n SECRET_KEY = env ( \"<STR_LIT>\" , default = '<STR_LIT>' ) \n EMAIL_HOST = '<STR_LIT:localhost>' \n EMAIL_PORT = <NUM_LIT> \n EMAIL_BACKEND = env ( '<STR_LIT>' , \n default = '<STR_LIT>' ) \n CACHES = { \n '<STR_LIT:default>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n } \n MIDDLEWARE_CLASSES += ( '<STR_LIT>' , ) \n INSTALLED_APPS += ( '<STR_LIT>' , ) \n INTERNAL_IPS = ( '<STR_LIT:127.0.0.1>' , '<STR_LIT>' , ) \n DEBUG_TOOLBAR_CONFIG = { \n '<STR_LIT>' : [ \n '<STR_LIT>' , \n ] , \n '<STR_LIT>' : <mask0> , \n } \n TEST_RUNNER = '<STR_LIT>' \n", "gt": "True"}
{"input": "\n from django . contrib import messages \n from django . contrib . auth import logout , login , authenticate \n from django . contrib . auth . decorators import login_required \n from django . contrib . auth . models import User \n from django . http import HttpResponseBadRequest , Http404 \n from django . shortcuts import render , redirect , get_object_or_404 \n from reddit . forms import UserForm , ProfileForm \n from reddit . utils . helpers import post_only \n from users . models import RedditUser \n def user_profile ( request , username ) : \n user = get_object_or_404 ( User , username = username ) \n profile = RedditUser . objects . get ( user = user ) \n return render ( request , '<STR_LIT>' , { '<STR_LIT>' : profile } ) \n @ login_required \n def edit_profile ( request ) : \n user = RedditUser . objects . get ( user = request . user ) \n if request . method == '<STR_LIT:GET>' : \n profile_form = ProfileForm ( instance = user ) \n elif request . method == '<STR_LIT:POST>' : \n profile_form = ProfileForm ( request . POST , instance = user ) \n if profile_form . is_valid ( ) : \n profile = profile_form . save ( commit = False ) \n profile . update_profile_data ( ) \n profile . save ( ) \n messages . success ( request , \"<STR_LIT>\" ) \n else : \n raise Http404 \n return render ( request , '<STR_LIT>' , { '<STR_LIT>' : profile_form } ) \n def user_login ( request ) : \n \"\"\"<STR_LIT>\"\"\" \n if request . user . is_authenticated ( ) : \n messages . warning ( request , \"<STR_LIT>\" ) \n return render ( request , '<STR_LIT>' ) \n if request . method == \"<STR_LIT:POST>\" : \n username = request . POST . get ( '<STR_LIT:username>' ) \n password = request . POST . get ( '<STR_LIT:password>' ) \n if not username or not password : \n return HttpResponseBadRequest ( ) \n user = authenticate ( username = username , \n password = password ) \n if user : \n if user . is_active : \n login ( request , user ) \n redirect_url = request . POST . get ( '<STR_LIT>' ) or '<STR_LIT>' \n return redirect ( redirect_url ) \n else : \n return render ( request , '<STR_LIT>' , \n { '<STR_LIT>' : \"<STR_LIT>\" } ) \n else : \n return render ( request , '<STR_LIT>' , \n { '<STR_LIT>' : \"<STR_LIT>\" } ) \n return render ( request , '<STR_LIT>' ) \n @ post_only \n def user_logout ( request ) : \n \"\"\"<STR_LIT>\"\"\" \n if request . user . is_authenticated ( ) : \n redirect_page = request . POST . get ( '<STR_LIT>' , '<STR_LIT:/>' ) \n logout ( request ) \n messages . success ( request , '<STR_LIT>' ) \n return redirect ( redirect_page ) \n return redirect ( '<STR_LIT>' ) \n def register ( request ) : \n \"\"\"<STR_LIT>\"\"\" \n user_form = UserForm ( ) \n if request . user . is_authenticated ( ) : \n messages . warning ( request , \n '<STR_LIT>' ) \n return render ( request , '<STR_LIT>' , { '<STR_LIT>' : user_form } ) \n if request . method == \"<STR_LIT:POST>\" : \n user_form = UserForm ( request . POST ) \n if user_form . is_valid ( ) : \n user = user_form . save ( ) \n user . set_password ( user . password ) \n user . save ( ) \n reddit_user = RedditUser ( ) \n reddit_user . user = user \n reddit_user . save ( ) \n user = authenticate ( username = request . POST [ '<STR_LIT:username>' ] , \n password = request . POST [ '<STR_LIT:password>' ] ) \n login ( request , user ) \n return redirect ( '<STR_LIT>' ) \n return render ( request , '<STR_LIT>' , { '<STR_LIT>' : <mask0> } ) \n", "gt": "user_form"}
{"input": "\n import unittest2 \n from pymysql . tests import base \n from pymysql import util \n class TestNextset ( base . PyMySQLTestCase ) : \n def setUp ( self ) : \n super ( TestNextset , self ) . setUp ( ) \n self . con = self . connections [ <NUM_LIT:0> ] \n def test_nextset ( self ) : \n cur = self . con . cursor ( ) \n cur . execute ( \"<STR_LIT>\" ) \n self . assertEqual ( [ ( <NUM_LIT:1> , ) ] , list ( cur ) ) \n r = cur . nextset ( ) \n self . assertTrue ( r ) \n self . assertEqual ( [ ( <NUM_LIT:2> , ) ] , list ( cur ) ) \n self . assertIsNone ( cur . nextset ( ) ) \n def test_skip_nextset ( self ) : \n cur = self . con . cursor ( ) \n cur . execute ( \"<STR_LIT>\" ) \n self . assertEqual ( [ ( <NUM_LIT:1> , ) ] , list ( cur ) ) \n cur . execute ( \"<STR_LIT>\" ) \n self . assertEqual ( [ ( <NUM_LIT> , ) ] , list ( cur ) ) \n def test_ok_and_next ( self ) : \n cur = self . con . cursor ( ) \n cur . execute ( \"<STR_LIT>\" ) \n self . assertEqual ( [ ( <NUM_LIT:1> , ) ] , list ( cur ) ) \n self . assertTrue ( cur . nextset ( ) ) \n self . assertTrue ( cur . nextset ( ) ) \n self . assertEqual ( [ ( <NUM_LIT:2> , ) ] , list ( cur ) ) \n self . assertFalse ( bool ( cur . nextset ( ) ) ) \n @ unittest2 . expectedFailure \n def test_multi_cursor ( self ) : \n cur1 = self . con . cursor ( ) \n cur2 = self . con . cursor ( ) \n cur1 . execute ( \"<STR_LIT>\" ) \n cur2 . execute ( \"<STR_LIT>\" ) \n self . assertEqual ( [ ( <NUM_LIT:1> , ) ] , list ( cur1 ) ) \n self . assertEqual ( [ ( <NUM_LIT> , ) ] , list ( cur2 ) ) \n r = cur1 . nextset ( ) \n self . assertTrue ( r ) \n self . assertEqual ( [ ( <NUM_LIT:2> , ) ] , list ( cur1 ) ) \n self . assertIsNone ( cur1 . nextset ( ) ) \n def test_multi_statement_warnings ( self ) : \n cursor = self . con . cursor ( ) \n try : \n cursor . execute ( '<STR_LIT>' \n '<STR_LIT>' ) \n except TypeError : \n self . <mask0> ( ) \n", "gt": "fail"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from spyderlib . qt . QtGui import QVBoxLayout , QGroupBox , QLabel \n from spyderlib . qt . QtCore import SIGNAL , Qt \n from spyderlib . baseconfig import get_translation \n _ = get_translation ( \"<STR_LIT>\" , dirname = \"<STR_LIT>\" ) \n from spyderlib . utils . qthelpers import get_icon , create_action \n from spyderlib . plugins import SpyderPluginMixin , PluginConfigPage , runconfig \n from spyderplugins . widgets . memoryprofilergui import ( \n MemoryProfilerWidget , is_memoryprofiler_installed ) \n class MemoryProfilerConfigPage ( PluginConfigPage ) : \n \"\"\"<STR_LIT>\"\"\" \n def setup_page ( self ) : \n settings_group = QGroupBox ( _ ( \"<STR_LIT>\" ) ) \n use_color_box = self . create_checkbox ( \n _ ( \"<STR_LIT>\" ) , \n '<STR_LIT>' , default = True ) \n results_group = QGroupBox ( _ ( \"<STR_LIT>\" ) ) \n results_label1 = QLabel ( _ ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" ) ) \n results_label1 . setWordWrap ( True ) \n results_label2 = QLabel ( MemoryProfilerWidget . DATAPATH ) \n results_label2 . setTextInteractionFlags ( Qt . TextSelectableByMouse ) \n results_label2 . setWordWrap ( True ) \n settings_layout = QVBoxLayout ( ) \n settings_layout . addWidget ( use_color_box ) \n settings_group . setLayout ( settings_layout ) \n results_layout = QVBoxLayout ( ) \n results_layout . addWidget ( results_label1 ) \n results_layout . addWidget ( results_label2 ) \n results_group . setLayout ( results_layout ) \n vlayout = QVBoxLayout ( ) \n vlayout . addWidget ( settings_group ) \n vlayout . addWidget ( results_group ) \n vlayout . addStretch ( <NUM_LIT:1> ) \n self . setLayout ( vlayout ) \n class MemoryProfiler ( MemoryProfilerWidget , SpyderPluginMixin ) : \n \"\"\"<STR_LIT>\"\"\" \n CONF_SECTION = '<STR_LIT>' \n CONFIGWIDGET_CLASS = MemoryProfilerConfigPage \n def __init__ ( self , parent = None ) : \n MemoryProfilerWidget . __init__ ( self , parent = parent ) \n SpyderPluginMixin . __init__ ( self , parent ) \n self . initialize_plugin ( ) \n def get_plugin_title ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return _ ( \"<STR_LIT>\" ) \n def get_plugin_icon ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return get_icon ( '<STR_LIT>' ) \n def get_focus_widget ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . datatree \n def get_plugin_actions ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return [ ] \n def on_first_registration ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . main . tabify_plugins ( self . main . inspector , self ) \n self . dockwidget . hide ( ) \n def register_plugin ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . connect ( self , SIGNAL ( \"<STR_LIT>\" ) , \n self . main . editor . load ) \n self . connect ( self , SIGNAL ( '<STR_LIT>' ) , \n self . main . redirect_internalshell_stdio ) \n self . main . add_dockwidget ( self ) \n memoryprofiler_act = create_action ( self , _ ( \"<STR_LIT>\" ) , \n icon = self . get_plugin_icon ( ) , \n triggered = self . run_memoryprofiler ) \n memoryprofiler_act . setEnabled ( is_memoryprofiler_installed ( ) ) \n self . register_shortcut ( memoryprofiler_act , context = \"<STR_LIT>\" , \n name = \"<STR_LIT>\" , default = \"<STR_LIT>\" ) \n self . main . run_menu_actions += [ memoryprofiler_act ] \n self . main . editor . pythonfile_dependent_actions += [ memoryprofiler_act ] \n def refresh_plugin ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n def closing_plugin ( self , cancelable = False ) : \n \"\"\"<STR_LIT>\"\"\" \n return True \n def apply_plugin_settings ( self , options ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n def run_memoryprofiler ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . analyze ( self . main . editor . get_current_filename ( ) ) \n def analyze ( self , filename ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . dockwidget and not self . ismaximized : \n self . dockwidget . setVisible ( True ) \n self . dockwidget . setFocus ( ) \n self . dockwidget . raise_ ( ) \n pythonpath = self . main . get_spyder_pythonpath ( ) \n runconf = runconfig . get_run_configuration ( filename ) \n wdir , args = None , None \n if runconf is not None : \n if runconf . wdir_enabled : \n wdir = runconf . wdir \n if runconf . args_enabled : \n args = runconf . args \n MemoryProfilerWidget . analyze ( \n self , filename , wdir = wdir , args = args , pythonpath = pythonpath , \n use_colors = self . get_option ( '<STR_LIT>' , True ) ) \n PLUGIN_CLASS = <mask0> \n", "gt": "MemoryProfiler"}
{"input": "\n from google . protobuf import descriptor as _descriptor \n from google . protobuf import message as _message \n from google . protobuf import reflection as _reflection \n from google . protobuf import descriptor_pb2 \n DESCRIPTOR = _descriptor . FileDescriptor ( \n name = '<STR_LIT>' , \n package = '<STR_LIT>' , \n serialized_pb = '<STR_LIT>' ) \n _PUSHNOTIFICATION = _descriptor . Descriptor ( \n name = '<STR_LIT>' , \n full_name = '<STR_LIT>' , \n filename = None , \n file = DESCRIPTOR , \n containing_type = None , \n fields = [ \n _descriptor . FieldDescriptor ( \n name = '<STR_LIT>' , full_name = '<STR_LIT>' , index = <NUM_LIT:0> , \n number = <NUM_LIT:1> , type = <NUM_LIT:3> , cpp_type = <NUM_LIT:2> , label = <NUM_LIT:1> , \n has_default_value = False , default_value = <NUM_LIT:0> , \n message_type = None , enum_type = None , containing_type = None , \n is_extension = False , extension_scope = None , \n options = None ) , \n _descriptor . FieldDescriptor ( \n name = '<STR_LIT:title>' , full_name = '<STR_LIT>' , index = <NUM_LIT:1> , \n number = <NUM_LIT:2> , type = <NUM_LIT:9> , cpp_type = <NUM_LIT:9> , label = <NUM_LIT:1> , \n has_default_value = False , default_value = unicode ( \"<STR_LIT>\" , \"<STR_LIT:utf-8>\" ) , \n message_type = None , enum_type = None , containing_type = None , \n is_extension = False , extension_scope = None , \n options = None ) , \n _descriptor . FieldDescriptor ( \n name = '<STR_LIT:content>' , full_name = '<STR_LIT>' , index = <NUM_LIT:2> , \n number = <NUM_LIT:3> , type = <NUM_LIT:9> , cpp_type = <NUM_LIT:9> , label = <NUM_LIT:1> , \n has_default_value = False , default_value = unicode ( \"<STR_LIT>\" , \"<STR_LIT:utf-8>\" ) , \n message_type = None , enum_type = None , containing_type = None , \n is_extension = False , extension_scope = None , \n options = None ) , \n _descriptor . FieldDescriptor ( \n name = '<STR_LIT>' , full_name = '<STR_LIT>' , index = <NUM_LIT:3> , \n number = <NUM_LIT:4> , type = <NUM_LIT:9> , cpp_type = <NUM_LIT:9> , label = <NUM_LIT:1> , \n has_default_value = False , default_value = unicode ( \"<STR_LIT>\" , \"<STR_LIT:utf-8>\" ) , \n message_type = None , enum_type = None , containing_type = None , \n is_extension = False , extension_scope = None , \n options = None ) , \n ] , \n extensions = [ \n ] , \n nested_types = [ ] , \n enum_types = [ \n ] , \n options = None , \n is_extendable = False , \n extension_ranges = [ ] , \n serialized_start = <NUM_LIT> , \n serialized_end = <NUM_LIT> , \n ) \n _BATCHNOTIFICATIONREQUEST = _descriptor . Descriptor ( \n name = '<STR_LIT>' , \n full_name = '<STR_LIT>' , \n filename = None , \n file = DESCRIPTOR , \n containing_type = None , \n fields = [ \n _descriptor . FieldDescriptor ( \n name = '<STR_LIT>' , full_name = '<STR_LIT>' , index = <NUM_LIT:0> , \n number = <NUM_LIT:1> , type = <NUM_LIT:11> , cpp_type = <NUM_LIT:10> , label = <NUM_LIT:3> , \n has_default_value = False , default_value = [ ] , \n message_type = None , enum_type = None , containing_type = None , \n is_extension = False , extension_scope = None , \n options = None ) , \n ] , \n extensions = [ \n ] , \n nested_types = [ ] , \n enum_types = [ \n ] , \n options = None , \n is_extendable = False , \n extension_ranges = [ ] , \n serialized_start = <NUM_LIT> , \n serialized_end = <NUM_LIT> , \n ) \n _BATCHNOTIFICATIONREQUEST . fields_by_name [ '<STR_LIT>' ] . message_type = _PUSHNOTIFICATION \n DESCRIPTOR . message_types_by_name [ '<STR_LIT>' ] = _PUSHNOTIFICATION \n DESCRIPTOR . message_types_by_name [ '<STR_LIT>' ] = _BATCHNOTIFICATIONREQUEST \n class PushNotification ( _message . Message ) : \n __metaclass__ = _reflection . GeneratedProtocolMessageType \n DESCRIPTOR = _PUSHNOTIFICATION \n class BatchNotificationRequest ( _message . Message ) : \n __metaclass__ = _reflection . GeneratedProtocolMessageType \n DESCRIPTOR = _BATCHNOTIFICATIONREQUEST \n DESCRIPTOR . has_options = True \n DESCRIPTOR . _options = _descriptor . _ParseOptions ( descriptor_pb2 . <mask0> ( ) , '<STR_LIT>' ) \n", "gt": "FileOptions"}
{"input": "\n import pytest \n from pushkin import pushkin_cli \n import tornado . web \n from pushkin import context \n from pushkin . database import database \n from pushkin . request . request_processor import RequestProcessor \n from pushkin . requesthandlers . events import JsonEventHandler \n from pushkin . requesthandlers . notifications import JsonNotificationHandler \n from pushkin import test_config_ini_path \n from pushkin import config \n @ pytest . fixture \n def setup_database ( ) : \n database . create_database ( ) \n @ pytest . fixture \n def mock_processor ( mocker ) : \n '''<STR_LIT>''' \n mocker . patch ( '<STR_LIT>' ) \n mocker . patch ( '<STR_LIT>' ) \n @ pytest . fixture \n def app ( ) : \n pushkin_cli . CONFIGURATION_FILENAME = test_config_ini_path \n pushkin_cli . init ( ) \n return pushkin_cli . create_app ( ) \n @ pytest . fixture \n def notification_batch_json ( ) : \n '''<STR_LIT>''' \n return '''<STR_LIT>''' \n @ pytest . fixture \n def post_notification_url ( base_url ) : \n return base_url + config . json_notification_handler_url \n @ pytest . fixture \n def event_batch_json ( ) : \n '''<STR_LIT>''' \n return '''<STR_LIT>''' \n @ pytest . fixture \n def post_event_url ( base_url ) : \n return base_url + config . json_event_handler_url \n @ pytest . mark . gen_test \n @ pytest . mark . parametrize ( \"<STR_LIT:input>\" , [ \n ( '<STR_LIT>' ) , \n ( '<STR_LIT>' ) , \n ] ) \n def test_post_notification_empty_request ( setup_database , mock_processor , http_client , post_notification_url , input ) : \n '''<STR_LIT>''' \n request = tornado . httpclient . HTTPRequest ( post_notification_url , method = '<STR_LIT:POST>' , body = input ) \n with pytest . raises ( tornado . httpclient . HTTPError ) : \n yield http_client . fetch ( request ) \n assert not context . request_processor . submit . called \n @ pytest . mark . gen_test \n def test_post_notification ( setup_database , mock_processor , http_client , post_notification_url , \n notification_batch_json ) : \n '''<STR_LIT>''' \n request = tornado . httpclient . HTTPRequest ( post_notification_url , method = '<STR_LIT:POST>' , body = notification_batch_json ) \n response = yield http_client . fetch ( request ) \n assert response . code == <NUM_LIT:200> \n assert context . request_processor . submit . called \n @ pytest . mark . gen_test \n @ pytest . mark . parametrize ( \"<STR_LIT:input>\" , [ \n ( '<STR_LIT>' ) , \n ( '<STR_LIT>' ) , \n ] ) \n def test_post_event_empty_request ( setup_database , mock_processor , http_client , post_event_url , input ) : \n '''<STR_LIT>''' \n request = tornado . httpclient . HTTPRequest ( post_event_url , method = '<STR_LIT:POST>' , body = input ) \n with pytest . raises ( tornado . httpclient . HTTPError ) : \n yield http_client . fetch ( request ) \n assert not context . request_processor . submit . called \n @ pytest . mark . gen_test \n def test_post_event ( setup_database , mock_processor , http_client , post_event_url , event_batch_json ) : \n '''<STR_LIT>''' \n context . request_processor . submit . return_value = True \n request = tornado . httpclient . HTTPRequest ( post_event_url , method = '<STR_LIT:POST>' , body = event_batch_json ) \n response = yield http_client . fetch ( request ) \n assert response . code == <NUM_LIT:200> \n assert context . request_processor . submit . called \n @ pytest . mark . gen_test \n def test_post_event_service_unavailable ( setup_database , mock_processor , http_client , post_event_url , event_batch_json , \n app ) : \n '''<STR_LIT>''' \n context . request_processor . submit . return_value = False \n request = tornado . httpclient . HTTPRequest ( post_event_url , method = '<STR_LIT:POST>' , body = event_batch_json ) \n RequestProcessor . submit . return_value = False \n with pytest . raises ( tornado . httpclient . HTTPError ) : \n yield http_client . fetch ( <mask0> ) \n", "gt": "request"}
{"input": "\n import json \n import os \n import tempfile \n import unittest \n from zipfile import ZipFile \n import shutil \n from nordicsemi . dfu . package import Package \n class TestPackage ( unittest . TestCase ) : \n def setUp ( self ) : \n self . work_directory = tempfile . mkdtemp ( prefix = \"<STR_LIT>\" ) \n def tearDown ( self ) : \n shutil . rmtree ( self . work_directory , ignore_errors = True ) \n def test_generate_package_application ( self ) : \n self . p = Package ( \n dev_type = <NUM_LIT:1> , \n dev_rev = <NUM_LIT:2> , \n app_version = <NUM_LIT:100> , \n sd_req = [ <NUM_LIT> , <NUM_LIT> ] , \n app_fw = \"<STR_LIT>\" \n ) \n pkg_name = \"<STR_LIT>\" \n self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n expected_zip_content = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n with ZipFile ( pkg_name , '<STR_LIT:r>' ) as pkg : \n infolist = pkg . infolist ( ) \n for file_information in infolist : \n self . assertTrue ( file_information . filename in expected_zip_content ) \n self . assertGreater ( file_information . file_size , <NUM_LIT:0> ) \n pkg . extractall ( self . work_directory ) \n with open ( os . path . join ( self . work_directory , '<STR_LIT>' ) , '<STR_LIT:r>' ) as f : \n _json = json . load ( f ) \n self . assertEqual ( u'<STR_LIT>' , _json [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n self . assertEqual ( u'<STR_LIT>' , _json [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n self . assertTrue ( u'<STR_LIT>' not in _json [ '<STR_LIT>' ] ) \n self . assertTrue ( u'<STR_LIT>' not in _json [ '<STR_LIT>' ] ) \n self . assertTrue ( u'<STR_LIT>' not in _json [ '<STR_LIT>' ] ) \n def test_generate_package_sd_bl ( self ) : \n self . p = Package ( dev_type = <NUM_LIT:1> , \n dev_rev = <NUM_LIT:2> , \n app_version = <NUM_LIT:100> , \n sd_req = [ <NUM_LIT> , <NUM_LIT> ] , \n softdevice_fw = \"<STR_LIT>\" , \n bootloader_fw = \"<STR_LIT>\" ) \n pkg_name = \"<STR_LIT>\" \n self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n expected_zip_content = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n with ZipFile ( pkg_name , '<STR_LIT:r>' ) as pkg : \n infolist = pkg . infolist ( ) \n for file_information in infolist : \n self . assertTrue ( file_information . filename in expected_zip_content ) \n self . assertGreater ( file_information . file_size , <NUM_LIT:0> ) \n pkg . extractall ( self . work_directory ) \n with open ( os . path . join ( self . work_directory , '<STR_LIT>' ) , '<STR_LIT:r>' ) as f : \n _json = json . load ( f ) \n self . assertEqual ( u'<STR_LIT>' , _json [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n self . assertEqual ( u'<STR_LIT>' , _json [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n def test_unpack_package_a ( self ) : \n self . p = Package ( dev_type = <NUM_LIT:1> , \n dev_rev = <NUM_LIT:2> , \n app_version = <NUM_LIT:100> , \n sd_req = [ <NUM_LIT> , <NUM_LIT> ] , \n softdevice_fw = \"<STR_LIT>\" , \n dfu_ver = <NUM_LIT> ) \n pkg_name = os . path . join ( self . work_directory , \"<STR_LIT>\" ) \n self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n unpacked_dir = os . path . join ( self . work_directory , \"<STR_LIT>\" ) \n manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n self . assertIsNotNone ( manifest ) \n self . assertEqual ( u'<STR_LIT>' , manifest . softdevice . bin_file ) \n self . assertEqual ( <NUM_LIT:0> , manifest . softdevice . init_packet_data . ext_packet_id ) \n self . assertIsNotNone ( manifest . softdevice . init_packet_data . firmware_crc16 ) \n def test_unpack_package_b ( self ) : \n self . p = Package ( dev_type = <NUM_LIT:1> , \n dev_rev = <NUM_LIT:2> , \n app_version = <NUM_LIT:100> , \n sd_req = [ <NUM_LIT> , <NUM_LIT> ] , \n softdevice_fw = \"<STR_LIT>\" , \n dfu_ver = <NUM_LIT> ) \n pkg_name = os . path . join ( self . work_directory , \"<STR_LIT>\" ) \n self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n unpacked_dir = os . path . join ( self . work_directory , \"<STR_LIT>\" ) \n manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n self . assertIsNotNone ( manifest ) \n self . assertEqual ( u'<STR_LIT>' , manifest . softdevice . bin_file ) \n self . assertEqual ( <NUM_LIT:1> , manifest . softdevice . init_packet_data . ext_packet_id ) \n self . assertIsNone ( manifest . softdevice . init_packet_data . firmware_crc16 ) \n self . assertIsNotNone ( manifest . softdevice . init_packet_data . firmware_hash ) \n def test_unpack_package_c ( self ) : \n self . p = Package ( dev_type = <NUM_LIT:1> , \n dev_rev = <NUM_LIT:2> , \n app_version = <NUM_LIT:100> , \n sd_req = [ <NUM_LIT> , <NUM_LIT> ] , \n softdevice_fw = \"<STR_LIT>\" , \n key_file = \"<STR_LIT>\" ) \n pkg_name = os . path . join ( self . work_directory , \"<STR_LIT>\" ) \n self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n unpacked_dir = os . path . join ( self . work_directory , \"<STR_LIT>\" ) \n manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n self . assertIsNotNone ( manifest ) \n self . assertEqual ( u'<STR_LIT>' , manifest . softdevice . bin_file ) \n self . assertEqual ( <NUM_LIT:2> , manifest . softdevice . init_packet_data . ext_packet_id ) \n self . assertIsNone ( manifest . softdevice . init_packet_data . firmware_crc16 ) \n self . assertIsNotNone ( manifest . softdevice . init_packet_data . firmware_hash ) \n self . assertIsNotNone ( manifest . softdevice . init_packet_data . init_packet_ecds ) \n self . assertEqual ( manifest . dfu_version , <NUM_LIT> ) \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n from agamotto . utils import execute , grepc \n def installed ( package ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n return grepc ( execute ( \"<STR_LIT>\" % package ) , package ) > <NUM_LIT:0> \n except Exception , _e : \n return False \n def is_installed ( package ) : \n \"\"\"<STR_LIT>\"\"\" \n return installed ( <mask0> ) \n", "gt": "package"}
{"input": "\n __author__ = '<STR_LIT>' \n import wx \n from wx import AboutBox , AboutDialogInfo , ClientDC \n from wx . lib . wordwrap import wordwrap \n from odmtools . meta import data \n class frmAbout ( wx . Dialog ) : \n def __init__ ( self , parent ) : \n self . parent = parent \n info = AboutDialogInfo ( ) \n info . Name = data . app_name \n info . Version = data . version \n info . Copyright = data . copyright \n info . Description = wordwrap ( data . description , <NUM_LIT> , ClientDC ( parent ) ) \n info . WebSite = data . website \n info . Developers = data . developers \n info . License = wordwrap ( data . license , <NUM_LIT> , ClientDC ( parent ) ) \n AboutBox ( <mask0> ) \n", "gt": "info"}
{"input": "\n import wx \n import wx . grid \n import wx . richtext \n from odmtools . odmdata import Method \n [ wxID_PNLMETHOD , wxID_PNLMETHODSLISTCTRL1 , wxID_PNLMETHODSRBCREATENEW , \n wxID_PNLMETHODSRBGENERATE , wxID_PNLMETHODSRBSELECT , \n wxID_PNLMETHODSRICHTEXTCTRL1 , \n ] = [ wx . NewId ( ) for _init_ctrls in range ( <NUM_LIT:6> ) ] \n from odmtools . common . logger import LoggerTool \n import logging \n tool = LoggerTool ( ) \n logger = tool . setupLogger ( __name__ , __name__ + '<STR_LIT>' , '<STR_LIT:w>' , logging . DEBUG ) \n class pnlMethod ( wx . Panel ) : \n def _init_ctrls ( self , prnt ) : \n wx . Panel . __init__ ( self , id = wxID_PNLMETHOD , name = u'<STR_LIT>' , \n parent = prnt , pos = wx . Point ( <NUM_LIT> , <NUM_LIT> ) , size = wx . Size ( <NUM_LIT> , <NUM_LIT> ) , \n style = wx . TAB_TRAVERSAL ) \n self . SetClientSize ( wx . Size ( <NUM_LIT> , <NUM_LIT> ) ) \n self . rbGenerate = wx . RadioButton ( id = wxID_PNLMETHODSRBGENERATE , \n label = u'<STR_LIT>' , name = u'<STR_LIT>' , \n parent = self , pos = wx . Point ( <NUM_LIT:16> , <NUM_LIT:8> ) , size = wx . Size ( <NUM_LIT> , <NUM_LIT:16> ) , style = <NUM_LIT:0> ) \n self . rbGenerate . SetValue ( True ) \n self . rbGenerate . Bind ( wx . EVT_RADIOBUTTON , self . OnRbGenerateRadiobutton , \n id = wxID_PNLMETHODSRBGENERATE ) \n self . rbSelect = wx . RadioButton ( id = wxID_PNLMETHODSRBSELECT , \n label = u'<STR_LIT>' , name = u'<STR_LIT>' , parent = self , \n pos = wx . Point ( <NUM_LIT:16> , <NUM_LIT:32> ) , size = wx . Size ( <NUM_LIT> , <NUM_LIT> ) , style = <NUM_LIT:0> ) \n self . rbSelect . SetValue ( False ) \n self . rbSelect . Bind ( wx . EVT_RADIOBUTTON , self . OnRbSelectRadiobutton , \n id = wxID_PNLMETHODSRBSELECT ) \n self . rbCreateNew = wx . RadioButton ( id = wxID_PNLMETHODSRBCREATENEW , \n label = u'<STR_LIT>' , name = u'<STR_LIT>' , parent = self , \n pos = wx . Point ( <NUM_LIT:16> , <NUM_LIT> ) , size = wx . Size ( <NUM_LIT> , <NUM_LIT> ) , style = <NUM_LIT:0> ) \n self . rbCreateNew . SetValue ( False ) \n self . rbCreateNew . Bind ( wx . EVT_RADIOBUTTON , self . OnRbCreateNewRadiobutton , \n id = wxID_PNLMETHODSRBCREATENEW ) \n self . txtMethodDescrip = wx . richtext . RichTextCtrl ( id = wxID_PNLMETHODSRICHTEXTCTRL1 , \n parent = self , pos = wx . Point ( <NUM_LIT:16> , <NUM_LIT> ) , size = wx . Size ( <NUM_LIT> , <NUM_LIT> ) , \n style = wx . richtext . RE_MULTILINE , value = u'<STR_LIT>' ) \n self . txtMethodDescrip . Enable ( False ) \n self . txtMethodDescrip . Bind ( wx . EVT_SET_FOCUS , self . OnTxtMethodDescripSetFocus ) \n self . txtMethodDescrip . Bind ( wx . EVT_KILL_FOCUS , self . OnTxtMethodDescripKillFocus ) \n self . lstMethods = wx . ListCtrl ( id = wxID_PNLMETHODSLISTCTRL1 , \n name = '<STR_LIT>' , parent = self , pos = wx . Point ( <NUM_LIT:16> , <NUM_LIT> ) , \n size = wx . Size ( <NUM_LIT> , <NUM_LIT> ) , style = wx . LC_REPORT | wx . LC_SINGLE_SEL ) \n self . lstMethods . InsertColumn ( <NUM_LIT:0> , '<STR_LIT>' ) \n self . lstMethods . InsertColumn ( <NUM_LIT:1> , '<STR_LIT>' ) \n self . lstMethods . InsertColumn ( <NUM_LIT:2> , '<STR_LIT:id>' ) \n self . lstMethods . SetColumnWidth ( <NUM_LIT:0> , <NUM_LIT:200> ) \n self . lstMethods . SetColumnWidth ( <NUM_LIT:1> , <NUM_LIT> ) \n self . lstMethods . SetColumnWidth ( <NUM_LIT:2> , <NUM_LIT:0> ) \n self . lstMethods . Enable ( False ) \n def __init__ ( self , parent , id , pos , size , style , name , sm , method ) : \n self . series_service = sm . get_series_service ( ) \n self . prev_val = method \n self . _init_ctrls ( parent ) \n def OnRbGenerateRadiobutton ( self , event ) : \n self . lstMethods . Enable ( False ) \n self . txtMethodDescrip . Enable ( False ) \n event . Skip ( ) \n def OnRbSelectRadiobutton ( self , event ) : \n self . lstMethods . Enable ( True ) \n self . txtMethodDescrip . Enable ( False ) \n event . Skip ( ) \n def OnRbCreateNewRadiobutton ( self , event ) : \n self . lstMethods . Enable ( False ) \n self . txtMethodDescrip . Enable ( True ) \n event . Skip ( ) \n def OnTxtMethodDescripSetFocus ( self , event ) : \n if self . txtMethodDescrip . GetValue ( ) == \"<STR_LIT>\" : \n self . txtMethodDescrip . SetValue ( \"<STR_LIT>\" ) \n event . Skip ( ) \n def OnTxtMethodDescripKillFocus ( self , event ) : \n if self . txtMethodDescrip . GetValue ( ) == \"<STR_LIT>\" : \n self . txtMethodDescrip . SetValue ( \"<STR_LIT>\" ) \n event . Skip ( ) \n def getMethod ( self ) : \n m = Method ( ) \n if self . rbGenerate . Value : \n genmethod = \"<STR_LIT>\" \n try : \n m = self . series_service . get_method_by_description ( genmethod ) \n except : \n m . description = genmethod \n elif self . rbSelect . Value : \n index = self . lstMethods . GetFirstSelected ( ) \n desc = self . lstMethods . GetItem ( index , <NUM_LIT:0> ) . GetText ( ) \n logger . debug ( desc ) \n m = self . series_service . get_method_by_description ( desc ) \n elif self . rbCreateNew . Value : \n m . description = self . txtMethodDescrip . GetValue ( ) \n return <mask0> \n", "gt": "m"}
{"input": "\n import wx \n import wx . xrc \n import wx . lib . masked as masked \n class clsDataFilters ( wx . Dialog ) : \n def __init__ ( self , parent ) : \n wx . Dialog . __init__ ( self , parent , id = wx . ID_ANY , title = u\"<STR_LIT>\" , pos = wx . Point ( <NUM_LIT> , <NUM_LIT> ) , \n size = wx . Size ( <NUM_LIT> , <NUM_LIT> ) , style = wx . DEFAULT_DIALOG_STYLE | wx . RESIZE_BORDER ) \n self . SetSizeHintsSz ( wx . Size ( <NUM_LIT> , <NUM_LIT> ) , wx . DefaultSize ) \n bSizer1 = wx . BoxSizer ( wx . VERTICAL ) \n bSizer3 = wx . BoxSizer ( wx . VERTICAL ) \n bsValueThresh = wx . BoxSizer ( wx . HORIZONTAL ) \n self . rbThreshold = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . Point ( <NUM_LIT:10> , <NUM_LIT:8> ) , wx . DefaultSize , wx . RB_GROUP ) \n self . rbThreshold . SetValue ( True ) \n bsValueThresh . Add ( self . rbThreshold , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n sbThreshold = wx . StaticBoxSizer ( wx . StaticBox ( self , wx . ID_ANY , u\"<STR_LIT>\" ) , wx . VERTICAL ) \n fgSizer1 = wx . FlexGridSizer ( <NUM_LIT:0> , <NUM_LIT:2> , <NUM_LIT:0> , <NUM_LIT:0> ) \n fgSizer1 . SetFlexibleDirection ( wx . HORIZONTAL ) \n fgSizer1 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n self . lblChangegt = wx . StaticText ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n self . lblChangegt . Wrap ( - <NUM_LIT:1> ) \n fgSizer1 . Add ( self . lblChangegt , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . txtThreshValGT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size ( <NUM_LIT> , - <NUM_LIT:1> ) , <NUM_LIT:0> ) \n fgSizer1 . Add ( self . txtThreshValGT , <NUM_LIT:1> , wx . ALL | wx . EXPAND , <NUM_LIT:5> ) \n self . lblChangelt = wx . StaticText ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n self . lblChangelt . Wrap ( - <NUM_LIT:1> ) \n fgSizer1 . Add ( self . lblChangelt , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . txtThreshValLT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size ( <NUM_LIT> , - <NUM_LIT:1> ) , <NUM_LIT:0> ) \n fgSizer1 . Add ( self . txtThreshValLT , <NUM_LIT:1> , wx . ALL | wx . EXPAND , <NUM_LIT:5> ) \n sbThreshold . Add ( fgSizer1 , <NUM_LIT:0> , wx . EXPAND , <NUM_LIT:5> ) \n bsValueThresh . Add ( sbThreshold , <NUM_LIT:1> , <NUM_LIT:0> , <NUM_LIT:5> ) \n bSizer3 . Add ( bsValueThresh , <NUM_LIT:1> , wx . EXPAND , <NUM_LIT:5> ) \n bsGaps = wx . BoxSizer ( wx . HORIZONTAL ) \n self . rbDataGaps = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n bsGaps . Add ( self . rbDataGaps , <NUM_LIT:1> , wx . ALL , <NUM_LIT:5> ) \n sbGaps = wx . StaticBoxSizer ( wx . StaticBox ( self , wx . ID_ANY , u\"<STR_LIT>\" ) , wx . VERTICAL ) \n fgSizer2 = wx . FlexGridSizer ( <NUM_LIT:0> , <NUM_LIT:2> , <NUM_LIT:0> , <NUM_LIT:0> ) \n fgSizer2 . SetFlexibleDirection ( wx . HORIZONTAL ) \n fgSizer2 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n self . lblGapsVal = wx . StaticText ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n self . lblGapsVal . Wrap ( - <NUM_LIT:1> ) \n fgSizer2 . Add ( self . lblGapsVal , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . txtGapsVal = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size ( <NUM_LIT> , - <NUM_LIT:1> ) , <NUM_LIT:0> ) \n fgSizer2 . Add ( self . txtGapsVal , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . lblGapsTime = wx . StaticText ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n self . lblGapsTime . Wrap ( - <NUM_LIT:1> ) \n fgSizer2 . Add ( self . lblGapsTime , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n cbGapTimeChoices = [ u\"<STR_LIT>\" , u\"<STR_LIT>\" , u\"<STR_LIT>\" , u\"<STR_LIT>\" ] \n self . cbGapTime = wx . ComboBox ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . Size ( <NUM_LIT> , - <NUM_LIT:1> ) , cbGapTimeChoices , \n wx . CB_READONLY ) \n fgSizer2 . Add ( self . cbGapTime , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n sbGaps . Add ( fgSizer2 , <NUM_LIT:1> , wx . EXPAND , <NUM_LIT:5> ) \n bsGaps . Add ( sbGaps , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:5> ) \n bSizer3 . Add ( bsGaps , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:5> ) \n bsDate = wx . BoxSizer ( wx . HORIZONTAL ) \n self . rbDate = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n bsDate . Add ( self . rbDate , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n sbDate = wx . StaticBoxSizer ( wx . StaticBox ( self , wx . ID_ANY , u\"<STR_LIT>\" ) , wx . VERTICAL ) \n fgSizer3 = wx . FlexGridSizer ( <NUM_LIT:0> , <NUM_LIT:4> , <NUM_LIT:0> , <NUM_LIT:0> ) \n fgSizer3 . SetFlexibleDirection ( wx . HORIZONTAL ) \n fgSizer3 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n self . lblDateAfter = wx . StaticText ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n self . lblDateAfter . Wrap ( - <NUM_LIT:1> ) \n fgSizer3 . Add ( self . lblDateAfter , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . dpAfter = wx . DatePickerCtrl ( self , wx . ID_ANY , wx . DefaultDateTime , wx . DefaultPosition , wx . Size ( <NUM_LIT> , - <NUM_LIT:1> ) , \n wx . DP_DROPDOWN | wx . DP_SHOWCENTURY ) \n fgSizer3 . Add ( self . dpAfter , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . sbAfter = wx . SpinButton ( self , wx . ID_ANY , wx . DefaultPosition , wx . Size ( <NUM_LIT:15> , - <NUM_LIT:1> ) , <NUM_LIT:0> ) \n self . tpAfter = masked . TimeCtrl ( self , wx . ID_ANY , pos = wx . DefaultPosition , size = wx . Size ( <NUM_LIT> , - <NUM_LIT:1> ) , \n name = \"<STR_LIT>\" , \n fmt24hr = True , spinButton = self . sbAfter , oob_color = \"<STR_LIT>\" ) \n fgSizer3 . Add ( self . tpAfter , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n fgSizer3 . Add ( self . sbAfter , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . lblDateBefore = wx . StaticText ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n self . lblDateBefore . Wrap ( - <NUM_LIT:1> ) \n fgSizer3 . Add ( self . lblDateBefore , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . dpBefore = wx . DatePickerCtrl ( self , wx . ID_ANY , wx . DefaultDateTime , wx . DefaultPosition , wx . Size ( <NUM_LIT> , - <NUM_LIT:1> ) , \n wx . DP_DROPDOWN | wx . DP_SHOWCENTURY ) \n fgSizer3 . Add ( self . dpBefore , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . sbBefore = wx . SpinButton ( self , wx . ID_ANY , wx . DefaultPosition , wx . Size ( <NUM_LIT:15> , - <NUM_LIT:1> ) , wx . SP_WRAP ) \n self . tpBefore = masked . TimeCtrl ( self , wx . ID_ANY , pos = wx . DefaultPosition , size = wx . Size ( <NUM_LIT> , - <NUM_LIT:1> ) , \n name = \"<STR_LIT>\" , \n fmt24hr = True , spinButton = self . sbBefore , oob_color = '<STR_LIT>' ) \n fgSizer3 . Add ( self . tpBefore , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n fgSizer3 . Add ( self . sbBefore , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n sbDate . Add ( fgSizer3 , <NUM_LIT:1> , wx . EXPAND , <NUM_LIT:5> ) \n bsDate . Add ( sbDate , <NUM_LIT:1> , wx . EXPAND , <NUM_LIT:5> ) \n bSizer3 . Add ( bsDate , <NUM_LIT:0> , wx . EXPAND , <NUM_LIT:5> ) \n bsValChange = wx . BoxSizer ( wx . HORIZONTAL ) \n self . rbVChangeThresh = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n bsValChange . Add ( self . rbVChangeThresh , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n sbValChange = wx . StaticBoxSizer ( wx . StaticBox ( self , wx . ID_ANY , u\"<STR_LIT>\" ) , wx . VERTICAL ) \n fgSizer4 = wx . FlexGridSizer ( <NUM_LIT:0> , <NUM_LIT:2> , <NUM_LIT:0> , <NUM_LIT:0> ) \n fgSizer4 . SetFlexibleDirection ( wx . HORIZONTAL ) \n fgSizer4 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n self . lblChangeGT = wx . StaticText ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n self . lblChangeGT . Wrap ( - <NUM_LIT:1> ) \n fgSizer4 . Add ( self . lblChangeGT , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . txtVChangeGT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size ( <NUM_LIT> , - <NUM_LIT:1> ) , <NUM_LIT:0> ) \n fgSizer4 . Add ( self . txtVChangeGT , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . lblChangeLT = wx . StaticText ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . DefaultSize , <NUM_LIT:0> ) \n self . lblChangeLT . Wrap ( - <NUM_LIT:1> ) \n fgSizer4 . Add ( self . lblChangeLT , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n self . txtVChangeLT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size ( <NUM_LIT> , - <NUM_LIT:1> ) , <NUM_LIT:0> ) \n fgSizer4 . Add ( self . txtVChangeLT , <NUM_LIT:0> , wx . ALL , <NUM_LIT:5> ) \n sbValChange . Add ( fgSizer4 , <NUM_LIT:1> , wx . EXPAND , <NUM_LIT:5> ) \n bsValChange . Add ( sbValChange , <NUM_LIT:1> , <NUM_LIT:0> , <NUM_LIT:5> ) \n bSizer3 . Add ( bsValChange , <NUM_LIT:1> , wx . EXPAND , <NUM_LIT:5> ) \n self . chkToggleFilterSelection = wx . CheckBox ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , \n wx . DefaultSize , <NUM_LIT:0> ) \n bSizer3 . Add ( self . chkToggleFilterSelection , <NUM_LIT:1> , wx . ALL | wx . EXPAND , <NUM_LIT:5> ) \n bsButtons = wx . BoxSizer ( wx . HORIZONTAL ) \n self . btnOK = wx . Button ( self , wx . ID_ANY , u\"<STR_LIT:OK>\" , wx . DefaultPosition , wx . Size ( <NUM_LIT:64> , <NUM_LIT> ) , <NUM_LIT:0> ) \n bsButtons . Add ( self . btnOK , <NUM_LIT:1> , wx . ALL | wx . EXPAND , <NUM_LIT:5> ) \n self . btnCancel = wx . Button ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . Size ( <NUM_LIT:64> , <NUM_LIT> ) , <NUM_LIT:0> ) \n bsButtons . Add ( self . btnCancel , <NUM_LIT:1> , wx . ALL | wx . EXPAND , <NUM_LIT:5> ) \n self . btnApply = wx . Button ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . Size ( <NUM_LIT:64> , <NUM_LIT> ) , <NUM_LIT:0> ) \n bsButtons . Add ( self . btnApply , <NUM_LIT:1> , wx . ALL | wx . EXPAND , <NUM_LIT:5> ) \n self . btnClear = wx . Button ( self , wx . ID_ANY , u\"<STR_LIT>\" , wx . DefaultPosition , wx . Size ( <NUM_LIT:64> , <NUM_LIT> ) , <NUM_LIT:0> ) \n bsButtons . Add ( self . btnClear , <NUM_LIT:1> , wx . ALL | wx . EXPAND , <NUM_LIT:5> ) \n bSizer3 . Add ( bsButtons , <NUM_LIT:1> , wx . EXPAND , <NUM_LIT:0> ) \n bSizer1 . Add ( bSizer3 , <NUM_LIT:1> , wx . EXPAND , <NUM_LIT:5> ) \n self . SetSizer ( bSizer1 ) \n self . Layout ( ) \n self . Centre ( wx . BOTH ) \n self . txtThreshValGT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . txtThreshValLT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . txtGapsVal . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . cbGapTime . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . dpAfter . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . tpAfter . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . sbAfter . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . dpBefore . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . tpBefore . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . sbBefore . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . txtVChangeGT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . txtVChangeLT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n self . chkToggleFilterSelection . Bind ( wx . EVT_CHECKBOX , self . onCheckBox ) \n self . btnClear . Bind ( wx . EVT_BUTTON , self . onBtnClearButton ) \n self . btnOK . Bind ( wx . EVT_BUTTON , self . onBtnOKButton ) \n self . btnCancel . Bind ( wx . EVT_BUTTON , self . onBtnCancelButton ) \n self . btnApply . Bind ( wx . EVT_BUTTON , self . onBtnApplyButton ) \n def __del__ ( self ) : \n pass \n def onSetFocus ( self , event ) : \n event . Skip ( ) \n def onCheckBox ( self , event ) : \n event . Skip ( ) \n def onBtnClearButton ( self , event ) : \n event . Skip ( ) \n def onBtnOKButton ( self , event ) : \n event . Skip ( ) \n def onBtnCancelButton ( self , event ) : \n event . Skip ( ) \n def onBtnApplyButton ( self , event ) : \n event . <mask0> ( ) \n", "gt": "Skip"}
{"input": "\n from odmtools . odmdata import SessionFactory \n class TestSessionFactory : \n def setup ( self ) : \n self . connection_string = \"<STR_LIT>\" \n self . session_factory = SessionFactory ( self . connection_string , echo = True ) \n def test_create_session_factory ( self ) : \n assert repr ( self . session_factory ) == \"<STR_LIT>\" % self . connection_string \n assert self . session_factory . Session != None \n def test_get_session ( self ) : \n session = self . session_factory . get_session ( ) \n assert '<STR_LIT>' in repr ( <mask0> ) \n", "gt": "session"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n class DataTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class DimTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class ArityTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class IndexTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class NameTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class SetTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class SizeTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class SubsetIndexOutOfBounds ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class SparsityTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class MapTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class DataSetTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class MatTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class DatTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class KernelTypeError ( TypeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class DataValueError ( ValueError ) : \n \"\"\"<STR_LIT>\"\"\" \n class IndexValueError ( ValueError ) : \n \"\"\"<STR_LIT>\"\"\" \n class ModeValueError ( ValueError ) : \n \"\"\"<STR_LIT>\"\"\" \n class IterateValueError ( ValueError ) : \n \"\"\"<STR_LIT>\"\"\" \n class SetValueError ( ValueError ) : \n \"\"\"<STR_LIT>\"\"\" \n class MapValueError ( ValueError ) : \n \"\"\"<STR_LIT>\"\"\" \n class ConfigurationError ( RuntimeError ) : \n \"\"\"<STR_LIT>\"\"\" \n class CompilationError ( <mask0> ) : \n \"\"\"<STR_LIT>\"\"\" \n", "gt": "RuntimeError"}
{"input": "\n import pytest \n import numpy \n from random import randrange \n from pyop2 import plan as _plan \n from pyop2 import op2 \n backends = [ '<STR_LIT>' , '<STR_LIT>' ] \n valuetype = numpy . float64 \n NUM_ELE = <NUM_LIT:12> \n NUM_NODES = <NUM_LIT> \n NUM_ENTRIES = <NUM_LIT:4> \n class TestColoring : \n \"\"\"<STR_LIT>\"\"\" \n @ pytest . fixture \n def nodes ( cls ) : \n return op2 . Set ( NUM_NODES , \"<STR_LIT>\" ) \n @ pytest . fixture \n def elements ( cls ) : \n return op2 . Set ( NUM_ELE , \"<STR_LIT>\" ) \n @ pytest . fixture \n def dnodes ( cls , nodes ) : \n return op2 . DataSet ( nodes , <NUM_LIT:1> , \"<STR_LIT>\" ) \n @ pytest . fixture \n def elem_node_map ( cls ) : \n v = [ randrange ( NUM_ENTRIES ) for i in range ( NUM_ELE * <NUM_LIT:3> ) ] \n return numpy . asarray ( v , dtype = numpy . uint32 ) \n @ pytest . fixture \n def elem_node ( cls , elements , nodes , elem_node_map ) : \n return op2 . Map ( elements , nodes , <NUM_LIT:3> , elem_node_map , \"<STR_LIT>\" ) \n @ pytest . fixture \n def mat ( cls , elem_node , dnodes ) : \n sparsity = op2 . Sparsity ( ( dnodes , dnodes ) , ( elem_node , elem_node ) , \"<STR_LIT>\" ) \n return op2 . Mat ( sparsity , valuetype , \"<STR_LIT>\" ) \n @ pytest . fixture \n def x ( cls , dnodes ) : \n return op2 . Dat ( dnodes , numpy . zeros ( NUM_NODES , dtype = numpy . uint32 ) , numpy . uint32 , \"<STR_LIT:x>\" ) \n def test_thread_coloring ( self , backend , skip_opencl , elements , elem_node_map , elem_node , mat , x ) : \n assert NUM_ELE % <NUM_LIT:2> == <NUM_LIT:0> , \"<STR_LIT>\" \n plan = _plan . Plan ( elements . all_part , \n mat ( op2 . INC , ( elem_node [ op2 . i [ <NUM_LIT:0> ] ] , \n elem_node [ op2 . i [ <NUM_LIT:1> ] ] ) ) , \n x ( op2 . WRITE , elem_node [ <NUM_LIT:0> ] ) , \n partition_size = NUM_ELE / <NUM_LIT:2> , \n matrix_coloring = True ) \n assert plan . nblocks == <NUM_LIT:2> \n eidx = <NUM_LIT:0> \n for p in range ( plan . nblocks ) : \n for thrcol in range ( plan . nthrcol [ p ] ) : \n counter = numpy . zeros ( NUM_NODES , dtype = numpy . uint32 ) \n for e in range ( eidx , eidx + plan . nelems [ p ] ) : \n if plan . thrcol [ e ] == thrcol : \n counter [ elem_node . values [ e ] [ <NUM_LIT:0> ] ] += <NUM_LIT:1> \n assert ( counter < <NUM_LIT:2> ) . all ( ) \n eidx += plan . nelems [ <mask0> ] \n", "gt": "p"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from matplotlib import pyplot as plt \n import oscaar \n import astrometry \n import photometry \n import dataBank \n import systematics \n import IO \n import pyfits \n plt . ion ( ) \n data = dataBank . dataBank ( ) \n allStars = data . getDict ( ) \n outputPath = data . outputPath \n N_exposures = len ( data . getPaths ( ) ) \n meanDarkFrame = data . getMeanDarkFrame ( ) \n masterFlat = data . masterFlat \n plottingThings , statusBarFig , statusBarAx = IO . plottingSettings ( data . trackPlots , data . photPlots ) \n for expNumber in xrange ( N_exposures ) : \n if statusBarAx is not None and expNumber % <NUM_LIT:15> == <NUM_LIT:0> : \n plt . cla ( ) \n statusBarAx . set_title ( '<STR_LIT>' ) \n statusBarAx . set_xlim ( [ <NUM_LIT:0> , <NUM_LIT:100> ] ) \n statusBarAx . set_xlabel ( '<STR_LIT>' ) \n statusBarAx . get_yaxis ( ) . set_ticks ( [ ] ) \n statusBarAx . barh ( [ <NUM_LIT:0> ] , [ <NUM_LIT> * expNumber / len ( data . getPaths ( ) ) ] , \n [ <NUM_LIT:1> ] , color = '<STR_LIT:k>' ) \n image = ( pyfits . getdata ( data . getPaths ( ) [ expNumber ] ) - meanDarkFrame ) / masterFlat \n data . storeTime ( expNumber ) \n for star in allStars : \n est_x , est_y = data . centroidInitialGuess ( expNumber , star ) \n x , y , radius , trackFlag = astrometry . trackSmooth ( image , est_x , est_y , \n data . smoothConst , \n plottingThings , \n zoom = data . trackingZoom , \n plots = data . trackPlots ) \n data . storeCentroid ( star , expNumber , x , y ) \n fluxes , errors , photFlags = photometry . multirad ( image , x , y , \n data . apertureRadii , \n plottingThings , \n ccdGain = data . ccdGain , \n plots = data . photPlots ) \n photFlag = any ( photFlags ) \n data . storeFluxes ( star , expNumber , fluxes , errors ) \n if trackFlag or photFlag and not data . getFlag ( ) : \n data . setFlag ( star , False ) \n if data . trackPlots or data . photPlots : \n plt . draw ( ) \n if statusBarAx is not None and expNumber % <NUM_LIT:15> == <NUM_LIT:0> : \n plt . draw ( ) \n plt . close ( ) \n data . scaleFluxes_multirad ( ) \n meanComparisonStars , meanComparisonStarErrors = data . calcMeanComparison_multirad ( ccdGain = data . ccdGain ) \n lightCurves , lightCurveErrors = data . computeLightCurve_multirad ( meanComparisonStars , \n meanComparisonStarErrors ) \n oscaar . IO . save ( data , outputPath ) \n data . <mask0> ( ) \n", "gt": "plotLightCurve_multirad"}
{"input": "\n '''<STR_LIT>''' \n from RESTfulResource import RESTfulResource \n from urlparse import urlparse \n class Observers ( RESTfulResource ) : \n def __init__ ( self ) : \n RESTfulResource . __init__ ( self ) \n self . __schemes = [ '<STR_LIT:http>' , '<STR_LIT>' , '<STR_LIT>' ] \n self . __observers = [ ] \n def onUpdate ( self , resource ) : \n self . __onUpdate ( resource ) \n def __onUpdate ( self , resource ) : \n for self . __observer in self . __observers : \n self . __notify ( self . __observer , resource ) \n def __notify ( self , observer , resource ) : \n if type ( observer ) is not callable : \n urlObject = urlparse ( observer ) \n if urlObject . scheme == '<STR_LIT:http>' : \n self . __httpNotify ( observer , resource ) \n elif urlObject . scheme == '<STR_LIT>' : \n self . __coapNotify ( observer , resource ) \n elif urlObject . scheme == '<STR_LIT>' : \n self . __callbackNotify ( observer , resource ) \n else : \n observer ( resource ) \n def __httpNotify ( self , targetURI , resource ) : \n print '<STR_LIT>' \n def __coapNotify ( self , targetURI , resource ) : \n print '<STR_LIT>' \n def __callbackNotify ( self , observer , resource ) : \n print '<STR_LIT>' \n def get ( self , targetURI = None ) : \n if targetURI != None : \n if targetURI in self . __observers : \n return targetURI \n return None \n return self . __observers \n def set ( self , targetURI ) : \n self . create ( targetURI ) \n def create ( self , targetURI ) : \n if urlparse ( targetURI ) . scheme not in self . __schemes : \n return None \n if targetURI not in self . __observers : \n self . __observers . append ( targetURI ) \n return targetURI \n def delete ( self , targetURI ) : \n if targetURI in self . __observers : \n self . __observers . remove ( targetURI ) \n return targetURI \n return <mask0> \n", "gt": "None"}
{"input": "\n import json \n import time \n import requests \n from pywechat . excepts import WechatError \n class Basic ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , app_id , app_secret ) : \n \"\"\"<STR_LIT>\"\"\" \n self . __app_id = app_id \n self . __app_secret = app_secret \n self . __access_token = self . access_token \n self . __token_expires_at = None \n @ property \n def access_token ( self ) : \n '''<STR_LIT>''' \n if self . __access_token and self . __token_expires_at : \n if self . __token_expires_at - time . time ( ) > <NUM_LIT> : \n return self . __access_token \n self . _grant_access_token ( ) \n return self . __access_token \n def _send_request ( self , method , url , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n if not kwargs . get ( '<STR_LIT>' ) : \n kwargs [ '<STR_LIT>' ] = { \n \"<STR_LIT>\" : self . access_token \n } \n if kwargs . get ( '<STR_LIT:data>' ) : \n data = json . dumps ( kwargs [ '<STR_LIT:data>' ] ) . encode ( '<STR_LIT:utf-8>' ) \n kwargs [ \"<STR_LIT:data>\" ] = data \n request = requests . request ( \n method = method , \n url = url , \n ** kwargs \n ) \n request . raise_for_status ( ) \n json_data = request . json ( ) \n self . _check_wechat_error ( json_data ) \n return json_data \n @ classmethod \n def _check_wechat_error ( cls , json_data ) : \n \"\"\"<STR_LIT>\"\"\" \n errcode = json_data . get ( '<STR_LIT>' ) \n if errcode and errcode != <NUM_LIT:0> : \n raise WechatError ( errcode , json_data . get ( '<STR_LIT>' ) ) \n def _grant_access_token ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n url = '<STR_LIT>' \n params = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : self . __app_id , \n \"<STR_LIT>\" : self . __app_secret \n } \n json_data = self . _send_request ( '<STR_LIT>' , url , params = params ) \n self . __access_token = json_data . get ( '<STR_LIT>' ) \n self . __token_expires_at = int ( \n time . time ( ) ) + json_data . get ( '<STR_LIT>' ) \n return json_data \n def _get_wechat_server_ips ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n url = \"<STR_LIT>\" \n params = { \n \"<STR_LIT>\" : self . access_token \n } \n json_data = self . _send_request ( '<STR_LIT>' , url , params = params ) \n return <mask0> \n", "gt": "json_data"}
{"input": "\n import json \n from fabric . api import * \n from . deployer . configuration import Configuration \n from . deployer . helpers import mkdir , rmdir \n from . deployer . standard_packages import package_list \n import os \n from StringIO import StringIO \n site_settings = { \n \"<STR_LIT>\" : '<STR_LIT>' , \n \"<STR_LIT>\" : '<STR_LIT>' , \n \"<STR_LIT>\" : '<STR_LIT>' , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : '<STR_LIT>' , \n } \n def load_configuration ( ) : \n with open ( os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , '<STR_LIT>' ) , '<STR_LIT:r>' ) as f : \n configuration_file = '<STR_LIT>' . join ( f . readlines ( ) ) \n conf = json . JSONDecoder ( ) . decode ( configuration_file ) \n print \"<STR_LIT>\" . format ( conf [ '<STR_LIT>' ] ) \n env . hosts . append ( conf [ '<STR_LIT>' ] ) \n env . hosts_data = Configuration ( conf , site_settings ) \n load_configuration ( ) \n @ task \n def install_requirements ( ) : \n sudo ( '<STR_LIT>' ) \n sudo ( '<STR_LIT>' ) \n sudo ( '<STR_LIT>' . format ( package_list ( ) ) ) \n @ task \n def create_folders ( ) : \n mkdir ( env . hosts_data . base_path ( ) ) \n mkdir ( env . hosts_data . log_path ( ) ) \n @ task \n def create_virtual_environment ( ) : \n with cd ( env . hosts_data . base_path ( ) ) : \n run ( '<STR_LIT>' . format ( env . hosts_data . virtualenv_path ( ) ) ) \n with prefix ( \"<STR_LIT>\" . format ( env . hosts_data . virtualenv_activate_path ( ) ) ) : \n run ( '<STR_LIT>' ) \n run ( '<STR_LIT>' . format ( env . hosts_data . requirements_path ( ) ) ) \n if env . hosts_data . is_mysql ( ) : \n run ( '<STR_LIT>' ) \n run ( '<STR_LIT>' ) \n @ task \n def create_local_settings ( ) : \n rmdir ( env . hosts_data . local_settings_path ( ) ) \n put ( StringIO ( env . hosts_data . local_settings ( ) ) , env . hosts_data . local_settings_path ( ) ) \n @ task \n def create_gunicorn_config ( ) : \n rmdir ( env . hosts_data . gunicorn_config_path ( ) ) \n put ( StringIO ( env . hosts_data . gunicorn_config ( ) ) , env . hosts_data . gunicorn_config_path ( ) ) \n sudo ( '<STR_LIT>' . format ( env . hosts_data . gunicorn_config_path ( ) ) ) \n @ task \n def create_demo_superuser ( ) : \n with cd ( env . hosts_data . app_path ( ) ) : \n with prefix ( \"<STR_LIT>\" . format ( env . hosts_data . virtualenv_activate_path ( ) ) ) : \n commands = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" \n ] \n run ( '<STR_LIT:U+0020>' . join ( commands ) ) \n @ task \n def create_gunicorn_supervisor ( ) : \n rmdir ( env . hosts_data . gunicorn_supervisor_config_path ( ) ) \n put ( \n StringIO ( env . hosts_data . gunicorn_supervisor_config ( ) ) , \n env . hosts_data . gunicorn_supervisor_config_path ( ) , \n use_sudo = True \n ) \n sudo ( '<STR_LIT>' ) \n sudo ( '<STR_LIT>' ) \n @ task \n def create_celery_supervisor ( ) : \n rmdir ( env . hosts_data . celery_supervisor_config_path ( ) ) \n put ( \n StringIO ( env . hosts_data . celery_supervisor_config ( ) ) , \n env . hosts_data . celery_supervisor_config_path ( ) , \n use_sudo = True \n ) \n sudo ( '<STR_LIT>' ) \n sudo ( '<STR_LIT>' ) \n @ task \n def create_nginx_config ( ) : \n put ( StringIO ( env . hosts_data . nginx_config ( ) ) , env . hosts_data . nginx_available_path ( ) , use_sudo = True ) \n sudo ( '<STR_LIT>' . format ( env . hosts_data . nginx_available_path ( ) , env . hosts_data . nginx_enabled_path ( ) ) ) \n sudo ( '<STR_LIT>' ) \n @ task \n def delete_nginx_config ( ) : \n rmdir ( env . hosts_data . nginx_enabled_path ( ) , sudo_access = True ) \n rmdir ( env . hosts_data . nginx_available_path ( ) , sudo_access = True ) \n sudo ( '<STR_LIT>' ) \n @ task \n def migrate_database ( ) : \n with cd ( env . hosts_data . app_path ( ) ) : \n with prefix ( \"<STR_LIT>\" . format ( env . hosts_data . virtualenv_activate_path ( ) ) ) : \n run ( \"<STR_LIT>\" ) \n run ( \"<STR_LIT>\" ) \n @ task \n def collect_static ( ) : \n with cd ( env . hosts_data . app_path ( ) ) : \n with prefix ( \"<STR_LIT>\" . format ( env . hosts_data . virtualenv_activate_path ( ) ) ) : \n run ( \"<STR_LIT>\" ) \n @ task \n def delete_folders ( ) : \n rmdir ( env . hosts_data . base_path ( ) ) \n @ task \n def delete_gunicorn_supervisor ( ) : \n server_stop ( ) \n rmdir ( env . hosts_data . gunicorn_supervisor_config_path ( ) , sudo_access = True ) \n sudo ( '<STR_LIT>' ) \n sudo ( '<STR_LIT>' ) \n @ task \n def delete_celery_supervisor ( ) : \n server_stop ( ) \n rmdir ( env . hosts_data . celery_supervisor_config_path ( ) , sudo_access = True ) \n sudo ( '<STR_LIT>' ) \n sudo ( '<STR_LIT>' ) \n @ task ( default = True ) \n def make_deploy ( ) : \n install_requirements ( ) \n create_folders ( ) \n run ( env . hosts_data . git_clone_command ( ) ) \n with cd ( env . hosts_data . app_path ( ) ) : \n run ( env . hosts_data . git_checkout_command ( ) ) \n create_virtual_environment ( ) \n create_local_settings ( ) \n migrate_database ( ) \n create_demo_superuser ( ) \n collect_static ( ) \n create_gunicorn_config ( ) \n create_gunicorn_supervisor ( ) \n create_celery_supervisor ( ) \n create_nginx_config ( ) \n @ task \n def server_status ( ) : \n sudo ( '<STR_LIT>' . format ( env . hosts_data . application_name ( ) ) ) \n @ task \n def server_stop ( ) : \n sudo ( '<STR_LIT>' . format ( env . hosts_data . application_name ( ) ) ) \n sudo ( '<STR_LIT>' . format ( env . hosts_data . application_name ( ) ) ) \n @ task \n def server_start ( ) : \n sudo ( '<STR_LIT>' . format ( env . hosts_data . application_name ( ) ) ) \n sudo ( '<STR_LIT>' . format ( env . hosts_data . application_name ( ) ) ) \n @ task \n def server_restart ( ) : \n sudo ( '<STR_LIT>' . format ( env . hosts_data . application_name ( ) ) ) \n sudo ( '<STR_LIT>' . format ( env . hosts_data . application_name ( ) ) ) \n @ task \n def destroy_deploy ( ) : \n delete_folders ( ) \n delete_gunicorn_supervisor ( ) \n delete_celery_supervisor ( ) \n delete_nginx_config ( ) \n @ task \n def update_deploy ( ) : \n server_stop ( ) \n with cd ( env . hosts_data . app_path ( ) ) : \n with prefix ( \"<STR_LIT>\" . format ( env . hosts_data . virtualenv_activate_path ( ) ) ) : \n run ( '<STR_LIT>' ) \n run ( '<STR_LIT>' . format ( env . hosts_data . requirements_path ( ) ) ) \n migrate_database ( ) \n collect_static ( ) \n <mask0> ( ) \n", "gt": "server_start"}
{"input": "\n from django . conf . urls import patterns , url , include \n from haystack . views import search_view_factory , SearchView \n from offers . feeds import OfferFeed , OfferAtomFeed \n from offers . forms import OfferSearchForm \n from . import views as offer_views \n urlpatterns = patterns ( '<STR_LIT>' , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = '<STR_LIT>' ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = '<STR_LIT>' ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , search_view_factory ( \n view_class = SearchView , \n form_class = OfferSearchForm , \n template = '<STR_LIT>' , \n results_per_page = <NUM_LIT:8> , \n ) , \n name = '<STR_LIT>' , \n ) , \n url ( r'<STR_LIT>' , OfferFeed ( ) , name = '<STR_LIT>' ) , \n url ( r'<STR_LIT>' , OfferAtomFeed ( ) , name = '<STR_LIT>' ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( \n r'<STR_LIT>' , \n '<STR_LIT>' , \n name = \"<STR_LIT>\" \n ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = \"<STR_LIT>\" ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , <mask0> = \"<STR_LIT>\" ) , \n ) \n", "gt": "name"}
{"input": "\n __productname__ = '<STR_LIT>' \n __version__ = \"<STR_LIT>\" \n __copyright__ = \"<STR_LIT>\" \n __author__ = \"<STR_LIT>\" \n __author_email__ = \"<STR_LIT>\" \n __description__ = \"<STR_LIT>\" \n __license__ = \"<STR_LIT>\" \n __homepage__ = \"<STR_LIT>\" \n from imapfw . init import Imapfw \n from imapfw import <mask0> \n", "gt": "runtime"}
{"input": "\n from imapfw import runtime \n from . manager import Manager \n class FolderManager ( Manager ) : \n def __init__ ( self ) : \n super ( FolderManager , self ) . __init__ ( ) \n self . rascal = runtime . <mask0> \n", "gt": "rascal"}
{"input": "\n '''<STR_LIT>''' \n from __future__ import unicode_literals \n from . . model . hashes import Hashes \n from . . one_drive_object_base import OneDriveObjectBase \n class File ( OneDriveObjectBase ) : \n def __init__ ( self , prop_dict = { } ) : \n self . _prop_dict = prop_dict \n @ property \n def hashes ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if \"<STR_LIT>\" in self . _prop_dict : \n if isinstance ( self . _prop_dict [ \"<STR_LIT>\" ] , OneDriveObjectBase ) : \n return self . _prop_dict [ \"<STR_LIT>\" ] \n else : \n self . _prop_dict [ \"<STR_LIT>\" ] = Hashes ( self . _prop_dict [ \"<STR_LIT>\" ] ) \n return self . _prop_dict [ \"<STR_LIT>\" ] \n return None \n @ hashes . setter \n def hashes ( self , val ) : \n self . _prop_dict [ \"<STR_LIT>\" ] = val \n @ property \n def mime_type ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if \"<STR_LIT>\" in self . _prop_dict : \n return self . _prop_dict [ \"<STR_LIT>\" ] \n else : \n return None \n @ mime_type . setter \n def mime_type ( self , val ) : \n self . _prop_dict [ \"<STR_LIT>\" ] = <mask0> \n", "gt": "val"}
{"input": "\n '''<STR_LIT>''' \n class RequestBuilderBase ( object ) : \n def __init__ ( self , request_url , client ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _request_url = request_url \n self . _client = client \n def append_to_request_url ( self , url_segment ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _request_url + \"<STR_LIT:/>\" + <mask0> \n", "gt": "url_segment"}
{"input": "\n '''<STR_LIT>''' \n from __future__ import unicode_literals \n from . . collection_base import CollectionRequestBase , CollectionResponseBase , CollectionPageBase \n from . . request_builder_base import RequestBuilderBase \n from . . model . item import Item \n import json \n class SharedCollectionRequest ( CollectionRequestBase ) : \n def __init__ ( self , request_url , client , options ) : \n \"\"\"<STR_LIT>\"\"\" \n super ( SharedCollectionRequest , self ) . __init__ ( request_url , client , options ) \n def get ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . method = \"<STR_LIT:GET>\" \n collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n return self . _page_from_response ( collection_response ) \n class SharedCollectionRequestBuilder ( RequestBuilderBase ) : \n def __getitem__ ( self , key ) : \n \"\"\"<STR_LIT>\"\"\" \n return ItemRequestBuilder ( self . append_to_request_url ( str ( key ) ) , self . _client ) \n def request ( self , expand = None , select = None , top = None , order_by = None , options = None ) : \n \"\"\"<STR_LIT>\"\"\" \n req = SharedCollectionRequest ( self . _request_url , self . _client , options ) \n req . _set_query_options ( expand = expand , select = select , top = top , order_by = order_by ) \n return req \n def get ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . request ( ) . get ( ) \n class SharedCollectionResponse ( CollectionResponseBase ) : \n @ property \n def collection_page ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _collection_page : \n self . _collection_page . _prop_list = self . _prop_dict [ \"<STR_LIT:value>\" ] \n else : \n self . _collection_page = SharedCollectionPage ( self . _prop_dict [ \"<STR_LIT:value>\" ] ) \n return self . _collection_page \n class SharedCollectionPage ( CollectionPageBase ) : \n def __getitem__ ( self , index ) : \n \"\"\"<STR_LIT>\"\"\" \n return Item ( self . _prop_list [ index ] ) \n def shared ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for item in self . _prop_list : \n yield Item ( item ) \n def _init_next_page_request ( self , next_page_link , client , options ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _next_page_request = SharedCollectionRequest ( next_page_link , client , options ) \n from . . request . item_request_builder import <mask0> \n", "gt": "ItemRequestBuilder"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n <mask0> = '<STR_LIT>' \n", "gt": "__version__"}
{"input": "\n from __future__ import absolute_import , division , print_function \n import glob \n import os . path \n from cffi import FFI \n from cffi . verifier import Verifier \n __all__ = [ \"<STR_LIT>\" ] \n HEADERS = glob . glob ( \n os . path . join ( os . path . abspath ( os . path . dirname ( __file__ ) ) , \"<STR_LIT>\" ) \n ) \n ffi = FFI ( ) \n for header in sorted ( HEADERS ) : \n with open ( header , \"<STR_LIT:r>\" ) as hfile : \n ffi . cdef ( hfile . read ( ) ) \n ffi . verifier = Verifier ( \n ffi , \n \"<STR_LIT>\" , \n libraries = [ \"<STR_LIT>\" ] , \n ext_package = \"<STR_LIT>\" , \n ) \n class Library ( object ) : \n def __init__ ( self , ffi ) : \n self . ffi = ffi \n self . _lib = None \n def _compile_module ( * args , ** kwargs ) : \n raise RuntimeError ( \"<STR_LIT>\" ) \n self . ffi . verifier . compile_module = _compile_module \n def __getattr__ ( self , name ) : \n if self . _lib is None : \n self . _lib = self . ffi . verifier . load_library ( ) \n return getattr ( self . _lib , name ) \n lib = Library ( <mask0> ) \n", "gt": "ffi"}
{"input": "\n import sqlite3 \n def migrate ( database_path ) : \n print \"<STR_LIT>\" \n conn = sqlite3 . connect ( database_path ) \n conn . text_factory = str \n cursor = conn . cursor ( ) \n cursor . execute ( '''<STR_LIT>''' ) \n notifications = cursor . fetchall ( ) \n cursor . execute ( '''<STR_LIT>''' ) \n cursor . execute ( '''<STR_LIT>''' ) \n cursor . execute ( '''<STR_LIT>''' ) \n for n in notifications : \n cursor . execute ( '''<STR_LIT>''' , ( n [ <NUM_LIT:0> ] , n [ <NUM_LIT:1> ] , n [ <NUM_LIT:2> ] , n [ <NUM_LIT:3> ] , n [ <NUM_LIT:4> ] , n [ <NUM_LIT:5> ] , n [ <NUM_LIT:6> ] , n [ <NUM_LIT:7> ] , n [ <NUM_LIT:8> ] ) ) \n cursor . execute ( '''<STR_LIT>''' ) \n conn . commit ( ) \n conn . <mask0> ( ) \n", "gt": "close"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n from twisted . python import log \n DEBUG = <NUM_LIT:5> \n WARNING = <NUM_LIT:4> \n INFO = <NUM_LIT:3> \n ERROR = <NUM_LIT:2> \n CRITICAL = <NUM_LIT:1> \n levels = { \"<STR_LIT>\" : <NUM_LIT:5> , \"<STR_LIT>\" : <NUM_LIT:4> , \"<STR_LIT:info>\" : <NUM_LIT:3> , \"<STR_LIT:error>\" : <NUM_LIT:2> , \"<STR_LIT>\" : <NUM_LIT:1> } \n class FileLogObserver ( log . FileLogObserver ) : \n def __init__ ( self , f = None , level = \"<STR_LIT:info>\" , default = DEBUG ) : \n log . FileLogObserver . __init__ ( self , f or sys . stdout ) \n self . level = levels [ level ] \n self . default = default \n def emit ( self , eventDict ) : \n ll = eventDict . get ( '<STR_LIT>' , self . default ) \n if eventDict [ '<STR_LIT>' ] or '<STR_LIT>' in eventDict or self . level >= ll : \n log . FileLogObserver . emit ( self , eventDict ) \n class Logger ( object ) : \n def __init__ ( self , ** kwargs ) : \n self . kwargs = kwargs \n def msg ( self , message , ** kw ) : \n kw . update ( self . kwargs ) \n if '<STR_LIT>' in kw and not isinstance ( kw [ '<STR_LIT>' ] , str ) : \n kw [ '<STR_LIT>' ] = kw [ '<STR_LIT>' ] . __class__ . __name__ \n log . msg ( message , ** kw ) \n def info ( self , message , ** kw ) : \n kw [ '<STR_LIT>' ] = INFO \n self . msg ( \"<STR_LIT>\" % message , ** kw ) \n def debug ( self , message , ** kw ) : \n kw [ '<STR_LIT>' ] = DEBUG \n self . msg ( \"<STR_LIT>\" % message , ** kw ) \n def warning ( self , message , ** kw ) : \n kw [ '<STR_LIT>' ] = WARNING \n self . msg ( \"<STR_LIT>\" % message , ** kw ) \n def error ( self , message , ** kw ) : \n kw [ '<STR_LIT>' ] = ERROR \n self . msg ( \"<STR_LIT>\" % message , ** kw ) \n def critical ( self , message , ** kw ) : \n kw [ '<STR_LIT>' ] = CRITICAL \n self . msg ( \"<STR_LIT>\" % message , ** kw ) \n try : \n theLogger \n except NameError : \n theLogger = Logger ( ) \n msg = theLogger . msg \n info = theLogger . info \n debug = theLogger . debug \n warning = theLogger . warning \n error = theLogger . error \n critical = theLogger . <mask0> \n", "gt": "critical"}
{"input": "\n import sys \n _b = sys . version_info [ <NUM_LIT:0> ] < <NUM_LIT:3> and ( lambda x : x ) or ( lambda x : x . encode ( '<STR_LIT>' ) ) \n from google . protobuf import descriptor as _descriptor \n from google . protobuf import message as _message \n from google . protobuf import reflection as _reflection \n from google . protobuf import symbol_database as _symbol_database \n from google . protobuf import descriptor_pb2 \n _sym_db = _symbol_database . Default ( ) \n DESCRIPTOR = _descriptor . FileDescriptor ( \n name = '<STR_LIT>' , \n package = '<STR_LIT>' , \n serialized_pb = _b ( '<STR_LIT>' ) \n ) \n _sym_db . RegisterFileDescriptor ( DESCRIPTOR ) \n _PEERSEEDS = _descriptor . Descriptor ( \n name = '<STR_LIT>' , \n full_name = '<STR_LIT>' , \n filename = None , \n file = DESCRIPTOR , \n containing_type = None , \n fields = [ \n _descriptor . FieldDescriptor ( \n name = '<STR_LIT>' , full_name = '<STR_LIT>' , index = <NUM_LIT:0> , \n number = <NUM_LIT:1> , type = <NUM_LIT:12> , cpp_type = <NUM_LIT:9> , label = <NUM_LIT:3> , \n has_default_value = False , default_value = [ ] , \n message_type = None , enum_type = None , containing_type = None , \n is_extension = False , extension_scope = None , \n options = None ) , \n _descriptor . FieldDescriptor ( \n name = '<STR_LIT>' , full_name = '<STR_LIT>' , index = <NUM_LIT:1> , \n number = <NUM_LIT:2> , type = <NUM_LIT:12> , cpp_type = <NUM_LIT:9> , label = <NUM_LIT:2> , \n has_default_value = False , default_value = _b ( \"<STR_LIT>\" ) , \n message_type = None , enum_type = None , containing_type = None , \n is_extension = False , extension_scope = None , \n options = None ) , \n ] , \n extensions = [ \n ] , \n nested_types = [ ] , \n enum_types = [ \n ] , \n options = None , \n is_extendable = False , \n extension_ranges = [ ] , \n oneofs = [ \n ] , \n serialized_start = <NUM_LIT:15> , \n serialized_end = <NUM_LIT> , \n ) \n DESCRIPTOR . message_types_by_name [ '<STR_LIT>' ] = _PEERSEEDS \n PeerSeeds = _reflection . GeneratedProtocolMessageType ( '<STR_LIT>' , ( _message . Message , ) , dict ( \n DESCRIPTOR = _PEERSEEDS , \n __module__ = '<STR_LIT>' \n ) ) \n _sym_db . RegisterMessage ( <mask0> ) \n", "gt": "PeerSeeds"}
{"input": "\n from kmip . core . enums import Tags \n from kmip . core . primitives import Struct \n from kmip . core . primitives import ByteString \n from kmip . core . utils import BytearrayStream \n class RawKey ( ByteString ) : \n def __init__ ( self , value = None ) : \n super ( RawKey , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n class OpaqueKey ( ByteString ) : \n def __init__ ( self , value = None ) : \n super ( OpaqueKey , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n class PKCS1Key ( ByteString ) : \n def __init__ ( self , value = None ) : \n super ( PKCS1Key , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n class PKCS8Key ( ByteString ) : \n def __init__ ( self , value = None ) : \n super ( PKCS8Key , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n class X509Key ( ByteString ) : \n def __init__ ( self , value = None ) : \n super ( X509Key , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n class ECPrivateKey ( ByteString ) : \n def __init__ ( self , value = None ) : \n super ( ECPrivateKey , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n class TransparentSymmetricKey ( Struct ) : \n class Key ( ByteString ) : \n def __init__ ( self , value = None ) : \n super ( TransparentSymmetricKey . Key , self ) . __init__ ( value , Tags . KEY ) \n def __init__ ( self , key = None ) : \n super ( TransparentSymmetricKey , self ) . __init__ ( Tags . KEY_MATERIAL ) \n self . key = key \n self . validate ( ) \n def read ( self , istream ) : \n super ( TransparentSymmetricKey , self ) . read ( istream ) \n tstream = BytearrayStream ( istream . read ( self . length ) ) \n self . key = TransparentSymmetricKey . Key ( ) \n self . key . read ( tstream ) \n self . is_oversized ( tstream ) \n self . validate ( ) \n def write ( self , ostream ) : \n tstream = BytearrayStream ( ) \n self . key . write ( tstream ) \n self . length = tstream . length ( ) \n super ( TransparentSymmetricKey , self ) . write ( ostream ) \n ostream . write ( tstream . buffer ) \n def validate ( self ) : \n self . __validate ( ) \n def __validate ( self ) : \n <mask0> \n", "gt": "pass"}
{"input": "\n import logging \n import sys \n from kmip . core import enums \n from kmip . demos import utils \n from kmip . pie import client \n from kmip . pie import objects \n if __name__ == '<STR_LIT:__main__>' : \n logger = utils . build_console_logger ( logging . INFO ) \n parser = utils . build_cli_parser ( ) \n opts , args = parser . parse_args ( sys . argv [ <NUM_LIT:1> : ] ) \n config = opts . config \n value = b'<STR_LIT>' \n opaque_type = enums . OpaqueDataType . NONE \n name = '<STR_LIT>' \n obj = objects . OpaqueObject ( value , opaque_type , name ) \n with client . ProxyKmipClient ( config = config ) as client : \n try : \n uid = client . register ( obj ) \n logger . info ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( uid ) ) \n except Exception as e : \n logger . error ( <mask0> ) \n", "gt": "e"}
{"input": "\n import logging \n import os \n import six \n from six . moves import configparser \n from kmip . core import exceptions \n class KmipServerConfig ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _logger = logging . getLogger ( '<STR_LIT>' ) \n self . settings = dict ( ) \n self . _expected_settings = [ \n '<STR_LIT>' , \n '<STR_LIT:port>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] \n def set_setting ( self , setting , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if setting not in self . _expected_settings : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" . format ( setting ) \n ) \n if setting == '<STR_LIT>' : \n self . _set_hostname ( value ) \n elif setting == '<STR_LIT:port>' : \n self . _set_port ( value ) \n elif setting == '<STR_LIT>' : \n self . _set_certificate_path ( value ) \n elif setting == '<STR_LIT>' : \n self . _set_key_path ( value ) \n elif setting == '<STR_LIT>' : \n self . _set_ca_path ( value ) \n else : \n self . _set_auth_suite ( value ) \n def load_settings ( self , path ) : \n \"\"\"<STR_LIT>\"\"\" \n if not os . path . exists ( path ) : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( path ) \n ) \n self . _logger . info ( \n \"<STR_LIT>\" . format ( path ) \n ) \n parser = configparser . SafeConfigParser ( ) \n parser . read ( path ) \n self . _parse_settings ( parser ) \n def _parse_settings ( self , parser ) : \n if not parser . has_section ( '<STR_LIT>' ) : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n settings = [ x [ <NUM_LIT:0> ] for x in parser . items ( '<STR_LIT>' ) ] \n for setting in settings : \n if setting not in self . _expected_settings : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( setting ) \n ) \n for setting in self . _expected_settings : \n if setting not in settings : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( setting ) \n ) \n if parser . has_option ( '<STR_LIT>' , '<STR_LIT>' ) : \n self . _set_hostname ( parser . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n if parser . has_option ( '<STR_LIT>' , '<STR_LIT:port>' ) : \n self . _set_port ( parser . getint ( '<STR_LIT>' , '<STR_LIT:port>' ) ) \n if parser . has_option ( '<STR_LIT>' , '<STR_LIT>' ) : \n self . _set_certificate_path ( parser . get ( \n '<STR_LIT>' , \n '<STR_LIT>' ) \n ) \n if parser . has_option ( '<STR_LIT>' , '<STR_LIT>' ) : \n self . _set_key_path ( parser . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n if parser . has_option ( '<STR_LIT>' , '<STR_LIT>' ) : \n self . _set_ca_path ( parser . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n if parser . has_option ( '<STR_LIT>' , '<STR_LIT>' ) : \n self . _set_auth_suite ( parser . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n def _set_hostname ( self , value ) : \n if isinstance ( value , six . string_types ) : \n self . settings [ '<STR_LIT>' ] = value \n else : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n ) \n def _set_port ( self , value ) : \n if isinstance ( value , six . integer_types ) : \n if <NUM_LIT:0> < value < <NUM_LIT> : \n self . settings [ '<STR_LIT:port>' ] = value \n else : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n ) \n else : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n ) \n def _set_certificate_path ( self , value ) : \n if value is None : \n self . settings [ '<STR_LIT>' ] = None \n elif isinstance ( value , six . string_types ) : \n if os . path . exists ( value ) : \n self . settings [ '<STR_LIT>' ] = value \n else : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n else : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n def _set_key_path ( self , value ) : \n if value is None : \n self . settings [ '<STR_LIT>' ] = None \n elif isinstance ( value , six . string_types ) : \n if os . path . exists ( value ) : \n self . settings [ '<STR_LIT>' ] = value \n else : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n else : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n def _set_ca_path ( self , value ) : \n if value is None : \n self . settings [ '<STR_LIT>' ] = None \n elif isinstance ( value , six . string_types ) : \n if os . path . exists ( value ) : \n self . settings [ '<STR_LIT>' ] = value \n else : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n else : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n def _set_auth_suite ( self , value ) : \n auth_suites = [ '<STR_LIT>' , '<STR_LIT>' ] \n if value not in auth_suites : \n raise exceptions . ConfigurationError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n else : \n self . settings [ '<STR_LIT>' ] = <mask0> \n", "gt": "value"}
{"input": "\n from six . moves import xrange \n from testtools import TestCase \n from kmip . core import utils \n from kmip . core . messages . contents import ProtocolVersion \n from kmip . core . messages . payloads import discover_versions \n class TestDiscoverVersionsRequestPayload ( TestCase ) : \n def setUp ( self ) : \n super ( TestDiscoverVersionsRequestPayload , self ) . setUp ( ) \n self . protocol_versions_empty = list ( ) \n self . protocol_versions_one = list ( ) \n self . protocol_versions_one . append ( ProtocolVersion . create ( <NUM_LIT:1> , <NUM_LIT:0> ) ) \n self . protocol_versions_two = list ( ) \n self . protocol_versions_two . append ( ProtocolVersion . create ( <NUM_LIT:1> , <NUM_LIT:1> ) ) \n self . protocol_versions_two . append ( ProtocolVersion . create ( <NUM_LIT:1> , <NUM_LIT:0> ) ) \n self . encoding_empty = utils . BytearrayStream ( ( \n b'<STR_LIT>' ) ) \n self . encoding_one = utils . BytearrayStream ( ( \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT:\\x00>' ) ) \n self . encoding_two = utils . BytearrayStream ( ( \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT>' ) ) \n def tearDown ( self ) : \n super ( TestDiscoverVersionsRequestPayload , self ) . tearDown ( ) \n def test_init_with_none ( self ) : \n discover_versions . DiscoverVersionsRequestPayload ( ) \n def test_init_with_args ( self ) : \n discover_versions . DiscoverVersionsRequestPayload ( \n self . protocol_versions_empty ) \n def test_validate_with_invalid_protocol_versions ( self ) : \n kwargs = { '<STR_LIT>' : '<STR_LIT>' } \n self . assertRaisesRegexp ( \n TypeError , \"<STR_LIT>\" , \n discover_versions . DiscoverVersionsRequestPayload , ** kwargs ) \n def test_validate_with_invalid_protocol_version ( self ) : \n kwargs = { '<STR_LIT>' : [ '<STR_LIT>' ] } \n self . assertRaisesRegexp ( \n TypeError , \"<STR_LIT>\" , \n discover_versions . DiscoverVersionsRequestPayload , ** kwargs ) \n def _test_read ( self , stream , payload , protocol_versions ) : \n payload . read ( stream ) \n expected = len ( protocol_versions ) \n observed = len ( payload . protocol_versions ) \n msg = \"<STR_LIT>\" \n msg += \"<STR_LIT>\" . format ( \n expected , observed ) \n self . assertEqual ( expected , observed , msg ) \n for i in xrange ( len ( protocol_versions ) ) : \n expected = protocol_versions [ i ] \n observed = payload . protocol_versions [ i ] \n msg = \"<STR_LIT>\" \n msg += \"<STR_LIT>\" . format ( expected , observed ) \n self . assertEqual ( expected , observed , msg ) \n def test_read_with_empty_protocol_list ( self ) : \n stream = self . encoding_empty \n payload = discover_versions . DiscoverVersionsRequestPayload ( ) \n protocol_versions = self . protocol_versions_empty \n self . _test_read ( stream , payload , protocol_versions ) \n def test_read_with_one_protocol_version ( self ) : \n stream = self . encoding_one \n payload = discover_versions . DiscoverVersionsRequestPayload ( ) \n protocol_versions = self . protocol_versions_one \n self . _test_read ( stream , payload , protocol_versions ) \n def test_read_with_two_protocol_versions ( self ) : \n stream = self . encoding_two \n payload = discover_versions . DiscoverVersionsRequestPayload ( ) \n protocol_versions = self . protocol_versions_two \n self . _test_read ( stream , payload , protocol_versions ) \n def _test_write ( self , payload , expected ) : \n stream = utils . BytearrayStream ( ) \n payload . write ( stream ) \n length_expected = len ( expected ) \n length_received = len ( stream ) \n msg = \"<STR_LIT>\" \n msg += \"<STR_LIT>\" . format ( \n length_expected , length_received ) \n self . assertEqual ( length_expected , length_received , msg ) \n msg = \"<STR_LIT>\" \n msg += \"<STR_LIT>\" . format ( expected , stream ) \n self . assertEqual ( expected , stream , msg ) \n def test_write_with_empty_protocol_list ( self ) : \n payload = discover_versions . DiscoverVersionsRequestPayload ( \n self . protocol_versions_empty ) \n expected = self . encoding_empty \n self . _test_write ( payload , expected ) \n def test_write_with_one_protocol_version ( self ) : \n payload = discover_versions . DiscoverVersionsRequestPayload ( \n self . protocol_versions_one ) \n expected = self . encoding_one \n self . _test_write ( payload , expected ) \n def test_write_with_two_protocol_versions ( self ) : \n payload = discover_versions . DiscoverVersionsRequestPayload ( \n self . protocol_versions_two ) \n expected = self . encoding_two \n self . _test_write ( payload , expected ) \n class TestDiscoverVersionsResponsePayload ( TestCase ) : \n def setUp ( self ) : \n super ( TestDiscoverVersionsResponsePayload , self ) . setUp ( ) \n self . protocol_versions_empty = list ( ) \n self . protocol_versions_one = list ( ) \n self . protocol_versions_one . append ( ProtocolVersion . create ( <NUM_LIT:1> , <NUM_LIT:0> ) ) \n self . protocol_versions_two = list ( ) \n self . protocol_versions_two . append ( ProtocolVersion . create ( <NUM_LIT:1> , <NUM_LIT:1> ) ) \n self . protocol_versions_two . append ( ProtocolVersion . create ( <NUM_LIT:1> , <NUM_LIT:0> ) ) \n self . encoding_empty = utils . BytearrayStream ( ( \n b'<STR_LIT>' ) ) \n self . encoding_one = utils . BytearrayStream ( ( \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT:\\x00>' ) ) \n self . encoding_two = utils . BytearrayStream ( ( \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT>' \n b'<STR_LIT>' ) ) \n def tearDown ( self ) : \n super ( TestDiscoverVersionsResponsePayload , self ) . tearDown ( ) \n def test_init_with_none ( self ) : \n discover_versions . DiscoverVersionsResponsePayload ( ) \n def test_init_with_args ( self ) : \n discover_versions . DiscoverVersionsResponsePayload ( \n self . protocol_versions_empty ) \n def test_validate_with_invalid_protocol_versions ( self ) : \n kwargs = { '<STR_LIT>' : '<STR_LIT>' } \n self . assertRaisesRegexp ( \n TypeError , \"<STR_LIT>\" , \n discover_versions . DiscoverVersionsResponsePayload , ** kwargs ) \n def test_validate_with_invalid_protocol_version ( self ) : \n kwargs = { '<STR_LIT>' : [ '<STR_LIT>' ] } \n self . assertRaisesRegexp ( \n TypeError , \"<STR_LIT>\" , \n discover_versions . DiscoverVersionsResponsePayload , ** kwargs ) \n def _test_read ( self , stream , payload , protocol_versions ) : \n payload . read ( stream ) \n expected = len ( protocol_versions ) \n observed = len ( payload . protocol_versions ) \n msg = \"<STR_LIT>\" \n msg += \"<STR_LIT>\" . format ( \n expected , observed ) \n self . assertEqual ( expected , observed , msg ) \n for i in xrange ( len ( protocol_versions ) ) : \n expected = protocol_versions [ i ] \n observed = payload . protocol_versions [ i ] \n msg = \"<STR_LIT>\" \n msg += \"<STR_LIT>\" . format ( expected , observed ) \n self . assertEqual ( expected , observed , msg ) \n def test_read_with_empty_protocol_list ( self ) : \n stream = self . encoding_empty \n payload = discover_versions . DiscoverVersionsResponsePayload ( ) \n protocol_versions = self . protocol_versions_empty \n self . _test_read ( stream , payload , protocol_versions ) \n def test_read_with_one_protocol_version ( self ) : \n stream = self . encoding_one \n payload = discover_versions . DiscoverVersionsResponsePayload ( ) \n protocol_versions = self . protocol_versions_one \n self . _test_read ( stream , payload , protocol_versions ) \n def test_read_with_two_protocol_versions ( self ) : \n stream = self . encoding_two \n payload = discover_versions . DiscoverVersionsResponsePayload ( ) \n protocol_versions = self . protocol_versions_two \n self . _test_read ( stream , payload , protocol_versions ) \n def _test_write ( self , payload , expected ) : \n stream = utils . BytearrayStream ( ) \n payload . write ( stream ) \n length_expected = len ( expected ) \n length_received = len ( stream ) \n msg = \"<STR_LIT>\" \n msg += \"<STR_LIT>\" . format ( \n length_expected , length_received ) \n self . assertEqual ( length_expected , length_received , msg ) \n msg = \"<STR_LIT>\" \n msg += \"<STR_LIT>\" . format ( expected , stream ) \n self . assertEqual ( expected , stream , msg ) \n def test_write_with_empty_protocol_list ( self ) : \n payload = discover_versions . DiscoverVersionsResponsePayload ( \n self . protocol_versions_empty ) \n expected = self . encoding_empty \n self . _test_write ( payload , expected ) \n def test_write_with_one_protocol_version ( self ) : \n payload = discover_versions . DiscoverVersionsResponsePayload ( \n self . protocol_versions_one ) \n expected = self . encoding_one \n self . _test_write ( payload , expected ) \n def test_write_with_two_protocol_versions ( self ) : \n payload = discover_versions . DiscoverVersionsResponsePayload ( \n self . protocol_versions_two ) \n expected = self . encoding_two \n self . _test_write ( payload , <mask0> ) \n", "gt": "expected"}
{"input": "\n import binascii \n import testtools \n from kmip . core import enums \n from kmip . pie . objects import ManagedObject , OpaqueObject \n from kmip . pie import sqltypes \n from sqlalchemy import create_engine \n from sqlalchemy . orm import sessionmaker \n class TestOpaqueObject ( testtools . TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n super ( TestOpaqueObject , self ) . setUp ( ) \n self . bytes_a = ( \n b'<STR_LIT>' ) \n self . bytes_b = ( \n b'<STR_LIT>' ) \n self . engine = create_engine ( '<STR_LIT>' , echo = True ) \n sqltypes . Base . metadata . create_all ( self . engine ) \n def tearDown ( self ) : \n super ( TestOpaqueObject , self ) . tearDown ( ) \n def test_init ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n obj = OpaqueObject ( \n self . bytes_a , enums . OpaqueDataType . NONE ) \n self . assertEqual ( obj . value , self . bytes_a ) \n self . assertEqual ( obj . opaque_type , enums . OpaqueDataType . NONE ) \n self . assertEqual ( obj . names , [ '<STR_LIT>' ] ) \n def test_init_with_args ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n obj = OpaqueObject ( \n self . bytes_a , \n enums . OpaqueDataType . NONE , \n name = '<STR_LIT>' ) \n self . assertEqual ( obj . value , self . bytes_a ) \n self . assertEqual ( obj . opaque_type , enums . OpaqueDataType . NONE ) \n self . assertEqual ( obj . names , [ '<STR_LIT>' ] ) \n def test_get_object_type ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n expected = enums . ObjectType . OPAQUE_DATA \n obj = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n observed = obj . object_type \n self . assertEqual ( expected , observed ) \n def test_validate_on_invalid_value ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n args = ( <NUM_LIT:0> , enums . OpaqueDataType . NONE ) \n self . assertRaises ( TypeError , OpaqueObject , * args ) \n def test_validate_on_invalid_data_type ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n args = ( self . bytes_a , '<STR_LIT>' ) \n self . assertRaises ( TypeError , OpaqueObject , * args ) \n def test_validate_on_invalid_name ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n args = ( self . bytes_a , enums . OpaqueDataType . NONE ) \n kwargs = { '<STR_LIT:name>' : <NUM_LIT:0> } \n self . assertRaises ( TypeError , OpaqueObject , * args , ** kwargs ) \n def test_repr ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n obj = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n args = \"<STR_LIT>\" . format ( \n binascii . hexlify ( self . bytes_a ) , enums . OpaqueDataType . NONE ) \n expected = \"<STR_LIT>\" . format ( args ) \n observed = repr ( obj ) \n self . assertEqual ( expected , observed ) \n def test_str ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n obj = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n expected = str ( binascii . hexlify ( self . bytes_a ) ) \n observed = str ( obj ) \n self . assertEqual ( expected , observed ) \n def test_equal_on_equal ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n b = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n self . assertTrue ( a == b ) \n self . assertTrue ( b == a ) \n def test_equal_on_not_equal_value ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n b = OpaqueObject ( self . bytes_b , enums . OpaqueDataType . NONE ) \n self . assertFalse ( a == b ) \n self . assertFalse ( b == a ) \n def test_equal_on_not_equal_data_type ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n b = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n b . opaque_type = \"<STR_LIT>\" \n self . assertFalse ( a == b ) \n self . assertFalse ( b == a ) \n def test_equal_on_type_mismatch ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n b = \"<STR_LIT>\" \n self . assertFalse ( a == b ) \n self . assertFalse ( b == a ) \n def test_not_equal_on_equal ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n b = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n self . assertFalse ( a != b ) \n self . assertFalse ( b != a ) \n def test_not_equal_on_not_equal_value ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n b = OpaqueObject ( self . bytes_b , enums . OpaqueDataType . NONE ) \n self . assertTrue ( a != b ) \n self . assertTrue ( b != a ) \n def test_not_equal_on_not_equal_data_type ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n b = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n b . opaque_type = \"<STR_LIT>\" \n self . assertTrue ( a != b ) \n self . assertTrue ( b != a ) \n def test_not_equal_on_type_mismatch ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n b = \"<STR_LIT>\" \n self . assertTrue ( a != b ) \n self . assertTrue ( b != a ) \n def test_save ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n obj = OpaqueObject ( \n self . bytes_a , enums . OpaqueDataType . NONE ) \n Session = sessionmaker ( bind = self . engine ) \n session = Session ( ) \n session . add ( obj ) \n session . commit ( ) \n def test_get ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n test_name = '<STR_LIT>' \n obj = OpaqueObject ( \n self . bytes_a , enums . OpaqueDataType . NONE , name = test_name ) \n Session = sessionmaker ( bind = self . engine ) \n session = Session ( ) \n session . add ( obj ) \n session . commit ( ) \n session = Session ( ) \n get_obj = session . query ( OpaqueObject ) . filter ( \n ManagedObject . unique_identifier == obj . unique_identifier \n ) . one ( ) \n session . commit ( ) \n self . assertEqual ( <NUM_LIT:1> , len ( get_obj . names ) ) \n self . assertEqual ( [ test_name ] , get_obj . names ) \n self . assertEqual ( self . bytes_a , get_obj . value ) \n self . assertEqual ( enums . ObjectType . OPAQUE_DATA , get_obj . object_type ) \n self . assertEqual ( enums . OpaqueDataType . NONE , get_obj . opaque_type ) \n def test_add_multiple_names ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n expected_names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n obj = OpaqueObject ( \n self . bytes_a , enums . OpaqueDataType . NONE , name = expected_names [ <NUM_LIT:0> ] ) \n obj . names . append ( expected_names [ <NUM_LIT:1> ] ) \n obj . names . append ( expected_names [ <NUM_LIT:2> ] ) \n self . assertEquals ( <NUM_LIT:3> , obj . name_index ) \n expected_mo_names = list ( ) \n for i , name in enumerate ( expected_names ) : \n expected_mo_names . append ( sqltypes . ManagedObjectName ( name , i ) ) \n self . assertEquals ( expected_mo_names , obj . _names ) \n Session = sessionmaker ( bind = self . engine ) \n session = Session ( ) \n session . add ( obj ) \n session . commit ( ) \n session = Session ( ) \n get_obj = session . query ( OpaqueObject ) . filter ( \n ManagedObject . unique_identifier == obj . unique_identifier \n ) . one ( ) \n session . commit ( ) \n self . assertEquals ( expected_mo_names , get_obj . _names ) \n def test_remove_name ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n remove_index = <NUM_LIT:1> \n obj = OpaqueObject ( \n self . bytes_a , enums . OpaqueDataType . NONE , name = names [ <NUM_LIT:0> ] ) \n obj . names . append ( names [ <NUM_LIT:1> ] ) \n obj . names . append ( names [ <NUM_LIT:2> ] ) \n obj . names . pop ( remove_index ) \n self . assertEquals ( <NUM_LIT:3> , obj . name_index ) \n expected_names = list ( ) \n expected_mo_names = list ( ) \n for i , name in enumerate ( names ) : \n if i != remove_index : \n expected_names . append ( name ) \n expected_mo_names . append ( sqltypes . ManagedObjectName ( name , i ) ) \n self . assertEquals ( expected_names , obj . names ) \n self . assertEquals ( expected_mo_names , obj . _names ) \n Session = sessionmaker ( bind = self . engine ) \n session = Session ( ) \n session . add ( obj ) \n session . commit ( ) \n session = Session ( ) \n get_obj = session . query ( OpaqueObject ) . filter ( \n ManagedObject . unique_identifier == obj . unique_identifier \n ) . one ( ) \n session . commit ( ) \n self . assertEquals ( expected_names , get_obj . names ) \n self . assertEquals ( expected_mo_names , get_obj . _names ) \n def test_remove_and_add_name ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n obj = OpaqueObject ( \n self . bytes_a , enums . OpaqueDataType . NONE , name = names [ <NUM_LIT:0> ] ) \n obj . names . append ( names [ <NUM_LIT:1> ] ) \n obj . names . append ( names [ <NUM_LIT:2> ] ) \n obj . names . pop ( ) \n obj . names . pop ( ) \n obj . names . append ( '<STR_LIT>' ) \n self . assertEquals ( <NUM_LIT:4> , obj . name_index ) \n expected_names = [ '<STR_LIT>' , '<STR_LIT>' ] \n expected_mo_names = list ( ) \n expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ <NUM_LIT:0> ] , \n <NUM_LIT:0> ) ) \n expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ <NUM_LIT:1> ] , \n <NUM_LIT:3> ) ) \n self . assertEquals ( expected_names , obj . names ) \n self . assertEquals ( expected_mo_names , obj . _names ) \n Session = sessionmaker ( bind = self . engine ) \n session = Session ( ) \n session . add ( obj ) \n session . commit ( ) \n session = Session ( ) \n get_obj = session . query ( OpaqueObject ) . filter ( \n ManagedObject . unique_identifier == obj . unique_identifier \n ) . one ( ) \n session . commit ( ) \n self . assertEquals ( expected_names , get_obj . names ) \n self . assertEquals ( expected_mo_names , get_obj . _names ) \n def test_update_with_add_name ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n first_name = '<STR_LIT>' \n obj = OpaqueObject ( \n self . bytes_a , enums . OpaqueDataType . NONE , name = first_name ) \n Session = sessionmaker ( bind = self . engine ) \n session = Session ( ) \n session . add ( obj ) \n session . commit ( ) \n added_name = '<STR_LIT>' \n expected_names = [ first_name , added_name ] \n expected_mo_names = list ( ) \n for i , name in enumerate ( expected_names ) : \n expected_mo_names . append ( sqltypes . ManagedObjectName ( name , i ) ) \n session = Session ( ) \n update_obj = session . query ( OpaqueObject ) . filter ( \n ManagedObject . unique_identifier == obj . unique_identifier \n ) . one ( ) \n update_obj . names . append ( added_name ) \n session . commit ( ) \n session = Session ( ) \n get_obj = session . query ( OpaqueObject ) . filter ( \n ManagedObject . unique_identifier == obj . unique_identifier \n ) . one ( ) \n session . commit ( ) \n self . assertEquals ( expected_names , get_obj . names ) \n self . assertEquals ( expected_mo_names , get_obj . _names ) \n def test_update_with_remove_name ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n remove_index = <NUM_LIT:1> \n obj = OpaqueObject ( \n self . bytes_a , enums . OpaqueDataType . NONE , name = names [ <NUM_LIT:0> ] ) \n obj . names . append ( names [ <NUM_LIT:1> ] ) \n obj . names . append ( names [ <NUM_LIT:2> ] ) \n Session = sessionmaker ( bind = self . engine ) \n session = Session ( ) \n session . add ( obj ) \n session . commit ( ) \n expected_names = list ( ) \n expected_mo_names = list ( ) \n for i , name in enumerate ( names ) : \n if i != remove_index : \n expected_names . append ( name ) \n expected_mo_names . append ( sqltypes . ManagedObjectName ( name , i ) ) \n session = Session ( ) \n update_obj = session . query ( OpaqueObject ) . filter ( \n ManagedObject . unique_identifier == obj . unique_identifier \n ) . one ( ) \n update_obj . names . pop ( remove_index ) \n session . commit ( ) \n session = Session ( ) \n get_obj = session . query ( OpaqueObject ) . filter ( \n ManagedObject . unique_identifier == obj . unique_identifier \n ) . one ( ) \n session . commit ( ) \n self . assertEquals ( expected_names , get_obj . names ) \n self . assertEquals ( expected_mo_names , get_obj . _names ) \n def test_update_with_remove_and_add_name ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n obj = OpaqueObject ( \n self . bytes_a , enums . OpaqueDataType . NONE , name = names [ <NUM_LIT:0> ] ) \n obj . names . append ( names [ <NUM_LIT:1> ] ) \n obj . names . append ( names [ <NUM_LIT:2> ] ) \n Session = sessionmaker ( bind = self . engine ) \n session = Session ( ) \n session . add ( obj ) \n session . commit ( ) \n session = Session ( ) \n update_obj = session . query ( OpaqueObject ) . filter ( \n ManagedObject . unique_identifier == obj . unique_identifier \n ) . one ( ) \n update_obj . names . pop ( ) \n update_obj . names . pop ( ) \n update_obj . names . append ( '<STR_LIT>' ) \n session . commit ( ) \n expected_names = [ '<STR_LIT>' , '<STR_LIT>' ] \n expected_mo_names = list ( ) \n expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ <NUM_LIT:0> ] , \n <NUM_LIT:0> ) ) \n expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ <NUM_LIT:1> ] , \n <NUM_LIT:3> ) ) \n session = Session ( ) \n get_obj = session . query ( OpaqueObject ) . filter ( \n ManagedObject . unique_identifier == obj . unique_identifier \n ) . one ( ) \n session . commit ( ) \n self . assertEquals ( expected_names , get_obj . names ) \n self . assertEquals ( expected_mo_names , get_obj . <mask0> ) \n", "gt": "_names"}
{"input": "\n from setuptools import setup , find_packages \n kwargs = { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : [ '<STR_LIT>' , \n '<STR_LIT>' ] , \n '<STR_LIT:description>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : True , \n '<STR_LIT>' : [ '<STR_LIT>' ] , \n '<STR_LIT>' : [ '<STR_LIT>' ] , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:name>' : '<STR_LIT>' , \n '<STR_LIT>' : { '<STR_LIT>' : [ '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' ] } , \n '<STR_LIT>' : { '<STR_LIT>' : '<STR_LIT:src>' } , \n '<STR_LIT>' : [ '<STR_LIT>' , '<STR_LIT>' ] , \n '<STR_LIT:url>' : '<STR_LIT>' , \n '<STR_LIT:version>' : '<STR_LIT>' , \n '<STR_LIT>' : False } \n setup ( ** <mask0> ) \n", "gt": "kwargs"}
{"input": "\n import os . path \n import setuptools \n import sys \n from numpy . distutils . core import setup \n from numpy . distutils . misc_util import Configuration \n include_dirs = [ ] \n library_dirs = [ ] \n if sys . platform == '<STR_LIT:win32>' : \n import types \n def _lib_dir_option ( self , dir ) : \n return '<STR_LIT>' % dir \n from distutils . msvc9compiler import MSVCCompiler \n setattr ( MSVCCompiler , '<STR_LIT>' , \n types . MethodType ( _lib_dir_option , None , MSVCCompiler ) ) \n sdkdir = os . environ . get ( '<STR_LIT>' ) \n if sdkdir : \n include_dirs . append ( os . path . join ( sdkdir , '<STR_LIT>' ) ) \n library_dirs . append ( os . path . join ( sdkdir , '<STR_LIT>' ) ) \n path = os . environ [ '<STR_LIT>' ] . split ( '<STR_LIT:;>' ) \n path . append ( os . path . join ( sdkdir , '<STR_LIT>' ) ) \n os . environ [ '<STR_LIT>' ] = '<STR_LIT:;>' . join ( path ) \n config = Configuration ( name = '<STR_LIT>' ) \n config . add_extension ( '<STR_LIT>' , \n sources = [ '<STR_LIT>' , \n '<STR_LIT>' ] , \n include_dirs = include_dirs , \n library_dirs = library_dirs ) \n config . add_data_files ( '<STR_LIT>' , '<STR_LIT>' ) \n kwds = { '<STR_LIT>' : [ '<STR_LIT>' ] , \n '<STR_LIT:version>' : '<STR_LIT>' , \n '<STR_LIT>' : False , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:url>' : '<STR_LIT>' , \n '<STR_LIT>' : { '<STR_LIT>' : [ '<STR_LIT>' ] } , \n } \n kwds . update ( config . todict ( ) ) \n setup ( ** <mask0> ) \n", "gt": "kwds"}
{"input": "\n import sys , os \n extensions = [ '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' \n ] \n templates_path = [ '<STR_LIT>' ] \n source_suffix = '<STR_LIT>' \n master_doc = '<STR_LIT:index>' \n project = u'<STR_LIT>' \n copyright = u'<STR_LIT:None>' \n version = '<STR_LIT>' \n release = '<STR_LIT>' \n today_fmt = '<STR_LIT>' \n exclude_trees = [ '<STR_LIT>' ] \n pygments_style = '<STR_LIT>' \n html_style = '<STR_LIT>' \n html_last_updated_fmt = '<STR_LIT>' \n html_theme = \"<STR_LIT:default>\" \n html_theme_options = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n todo_include_todos = True \n intersphinx_mapping = { '<STR_LIT>' : None } \n <mask0> = '<STR_LIT>' \n", "gt": "autodoc_member_order"}
{"input": "\n import glob \n import os . path \n from dirwalk import includingWalk \n from os import system \n from subprocess import Popen , PIPE , STDOUT \n from compmodtimes import compmodtimes \n from PIL import Image \n def resize_image ( fname , max_width = <NUM_LIT> ) : \n im = Image . open ( fname ) \n width , height = tuple ( im . getbbox ( ) [ <NUM_LIT:2> : ] ) \n print '<STR_LIT>' , height , '<STR_LIT>' , width \n if width > max_width : \n wrat = max_width / float ( width ) \n new_w = int ( width * wrat ) \n new_h = int ( height * wrat ) \n newim = im . transform ( ( new_w , new_h ) , Image . EXTENT , \n im . getbbox ( ) , Image . BICUBIC ) \n newim . save ( fname ) \n for diafile in includingWalk ( \"<STR_LIT:..>\" , [ \"<STR_LIT>\" ] ) : \n pth = os . path . split ( diafile ) \n dest = pth [ <NUM_LIT:1> ] . split ( '<STR_LIT:.>' ) [ <NUM_LIT:0> ] \n retcode = compmodtimes ( diafile , '<STR_LIT>' + dest + '<STR_LIT>' ) \n if retcode == - <NUM_LIT:1> or retcode == <NUM_LIT:0> : \n print '<STR_LIT>' + dest + '<STR_LIT>' \n else : \n cmd = '<STR_LIT>' + dest + '<STR_LIT>' + diafile \n system ( cmd ) \n resize_image ( os . path . abspath ( os . path . join ( '<STR_LIT>' , <mask0> + '<STR_LIT>' ) ) ) \n", "gt": "dest"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import numpy as np \n from openmdao . main . api import Assembly , Component \n from openmdao . lib . datatypes . api import Float \n from openmdao . lib . drivers . api import CaseIteratorDriver \n from openmdao . lib . components . api import MultiFiMetaModel \n from openmdao . lib . surrogatemodels . api import MultiFiCoKrigingSurrogate , KrigingSurrogate \n class Model ( Component ) : \n x = Float ( <NUM_LIT:0> , iotype = \"<STR_LIT>\" ) \n f_x = Float ( <NUM_LIT:0.0> , iotype = \"<STR_LIT>\" ) \n def execute ( self ) : \n x = self . x \n self . f_x = ( ( <NUM_LIT:6> * x - <NUM_LIT:2> ) ** <NUM_LIT:2> ) * np . sin ( ( <NUM_LIT:6> * x - <NUM_LIT:2> ) * <NUM_LIT:2> ) \n class LowFidelityModel ( Component ) : \n x = Float ( <NUM_LIT:0.0> , iotype = \"<STR_LIT>\" ) \n f_x = Float ( <NUM_LIT:0.0> , iotype = \"<STR_LIT>\" ) \n def execute ( self ) : \n x = self . x \n self . f_x = <NUM_LIT:0.5> * ( ( <NUM_LIT:6> * x - <NUM_LIT:2> ) ** <NUM_LIT:2> ) * np . sin ( ( <NUM_LIT:6> * x - <NUM_LIT:2> ) * <NUM_LIT:2> ) + ( x - <NUM_LIT:0.5> ) * <NUM_LIT> - <NUM_LIT:5> \n class HighFidelityModel ( Model ) : \n pass \n class CasesBuilder ( Assembly ) : \n def __init__ ( self , model , cases ) : \n self . instance = model \n self . cases = cases \n super ( CasesBuilder , self ) . __init__ ( ) \n def configure ( self ) : \n self . add ( \"<STR_LIT>\" , self . instance ) \n self . add ( \"<STR_LIT>\" , CaseIteratorDriver ( ) ) \n self . driver . workflow . add ( '<STR_LIT>' ) \n self . driver . add_parameter ( \"<STR_LIT>\" , low = <NUM_LIT:0> , high = <NUM_LIT:1> ) \n self . driver . add_response ( \"<STR_LIT>\" ) \n self . driver . case_inputs . model . x = self . cases \n self . create_passthrough ( '<STR_LIT>' ) \n self . create_passthrough ( '<STR_LIT>' ) \n class Simulation ( Assembly ) : \n def __init__ ( self , surrogate , nfi = <NUM_LIT:1> ) : \n self . surrogate = surrogate \n self . nfi = nfi \n super ( Simulation , self ) . __init__ ( ) \n def configure ( self ) : \n doe_e = [ <NUM_LIT:0.0> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT:1.0> ] \n doe_c = [ <NUM_LIT:0.1> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT:0.5> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] + doe_e \n self . add ( '<STR_LIT>' , CasesBuilder ( HighFidelityModel ( ) , doe_e ) ) \n self . add ( '<STR_LIT>' , CasesBuilder ( LowFidelityModel ( ) , doe_c ) ) \n self . add ( \"<STR_LIT>\" , MultiFiMetaModel ( params = ( '<STR_LIT:x>' , ) , \n responses = ( '<STR_LIT>' , ) , nfi = self . nfi ) ) \n self . meta_model . default_surrogate = self . surrogate \n self . connect ( '<STR_LIT>' , '<STR_LIT>' ) \n self . connect ( '<STR_LIT>' , '<STR_LIT>' ) \n if self . nfi > <NUM_LIT:1> : \n self . connect ( '<STR_LIT>' , '<STR_LIT>' ) \n self . connect ( '<STR_LIT>' , '<STR_LIT>' ) \n self . add ( '<STR_LIT>' , CaseIteratorDriver ( ) ) \n self . add ( '<STR_LIT>' , Model ( ) ) \n self . mm_checker . add_parameter ( \"<STR_LIT>\" , low = <NUM_LIT:0> , high = <NUM_LIT:1> ) \n self . mm_checker . add_parameter ( \"<STR_LIT>\" , low = <NUM_LIT:0> , high = <NUM_LIT:1> ) \n self . mm_checker . add_response ( \"<STR_LIT>\" ) \n self . mm_checker . add_response ( \"<STR_LIT>\" ) \n ngrid = <NUM_LIT:100> \n self . mm_checker . case_inputs . meta_model . x = np . linspace ( <NUM_LIT:0> , <NUM_LIT:1> , ngrid ) \n self . mm_checker . case_inputs . model . x = np . linspace ( <NUM_LIT:0> , <NUM_LIT:1> , ngrid ) \n self . driver . workflow . add ( '<STR_LIT>' ) \n if self . nfi > <NUM_LIT:1> : \n self . driver . workflow . add ( '<STR_LIT>' ) \n self . driver . workflow . add ( '<STR_LIT>' ) \n if __name__ == \"<STR_LIT:__main__>\" : \n surrogate = MultiFiCoKrigingSurrogate ( ) \n sim_cok = Simulation ( surrogate , nfi = <NUM_LIT:2> ) \n sim_cok . run ( ) \n predicted_cok = np . array ( [ d . mu for d in sim_cok . mm_checker . case_outputs . meta_model . f_x ] ) \n sigma_cok = np . array ( [ d . sigma for d in sim_cok . mm_checker . case_outputs . meta_model . f_x ] ) \n surrogate = KrigingSurrogate ( ) \n sim_k = Simulation ( surrogate , nfi = <NUM_LIT:1> ) \n sim_k . run ( ) \n predicted_k = np . array ( [ d . mu for d in sim_k . mm_checker . case_outputs . meta_model . f_x ] ) \n sigma_k = np . array ( [ d . sigma for d in sim_k . mm_checker . case_outputs . meta_model . f_x ] ) \n actual = sim_k . mm_checker . case_outputs . model . f_x \n check = sim_k . mm_checker . case_inputs . meta_model . x \n import pylab as plt \n plt . figure ( <NUM_LIT:2> ) \n plt . ioff ( ) \n plt . plot ( check , actual , '<STR_LIT:k>' , label = '<STR_LIT>' ) \n plt . plot ( sim_cok . hifi_cases . x , sim_cok . hifi_cases . f_x , '<STR_LIT>' , label = \"<STR_LIT>\" ) \n plt . plot ( sim_cok . lofi_cases . x , sim_cok . lofi_cases . f_x , '<STR_LIT>' , label = \"<STR_LIT>\" ) \n plt . plot ( check , predicted_cok , '<STR_LIT:g>' , label = '<STR_LIT>' ) \n plt . plot ( check , predicted_cok + <NUM_LIT:2> * sigma_cok , '<STR_LIT:g>' , alpha = <NUM_LIT:0.5> , label = '<STR_LIT>' ) \n plt . plot ( check , predicted_cok - <NUM_LIT:2> * sigma_cok , '<STR_LIT:g>' , alpha = <NUM_LIT:0.5> ) \n plt . fill_between ( check , predicted_cok + <NUM_LIT:2> * sigma_cok , \n predicted_cok - <NUM_LIT:2> * sigma_cok , facecolor = '<STR_LIT:g>' , alpha = <NUM_LIT> ) \n plt . plot ( check , predicted_k , '<STR_LIT:b>' , label = '<STR_LIT>' ) \n plt . plot ( check , predicted_k + <NUM_LIT:2> * sigma_k , '<STR_LIT:b>' , alpha = <NUM_LIT:0.5> , label = '<STR_LIT>' ) \n plt . plot ( check , predicted_k - <NUM_LIT:2> * sigma_k , '<STR_LIT:b>' , alpha = <NUM_LIT:0.5> ) \n plt . fill_between ( check , predicted_k + <NUM_LIT:2> * sigma_k , \n predicted_k - <NUM_LIT:2> * sigma_k , facecolor = '<STR_LIT:b>' , alpha = <NUM_LIT> ) \n plt . legend ( loc = '<STR_LIT>' ) \n plt . show ( ) \n error = <NUM_LIT:0.> \n for a , p in zip ( actual , predicted_cok ) : \n error += ( a - p ) ** <NUM_LIT:2> \n error = ( error / len ( actual ) ) \n print \"<STR_LIT>\" % error \n error = <NUM_LIT:0.> \n for a , p in zip ( actual , predicted_k ) : \n error += ( a - p ) ** <NUM_LIT:2> \n error = ( error / len ( actual ) ) \n print \"<STR_LIT>\" % <mask0> \n", "gt": "error"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n import os \n import shutil \n import urllib2 \n import subprocess \n import codecs \n from optparse import OptionParser \n def has_setuptools ( ) : \n try : \n import setuptools \n except ImportError : \n return False \n return True \n def make_new_setupfile ( setupfile ) : \n \"\"\"<STR_LIT>\"\"\" \n setupfile = os . path . abspath ( setupfile ) \n newsetupfile = os . path . join ( os . path . dirname ( setupfile ) , \n '<STR_LIT>' + os . path . basename ( setupfile ) ) \n startdir = os . getcwd ( ) \n os . chdir ( os . path . dirname ( setupfile ) ) \n try : \n print \"<STR_LIT>\" \n if not os . path . isfile ( '<STR_LIT>' ) : \n print \"<STR_LIT>\" \n resp = urllib2 . urlopen ( '<STR_LIT>' ) \n with open ( '<STR_LIT>' , '<STR_LIT:wb>' ) as easyf : \n shutil . copyfileobj ( resp . fp , easyf ) \n print '<STR_LIT>' \n print \"<STR_LIT>\" % setupfile \n if not os . path . isfile ( setupfile ) : \n raise IOError ( \"<STR_LIT>\" % setupfile ) \n setupf = open ( setupfile , '<STR_LIT:r>' ) \n setup_contents = setupf . read ( ) \n setupf . close ( ) \n with open ( newsetupfile , '<STR_LIT:wb>' ) as newf : \n newf . write ( \"<STR_LIT>\" ) \n newf . write ( \"<STR_LIT>\" ) \n newf . write ( setup_contents ) \n finally : \n os . chdir ( startdir ) \n return newsetupfile \n def build_dist ( srcdir , destdir = '<STR_LIT:.>' , build_type = '<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n startdir = os . getcwd ( ) \n destdir = os . path . abspath ( os . path . expanduser ( destdir ) ) . replace ( '<STR_LIT:\\\\>' , '<STR_LIT:/>' ) \n srcdir = os . path . abspath ( os . path . expanduser ( srcdir ) ) . replace ( '<STR_LIT:\\\\>' , '<STR_LIT:/>' ) \n setupname = os . path . join ( srcdir , '<STR_LIT>' ) \n if not has_setuptools ( ) : \n setupname = make_new_setupfile ( setupname ) \n dirfiles = set ( os . listdir ( destdir ) ) \n print \"<STR_LIT>\" % srcdir \n cmd = [ sys . executable . replace ( '<STR_LIT:\\\\>' , '<STR_LIT:/>' ) , \n os . path . basename ( setupname ) , \n ] \n cmd . extend ( build_type . split ( '<STR_LIT:U+0020>' ) ) \n cmd . extend ( [ '<STR_LIT>' , destdir ] ) \n os . chdir ( srcdir ) \n out = codecs . open ( '<STR_LIT>' , '<STR_LIT:wb>' , \n encoding = '<STR_LIT:ascii>' , errors = '<STR_LIT:replace>' ) \n print '<STR_LIT>' % '<STR_LIT:U+0020>' . join ( cmd ) \n try : \n p = subprocess . Popen ( '<STR_LIT:U+0020>' . join ( cmd ) , \n stdout = out , stderr = subprocess . STDOUT , \n shell = True ) \n p . wait ( ) \n finally : \n out . close ( ) \n with open ( '<STR_LIT>' , '<STR_LIT:r>' ) as f : \n print f . read ( ) \n os . chdir ( startdir ) \n newfiles = set ( os . listdir ( destdir ) ) - dirfiles \n if len ( newfiles ) != <NUM_LIT:1> : \n raise RuntimeError ( \"<STR_LIT>\" % \n list ( newfiles ) ) \n if p . returncode != <NUM_LIT:0> : \n raise RuntimeError ( \"<STR_LIT>\" % \n ( srcdir , p . returncode ) ) \n distfile = os . path . join ( destdir , newfiles . pop ( ) ) \n print '<STR_LIT>' % distfile \n return distfile \n if __name__ == '<STR_LIT:__main__>' : \n parser = OptionParser ( usage = \"<STR_LIT>\" ) \n parser . add_option ( \"<STR_LIT>\" , \"<STR_LIT>\" , action = \"<STR_LIT:store>\" , type = '<STR_LIT:string>' , \n dest = '<STR_LIT>' , \n help = \"<STR_LIT>\" ) \n parser . add_option ( \"<STR_LIT>\" , \"<STR_LIT>\" , action = \"<STR_LIT:store>\" , type = '<STR_LIT:string>' , \n dest = '<STR_LIT>' , default = '<STR_LIT:.>' , \n help = \"<STR_LIT>\" ) \n parser . add_option ( \"<STR_LIT>\" , \"<STR_LIT>\" , action = \"<STR_LIT:store>\" , type = '<STR_LIT:string>' , \n dest = '<STR_LIT>' , default = '<STR_LIT>' , \n help = \"<STR_LIT>\" ) \n ( options , args ) = parser . parse_args ( sys . argv [ <NUM_LIT:1> : ] ) \n retcode = - <NUM_LIT:1> \n startdir = os . getcwd ( ) \n if not options . srcdir : \n print \"<STR_LIT>\" \n parser . print_help ( ) \n sys . exit ( retcode ) \n srcdir = os . path . abspath ( os . path . expanduser ( options . srcdir ) ) \n destdir = os . path . abspath ( os . path . expanduser ( options . destdir ) ) \n if not os . path . exists ( srcdir ) : \n print \"<STR_LIT>\" % srcdir \n sys . exit ( retcode ) \n try : \n distfile = build_dist ( srcdir , destdir , options . buildtype ) \n finally : \n os . chdir ( <mask0> ) \n", "gt": "startdir"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from openmdao . lib . casehandlers . caseset import CaseArray , CaseSet , caseiter_to_caseset \n from openmdao . lib . casehandlers . csvcase import CSVCaseIterator , CSVCaseRecorder \n from openmdao . lib . casehandlers . dbcase import DBCaseIterator , DBCaseRecorder , case_db_to_dict \n from openmdao . lib . casehandlers . dumpcase import DumpCaseRecorder \n from openmdao . lib . casehandlers . jsoncase import JSONCaseRecorder , BSONCaseRecorder , verify_json \n from openmdao . lib . casehandlers . listcase import ListCaseRecorder , ListCaseIterator \n from openmdao . lib . casehandlers . caseset import CaseArray , CaseSet , caseiter_to_caseset \n from openmdao . lib . casehandlers . filters import SequenceCaseFilter , SliceCaseFilter , ExprCaseFilter \n from openmdao . lib . casehandlers . query import CaseDataset \n from openmdao . lib . casehandlers . csv_post_processor import caseset_query_to_csv \n from openmdao . lib . casehandlers . dump_post_processor import caseset_query_dump \n from openmdao . lib . casehandlers . html_post_processor import caseset_query_to_html \n try : \n from openmdao . lib . casehandlers . query_hdf5 import CaseDatasetHDF5 \n from openmdao . lib . casehandlers . hdf5case import HDF5CaseRecorder \n except ImportError : \n <mask0> \n", "gt": "pass"}
{"input": "\n from weakref import ref \n import numpy as np \n from openmdao . main . api import VariableTree \n from openmdao . lib . casehandlers . query import DictList , ListResult \n _GLOBAL_DICT = dict ( __builtins__ = None ) \n class CaseDatasetHDF5 ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , filename , format ) : \n format = format . lower ( ) \n if format == '<STR_LIT>' : \n self . _reader = _HDF5Reader ( filename ) \n else : \n raise ValueError ( \"<STR_LIT>\" ) \n self . _query_id = self . _query_itername = self . _parent_id = self . _parent_itername = self . _driver_id = self . _driver_name = None \n self . _case_ids = self . _drivers = self . _case_iternames = None \n self . metadata_names = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' ] \n @ property \n def data ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return QueryHDF5 ( self ) \n @ property \n def drivers ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _reader . drivers ( ) \n @ property \n def simulation_info ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _reader . simulation_info \n def _fetch ( self , query ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _setup ( query ) \n if query . vnames : \n tmp = [ ] \n for name in self . metadata_names : \n if name in query . vnames : \n tmp . append ( name ) \n self . metadata_names = tmp \n names = query . vnames \n else : \n if query . driver_name : \n driver_info = self . _drivers [ self . _driver_name ] \n prefix = driver_info [ '<STR_LIT>' ] \n all_names = [ prefix + name \n for name in driver_info [ '<STR_LIT>' ] ] \n else : \n all_names = [ ] \n for driver_info in self . _drivers . values ( ) : \n prefix = driver_info [ '<STR_LIT>' ] \n all_names . extend ( [ prefix + name \n for name in driver_info [ '<STR_LIT>' ] ] ) \n names = sorted ( all_names + self . metadata_names ) \n if query . names : \n return names \n nan = float ( '<STR_LIT>' ) \n rows = ListResult ( ) \n state = { } \n for case_data in self . _reader . cases ( ) : \n data = case_data [ '<STR_LIT:data>' ] \n metadata = case_data [ '<STR_LIT>' ] \n case_id = metadata [ '<STR_LIT>' ] \n case_driver_id = metadata [ '<STR_LIT>' ] \n case_driver_name = metadata [ '<STR_LIT>' ] \n case_itername = metadata [ '<STR_LIT>' ] \n prefix = self . _drivers [ case_driver_name ] [ '<STR_LIT>' ] \n if prefix : \n pass \n else : \n data = data . copy ( ) \n state . update ( data ) \n if self . _driver_name is not None and case_driver_name != self . _driver_name : \n continue \n if self . _case_iternames is None or case_itername in self . _case_iternames : \n for name in self . metadata_names : \n data [ name ] = case_data [ '<STR_LIT>' ] [ name ] \n row = DictList ( names ) \n for name in names : \n if query . local_only : \n if name in self . metadata_names : \n row . append ( data [ name ] ) \n else : \n driver = self . _drivers [ case_driver_name ] \n lnames = [ prefix + rec for rec in driver [ '<STR_LIT>' ] ] \n if name in lnames : \n row . append ( data [ name ] ) \n else : \n row . append ( nan ) \n elif name in state : \n row . append ( state [ name ] ) \n elif name in data : \n row . append ( data [ name ] ) \n else : \n row . append ( nan ) \n rows . append ( row ) \n if case_itername == self . _query_itername or case_itername == self . _parent_itername : \n break \n if self . _query_id and not rows : \n raise ValueError ( '<STR_LIT>' % self . _query_id ) \n if query . transpose : \n tmp = DictList ( names ) \n for i in range ( len ( rows [ <NUM_LIT:0> ] ) ) : \n tmp . append ( [ row [ i ] for row in rows ] ) \n tmp . cds = self \n return tmp \n rows . cds = self \n return rows \n def _write ( self , query , out , format ) : \n raise NotImplementedError \n def _setup ( self , query ) : \n \"\"\"<STR_LIT>\"\"\" \n if query . vnames is not None : \n bad = [ ] \n metadata = self . simulation_info [ '<STR_LIT>' ] \n expressions = self . simulation_info [ '<STR_LIT>' ] \n for name in query . vnames : \n if name not in metadata and name not in [ e [ '<STR_LIT>' ] for e in expressions . values ( ) ] and name not in self . metadata_names : \n bad . append ( name ) \n if bad : \n raise RuntimeError ( '<STR_LIT>' % bad ) \n self . _drivers = { } \n self . _driver_id = None \n self . _driver_name = None \n for driver_info in self . _reader . drivers ( ) : \n _id = driver_info [ '<STR_LIT>' ] \n name = driver_info [ '<STR_LIT:name>' ] \n prefix , _ , name = name . rpartition ( '<STR_LIT:.>' ) \n if prefix : \n prefix += '<STR_LIT:.>' \n driver_info [ '<STR_LIT>' ] = prefix \n self . _drivers [ driver_info [ '<STR_LIT:name>' ] ] = driver_info \n if ( driver_info [ '<STR_LIT:name>' ] ) == query . driver_name : \n self . _driver_name = query . driver_name \n if query . driver_name : \n if self . _driver_name is None : \n raise ValueError ( '<STR_LIT>' % query . driver_name ) \n self . _case_ids = None \n self . _query_id = None \n self . _parent_id = None \n if query . case_itername is not None : \n self . _query_itername = query . case_itername \n self . _case_iternames = set ( ( self . _query_itername , ) ) \n self . _driver_name = None \n elif query . parent_itername is not None : \n self . _parent_itername = query . parent_itername \n self . _case_iternames = set ( ( self . _parent_itername , ) ) \n parent_itername_parts = self . _parent_itername . split ( '<STR_LIT:->' ) \n for case_data in self . _reader . cases ( ) : \n itername = case_data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n itername_parts = itername . split ( '<STR_LIT:->' ) \n if len ( parent_itername_parts ) + <NUM_LIT:1> == len ( itername_parts ) and itername_parts [ : - <NUM_LIT:1> ] == parent_itername_parts : \n self . _case_iternames . add ( itername ) \n def restore ( self , assembly , case_id ) : \n \"\"\"<STR_LIT>\"\"\" \n raise NotImplementedError \n def _set ( self , assembly , name , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( value , dict ) : \n curr = assembly . get ( name ) \n if isinstance ( curr , VariableTree ) : \n for key , val in value . items ( ) : \n self . _set ( assembly , '<STR_LIT:.>' . join ( ( name , key ) ) , val ) \n elif '<STR_LIT:[>' in name : \n if isinstance ( value , unicode ) : \n value = str ( value ) \n exec ( '<STR_LIT>' % name , _GLOBAL_DICT , locals ( ) ) \n else : \n for key , val in value . items ( ) : \n if isinstance ( val , unicode ) : \n value [ key ] = str ( val ) \n assembly . set ( name , value ) \n else : \n if isinstance ( value , unicode ) : \n value = str ( value ) \n if '<STR_LIT:[>' in name : \n exec ( '<STR_LIT>' % name , _GLOBAL_DICT , locals ( ) ) \n else : \n assembly . set ( name , value ) \n class QueryHDF5 ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , dataset ) : \n self . _dataset = dataset \n self . driver_name = None \n self . case_id = None \n self . case_itername = None \n self . parent_id = None \n self . parent_itername = None \n self . vnames = None \n self . local_only = False \n self . names = False \n self . transpose = False \n def fetch ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _dataset . _fetch ( self ) \n def write ( self , out , format = None ) : \n \"\"\"<STR_LIT>\"\"\" \n raise NotImplementedError \n def driver ( self , driver_name ) : \n \"\"\"<STR_LIT>\"\"\" \n self . driver_name = driver_name \n return self \n def case ( self , case_itername ) : \n \"\"\"<STR_LIT>\"\"\" \n self . case_itername = case_itername \n self . parent_itername = None \n return self \n def parent_case ( self , parent_case_id ) : \n \"\"\"<STR_LIT>\"\"\" \n self . parent_id = parent_case_id \n self . parent_itername = parent_case_id \n self . case_id = None \n return self \n def vars ( self , * args ) : \n \"\"\"<STR_LIT>\"\"\" \n self . vnames = [ ] \n for arg in args : \n if isinstance ( arg , basestring ) : \n self . vnames . append ( arg ) \n else : \n self . vnames . extend ( arg ) \n return self \n def local ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . local_only = True \n return self \n def by_case ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . transpose = False \n return self \n def by_variable ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . transpose = True \n return self \n def var_names ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . names = True \n return self \n class _HDF5Reader ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , filename ) : \n import h5py \n self . _inp = h5py . File ( filename , '<STR_LIT:r>' ) \n self . _simulation_info = self . read_simulation_info ( ) \n self . _state = '<STR_LIT>' \n self . _info = None \n @ property \n def simulation_info ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _simulation_info \n def read_iteration_case_from_hdf5 ( self , hdf5file , driver_name , iteration_case_name ) : \n info = { } \n driver_grp = self . _inp [ '<STR_LIT>' ] [ driver_name ] \n iteration_grp = self . _inp [ '<STR_LIT>' ] [ driver_name ] [ iteration_case_name ] \n info [ '<STR_LIT>' ] = self . read_from_hdf5 ( iteration_grp [ '<STR_LIT>' ] ) \n data_grp = iteration_grp [ '<STR_LIT:data>' ] \n info [ '<STR_LIT:data>' ] = { } \n float_names = driver_grp [ '<STR_LIT>' ] \n int_names = driver_grp [ '<STR_LIT>' ] \n str_names = driver_grp [ '<STR_LIT>' ] \n for i , name in enumerate ( float_names ) : \n info [ '<STR_LIT:data>' ] [ name ] = data_grp [ '<STR_LIT>' ] [ i ] \n for i , name in enumerate ( str_names ) : \n info [ '<STR_LIT:data>' ] [ name ] = data_grp [ '<STR_LIT>' ] [ i ] \n for i , name in enumerate ( int_names ) : \n info [ '<STR_LIT:data>' ] [ name ] = data_grp [ '<STR_LIT>' ] [ i ] \n for name in data_grp . keys ( ) : \n if name not in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : \n if '<STR_LIT>' in data_grp [ name ] . attrs : \n info [ '<STR_LIT:data>' ] [ name ] = { } \n for n , v in data_grp [ name ] . items ( ) : \n info [ '<STR_LIT:data>' ] [ name ] [ n ] = self . read_from_hdf5 ( data_grp [ name ] [ n ] ) \n info [ '<STR_LIT:data>' ] [ name ] = self . read_from_hdf5 ( data_grp [ name ] ) \n return info \n def read_from_hdf5 ( self , value ) : \n import h5py \n if isinstance ( value , h5py . _hl . group . Group ) : \n d = { } \n group = value \n for name , value in group . attrs . items ( ) : \n d [ name ] = self . read_from_hdf5 ( value ) \n for name , value in group . items ( ) : \n d [ name ] = self . read_from_hdf5 ( value ) \n return d \n elif value . dtype . names : \n d = { } \n for name in value . dtype . names : \n d [ name ] = value [ name ] [ <NUM_LIT:0> ] \n return d \n else : \n return value [ ( ) ] \n def read_simulation_info ( self ) : \n sim_info_grp = self . _inp [ '<STR_LIT>' ] \n sim_info = { } \n for name , value in sim_info_grp . attrs . items ( ) : \n sim_info [ name ] = self . read_from_hdf5 ( value ) \n for name , value in sim_info_grp . items ( ) : \n sim_info [ name ] = self . read_from_hdf5 ( value ) \n return sim_info \n def drivers ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n driver_info = [ ] \n for name in self . _inp . keys ( ) : \n if name . startswith ( '<STR_LIT>' ) : \n driver_info . append ( self . read_from_hdf5 ( self . _inp [ name ] ) ) \n return driver_info \n def cases ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n iteration_cases_grp = self . _inp [ '<STR_LIT>' ] \n case_timestamps = { } \n for driver_name in iteration_cases_grp : \n for iteration_case_name in iteration_cases_grp [ driver_name ] : \n if iteration_case_name . startswith ( '<STR_LIT>' ) : \n timestamp = iteration_cases_grp [ driver_name ] [ iteration_case_name ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT:0> ] \n case_timestamps [ timestamp ] = ( driver_name , iteration_case_name ) \n sorted_timestamps = sorted ( case_timestamps ) \n for timestamp in sorted_timestamps : \n driver_name , iteration_case_name = case_timestamps [ timestamp ] \n info = self . read_iteration_case_from_hdf5 ( self . _inp , driver_name , iteration_case_name ) \n yield info \n def _next ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n <mask0> \n", "gt": "pass"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from openmdao . main . api import Component \n from openmdao . main . datatypes . api import Float \n import time \n class SleepComponent ( Component ) : \n \"\"\"<STR_LIT>\"\"\" \n sleep_time = Float ( <NUM_LIT:0.0> , iotype = '<STR_LIT>' , desc = '<STR_LIT>' ) \n def execute ( self ) : \n time . sleep ( self . <mask0> ) \n", "gt": "sleep_time"}
{"input": "\n import copy \n from openmdao . lib . datatypes . domain . flow import FlowSolution \n from openmdao . lib . datatypes . domain . grid import GridCoordinates \n CARTESIAN = '<STR_LIT>' \n CYLINDRICAL = '<STR_LIT>' \n _COORD_SYSTEMS = ( CARTESIAN , CYLINDRICAL ) \n class Zone ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n self . grid_coordinates = GridCoordinates ( ) \n self . flow_solution = FlowSolution ( ) \n self . reference_state = None \n self . _coordinate_system = CARTESIAN \n self . right_handed = True \n self . symmetry = None \n self . symmetry_axis = None \n self . symmetry_instances = <NUM_LIT:1> \n @ property \n def shape ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . grid_coordinates . shape \n @ property \n def extent ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . grid_coordinates . extent \n def _get_coord_sys ( self ) : \n return self . _coordinate_system \n def _set_coord_sys ( self , sys ) : \n if sys in _COORD_SYSTEMS : \n self . _coordinate_system = sys \n else : \n raise ValueError ( '<STR_LIT>' % sys ) \n coordinate_system = property ( _get_coord_sys , _set_coord_sys , \n doc = '<STR_LIT>' ) \n def copy ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return copy . deepcopy ( self ) \n def is_equivalent ( self , other , logger , tolerance = <NUM_LIT:0.> ) : \n \"\"\"<STR_LIT>\"\"\" \n if not isinstance ( other , Zone ) : \n logger . debug ( '<STR_LIT>' ) \n return False \n if self . coordinate_system != other . coordinate_system : \n logger . debug ( '<STR_LIT>' ) \n return False \n if self . right_handed != other . right_handed : \n logger . debug ( '<STR_LIT>' ) \n return False \n if self . symmetry != other . symmetry : \n logger . debug ( '<STR_LIT>' ) \n return False \n if self . symmetry_axis != other . symmetry_axis : \n logger . debug ( '<STR_LIT>' ) \n return False \n if self . symmetry_instances != other . symmetry_instances : \n logger . debug ( '<STR_LIT>' ) \n return False \n if not self . grid_coordinates . is_equivalent ( other . grid_coordinates , \n logger , tolerance ) : \n return False \n if not self . flow_solution . is_equivalent ( other . flow_solution , logger , \n tolerance ) : \n return False \n return True \n def extract ( self , imin , imax , jmin = None , jmax = None , kmin = None , kmax = None , \n grid_ghosts = None , flow_ghosts = None ) : \n \"\"\"<STR_LIT>\"\"\" \n zone = Zone ( ) \n zone . grid_coordinates = self . grid_coordinates . extract ( imin , imax , jmin , jmax , kmin , kmax , \n grid_ghosts ) \n zone . flow_solution = self . flow_solution . extract ( imin , imax , jmin , jmax , kmin , kmax , \n flow_ghosts ) \n if self . reference_state is not None : \n zone . reference_state = self . reference_state . copy ( ) \n zone . coordinate_system = self . coordinate_system \n zone . right_handed = self . right_handed \n zone . symmetry = self . symmetry \n zone . symmetry_axis = self . symmetry_axis \n zone . symmetry_instances = self . symmetry_instances \n return zone \n def extend ( self , axis , delta , grid_points , flow_points , normal = None ) : \n \"\"\"<STR_LIT>\"\"\" \n zone = Zone ( ) \n if grid_points > <NUM_LIT:0> : \n zone . grid_coordinates = self . grid_coordinates . extend ( axis , delta , grid_points , normal ) \n else : \n zone . grid_coordinates = self . grid_coordinates . copy ( ) \n if flow_points > <NUM_LIT:0> : \n zone . flow_solution = self . flow_solution . extend ( axis , delta , flow_points ) \n else : \n zone . flow_solution = self . flow_solution . copy ( ) \n if self . reference_state is not None : \n zone . reference_state = self . reference_state . copy ( ) \n zone . coordinate_system = self . coordinate_system \n zone . right_handed = self . right_handed \n zone . symmetry = self . symmetry \n zone . symmetry_axis = self . symmetry_axis \n zone . symmetry_instances = self . symmetry_instances \n return zone \n def make_cartesian ( self , axis = '<STR_LIT:z>' ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . coordinate_system != CARTESIAN : \n self . flow_solution . make_cartesian ( self . grid_coordinates , axis ) \n self . grid_coordinates . make_cartesian ( axis ) \n self . coordinate_system = CARTESIAN \n def make_cylindrical ( self , axis = '<STR_LIT:z>' ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . coordinate_system != CYLINDRICAL : \n self . grid_coordinates . make_cylindrical ( axis ) \n self . flow_solution . make_cylindrical ( self . grid_coordinates , axis ) \n self . coordinate_system = CYLINDRICAL \n def make_left_handed ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . right_handed : \n self . grid_coordinates . flip_z ( ) \n self . flow_solution . flip_z ( ) \n self . right_handed = False \n def make_right_handed ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if not self . right_handed : \n self . grid_coordinates . flip_z ( ) \n self . flow_solution . flip_z ( ) \n self . right_handed = True \n def translate ( self , delta_x , delta_y , delta_z ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . coordinate_system == CARTESIAN : \n self . grid_coordinates . translate ( delta_x , delta_y , delta_z ) \n else : \n raise RuntimeError ( '<STR_LIT>' ) \n def rotate_about_x ( self , deg ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . coordinate_system == CARTESIAN : \n self . grid_coordinates . rotate_about_x ( deg ) \n self . flow_solution . rotate_about_x ( deg ) \n else : \n raise RuntimeError ( '<STR_LIT>' ) \n def rotate_about_y ( self , deg ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . coordinate_system == CARTESIAN : \n self . grid_coordinates . rotate_about_y ( deg ) \n self . flow_solution . rotate_about_y ( deg ) \n else : \n raise RuntimeError ( '<STR_LIT>' ) \n def rotate_about_z ( self , deg ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . coordinate_system == CARTESIAN : \n self . grid_coordinates . rotate_about_z ( deg ) \n self . flow_solution . rotate_about_z ( deg ) \n else : \n raise RuntimeError ( '<STR_LIT>' ) \n def promote ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . grid_coordinates . promote ( ) \n self . flow_solution . promote ( ) \n def demote ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . grid_coordinates . demote ( ) \n self . flow_solution . <mask0> ( ) \n", "gt": "demote"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from math import isnan \n from numpy import zeros , array \n from slsqp . slsqp import slsqp , closeunit , pyflush \n from openmdao . main . datatypes . api import Enum , Float , Int , Str \n from openmdao . main . driver_uses_derivatives import Driver \n from openmdao . main . hasparameters import HasParameters \n from openmdao . main . hasconstraints import HasConstraints \n from openmdao . main . hasobjective import HasObjective \n from openmdao . main . interfaces import IHasParameters , IHasConstraints , IHasObjective , implements , IOptimizer \n from openmdao . util . decorators import add_delegate \n @ add_delegate ( HasParameters , HasConstraints , HasObjective ) \n class SLSQPdriver ( Driver ) : \n \"\"\"<STR_LIT>\"\"\" \n implements ( IHasParameters , IHasConstraints , IHasObjective , IOptimizer ) \n accuracy = Float ( <NUM_LIT> , iotype = '<STR_LIT>' , \n desc = '<STR_LIT>' ) \n maxiter = Int ( <NUM_LIT:50> , iotype = '<STR_LIT>' , \n desc = '<STR_LIT>' ) \n iprint = Enum ( <NUM_LIT:0> , [ <NUM_LIT:0> , <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] , iotype = '<STR_LIT>' , \n desc = '<STR_LIT>' ) \n iout = Int ( <NUM_LIT:6> , iotype = '<STR_LIT>' , \n desc = '<STR_LIT>' ) \n output_filename = Str ( '<STR_LIT>' , iotype = '<STR_LIT>' , \n desc = '<STR_LIT>' ) \n error_code = Int ( <NUM_LIT:0> , iotype = '<STR_LIT>' , \n desc = '<STR_LIT>' ) \n def __init__ ( self ) : \n super ( SLSQPdriver , self ) . __init__ ( ) \n self . error_messages = { \n - <NUM_LIT:1> : \"<STR_LIT>\" , \n <NUM_LIT:1> : \"<STR_LIT>\" , \n <NUM_LIT:2> : \"<STR_LIT>\" , \n <NUM_LIT:3> : \"<STR_LIT>\" , \n <NUM_LIT:4> : \"<STR_LIT>\" , \n <NUM_LIT:5> : \"<STR_LIT>\" , \n <NUM_LIT:6> : \"<STR_LIT>\" , \n <NUM_LIT:7> : \"<STR_LIT>\" , \n <NUM_LIT:8> : \"<STR_LIT>\" , \n <NUM_LIT:9> : \"<STR_LIT>\" , \n } \n self . x = zeros ( <NUM_LIT:0> , '<STR_LIT:d>' ) \n self . x_lower_bounds = zeros ( <NUM_LIT:0> , '<STR_LIT:d>' ) \n self . x_upper_bounds = zeros ( <NUM_LIT:0> , '<STR_LIT:d>' ) \n self . inputs = None \n self . obj = None \n self . con = None \n self . nparam = None \n self . ncon = None \n self . neqcon = None \n self . ff = <NUM_LIT:0> \n self . nfunc = <NUM_LIT:0> \n self . ngrad = <NUM_LIT:0> \n self . _continue = None \n def start_iteration ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n super ( SLSQPdriver , self ) . run_iteration ( ) \n self . inputs = self . list_param_group_targets ( ) \n self . obj = self . list_objective_targets ( ) \n self . con = self . list_constraint_targets ( ) \n self . nparam = self . total_parameters ( ) \n self . ncon = self . total_constraints ( ) \n self . neqcon = self . total_eq_constraints ( ) \n self . x = self . eval_parameters ( self . parent ) \n self . x_lower_bounds = self . get_lower_bounds ( ) \n self . x_upper_bounds = self . get_upper_bounds ( ) \n self . ff = <NUM_LIT:0> \n self . nfunc = <NUM_LIT:0> \n self . ngrad = <NUM_LIT:0> \n self . _continue = True \n def run_iteration ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n n = self . nparam \n m = self . ncon \n meq = self . neqcon \n la = max ( m , <NUM_LIT:1> ) \n gg = zeros ( [ la ] , '<STR_LIT:d>' ) \n df = zeros ( [ n + <NUM_LIT:1> ] , '<STR_LIT:d>' ) \n dg = zeros ( [ la , n + <NUM_LIT:1> ] , '<STR_LIT:d>' ) \n mineq = m - meq + <NUM_LIT:2> * ( n + <NUM_LIT:1> ) \n lsq = ( n + <NUM_LIT:1> ) * ( ( n + <NUM_LIT:1> ) + <NUM_LIT:1> ) + meq * ( ( n + <NUM_LIT:1> ) + <NUM_LIT:1> ) + mineq * ( ( n + <NUM_LIT:1> ) + <NUM_LIT:1> ) \n lsi = ( ( n + <NUM_LIT:1> ) - meq + <NUM_LIT:1> ) * ( mineq + <NUM_LIT:2> ) + <NUM_LIT:2> * mineq \n lsei = ( ( n + <NUM_LIT:1> ) + mineq ) * ( ( n + <NUM_LIT:1> ) - meq ) + <NUM_LIT:2> * meq + ( n + <NUM_LIT:1> ) \n slsqpb = ( n + <NUM_LIT:1> ) * ( n / <NUM_LIT:2> ) + <NUM_LIT:2> * m + <NUM_LIT:3> * n + <NUM_LIT:3> * ( n + <NUM_LIT:1> ) + <NUM_LIT:1> \n lw = lsq + lsi + lsei + slsqpb + n + m \n w = zeros ( [ lw ] , '<STR_LIT:d>' ) \n ljw = max ( mineq , ( n + <NUM_LIT:1> ) - meq ) \n jw = zeros ( [ ljw ] , '<STR_LIT:i>' ) \n try : \n dg , self . error_code , self . nfunc , self . ngrad = slsqp ( self . ncon , self . neqcon , la , self . nparam , \n self . x , self . x_lower_bounds , self . x_upper_bounds , \n self . ff , gg , df , dg , self . accuracy , self . maxiter , \n self . iprint - <NUM_LIT:1> , self . iout , self . output_filename , \n self . error_code , w , lw , jw , ljw , \n self . nfunc , self . ngrad , \n self . _func , self . _grad ) \n except Exception as err : \n self . _logger . error ( str ( err ) ) \n raise \n if self . iprint > <NUM_LIT:0> : \n closeunit ( self . iout ) \n if self . error_code != <NUM_LIT:0> : \n self . _logger . warning ( self . error_messages [ self . error_code ] ) \n self . _continue = False \n def _func ( self , m , me , la , n , f , g , xnew ) : \n \"\"\"<STR_LIT>\"\"\" \n self . set_parameters ( xnew ) \n super ( SLSQPdriver , self ) . run_iteration ( ) \n f = self . eval_objective ( ) \n if isnan ( f ) : \n msg = \"<STR_LIT>\" \n self . raise_exception ( msg , RuntimeError ) \n if self . ncon > <NUM_LIT:0> : \n g = - <NUM_LIT:1.> * array ( self . eval_constraints ( self . parent ) ) \n if self . iprint > <NUM_LIT:0> : \n pyflush ( self . iout ) \n return f , g \n def _grad ( self , m , me , la , n , f , g , df , dg , xnew ) : \n \"\"\"<STR_LIT>\"\"\" \n J = self . _calc_gradient ( self . inputs , self . obj + self . con ) \n df [ <NUM_LIT:0> : self . nparam ] = J [ <NUM_LIT:0> , : ] . ravel ( ) \n if self . ncon > <NUM_LIT:0> : \n dg [ <NUM_LIT:0> : self . ncon , <NUM_LIT:0> : self . nparam ] = - J [ <NUM_LIT:1> : <NUM_LIT:1> + self . ncon , : ] \n return df , dg \n def requires_derivs ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return <mask0> \n", "gt": "True"}
{"input": "\n from sellar import SellarProblem , SellarProblemWithDeriv \n from branin import BraninProblem \n from scalable import UnitScalableProblem \n from polyscale import <mask0> \n", "gt": "PolyScalableProblem"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import datetime \n import copy \n import pprint \n import socket \n import sys \n import weakref \n copy . _copy_dispatch [ weakref . ref ] = copy . _copy_immutable \n copy . _deepcopy_dispatch [ weakref . ref ] = copy . _deepcopy_atomic \n copy . _deepcopy_dispatch [ weakref . KeyedRef ] = copy . _deepcopy_atomic \n from zope . interface import Interface , implements \n from numpy import ndarray \n from traits . api import HasTraits , Missing , Python , push_exception_handler , TraitType , CTrait \n from traits . has_traits import FunctionType , _clone_trait , MetaHasTraits \n from traits . trait_base import not_none \n from multiprocessing import connection \n from openmdao . main . datatypes . file import FileRef \n from openmdao . main . datatypes . list import List \n from openmdao . main . datatypes . slot import Slot \n from openmdao . main . datatypes . vtree import VarTree \n from openmdao . main . interfaces import ICaseIterator , IResourceAllocator , IContainer , IVariableTree , IContainerProxy , IOverrideSet \n from openmdao . main . mp_support import ObjectManager , is_instance , CLASSES_TO_PROXY , has_interface \n from openmdao . main . rbac import rbac \n from openmdao . main . variable import Variable , is_legal_name , _missing \n from openmdao . main . array_helpers import flattened_value , get_index \n from openmdao . util . log import Logger , logger \n from openmdao . util import eggloader , eggsaver , eggobserver \n from openmdao . util . eggsaver import SAVE_CPICKLE \n from openmdao . util . typegroups import int_types , complex_or_real_types \n _copydict = { \n '<STR_LIT>' : copy . deepcopy , \n '<STR_LIT>' : copy . copy \n } \n _iodict = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT:input>' } \n __missing__ = object ( ) \n def get_closest_proxy ( obj , pathname ) : \n \"\"\"<STR_LIT>\"\"\" \n names = pathname . split ( '<STR_LIT:.>' ) \n i = <NUM_LIT:0> \n for name in names : \n if IContainerProxy . providedBy ( obj ) : \n return ( obj , '<STR_LIT:.>' . join ( names [ i : ] ) ) \n try : \n obj = getattr ( obj , name ) \n except AttributeError : \n break \n i += <NUM_LIT:1> \n return ( obj , '<STR_LIT:.>' . join ( names [ i : ] ) ) \n def proxy_parent ( obj , pathname ) : \n \"\"\"<STR_LIT>\"\"\" \n names = pathname . split ( '<STR_LIT:.>' ) \n i = <NUM_LIT:0> \n for name in names [ : - <NUM_LIT:1> ] : \n if IContainerProxy . providedBy ( obj ) : \n return ( obj , '<STR_LIT:.>' . join ( names [ i : ] ) ) \n try : \n obj = getattr ( obj , name ) \n except AttributeError : \n break \n i += <NUM_LIT:1> \n return ( obj , '<STR_LIT:.>' . join ( names [ i : ] ) ) \n push_exception_handler ( handler = lambda o , t , ov , nv : None , \n reraise_exceptions = True , \n main = True , \n locked = True ) \n class _MetaSafe ( MetaHasTraits ) : \n \"\"\"<STR_LIT>\"\"\" \n def __new__ ( mcs , class_name , bases , class_dict ) : \n for name , obj in class_dict . items ( ) : \n if isinstance ( obj , Variable ) : \n for base in bases : \n if name in base . __dict__ : \n raise NameError ( '<STR_LIT>' \n % ( class_name , name , base . __name__ ) ) \n return super ( _MetaSafe , mcs ) . __new__ ( mcs , class_name , bases , class_dict ) \n class SafeHasTraits ( HasTraits ) : \n \"\"\"<STR_LIT>\"\"\" \n __metaclass__ = _MetaSafe \n def _check_bad_default ( name , trait , obj = None ) : \n if trait . vartypename not in [ '<STR_LIT>' , '<STR_LIT>' ] and trait . required is True and not trait . assumed_default and trait . _illegal_default_ is True : \n msg = \"<STR_LIT>\" % name \n if obj is None : \n raise RuntimeError ( msg ) \n else : \n obj . raise_exception ( msg , RuntimeError ) \n class Container ( SafeHasTraits ) : \n \"\"\"<STR_LIT>\"\"\" \n implements ( IContainer ) \n def __init__ ( self ) : \n self . _parent = None \n self . _name = None \n super ( Container , self ) . __init__ ( ) \n self . _call_cpath_updated = True \n self . _call_configure = True \n self . _managers = { } \n self . _added_traits = { } \n self . _getcache = { } \n self . _setcache = { } \n self . _copycache = { } \n self . _cached_traits_ = None \n self . _repair_trait_info = None \n self . _trait_metadata = { } \n self . _logger = Logger ( '<STR_LIT>' ) \n for name , obj in self . items ( ) : \n if isinstance ( obj , FileRef ) : \n setattr ( self , name , obj . copy ( owner = self ) ) \n for name , obj in self . __class__ . __dict__ [ '<STR_LIT>' ] . items ( ) : \n ttype = obj . trait_type \n if isinstance ( ttype , VarTree ) : \n variable_tree = getattr ( self , name ) \n if not obj . required : \n new_tree = variable_tree . copy ( ) \n setattr ( self , name , new_tree ) \n if obj . required : \n _check_bad_default ( name , obj , self ) \n @ property \n def parent ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _parent \n @ parent . setter \n def parent ( self , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _parent is not value : \n self . _parent = value \n self . _fix_loggers ( self , recurse = True ) \n self . _branch_moved ( ) \n def _branch_moved ( self ) : \n self . _call_cpath_updated = True \n for n , cont in self . items ( ) : \n if is_instance ( cont , Container ) and cont is not self . _parent : \n cont . _branch_moved ( ) \n @ property \n def name ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _name is None : \n if self . parent : \n self . _name = find_name ( self . parent , self ) \n self . _fix_loggers ( self , recurse = True ) \n elif self . _call_cpath_updated is False : \n self . _name = '<STR_LIT>' \n else : \n return '<STR_LIT>' \n return self . _name \n @ name . setter \n def name ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n if not is_legal_name ( name ) : \n raise NameError ( \"<STR_LIT>\" % name ) \n if self . _name != name : \n self . _name = name \n self . _fix_loggers ( self , recurse = True ) \n def _fix_loggers ( self , container , recurse ) : \n \"\"\"<STR_LIT>\"\"\" \n container . _logger . rename ( container . get_pathname ( ) . replace ( '<STR_LIT:.>' , '<STR_LIT:U+002C>' ) ) \n if recurse : \n for name in container . list_containers ( ) : \n obj = getattr ( container , name ) \n self . _fix_loggers ( obj , recurse ) \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def get_pathname ( self , rel_to_scope = None ) : \n \"\"\"<STR_LIT>\"\"\" \n path = [ ] \n obj = self \n name = obj . name \n while obj is not rel_to_scope and name : \n path . append ( name ) \n obj = obj . parent \n if obj is None : \n break \n name = obj . name \n return '<STR_LIT:.>' . join ( path [ : : - <NUM_LIT:1> ] ) \n def get_trait ( self , name , copy = False ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _cached_traits_ is None : \n self . _cached_traits_ = self . traits ( ) \n self . _cached_traits_ . update ( self . _instance_traits ( ) ) \n if copy : \n if self . _cached_traits_ . get ( name ) : \n return self . trait ( name , copy = copy ) \n else : \n return None \n else : \n return self . _cached_traits_ . get ( name ) \n def __deepcopy__ ( self , memo ) : \n \"\"\"<STR_LIT>\"\"\" \n id_self = id ( self ) \n if id_self in memo : \n return memo [ id_self ] \n memo [ '<STR_LIT>' ] = \"<STR_LIT>\" \n saved_p = self . _parent \n saved_c = self . _cached_traits_ \n saved_s = self . _setcache \n saved_g = self . _getcache \n self . _parent = None \n self . _cached_traits_ = None \n self . _getcache = { } \n self . _setcache = { } \n try : \n result = super ( Container , self ) . __deepcopy__ ( memo ) \n finally : \n self . _parent = saved_p \n self . _cached_traits_ = saved_c \n self . _getcache = saved_g \n self . _setcache = saved_s \n olditraits = self . _instance_traits ( ) \n for name , trait in olditraits . items ( ) : \n if trait . type is not '<STR_LIT>' and name in self . _added_traits : \n if isinstance ( trait . trait_type , VarTree ) : \n if name not in result . _added_traits : \n result . add_trait ( name , _clone_trait ( trait ) ) \n else : \n result . add_trait ( name , _clone_trait ( trait ) ) \n if name in self . __dict__ : \n result . __dict__ [ name ] = copy . deepcopy ( self . __dict__ [ name ] ) \n return result \n def __getstate__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n state = super ( Container , self ) . __getstate__ ( ) \n dct = { } \n for name , trait in state [ '<STR_LIT>' ] . items ( ) : \n if trait . transient is not True : \n dct [ name ] = trait \n state [ '<STR_LIT>' ] = dct \n state [ '<STR_LIT>' ] = None \n state [ '<STR_LIT>' ] = { } \n state [ '<STR_LIT>' ] = { } \n return state \n def __setstate__ ( self , state ) : \n \"\"\"<STR_LIT>\"\"\" \n super ( Container , self ) . __setstate__ ( { } ) \n self . __dict__ . update ( state ) \n self . _repair_trait_info = { } \n self . _cached_traits_ = None \n traits = self . _alltraits ( ) \n for name , trait in self . _added_traits . items ( ) : \n if name not in traits : \n self . add_trait ( name , trait , refresh = False ) \n fixups = [ ] \n for name , trait in traits . items ( ) : \n try : \n get = trait . trait_type . get \n except AttributeError : \n continue \n if get is not None : \n if name not in self . _added_traits : \n try : \n val = getattr ( self , name ) \n self . remove_trait ( name ) \n self . add_trait ( name , trait ) \n setattr ( self , name , val ) \n except Exception as exc : \n self . _logger . warning ( '<STR_LIT>' , \n name , val , exc ) \n fixups . append ( ( name , trait ) ) \n self . _repair_trait_info [ '<STR_LIT>' ] = fixups \n fixups = [ ] \n for name , val in self . __dict__ . items ( ) : \n if not name . startswith ( '<STR_LIT>' ) and not self . get_trait ( name ) : \n try : \n setattr ( self , name , val ) \n except Exception as exc : \n self . _logger . warning ( '<STR_LIT>' , \n name , val , exc ) \n fixups . append ( ( name , val ) ) \n self . _repair_trait_info [ '<STR_LIT>' ] = fixups \n fixups = [ ] \n for name , trait in self . _alltraits ( ) . items ( ) : \n if isinstance ( trait . trait_type , List ) : \n try : \n setattr ( self , name , getattr ( self , name ) ) \n except Exception as exc : \n self . _logger . warning ( '<STR_LIT>' , \n name , val , exc ) \n fixups . append ( name ) \n self . _repair_trait_info [ '<STR_LIT:list>' ] = fixups \n self . _cached_traits_ = None \n def _repair_traits ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _repair_trait_info is None : \n return \n for name , trait in self . _repair_trait_info [ '<STR_LIT>' ] : \n val = getattr ( self , name ) \n self . remove_trait ( name ) \n self . add_trait ( name , trait ) \n setattr ( self , name , val ) \n for name , val in self . _repair_trait_info [ '<STR_LIT>' ] : \n setattr ( self , name , val ) \n for name in self . _repair_trait_info [ '<STR_LIT:list>' ] : \n setattr ( self , name , getattr ( self , name ) ) \n self . _repair_trait_info = None \n @ classmethod \n def add_class_trait ( cls , name , * trait ) : \n \"\"\"<STR_LIT>\"\"\" \n bases = [ cls ] \n bases . extend ( cls . __bases__ ) \n for base in bases : \n if name in base . __dict__ : \n raise NameError ( '<STR_LIT>' \n % ( name , base . __name__ ) ) \n for t in trait : \n _check_bad_default ( name , t ) \n break \n if name in cls . _trait_metadata : \n del cls . _trait_metadata [ name ] \n return super ( Container , cls ) . add_class_trait ( name , * trait ) \n def add_trait ( self , name , trait , refresh = True ) : \n \"\"\"<STR_LIT>\"\"\" \n if name . endswith ( '<STR_LIT>' ) and trait . type == '<STR_LIT>' : \n super ( Container , self ) . add_trait ( name , trait ) \n return \n bases = [ self . __class__ ] \n bases . extend ( self . __class__ . __bases__ ) \n for base in bases : \n if name in base . __dict__ : \n raise NameError ( '<STR_LIT>' \n % ( name , base . __name__ ) ) \n _check_bad_default ( name , trait , self ) \n if name not in self . _added_traits : \n self . _added_traits [ name ] = trait \n super ( Container , self ) . add_trait ( name , trait ) \n if self . _cached_traits_ is not None : \n self . _cached_traits_ [ name ] = self . trait ( name ) \n if name in self . _trait_metadata : \n del self . _trait_metadata [ name ] \n if refresh : \n getattr ( self , name ) \n def remove_trait ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n del self . _added_traits [ name ] \n except KeyError : \n pass \n try : \n del self . _cached_traits_ [ name ] \n except ( KeyError , TypeError ) : \n pass \n try : \n del self . _trait_metadata [ name ] \n except KeyError : \n pass \n super ( Container , self ) . remove_trait ( name ) \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def get_attr_w_copy ( self , path ) : \n \"\"\"<STR_LIT>\"\"\" \n obj = self . get ( path ) \n copy = self . _copycache . get ( path , _missing ) \n if copy is _missing : \n copy = self . get_metadata ( path . split ( '<STR_LIT:[>' , <NUM_LIT:1> ) [ <NUM_LIT:0> ] , '<STR_LIT>' ) \n self . _copycache [ path ] = copy \n if copy : \n if isinstance ( obj , Container ) : \n obj = obj . copy ( ) \n else : \n obj = _copydict [ copy ] ( obj ) \n return obj \n def _add_after_parent_set ( self , name , obj ) : \n pass \n def _prep_for_add ( self , name , obj ) : \n \"\"\"<STR_LIT>\"\"\" \n if '<STR_LIT:.>' in name : \n self . raise_exception ( \n '<STR_LIT>' % \n name , ValueError ) \n elif not is_legal_name ( name ) : \n self . raise_exception ( \"<STR_LIT>\" % name , \n NameError ) \n removed = False \n if has_interface ( obj , IContainer ) : \n if self . contains ( name ) and getattr ( self , name ) : \n self . remove ( name ) \n removed = True \n if has_interface ( obj , IContainer ) : \n self . _check_recursion ( obj ) \n if IContainerProxy . providedBy ( obj ) : \n obj . parent = self . _get_proxy ( obj ) \n else : \n obj . parent = self \n obj . name = name \n self . _add_after_parent_set ( name , obj ) \n if self . _call_cpath_updated is False : \n obj . cpath_updated ( ) \n return removed \n def _post_container_add ( self , name , obj , removed ) : \n pass \n def add ( self , name , obj ) : \n \"\"\"<STR_LIT>\"\"\" \n removed = self . _prep_for_add ( name , obj ) \n if has_interface ( obj , IContainer ) : \n setattr ( self , name , obj ) \n if self . _cached_traits_ is None : \n self . get_trait ( name ) \n else : \n self . _cached_traits_ [ name ] = self . trait ( name ) \n self . _post_container_add ( name , obj , removed ) \n elif is_instance ( obj , TraitType ) : \n self . add_trait ( name , obj ) \n else : \n setattr ( self , name , obj ) \n return obj \n def _check_recursion ( self , obj ) : \n \"\"\"<STR_LIT>\"\"\" \n ancestor = self \n while is_instance ( ancestor , Container ) : \n if obj is ancestor : \n self . raise_exception ( '<STR_LIT>' , \n ValueError ) \n ancestor = ancestor . parent \n def _get_proxy ( self , proxy ) : \n \"\"\"<STR_LIT>\"\"\" \n addr_type = connection . address_type ( proxy . _token . address ) \n addr = proxy . _token . address [ <NUM_LIT:0> ] if addr_type == '<STR_LIT>' else None \n key = ( addr_type , addr , proxy . _authkey ) \n try : \n manager = self . _managers [ key ] \n except KeyError : \n if addr_type == '<STR_LIT>' : \n ip_addr = socket . gethostbyname ( socket . gethostname ( ) ) \n address = ( ip_addr , <NUM_LIT:0> ) \n allowed_hosts = [ addr ] \n if addr == ip_addr : \n allowed_hosts . append ( '<STR_LIT:127.0.0.1>' ) \n else : \n address = None \n allowed_hosts = None \n name = self . name or '<STR_LIT>' \n access = addr if addr_type == '<STR_LIT>' else addr_type \n name = '<STR_LIT>' % ( name , access ) \n manager = ObjectManager ( self , address , authkey = proxy . _authkey , \n name = name , allowed_hosts = allowed_hosts ) \n self . _managers [ key ] = manager \n return manager . proxy \n def _check_rename ( self , oldname , newname ) : \n if '<STR_LIT:.>' in oldname or '<STR_LIT:.>' in newname : \n self . raise_exception ( \"<STR_LIT>\" \n \"<STR_LIT>\" % \n ( oldname , newname ) , RuntimeError ) \n if not self . contains ( oldname ) : \n self . raise_exception ( \"<STR_LIT>\" % \n ( oldname , newname , oldname ) , RuntimeError ) \n if self . contains ( newname ) : \n self . raise_exception ( \"<STR_LIT>\" % \n ( oldname , newname , newname ) , RuntimeError ) \n def rename ( self , oldname , newname ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _check_rename ( oldname , newname ) \n obj = self . remove ( oldname ) \n self . add ( newname , obj ) \n def remove ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n if '<STR_LIT:.>' in name : \n self . raise_exception ( \n '<STR_LIT>' % \n name , NameError ) \n try : \n obj = getattr ( self , name ) \n except AttributeError : \n return None \n trait = self . get_trait ( name ) \n if trait is None : \n delattr ( self , name ) \n else : \n if trait . is_trait_type ( Slot ) : \n try : \n setattr ( self , name , None ) \n except TypeError as err : \n self . raise_exception ( str ( err ) , RuntimeError ) \n else : \n self . remove_trait ( name ) \n return obj \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def configure ( self ) : \n pass \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def copy ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n cp = copy . deepcopy ( self ) \n cp . _relink ( ) \n return cp \n def _relink ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for name in self . list_containers ( ) : \n container = getattr ( self , name ) \n if container is not self . _parent : \n container . _parent = self \n container . _relink ( ) \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def cpath_updated ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _fix_loggers ( self , recurse = False ) \n self . _call_cpath_updated = False \n for cont in self . list_containers ( ) : \n cont = getattr ( self , cont ) \n if cont is not self . _parent : \n cont . cpath_updated ( ) \n def revert_to_defaults ( self , recurse = True ) : \n \"\"\"<STR_LIT>\"\"\" \n self . reset_traits ( iotype = '<STR_LIT>' ) \n if recurse : \n for cname in self . list_containers ( ) : \n getattr ( self , cname ) . revert_to_defaults ( recurse ) \n def _items ( self , visited , recurse = False , ** metadata ) : \n \"\"\"<STR_LIT>\"\"\" \n if id ( self ) not in visited : \n visited . add ( id ( self ) ) \n match_dict = self . _alltraits ( ** metadata ) \n if recurse : \n for name in self . list_containers ( ) : \n obj = getattr ( self , name ) \n if name in match_dict and id ( obj ) not in visited : \n yield ( name , obj ) \n if obj : \n for chname , child in obj . _items ( visited , recurse , \n ** metadata ) : \n yield ( '<STR_LIT:.>' . join ( ( name , chname ) ) , child ) \n for name , trait in match_dict . items ( ) : \n obj = getattr ( self , name , Missing ) \n if obj is not Missing : \n if is_instance ( obj , ( Container , VarTree ) ) and id ( obj ) not in visited : \n if not recurse : \n yield ( name , obj ) \n elif trait . iotype is not None : \n yield ( name , obj ) \n def items ( self , recurse = False , ** metadata ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _items ( set ( [ id ( self . parent ) ] ) , recurse , ** metadata ) \n def list_containers ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return [ n for n , v in self . items ( ) if is_instance ( v , Container ) ] \n def list_vars ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return [ k for k , v in self . items ( iotype = not_none ) ] \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def _alltraits ( self , traits = None , events = False , ** metadata ) : \n \"\"\"<STR_LIT>\"\"\" \n if traits is None : \n if self . _cached_traits_ : \n traits = self . _cached_traits_ \n else : \n traits = self . traits ( ) \n traits . update ( self . _instance_traits ( ) ) \n self . _cached_traits_ = traits \n result = { } \n for name , trait in traits . items ( ) : \n if not events and trait . type is '<STR_LIT>' : \n continue \n for meta_name , meta_eval in metadata . items ( ) : \n if type ( meta_eval ) is FunctionType : \n if not meta_eval ( getattr ( trait , meta_name ) ) : \n break \n elif meta_eval != getattr ( trait , meta_name ) : \n break \n else : \n result [ name ] = trait \n return result \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def contains ( self , path ) : \n \"\"\"<STR_LIT>\"\"\" \n childname , _ , restofpath = path . partition ( '<STR_LIT:.>' ) \n if restofpath : \n obj = getattr ( self , childname , Missing ) \n if obj is Missing : \n return False \n elif is_instance ( obj , Container ) : \n return obj . contains ( restofpath ) \n else : \n return hasattr ( obj , restofpath ) \n return hasattr ( self , path ) \n def _get_metadata_failed ( self , traitpath , metaname ) : \n self . raise_exception ( \"<STR_LIT>\" % traitpath , \n AttributeError ) \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def get_metadata ( self , traitpath , metaname = None ) : \n \"\"\"<STR_LIT>\"\"\" \n childname , _ , restofpath = traitpath . partition ( '<STR_LIT:.>' ) \n if restofpath : \n obj = getattr ( self , childname , Missing ) \n if obj is Missing : \n return self . _get_metadata_failed ( traitpath , metaname ) \n elif hasattr ( obj , '<STR_LIT>' ) : \n return obj . get_metadata ( restofpath , metaname ) \n else : \n t = self . get_trait ( childname ) \n if t is not None and t . iotype and metaname == '<STR_LIT>' : \n return t . iotype \n else : \n self . _get_metadata_failed ( traitpath , metaname ) \n varname , _ , _ = traitpath . partition ( '<STR_LIT:[>' ) \n try : \n mdict = self . _trait_metadata [ varname ] \n except KeyError : \n t = self . get_trait ( varname ) \n if t : \n t = t . trait_type \n mdict = t . _metadata . copy ( ) \n mdict . setdefault ( '<STR_LIT>' , t . __class__ . __name__ ) \n else : \n mdict = self . _get_metadata_failed ( traitpath , None ) \n self . _trait_metadata [ varname ] = mdict \n if metaname is None : \n return mdict \n else : \n return mdict . get ( metaname , None ) \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def set_metadata ( self , traitpath , metaname , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if metaname in ( '<STR_LIT>' , ) : \n self . raise_exception ( \"<STR_LIT>\" \n % ( metaname , traitpath ) , TypeError ) \n self . get_metadata ( traitpath ) [ metaname ] = value \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) , proxy_types = [ FileRef ] ) \n def get ( self , path ) : \n \"\"\"<STR_LIT>\"\"\" \n expr = self . _getcache . get ( path ) \n if expr is not None : \n return eval ( expr , self . __dict__ ) \n obj , restofpath = get_closest_proxy ( self , path ) \n if restofpath and IContainerProxy . providedBy ( obj ) : \n return obj . get ( restofpath ) \n expr = compile ( path , path , mode = '<STR_LIT>' ) \n try : \n val = eval ( expr , self . __dict__ ) \n except ( AttributeError , NameError ) as err : \n if not restofpath : \n return obj \n self . raise_exception ( str ( err ) , AttributeError ) \n else : \n self . _getcache [ path ] = expr \n return val \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) , proxy_types = [ FileRef ] ) \n def get_flattened_value ( self , path ) : \n \"\"\"<STR_LIT>\"\"\" \n return flattened_value ( path , self . get ( path ) ) \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def set_flattened_value ( self , path , value ) : \n obj , restofpath = proxy_parent ( self , path ) \n if restofpath and IContainerProxy . providedBy ( obj ) : \n obj . set_flattened_value ( restofpath , value ) \n return \n val = self . get ( path ) \n if not isinstance ( val , int_types ) and isinstance ( val , complex_or_real_types ) : \n self . set ( path , value [ <NUM_LIT:0> ] ) \n return \n elif hasattr ( val , '<STR_LIT>' ) : \n val . set_flattened_value ( value ) \n return \n elif isinstance ( val , ndarray ) : \n try : \n newshape = value . shape \n self . set ( path , value . reshape ( val . shape ) ) \n except Exception as err : \n self . reraise_exception ( \"<STR_LIT>\" \n % ( self . get_pathname ( ) , path , val . shape , newshape ) , \n sys . exc_info ( ) ) \n return \n val = self . get ( path . split ( '<STR_LIT:[>' , <NUM_LIT:1> ) [ <NUM_LIT:0> ] ) \n idx = get_index ( path ) \n if isinstance ( val , int_types ) : \n pass \n elif hasattr ( val , '<STR_LIT>' ) and idx is not None : \n if isinstance ( val [ idx ] , complex_or_real_types ) : \n val [ idx ] = value [ <NUM_LIT:0> ] \n else : \n val [ idx ] = value \n return \n elif IVariableTree . providedBy ( val ) : \n raise NotImplementedError ( \"<STR_LIT>\" ) \n elif hasattr ( val , '<STR_LIT>' ) : \n val . set_flattened_value ( value ) \n return \n self . raise_exception ( \"<STR_LIT>\" % path , TypeError ) \n def get_iotype ( self , name ) : \n return self . get_trait ( name ) . iotype \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def set ( self , path , value ) : \n \"\"\"<STR_LIT>\"\"\" \n _local_setter_ = value \n expr = self . _setcache . get ( path ) \n if expr is not None : \n exec ( expr ) \n return \n obj , restofpath = proxy_parent ( self , path ) \n if IOverrideSet . providedBy ( obj ) or ( restofpath and IContainerProxy . providedBy ( obj ) ) : \n obj . set ( restofpath , value ) \n return \n assign = \"<STR_LIT>\" % path \n expr = compile ( assign , assign , mode = '<STR_LIT>' ) \n try : \n exec ( expr ) \n except Exception as err : \n self . raise_exception ( str ( err ) , err . __class__ ) \n else : \n self . _setcache [ path ] = expr \n def save_to_egg ( self , name , version , py_dir = None , src_dir = None , \n src_files = None , child_objs = None , dst_dir = None , \n observer = None , need_requirements = True ) : \n \"\"\"<STR_LIT>\"\"\" \n assert name and isinstance ( name , basestring ) \n assert version and isinstance ( version , basestring ) \n if not version . endswith ( '<STR_LIT:.>' ) : \n version += '<STR_LIT:.>' \n now = datetime . datetime . now ( ) \n tstamp = '<STR_LIT>' % ( now . year , now . month , now . day , now . hour , now . minute ) \n version += tstamp \n observer = eggobserver . EggObserver ( observer , self . _logger ) \n entry_pts = [ ( self , name , _get_entry_group ( self ) ) ] \n if child_objs is not None : \n root_pathname = self . get_pathname ( ) \n root_start = root_pathname . rfind ( '<STR_LIT:.>' ) \n root_start = root_start + <NUM_LIT:1> if root_start >= <NUM_LIT:0> else <NUM_LIT:0> \n root_pathname += '<STR_LIT:.>' \n for child in child_objs : \n pathname = child . get_pathname ( ) \n if not pathname . startswith ( root_pathname ) : \n msg = '<STR_LIT>' % ( pathname , root_pathname ) \n observer . exception ( msg ) \n self . raise_exception ( msg , RuntimeError ) \n entry_pts . append ( ( child , pathname [ root_start : ] , \n _get_entry_group ( child ) ) ) \n parent = self . parent \n self . parent = None \n try : \n return eggsaver . save_to_egg ( entry_pts , version , py_dir , \n src_dir , src_files , dst_dir , \n self . _logger , observer . observer , \n need_requirements ) \n except Exception : \n self . reraise_exception ( info = sys . exc_info ( ) ) \n finally : \n self . parent = parent \n def save ( self , outstream , fmt = SAVE_CPICKLE , proto = - <NUM_LIT:1> ) : \n \"\"\"<STR_LIT>\"\"\" \n parent = self . parent \n self . parent = None \n try : \n eggsaver . save ( self , outstream , fmt , proto , self . _logger ) \n except Exception : \n self . reraise_exception ( info = sys . exc_info ( ) ) \n finally : \n self . parent = parent \n @ staticmethod \n def load_from_eggfile ( filename , observer = None , log = None ) : \n \"\"\"<STR_LIT>\"\"\" \n entry_group = '<STR_LIT>' \n entry_name = '<STR_LIT>' \n log = log or logger \n return eggloader . load_from_eggfile ( filename , entry_group , entry_name , \n log , observer ) \n @ staticmethod \n def load_from_eggpkg ( package , entry_name = None , instance_name = None , \n observer = None ) : \n \"\"\"<STR_LIT>\"\"\" \n entry_group = '<STR_LIT>' \n if not entry_name : \n entry_name = package \n return eggloader . load_from_eggpkg ( package , entry_group , entry_name , \n instance_name , logger , observer ) \n @ staticmethod \n def load ( instream , fmt = SAVE_CPICKLE , package = None , call_post_load = True , \n name = None ) : \n \"\"\"<STR_LIT>\"\"\" \n top = eggloader . load ( instream , fmt , package , logger ) \n top . cpath_updated ( ) \n if name : \n top . name = name \n if call_post_load : \n top . parent = None \n top . post_load ( ) \n return top \n def post_load ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _repair_traits ( ) \n for name in self . list_containers ( ) : \n getattr ( self , name ) . post_load ( ) \n @ rbac ( '<STR_LIT>' ) \n def pre_delete ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for name in self . list_containers ( ) : \n getattr ( self , name ) . pre_delete ( ) \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) , proxy_types = [ CTrait ] ) \n def get_dyn_trait ( self , pathname , iotype = None , trait = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if pathname . startswith ( '<STR_LIT>' ) : \n return None \n cname , _ , restofpath = pathname . partition ( '<STR_LIT:.>' ) \n if restofpath : \n child = getattr ( self , cname ) \n if is_instance ( child , Container ) : \n return child . get_dyn_trait ( restofpath , iotype , trait ) \n else : \n if deep_hasattr ( child , restofpath ) : \n return None \n else : \n trait = self . get_trait ( cname ) \n if trait is not None : \n if iotype is not None : \n if isinstance ( trait . trait_type , Python ) : \n obj = getattr ( self , cname ) \n t_iotype = getattr ( obj , '<STR_LIT>' , None ) \n else : \n t_iotype = self . get_iotype ( cname ) \n if ( iotype == '<STR_LIT>' and t_iotype not in ( '<STR_LIT>' , '<STR_LIT:state>' ) ) or ( iotype == '<STR_LIT>' and t_iotype not in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT:state>' , '<STR_LIT>' ) ) : \n self . raise_exception ( \"<STR_LIT>\" % \n ( pathname , _iodict [ iotype ] ) , \n RuntimeError ) \n return trait \n elif trait is None and self . contains ( cname ) : \n return None \n self . raise_exception ( \"<STR_LIT>\" % \n pathname , AttributeError ) \n @ rbac ( ( '<STR_LIT>' , '<STR_LIT:user>' ) ) \n def get_trait_typenames ( self , pathname , iotype = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if not pathname : \n obj = self \n else : \n trait = self . get_dyn_trait ( pathname , iotype = iotype ) \n if trait is None : \n return [ ] \n trait = trait . trait_type or trait . trait or trait \n if trait . target : \n trait = self . get_dyn_trait ( trait . target ) \n try : \n ttype = trait . trait_type \n except AttributeError : \n pass \n else : \n if ttype is not None : \n trait = ttype \n if isinstance ( trait , Python ) : \n obj = self . get ( pathname ) \n else : \n obj = trait \n names = [ ] \n Container . _bases ( type ( obj ) , names ) \n return names \n @ staticmethod \n def _bases ( cls , names ) : \n \"\"\"<STR_LIT>\"\"\" \n names . append ( '<STR_LIT>' % ( cls . __module__ , cls . __name__ ) ) \n for base in cls . __bases__ : \n Container . _bases ( base , names ) \n def raise_exception ( self , msg , exception_class = Exception ) : \n \"\"\"<STR_LIT>\"\"\" \n coords = '<STR_LIT>' \n obj = self \n while obj is not None : \n try : \n coords = obj . get_itername ( ) \n except AttributeError : \n try : \n obj = obj . parent \n except AttributeError : \n break \n else : \n break \n if coords : \n full_msg = '<STR_LIT>' % ( self . get_pathname ( ) , coords , msg ) \n else : \n full_msg = '<STR_LIT>' % ( self . get_pathname ( ) , msg ) \n raise exception_class ( full_msg ) \n def reraise_exception ( self , msg = '<STR_LIT>' , info = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if info is None : \n exc_type , exc_value , exc_traceback = sys . exc_info ( ) \n else : \n exc_type , exc_value , exc_traceback = info \n if msg : \n msg = '<STR_LIT>' % ( msg , exc_value ) \n else : \n msg = '<STR_LIT:%s>' % exc_value \n prefix = '<STR_LIT>' % self . get_pathname ( ) \n if not msg . startswith ( prefix ) : \n msg = prefix + msg \n new_exc = exc_type ( msg ) \n raise exc_type , new_exc , exc_traceback \n def build_trait ( self , ref_name , iotype = None , trait = None ) : \n \"\"\"<STR_LIT>\"\"\" \n self . raise_exception ( '<STR_LIT>' , NotImplementedError ) \n CLASSES_TO_PROXY . append ( Container ) \n CLASSES_TO_PROXY . append ( FileRef ) \n def _get_entry_group ( obj ) : \n \"\"\"<STR_LIT>\"\"\" \n if _get_entry_group . group_map is None : \n from openmdao . main . component import Component \n from openmdao . main . driver import Driver \n _get_entry_group . group_map = [ \n ( Variable , '<STR_LIT>' ) , \n ( Driver , '<STR_LIT>' ) , \n ( ICaseIterator , '<STR_LIT>' ) , \n ( IResourceAllocator , '<STR_LIT>' ) , \n ( Component , '<STR_LIT>' ) , \n ( Container , '<STR_LIT>' ) , \n ] \n for cls , group in _get_entry_group . group_map : \n if issubclass ( cls , Interface ) : \n if cls . providedBy ( obj ) : \n return group \n else : \n if isinstance ( obj , cls ) : \n return group \n return None \n _get_entry_group . group_map = None \n def dump ( cont , recurse = False , stream = None , ** metadata ) : \n \"\"\"<STR_LIT>\"\"\" \n pprint . pprint ( dict ( [ ( n , str ( v ) ) \n for n , v in cont . items ( recurse = recurse , \n ** metadata ) ] ) , \n stream ) \n def find_name ( parent , obj ) : \n \"\"\"<STR_LIT>\"\"\" \n for name , val in parent . __dict__ . items ( ) : \n if val is obj : \n return name \n return '<STR_LIT>' \n def get_default_name ( obj , scope ) : \n \"\"\"<STR_LIT>\"\"\" \n classname = obj . __class__ . __name__ . lower ( ) \n if scope is None : \n sdict = { } \n else : \n sdict = scope . __dict__ \n ver = <NUM_LIT:1> \n while '<STR_LIT>' % ( classname , ver ) in sdict : \n ver += <NUM_LIT:1> \n return '<STR_LIT>' % ( classname , ver ) \n def find_trait_and_value ( obj , pathname ) : \n \"\"\"<STR_LIT>\"\"\" \n names = pathname . split ( '<STR_LIT:.>' ) \n for name in names [ : - <NUM_LIT:1> ] : \n obj = getattr ( obj , name ) \n if is_instance ( obj , Container ) : \n objtrait = obj . get_trait ( names [ - <NUM_LIT:1> ] ) \n elif isinstance ( obj , HasTraits ) : \n objtrait = obj . trait ( names [ - <NUM_LIT:1> ] ) \n else : \n objtrait = None \n return ( objtrait , getattr ( obj , names [ - <NUM_LIT:1> ] ) ) \n def create_io_traits ( cont , obj_info , iotype = '<STR_LIT>' ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( obj_info , ( basestring , tuple ) ) : \n it = [ obj_info ] \n else : \n it = obj_info \n for entry in it : \n iostat = iotype \n trait = None \n if isinstance ( entry , basestring ) : \n ref_name = entry \n name = entry . replace ( '<STR_LIT:.>' , '<STR_LIT:_>' ) \n elif isinstance ( entry , tuple ) : \n ref_name = entry [ <NUM_LIT:0> ] \n name = entry [ <NUM_LIT:1> ] or ref_name . replace ( '<STR_LIT:.>' , '<STR_LIT:_>' ) \n try : \n iostat = entry [ <NUM_LIT:2> ] \n trait = entry [ <NUM_LIT:3> ] \n except IndexError : \n pass \n else : \n cont . raise_exception ( '<STR_LIT>' % entry , \n RuntimeError ) \n if '<STR_LIT:.>' in name : \n cont . raise_exception ( \"<STR_LIT>\" \n \"<STR_LIT>\" % name , NameError ) \n newtrait = cont . get_trait ( name ) \n if newtrait is not None : \n cont . raise_exception ( \n \"<STR_LIT>\" % name , \n RuntimeError ) \n if not cont . contains ( ref_name ) : \n cont . raise_exception ( \"<STR_LIT>\" \n \"<STR_LIT>\" % ref_name , AttributeError ) \n cont . add_trait ( name , cont . build_trait ( ref_name , iostat , <mask0> ) ) \n", "gt": "trait"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n __all__ = [ \"<STR_LIT>\" ] \n from traits . api import Instance \n from openmdao . main . variable import Variable , gui_excludes \n class VarTree ( Variable ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , default_value , allow_none = True , ** metadata ) : \n from openmdao . main . vartree import VariableTree \n if isinstance ( default_value , VariableTree ) : \n klass = default_value . __class__ \n if '<STR_LIT>' in metadata : \n default_value . _iotype = metadata [ '<STR_LIT>' ] \n else : \n metadata [ '<STR_LIT>' ] = default_value . iotype \n else : \n raise TypeError ( '<STR_LIT>' \n '<STR_LIT>' ) \n metadata . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) \n self . _allow_none = allow_none \n self . klass = klass \n self . _instance = Instance ( klass = klass , allow_none = False , factory = None , \n args = None , kw = None , ** metadata ) \n self . _instance . default_value = default_value \n super ( VarTree , self ) . __init__ ( default_value , ** metadata ) \n def validate ( self , obj , name , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if value is None : \n if self . _allow_none : \n return value \n self . validate_failed ( obj , name , value ) \n try : \n value = self . _instance . validate ( obj , name , value ) \n except Exception : \n obj . raise_exception ( '<STR_LIT>' % \n ( name , self . _instance . klass . __module__ , \n self . _instance . klass . __name__ , type ( value ) ) , \n TypeError ) \n return value \n def post_setattr ( self , obj , name , value ) : \n \"\"\"<STR_LIT>\"\"\" \n if value . parent is not obj : \n value . parent = obj \n value . name = name \n value . _iotype = self . iotype \n def get_attribute ( self , name , value , trait , meta ) : \n \"\"\"<STR_LIT>\"\"\" \n io_attr = { } \n io_attr [ '<STR_LIT:name>' ] = name \n io_attr [ '<STR_LIT:type>' ] = trait . trait_type . klass . __name__ \n io_attr [ '<STR_LIT>' ] = '<STR_LIT>' \n for field in meta : \n if field not in gui_excludes : \n io_attr [ field ] = meta [ field ] \n return io_attr , <mask0> \n", "gt": "None"}
{"input": "\n import os \n import sys \n import numpy \n from contextlib import contextmanager \n def _redirect_streams ( to_fd ) : \n \"\"\"<STR_LIT>\"\"\" \n original_stdout_fd = sys . stdout . fileno ( ) \n original_stderr_fd = sys . stderr . fileno ( ) \n sys . stdout . close ( ) \n sys . stderr . close ( ) \n os . dup2 ( to_fd , original_stdout_fd ) \n os . dup2 ( to_fd , original_stderr_fd ) \n sys . stdout = os . fdopen ( original_stdout_fd , '<STR_LIT:wb>' , <NUM_LIT:0> ) \n sys . stderr = os . fdopen ( original_stderr_fd , '<STR_LIT:wb>' , <NUM_LIT:0> ) \n def use_proc_files ( ) : \n if MPI is not None : \n rank = MPI . COMM_WORLD . rank \n sname = \"<STR_LIT>\" % rank \n ofile = open ( sname , '<STR_LIT:wb>' ) \n _redirect_streams ( ofile . fileno ( ) ) \n def under_mpirun ( ) : \n \"\"\"<STR_LIT>\"\"\" \n for name in os . environ . keys ( ) : \n if name . startswith ( '<STR_LIT>' ) or name . startswith ( '<STR_LIT>' ) : \n return True \n return False \n class PETSc ( object ) : \n def __init__ ( self ) : \n self . needs_ksp = False \n self . _PETSc = None \n @ property \n def installed ( self ) : \n try : \n if self . _PETSc is None : \n PETSc = _import_petsc ( ) \n del sys . modules [ '<STR_LIT>' ] \n self . _PETSc = PETSc \n return True \n except ImportError : \n self . _PETSc = None \n return False \n def __getattr__ ( self , name ) : \n if self . installed : \n return getattr ( self . _PETSc , name ) \n raise AttributeError ( name ) \n def create_petsc_vec ( comm , arr ) : \n if under_mpirun ( ) or PETSc . needs_ksp : \n if PETSc . installed and ( MPI is None or comm != MPI . COMM_NULL ) : \n return PETSc . Vec ( ) . createWithArray ( arr , comm = comm ) \n return None \n def _import_petsc ( ) : \n import petsc4py \n from petsc4py import PETSc \n return PETSc \n if under_mpirun ( ) : \n from mpi4py import MPI \n PETSc = _import_petsc ( ) \n PETSc . installed = True \n COMM_NULL = MPI . COMM_NULL \n else : \n MPI = None \n COMM_NULL = None \n PETSc = PETSc ( ) \n class MPI_info ( object ) : \n def __init__ ( self ) : \n self . requested_cpus = ( <NUM_LIT:1> , <NUM_LIT:1> ) \n self . comm = COMM_NULL \n @ property \n def size ( self ) : \n if MPI and self . comm != COMM_NULL : \n return self . comm . size \n return <NUM_LIT:1> \n @ property \n def rank ( self ) : \n if MPI : \n if self . comm != COMM_NULL : \n return self . comm . rank \n else : \n return - <NUM_LIT:1> \n return <NUM_LIT:0> \n def get_norm ( vec , order = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if MPI : \n vec . petsc_vec . assemble ( ) \n return vec . petsc_vec . norm ( ) \n else : \n return numpy . linalg . norm ( vec . array , ord = order ) \n idx_arr_type = PETSc . IntType if MPI else '<STR_LIT:i>' \n def make_idx_array ( start , end ) : \n \"\"\"<STR_LIT>\"\"\" \n return numpy . arange ( start , end , dtype = idx_arr_type ) \n def to_idx_array ( idxs ) : \n \"\"\"<STR_LIT>\"\"\" \n return numpy . array ( idxs , dtype = idx_arr_type ) \n def evenly_distrib_idxs ( num_divisions , arr_size ) : \n \"\"\"<STR_LIT>\"\"\" \n base = arr_size / num_divisions \n leftover = arr_size % num_divisions \n sizes = numpy . ones ( num_divisions , dtype = \"<STR_LIT:int>\" ) * base \n sizes [ : leftover ] += <NUM_LIT:1> \n offsets = numpy . zeros ( num_divisions , dtype = \"<STR_LIT:int>\" ) \n offsets [ <NUM_LIT:1> : ] = numpy . cumsum ( sizes ) [ : - <NUM_LIT:1> ] \n return sizes , offsets \n @ contextmanager \n def MPIContext ( ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n yield \n except : \n exc_type , exc_val , exc_tb = sys . exc_info ( ) \n if exc_val is not None : \n fail = True \n else : \n fail = False \n fails = MPI . COMM_WORLD . allgather ( fail ) \n if fail or not any ( fails ) : \n raise exc_type , exc_val , exc_tb \n else : \n for i , f in enumerate ( fails ) : \n if f : \n raise RuntimeError ( \"<STR_LIT>\" % i ) \n if os . environ . get ( '<STR_LIT>' ) : \n <mask0> ( ) \n", "gt": "use_proc_files"}
{"input": "\n import unittest \n from openmdao . main . api import Assembly , Component \n from openmdao . main . datatypes . api import Float , Array \n from openmdao . lib . drivers . api import CONMINdriver , BroydenSolver , SensitivityDriver , FixedPointIterator \n from openmdao . lib . optproblems import sellar \n class Dis12Linear ( Component ) : \n \"\"\"<STR_LIT>\"\"\" \n z1 = Float ( <NUM_LIT:0.> , iotype = '<STR_LIT>' ) \n z2 = Float ( <NUM_LIT:0.> , iotype = '<STR_LIT>' ) \n z_store = Array ( [ <NUM_LIT:0.> , <NUM_LIT:0.> ] , iotype = '<STR_LIT>' ) \n ssa_F = Array ( [ <NUM_LIT:0.0> ] , iotype = '<STR_LIT>' ) \n ssa_G = Array ( [ <NUM_LIT:0.0> , <NUM_LIT:0.0> ] , iotype = '<STR_LIT>' ) \n ssa_dF = Array ( [ <NUM_LIT:0.0> , <NUM_LIT:0.0> ] , iotype = '<STR_LIT>' ) \n ssa_dG = Array ( [ [ <NUM_LIT:0.0> , <NUM_LIT:0.0> ] , [ <NUM_LIT:0.0> , <NUM_LIT:0.0> ] ] , iotype = '<STR_LIT>' ) \n obj = Float ( <NUM_LIT:0.0> , iotype = '<STR_LIT>' ) \n con1 = Float ( <NUM_LIT:0.0> , iotype = '<STR_LIT>' ) \n con2 = Float ( <NUM_LIT:0.0> , iotype = '<STR_LIT>' ) \n def execute ( self ) : \n self . obj = self . ssa_F [ <NUM_LIT:0> ] + self . ssa_dF [ <NUM_LIT:0> ] * ( self . z_store [ <NUM_LIT:0> ] - self . z1 ) + self . ssa_dF [ <NUM_LIT:1> ] * ( self . z_store [ <NUM_LIT:1> ] - self . z2 ) \n self . con1 = self . ssa_G [ <NUM_LIT:0> ] + self . ssa_dG [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] * ( self . z_store [ <NUM_LIT:0> ] - self . z1 ) + self . ssa_dG [ <NUM_LIT:0> ] [ <NUM_LIT:1> ] * ( self . z_store [ <NUM_LIT:1> ] - self . z2 ) \n self . con2 = self . ssa_G [ <NUM_LIT:1> ] + self . ssa_dG [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] * ( self . z_store [ <NUM_LIT:0> ] - self . z1 ) + self . ssa_dG [ <NUM_LIT:1> ] [ <NUM_LIT:1> ] * ( self . z_store [ <NUM_LIT:1> ] - self . z2 ) \n class SellarBLISS ( Assembly ) : \n z_store = Array ( [ <NUM_LIT:0.> , <NUM_LIT:0.> ] , dtype = Float , iotype = '<STR_LIT>' ) \n def configure ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . add ( '<STR_LIT>' , Dis12Linear ( ) ) \n self . connect ( '<STR_LIT>' , '<STR_LIT>' ) \n self . add ( '<STR_LIT>' , CONMINdriver ( ) ) \n self . sysopt . add_parameter ( '<STR_LIT>' , low = - <NUM_LIT> , high = <NUM_LIT> , start = <NUM_LIT> ) \n self . sysopt . add_objective ( '<STR_LIT>' ) \n self . driver . workflow . add ( [ '<STR_LIT>' ] ) \n self . sysopt . workflow . add ( [ '<STR_LIT>' ] ) \n class BndryFullSubTestCase ( unittest . TestCase ) : \n def test_MDF ( self ) : \n prob = SellarBLISS ( ) \n prob . name = '<STR_LIT>' \n prob . run ( ) \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import numpy as np \n import unittest \n from openmdao . main . api import Assembly , Component , Driver , set_as_top \n from openmdao . main . datatypes . api import Float , Array \n from openmdao . main . hasconstraints import HasConstraints , HasEqConstraints , HasIneqConstraints , Constraint , Has2SidedConstraints \n from openmdao . main . interfaces import IHas2SidedConstraints , implements \n from openmdao . main . pseudocomp import SimpleEQConPComp , SimpleEQ0PComp \n from openmdao . main . test . simpledriver import SimpleDriver \n from openmdao . test . execcomp import ExecComp \n from openmdao . units . units import PhysicalQuantity \n from openmdao . util . decorators import add_delegate \n from openmdao . util . testutil import assert_rel_error \n @ add_delegate ( HasConstraints ) \n class MyDriver ( Driver ) : \n pass \n @ add_delegate ( HasEqConstraints ) \n class MyEqDriver ( Driver ) : \n pass \n @ add_delegate ( HasIneqConstraints ) \n class MyInEqDriver ( Driver ) : \n pass \n @ add_delegate ( HasConstraints , Has2SidedConstraints ) \n class My2SDriver ( Driver ) : \n implements ( IHas2SidedConstraints ) \n class SimpleUnits ( Component ) : \n a = Float ( iotype = '<STR_LIT>' , units = '<STR_LIT>' ) \n b = Float ( iotype = '<STR_LIT>' , units = '<STR_LIT>' ) \n c = Float ( iotype = '<STR_LIT>' , units = '<STR_LIT>' ) \n d = Float ( iotype = '<STR_LIT>' , units = '<STR_LIT>' ) \n arr = Array ( [ <NUM_LIT:1.> , <NUM_LIT> , <NUM_LIT> ] , iotype = '<STR_LIT>' , units = '<STR_LIT>' ) \n arr_out = Array ( [ <NUM_LIT:1.> , <NUM_LIT> , <NUM_LIT> ] , iotype = '<STR_LIT>' , units = '<STR_LIT>' ) \n def __init__ ( self ) : \n super ( SimpleUnits , self ) . __init__ ( ) \n self . a = <NUM_LIT:1> \n self . b = <NUM_LIT:2> \n self . c = <NUM_LIT:3> \n self . d = - <NUM_LIT:1> \n def execute ( self ) : \n self . c = PhysicalQuantity ( self . a + self . b , '<STR_LIT>' ) . in_units_of ( '<STR_LIT>' ) . value \n self . d = PhysicalQuantity ( self . a - self . b , '<STR_LIT>' ) . in_units_of ( '<STR_LIT>' ) . value \n class Simple ( Component ) : \n a = Float ( iotype = '<STR_LIT>' ) \n b = Float ( iotype = '<STR_LIT>' ) \n c = Float ( iotype = '<STR_LIT>' ) \n d = Float ( iotype = '<STR_LIT>' ) \n def __init__ ( self ) : \n super ( Simple , self ) . __init__ ( ) \n self . a = <NUM_LIT:1> \n self . b = <NUM_LIT:2> \n self . c = <NUM_LIT:3> \n self . d = - <NUM_LIT:1> \n def execute ( self ) : \n self . c = self . a + self . b \n self . d = self . a - self . b \n def list_deriv_vars ( self ) : \n return ( '<STR_LIT:a>' , '<STR_LIT:b>' ) , ( '<STR_LIT:c>' , '<STR_LIT:d>' ) \n def provideJ ( self ) : \n der = <NUM_LIT:1.0> \n return np . array ( [ [ der , der ] , [ der , der ] ] ) \n class HasConstraintsTestCase ( unittest . TestCase ) : \n def setUp ( self ) : \n self . asm = set_as_top ( Assembly ( ) ) \n self . asm . add ( '<STR_LIT>' , Simple ( ) ) \n self . asm . add ( '<STR_LIT>' , Simple ( ) ) \n self . asm . add ( '<STR_LIT>' , SimpleUnits ( ) ) \n self . asm . add ( '<STR_LIT>' , SimpleUnits ( ) ) \n def test_list_constraints ( self ) : \n drv = self . asm . add ( '<STR_LIT>' , MyDriver ( ) ) \n self . asm . run ( ) \n drv . add_constraint ( '<STR_LIT>' ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . assertEqual ( drv . list_constraints ( ) , \n [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def test_list_eq_constraints ( self ) : \n drv = self . asm . add ( '<STR_LIT>' , MyEqDriver ( ) ) \n drv . add_constraint ( '<STR_LIT>' ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . assertEqual ( drv . list_constraints ( ) , \n [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def test_list_ineq_constraints ( self ) : \n drv = self . asm . add ( '<STR_LIT>' , MyDriver ( ) ) \n drv . add_constraint ( '<STR_LIT>' ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . assertEqual ( drv . list_constraints ( ) , \n [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def _check_ineq_add_constraint ( self , drv ) : \n self . asm . add ( '<STR_LIT>' , drv ) \n try : \n drv . add_constraint ( '<STR_LIT>' ) \n except Exception as err : \n self . assertEqual ( str ( err ) , \"<STR_LIT>\" ) \n else : \n self . fail ( \"<STR_LIT>\" ) \n self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , <NUM_LIT:0> ) \n drv . add_constraint ( '<STR_LIT>' ) \n try : \n drv . add_constraint ( '<STR_LIT>' ) \n except Exception as err : \n self . assertEqual ( str ( err ) , \n '<STR_LIT>' \n '<STR_LIT>' ) \n else : \n self . fail ( \"<STR_LIT>\" ) \n self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , <NUM_LIT:1> ) \n drv . remove_constraint ( '<STR_LIT>' ) \n self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , <NUM_LIT:0> ) \n try : \n drv . remove_constraint ( '<STR_LIT>' ) \n except Exception as err : \n self . assertEqual ( str ( err ) , \n \"<STR_LIT>\" ) \n else : \n self . fail ( \"<STR_LIT>\" ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , <NUM_LIT:1> ) \n drv . add_constraint ( '<STR_LIT>' , name = '<STR_LIT>' ) \n self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , <NUM_LIT:2> ) \n try : \n drv . add_constraint ( '<STR_LIT>' , name = '<STR_LIT>' ) \n except Exception as err : \n self . assertEqual ( str ( err ) , '<STR_LIT>' ) \n else : \n self . fail ( \"<STR_LIT>\" ) \n self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , <NUM_LIT:2> ) \n drv . remove_constraint ( '<STR_LIT>' ) \n self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , <NUM_LIT:1> ) \n drv . clear_constraints ( ) \n self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , <NUM_LIT:0> ) \n try : \n drv . add_constraint ( '<STR_LIT>' ) \n except ValueError as err : \n self . assertEqual ( str ( err ) , \n \"<STR_LIT>\" ) \n else : \n self . fail ( '<STR_LIT>' ) \n def _check_eq_add_constraint ( self , drv ) : \n self . asm . add ( '<STR_LIT>' , drv ) \n self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , <NUM_LIT:0> ) \n self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , <NUM_LIT:0> ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , <NUM_LIT:1> ) \n try : \n drv . add_constraint ( '<STR_LIT>' ) \n except Exception as err : \n self . assertEqual ( str ( err ) , \n '<STR_LIT>' \n '<STR_LIT>' ) \n else : \n self . fail ( \"<STR_LIT>\" ) \n drv . remove_constraint ( '<STR_LIT>' ) \n self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , <NUM_LIT:0> ) \n try : \n drv . remove_constraint ( '<STR_LIT>' ) \n except Exception as err : \n self . assertEqual ( str ( err ) , \n \"<STR_LIT>\" ) \n else : \n self . fail ( \"<STR_LIT>\" ) \n self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , <NUM_LIT:0> ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , <NUM_LIT:1> ) \n drv . add_constraint ( '<STR_LIT>' , name = '<STR_LIT>' ) \n self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , <NUM_LIT:2> ) \n try : \n drv . add_constraint ( '<STR_LIT>' , name = '<STR_LIT>' ) \n except Exception as err : \n self . assertEqual ( str ( err ) , '<STR_LIT>' ) \n else : \n self . fail ( \"<STR_LIT>\" ) \n drv . remove_constraint ( '<STR_LIT>' ) \n self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , <NUM_LIT:1> ) \n drv . clear_constraints ( ) \n self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , <NUM_LIT:0> ) \n try : \n drv . add_constraint ( '<STR_LIT>' ) \n except ValueError as err : \n self . assertEqual ( str ( err ) , \n \"<STR_LIT>\" ) \n else : \n self . fail ( '<STR_LIT>' ) \n def _check_eq_eval_constraints ( self , drv ) : \n self . asm . add ( '<STR_LIT>' , drv ) \n vals = drv . eval_eq_constraints ( ) \n self . assertEqual ( len ( vals ) , <NUM_LIT:0> ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . asm . comp1 . a = <NUM_LIT:4> \n self . asm . comp1 . b = <NUM_LIT:5> \n self . asm . comp1 . c = <NUM_LIT:9> \n self . asm . comp1 . d = - <NUM_LIT:1> \n self . asm . run ( ) \n vals = drv . eval_eq_constraints ( ) \n self . assertEqual ( len ( vals ) , <NUM_LIT:1> ) \n self . assertEqual ( vals [ <NUM_LIT:0> ] , <NUM_LIT> ) \n vals = drv . get_eq_constraints ( ) \n self . assertEqual ( len ( vals ) , <NUM_LIT:1> ) \n self . assertTrue ( isinstance ( vals [ '<STR_LIT>' ] , Constraint ) ) \n def _check_ineq_eval_constraints ( self , drv ) : \n self . asm . add ( '<STR_LIT>' , drv ) \n vals = drv . eval_ineq_constraints ( ) \n self . assertEqual ( len ( vals ) , <NUM_LIT:0> ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . asm . comp1 . a = <NUM_LIT:4> \n self . asm . comp1 . b = <NUM_LIT:5> \n self . asm . comp1 . c = <NUM_LIT:9> \n self . asm . comp1 . d = - <NUM_LIT:1> \n self . asm . run ( ) \n vals = drv . eval_ineq_constraints ( ) \n self . assertEqual ( len ( vals ) , <NUM_LIT:1> ) \n self . assertEqual ( vals [ <NUM_LIT:0> ] , <NUM_LIT:1> ) \n vals = drv . get_ineq_constraints ( ) \n self . assertEqual ( len ( vals ) , <NUM_LIT:1> ) \n self . assertTrue ( isinstance ( vals [ '<STR_LIT>' ] , Constraint ) ) \n def test_constraint_scaler_adder ( self ) : \n drv = self . asm . add ( '<STR_LIT>' , MyDriver ( ) ) \n self . asm . comp1 . a = <NUM_LIT> \n self . asm . comp1 . b = <NUM_LIT> \n drv . add_constraint ( '<STR_LIT>' ) \n self . asm . run ( ) \n result = drv . eval_ineq_constraints ( ) \n self . assertEqual ( result [ <NUM_LIT:0> ] , - <NUM_LIT> ) \n drv . remove_constraint ( '<STR_LIT>' ) \n result = drv . eval_ineq_constraints ( ) \n self . assertEqual ( result , [ ] ) \n def test_add_constraint_eq_eq ( self ) : \n drv = MyDriver ( ) \n self . asm . add ( '<STR_LIT>' , drv ) \n try : \n drv . add_constraint ( '<STR_LIT>' ) \n except Exception as err : \n self . assertEqual ( str ( err ) , \"<STR_LIT>\" ) \n else : \n self . fail ( \"<STR_LIT>\" ) \n def test_add_constraint ( self ) : \n drv = MyDriver ( ) \n self . _check_eq_add_constraint ( drv ) \n self . _check_ineq_add_constraint ( drv ) \n def test_add_eq_constraint ( self ) : \n self . _check_eq_add_constraint ( MyEqDriver ( ) ) \n def test_add_ineq_constraint ( self ) : \n self . _check_ineq_add_constraint ( MyInEqDriver ( ) ) \n def test_implicit_constraint ( self ) : \n drv = self . asm . add ( '<STR_LIT>' , MyEqDriver ( ) ) \n try : \n drv . add_constraint ( '<STR_LIT>' ) \n except ValueError , err : \n self . assertEqual ( str ( err ) , \n \"<STR_LIT>\" ) \n else : \n self . fail ( '<STR_LIT>' ) \n def test_eval_constraint ( self ) : \n self . _check_eq_eval_constraints ( MyDriver ( ) ) \n self . _check_ineq_eval_constraints ( MyDriver ( ) ) \n def test_eval_eq_constraint ( self ) : \n self . _check_eq_eval_constraints ( MyEqDriver ( ) ) \n def test_eval_ineq_constraint ( self ) : \n self . _check_ineq_eval_constraints ( MyInEqDriver ( ) ) \n def test_pseudocomps ( self ) : \n self . asm . add ( '<STR_LIT>' , MyDriver ( ) ) \n self . asm . driver . workflow . add ( [ '<STR_LIT>' , '<STR_LIT>' ] ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _depgraph . list_connections ( ) , \n [ ] ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _pseudo_0 . _orig_expr , '<STR_LIT>' ) \n self . assertEqual ( set ( self . asm . _depgraph . list_connections ( drivers = False ) ) , \n set ( [ ( '<STR_LIT>' , '<STR_LIT>' ) , ( '<STR_LIT>' , '<STR_LIT>' ) ] ) ) \n self . asm . driver . remove_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _depgraph . list_connections ( drivers = False ) , [ ] ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( set ( self . asm . _depgraph . list_connections ( drivers = False ) ) , \n set ( [ ( '<STR_LIT>' , '<STR_LIT>' ) ] ) ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _pseudo_1 . _orig_expr , '<STR_LIT>' ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _pseudo_2 . _orig_expr , '<STR_LIT>' ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _pseudo_3 . _orig_expr , '<STR_LIT>' ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _pseudo_4 . _orig_expr , '<STR_LIT>' ) \n self . asm . driver . clear_constraints ( ) \n self . asm . comp1 . a = <NUM_LIT:2> \n self . asm . comp1 . b = <NUM_LIT:1> \n self . asm . comp2 . a = <NUM_LIT:4> \n self . asm . comp2 . b = <NUM_LIT:2> \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . run ( ) \n self . assertEqual ( self . asm . _pseudo_5 . out0 , <NUM_LIT:1.0> ) \n self . assertEqual ( self . asm . _pseudo_6 . out0 , - <NUM_LIT:1.0> ) \n self . assertEqual ( self . asm . _pseudo_7 . out0 , <NUM_LIT> ) \n def test_custom_pseudocomp_creation ( self ) : \n self . asm . add ( '<STR_LIT>' , MyDriver ( ) ) \n arg = { } \n result = { } \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _pseudo_0 . __class__ , SimpleEQ0PComp ) \n self . asm . run ( ) \n arg [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT> ] ) \n result [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT:0.0> ] ) \n self . asm . _pseudo_0 . apply_deriv ( arg , result ) \n self . assertEqual ( result [ '<STR_LIT>' ] [ <NUM_LIT:0> ] , <NUM_LIT> ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _pseudo_1 . __class__ , SimpleEQ0PComp ) \n self . asm . run ( ) \n arg [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT> ] ) \n result [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT:0.0> ] ) \n self . asm . _pseudo_1 . apply_deriv ( arg , result ) \n self . assertEqual ( result [ '<STR_LIT>' ] [ <NUM_LIT:0> ] , <NUM_LIT> ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _pseudo_2 . __class__ , SimpleEQConPComp ) \n self . asm . run ( ) \n arg [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT> ] ) \n arg [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT> ] ) \n result [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT:0.0> ] ) \n self . asm . _pseudo_2 . apply_deriv ( arg , result ) \n self . assertEqual ( result [ '<STR_LIT>' ] [ <NUM_LIT:0> ] , <NUM_LIT> ) \n self . asm . driver . clear_constraints ( ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _pseudo_3 . __class__ , SimpleEQConPComp ) \n self . asm . run ( ) \n arg [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT> ] ) \n arg [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT> ] ) \n result [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT:0.0> ] ) \n self . asm . _pseudo_3 . apply_deriv ( arg , result ) \n self . assertEqual ( result [ '<STR_LIT>' ] [ <NUM_LIT:0> ] , <NUM_LIT> ) \n self . asm . driver . clear_constraints ( ) \n self . asm . driver . add_constraint ( '<STR_LIT>' ) \n self . asm . _setup ( ) \n self . assertEqual ( self . asm . _pseudo_4 . __class__ , SimpleEQConPComp ) \n self . asm . run ( ) \n arg [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT> ] ) \n arg [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT> ] ) \n result [ '<STR_LIT>' ] = np . array ( [ <NUM_LIT:0.0> ] ) \n self . asm . _pseudo_4 . apply_deriv ( arg , result ) \n self . assertEqual ( result [ '<STR_LIT>' ] [ <NUM_LIT:0> ] , <NUM_LIT> ) \n def test_custom_jacobian ( self ) : \n class AComp ( Component ) : \n x = Array ( [ [ <NUM_LIT:1.0> , <NUM_LIT> ] , [ - <NUM_LIT> , <NUM_LIT> ] ] , iotype = '<STR_LIT>' ) \n y = Array ( np . zeros ( ( <NUM_LIT:2> , <NUM_LIT:2> ) ) , iotype = '<STR_LIT>' ) \n def __init__ ( self ) : \n super ( AComp , self ) . __init__ ( ) \n self . J = np . array ( [ [ <NUM_LIT> , - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , - <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT:0.1> , <NUM_LIT> , <NUM_LIT> , - <NUM_LIT> ] ] ) \n def execute ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n y = self . J . dot ( self . x . flatten ( ) ) \n self . y = y . reshape ( ( <NUM_LIT:2> , <NUM_LIT:2> ) ) \n def list_deriv_vars ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n input_keys = ( '<STR_LIT:x>' , ) \n output_keys = ( '<STR_LIT:y>' , ) \n return input_keys , output_keys \n def provideJ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . J \n def fake_jac ( ) : \n \"\"\"<STR_LIT>\"\"\" \n jacs = { } \n jacs [ '<STR_LIT>' ] = np . array ( [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] ) \n return jacs \n top = set_as_top ( Assembly ( ) ) \n top . add ( '<STR_LIT>' , SimpleDriver ( ) ) \n top . add ( '<STR_LIT>' , AComp ( ) ) \n top . driver . workflow . add ( '<STR_LIT>' ) \n top . driver . add_parameter ( '<STR_LIT>' , low = <NUM_LIT:10> , high = <NUM_LIT:10> ) \n top . driver . add_constraint ( '<STR_LIT>' , jacs = fake_jac ) \n top . _setup ( ) \n top . run ( ) \n J = top . driver . calc_gradient ( mode = '<STR_LIT>' , return_format = '<STR_LIT>' ) \n J = J [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n diff = np . abs ( J - top . comp . J ) \n assert_rel_error ( self , diff . max ( ) , <NUM_LIT:0.0> , <NUM_LIT> ) \n J = top . driver . calc_gradient ( mode = '<STR_LIT>' , return_format = '<STR_LIT>' ) \n J = J [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n diff = np . abs ( J - fake_jac ( ) [ '<STR_LIT>' ] ) \n assert_rel_error ( self , diff . max ( ) , <NUM_LIT:0.0> , <NUM_LIT> ) \n top . driver . clear_constraints ( ) \n top . _pseudo_count = <NUM_LIT:0> \n top . driver . add_constraint ( '<STR_LIT>' , jacs = fake_jac ) \n top . _setup ( ) \n top . run ( ) \n J = top . driver . calc_gradient ( mode = '<STR_LIT>' , return_format = '<STR_LIT>' ) \n J = J [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n diff = np . abs ( J - top . comp . J ) \n assert_rel_error ( self , diff . max ( ) , <NUM_LIT:0.0> , <NUM_LIT> ) \n J = top . driver . calc_gradient ( mode = '<STR_LIT>' , return_format = '<STR_LIT>' ) \n J = J [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n diff = np . abs ( J - fake_jac ( ) [ '<STR_LIT>' ] ) \n assert_rel_error ( self , diff . max ( ) , <NUM_LIT:0.0> , <NUM_LIT> ) \n top . driver . clear_constraints ( ) \n top . _pseudo_count = <NUM_LIT:0> \n top . driver . add_constraint ( '<STR_LIT>' , jacs = fake_jac ) \n top . _setup ( ) \n top . run ( ) \n J = top . driver . calc_gradient ( mode = '<STR_LIT>' , return_format = '<STR_LIT>' ) \n J = J [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n diff = np . abs ( J - top . comp . J ) \n assert_rel_error ( self , diff . max ( ) , <NUM_LIT:0.0> , <NUM_LIT> ) \n J = top . driver . calc_gradient ( mode = '<STR_LIT>' , return_format = '<STR_LIT>' ) \n J = J [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n diff = np . abs ( J - fake_jac ( ) [ '<STR_LIT>' ] ) \n assert_rel_error ( self , diff . max ( ) , <NUM_LIT:0.0> , <NUM_LIT> ) \n top . driver . clear_constraints ( ) \n top . _pseudo_count = <NUM_LIT:0> \n top . driver . add_constraint ( '<STR_LIT>' , jacs = fake_jac ) \n top . driver . gradient_options . lin_solver = '<STR_LIT>' \n top . _setup ( ) \n top . run ( ) \n J = top . driver . calc_gradient ( mode = '<STR_LIT>' , return_format = '<STR_LIT>' ) \n J = J [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n diff = np . abs ( J - top . comp . J ) \n assert_rel_error ( self , diff . max ( ) , <NUM_LIT:0.0> , <NUM_LIT> ) \n J = top . driver . calc_gradient ( mode = '<STR_LIT>' , return_format = '<STR_LIT>' ) \n J = J [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n diff = np . abs ( J - fake_jac ( ) [ '<STR_LIT>' ] ) \n assert_rel_error ( self , diff . max ( ) , <NUM_LIT:0.0> , <NUM_LIT> ) \n def fake_jac2 ( ) : \n \"\"\"<STR_LIT>\"\"\" \n jacs = { } \n jacs [ '<STR_LIT>' ] = np . array ( [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] ) \n return jacs \n top . driver . clear_constraints ( ) \n top . _pseudo_count = <NUM_LIT:0> \n top . driver . add_constraint ( '<STR_LIT>' , jacs = fake_jac2 ) \n top . _setup ( ) \n top . run ( ) \n J = top . driver . calc_gradient ( mode = '<STR_LIT>' , return_format = '<STR_LIT>' ) \n J = J [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n diff = np . abs ( J - top . comp . J ) \n assert_rel_error ( self , diff . max ( ) , <NUM_LIT:0.0> , <NUM_LIT> ) \n J = top . driver . calc_gradient ( mode = '<STR_LIT>' , return_format = '<STR_LIT>' ) \n J = J [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n J_abs = np . abs ( J ) \n assert_rel_error ( self , J_abs . max ( ) , <NUM_LIT:0.0> , <NUM_LIT> ) \n class Has2SidedConstraintsTestCase ( unittest . TestCase ) : \n def setUp ( self ) : \n self . asm = set_as_top ( Assembly ( ) ) \n self . asm . add ( '<STR_LIT>' , Simple ( ) ) \n self . asm . add ( '<STR_LIT>' , Simple ( ) ) \n self . asm . add ( '<STR_LIT>' , SimpleUnits ( ) ) \n self . asm . add ( '<STR_LIT>' , SimpleUnits ( ) ) \n def test_unsupported ( self ) : \n drv = self . asm . add ( '<STR_LIT>' , MyDriver ( ) ) \n self . asm . run ( ) \n try : \n drv . add_constraint ( '<STR_LIT>' ) \n except AttributeError as err : \n self . assertEqual ( str ( err ) , \"<STR_LIT>\" ) \n else : \n self . fail ( \"<STR_LIT>\" ) \n def test_get_2sided_constraints ( self ) : \n drv = self . asm . add ( '<STR_LIT>' , My2SDriver ( ) ) \n drv . add_constraint ( '<STR_LIT>' ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . asm . run ( ) \n cons = drv . get_2sided_constraints ( ) \n self . assertTrue ( len ( cons ) == <NUM_LIT:2> ) \n con1 = cons [ '<STR_LIT>' ] \n self . assertEqual ( self . asm . comp1 . a , con1 . evaluate ( self . asm ) [ <NUM_LIT:0> ] ) \n self . assertEqual ( con1 . low , - <NUM_LIT> ) \n self . assertEqual ( con1 . high , <NUM_LIT> ) \n con1 = cons [ '<STR_LIT>' ] \n self . assertEqual ( self . asm . comp1 . c , con1 . evaluate ( self . asm ) [ <NUM_LIT:0> ] ) \n self . assertEqual ( con1 . low , <NUM_LIT> ) \n self . assertEqual ( con1 . high , <NUM_LIT> ) \n cons = drv . get_constraints ( ) \n self . assertTrue ( len ( cons ) == <NUM_LIT:0> ) \n def test_list_constraints ( self ) : \n drv = self . asm . add ( '<STR_LIT>' , My2SDriver ( ) ) \n drv . add_constraint ( '<STR_LIT>' ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . asm . run ( ) \n cons = drv . list_constraints ( ) \n self . assertTrue ( '<STR_LIT>' in cons ) \n self . assertTrue ( '<STR_LIT>' in cons ) \n def test_gradient ( self ) : \n drv = self . asm . add ( '<STR_LIT>' , My2SDriver ( ) ) \n drv . add_constraint ( '<STR_LIT>' ) \n drv . add_constraint ( '<STR_LIT>' ) \n drv . add_constraint ( '<STR_LIT>' ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . asm . run ( ) \n J = drv . calc_gradient ( inputs = [ '<STR_LIT>' ] ) \n assert_rel_error ( self , J [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] , <NUM_LIT> , <NUM_LIT> ) \n assert_rel_error ( self , J [ <NUM_LIT:1> ] [ <NUM_LIT:0> ] , <NUM_LIT:1.0> , <NUM_LIT> ) \n assert_rel_error ( self , J [ <NUM_LIT:2> ] [ <NUM_LIT:0> ] , <NUM_LIT:1.0> , <NUM_LIT> ) \n assert_rel_error ( self , J [ <NUM_LIT:3> ] [ <NUM_LIT:0> ] , <NUM_LIT> , <NUM_LIT> ) \n def test_replace ( self ) : \n drv = self . asm . add ( '<STR_LIT>' , My2SDriver ( ) ) \n drv . add_constraint ( '<STR_LIT>' ) \n drv . add_constraint ( '<STR_LIT>' ) \n self . asm . run ( ) \n self . asm . replace ( '<STR_LIT>' , My2SDriver ( ) ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import unittest \n from openmdao . main . api import Component , Assembly \n from openmdao . main . datatypes . api import Float , List , Array \n from numpy import array , zeros \n class MyDefComp ( Component ) : \n f_in = Float ( <NUM_LIT> , iotype = '<STR_LIT>' ) \n f_out = Float ( iotype = '<STR_LIT>' ) \n arr_in = Array ( [ <NUM_LIT:1.> , <NUM_LIT> , <NUM_LIT> ] , iotype = '<STR_LIT>' ) \n list_in = List ( value = [ '<STR_LIT:a>' , '<STR_LIT:b>' , '<STR_LIT:c>' ] , iotype = '<STR_LIT>' ) \n def execute ( self ) : \n self . f_out = self . f_in + <NUM_LIT:1.> \n class MyNoDefComp ( Component ) : \n f_in = Float ( iotype = '<STR_LIT>' ) \n f_out = Float ( iotype = '<STR_LIT>' ) \n arr_in = Array ( iotype = '<STR_LIT>' ) \n list_in = List ( iotype = '<STR_LIT>' ) \n def execute ( self ) : \n self . f_out = self . f_in + <NUM_LIT:1.> \n class SetDefaultsTestCase ( unittest . TestCase ) : \n def test_set_to_unset_default ( self ) : \n comp = MyNoDefComp ( ) \n self . assertEqual ( <NUM_LIT:0.> , comp . f_in ) \n comp . f_in = <NUM_LIT> \n comp . arr_in = array ( [ <NUM_LIT> , <NUM_LIT> ] ) \n comp . list_in = [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ] \n comp . run ( ) \n comp . revert_to_defaults ( ) \n self . assertEqual ( <NUM_LIT:0.> , comp . f_in ) \n self . assertTrue ( all ( zeros ( <NUM_LIT:0> , '<STR_LIT:d>' ) == comp . arr_in ) ) \n self . assertEqual ( [ ] , comp . list_in ) \n def test_set_to_default ( self ) : \n comp = MyDefComp ( ) \n self . assertEqual ( <NUM_LIT> , comp . f_in ) \n comp . f_in = <NUM_LIT> \n comp . arr_in = array ( [ <NUM_LIT> , <NUM_LIT> ] ) \n self . assertFalse ( array ( [ <NUM_LIT:1.> , <NUM_LIT> , <NUM_LIT> ] ) == comp . arr_in ) \n comp . run ( ) \n comp . revert_to_defaults ( ) \n self . assertEqual ( <NUM_LIT> , comp . f_in ) \n self . assertTrue ( all ( array ( [ <NUM_LIT:1.> , <NUM_LIT> , <NUM_LIT> ] ) == comp . arr_in ) ) \n def test_set_recursive ( self ) : \n asm = Assembly ( ) \n asm . add ( '<STR_LIT>' , MyDefComp ( ) ) \n asm . add ( '<STR_LIT>' , MyNoDefComp ( ) ) \n self . assertEqual ( <NUM_LIT:0.> , asm . nodefcomp . f_in ) \n self . assertEqual ( <NUM_LIT> , asm . defcomp . f_in ) \n asm . nodefcomp . f_in = <NUM_LIT> \n asm . defcomp . f_in = <NUM_LIT> \n asm . revert_to_defaults ( ) \n self . assertEqual ( <NUM_LIT:0.> , asm . nodefcomp . f_in ) \n self . assertEqual ( <NUM_LIT> , asm . defcomp . f_in ) \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from setuptools import setup \n __author__ = '<STR_LIT>' \n setup ( \n name = '<STR_LIT:foo>' , \n version = '<STR_LIT>' , \n description = __doc__ , \n author = __author__ , \n packages = [ ] , \n py_modules = [ '<STR_LIT:foo>' ] , \n <mask0> = \"\"\"<STR_LIT>\"\"\" \n ) \n", "gt": "entry_points"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import re \n from pyparsing import CaselessLiteral , Combine , OneOrMore , Optional , TokenConverter , Word , nums , oneOf , printables , ParserElement , alphanums \n from numpy import append , array , zeros \n def _getformat ( val ) : \n if int ( val ) == val : \n return \"<STR_LIT>\" \n else : \n return \"<STR_LIT>\" \n class _SubHelper ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n self . newtext = \"<STR_LIT>\" \n self . replace_location = <NUM_LIT:0> \n self . current_location = <NUM_LIT:0> \n self . counter = <NUM_LIT:0> \n self . start_location = <NUM_LIT:0> \n self . end_location = <NUM_LIT:0> \n def set ( self , newtext , location ) : \n \"\"\"<STR_LIT>\"\"\" \n self . newtext = newtext \n self . replace_location = location \n self . current_location = <NUM_LIT:0> \n def set_array ( self , newtext , start_location , end_location ) : \n \"\"\"<STR_LIT>\"\"\" \n self . newtext = newtext \n self . start_location = start_location \n self . end_location = end_location \n self . current_location = <NUM_LIT:0> \n def replace ( self , text ) : \n \"\"\"<STR_LIT>\"\"\" \n self . current_location += <NUM_LIT:1> \n if self . current_location == self . replace_location : \n if isinstance ( self . newtext , float ) : \n return _getformat ( self . newtext ) % self . newtext \n else : \n return str ( self . newtext ) \n else : \n return text . group ( ) \n def replace_array ( self , text ) : \n \"\"\"<STR_LIT>\"\"\" \n self . current_location += <NUM_LIT:1> \n end = len ( self . newtext ) \n if self . current_location >= self . start_location and self . current_location <= self . end_location and self . counter < end : \n if isinstance ( self . newtext [ self . counter ] , float ) : \n val = self . newtext [ self . counter ] \n newval = _getformat ( val ) % val \n else : \n newval = str ( self . newtext [ self . counter ] ) \n self . counter += <NUM_LIT:1> \n return newval \n else : \n return text . group ( ) \n class ToInteger ( TokenConverter ) : \n \"\"\"<STR_LIT>\"\"\" \n def postParse ( self , instring , loc , tokenlist ) : \n \"\"\"<STR_LIT>\"\"\" \n return int ( tokenlist [ <NUM_LIT:0> ] ) \n class ToFloat ( TokenConverter ) : \n \"\"\"<STR_LIT>\"\"\" \n def postParse ( self , instring , loc , tokenlist ) : \n \"\"\"<STR_LIT>\"\"\" \n return float ( tokenlist [ <NUM_LIT:0> ] . replace ( '<STR_LIT:D>' , '<STR_LIT:E>' ) ) \n class ToNan ( TokenConverter ) : \n \"\"\"<STR_LIT>\"\"\" \n def postParse ( self , instring , loc , tokenlist ) : \n \"\"\"<STR_LIT>\"\"\" \n return float ( '<STR_LIT>' ) \n class ToInf ( TokenConverter ) : \n \"\"\"<STR_LIT>\"\"\" \n def postParse ( self , instring , loc , tokenlist ) : \n \"\"\"<STR_LIT>\"\"\" \n return float ( '<STR_LIT>' ) \n class InputFileGenerator ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n self . template_filename = [ ] \n self . output_filename = [ ] \n self . delimiter = \"<STR_LIT:U+0020>\" \n self . reg = re . compile ( '<STR_LIT>' ) \n self . data = [ ] \n self . current_row = <NUM_LIT:0> \n self . anchored = False \n def set_template_file ( self , filename ) : \n \"\"\"<STR_LIT>\"\"\" \n self . template_filename = filename \n templatefile = open ( filename , '<STR_LIT:r>' ) \n self . data = templatefile . readlines ( ) \n templatefile . close ( ) \n def set_generated_file ( self , filename ) : \n \"\"\"<STR_LIT>\"\"\" \n self . output_filename = filename \n def set_delimiters ( self , delimiter ) : \n \"\"\"<STR_LIT>\"\"\" \n self . delimiter = delimiter \n self . reg = re . compile ( '<STR_LIT>' + delimiter + '<STR_LIT>' ) \n def mark_anchor ( self , anchor , occurrence = <NUM_LIT:1> ) : \n \"\"\"<STR_LIT>\"\"\" \n if not isinstance ( occurrence , int ) : \n raise ValueError ( \"<STR_LIT>\" ) \n instance = <NUM_LIT:0> \n if occurrence > <NUM_LIT:0> : \n count = <NUM_LIT:0> \n max_lines = len ( self . data ) \n for index in xrange ( self . current_row , max_lines ) : \n line = self . data [ index ] \n if count == <NUM_LIT:0> and self . anchored : \n line = line . split ( anchor ) [ - <NUM_LIT:1> ] \n if line . find ( anchor ) > - <NUM_LIT:1> : \n instance += <NUM_LIT:1> \n if instance == occurrence : \n self . current_row += count \n self . anchored = True \n return \n count += <NUM_LIT:1> \n elif occurrence < <NUM_LIT:0> : \n max_lines = len ( self . data ) - <NUM_LIT:1> \n count = max_lines \n for index in xrange ( max_lines , - <NUM_LIT:1> , - <NUM_LIT:1> ) : \n line = self . data [ index ] \n if count == max_lines and self . anchored : \n line = line . split ( anchor ) [ <NUM_LIT:0> ] \n if line . find ( anchor ) > - <NUM_LIT:1> : \n instance += - <NUM_LIT:1> \n if instance == occurrence : \n self . current_row = count \n self . anchored = True \n return \n count -= <NUM_LIT:1> \n else : \n raise ValueError ( \"<STR_LIT>\" ) \n raise RuntimeError ( \"<STR_LIT>\" % ( anchor , self . template_filename ) ) \n def reset_anchor ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . current_row = <NUM_LIT:0> \n self . anchored = False \n def transfer_var ( self , value , row , field ) : \n \"\"\"<STR_LIT>\"\"\" \n j = self . current_row + row \n line = self . data [ j ] \n sub = _SubHelper ( ) \n sub . set ( value , field ) \n newline = re . sub ( self . reg , sub . replace , line ) \n self . data [ j ] = newline \n def transfer_array ( self , value , row_start , field_start , field_end , \n row_end = None , sep = \"<STR_LIT:U+002CU+0020>\" ) : \n \"\"\"<STR_LIT>\"\"\" \n if row_end == None : \n row_end = row_start \n sub = _SubHelper ( ) \n for row in range ( row_start , row_end + <NUM_LIT:1> ) : \n j = self . current_row + row \n line = self . data [ j ] \n if row == row_end : \n f_end = field_end \n else : \n f_end = <NUM_LIT> \n sub . set_array ( value , field_start , f_end ) \n field_start = <NUM_LIT:0> \n newline = re . sub ( self . reg , sub . replace_array , line ) \n self . data [ j ] = newline \n if sub . counter < len ( value ) : \n for val in value [ sub . counter : ] : \n newline = newline . rstrip ( ) + sep + str ( val ) \n self . data [ j ] = newline \n elif sub . counter > len ( value ) : \n raise ValueError ( \"<STR_LIT>\" ) \n self . data [ j ] += \"<STR_LIT:\\n>\" \n def transfer_2Darray ( self , value , row_start , row_end , field_start , \n field_end , sep = \"<STR_LIT:U+002CU+0020>\" ) : \n \"\"\"<STR_LIT>\"\"\" \n sub = _SubHelper ( ) \n i = <NUM_LIT:0> \n for row in range ( row_start , row_end + <NUM_LIT:1> ) : \n j = self . current_row + row \n line = self . data [ j ] \n sub . set_array ( value [ i , : ] , field_start , field_end ) \n newline = re . sub ( self . reg , sub . replace_array , line ) \n self . data [ j ] = newline \n sub . current_location = <NUM_LIT:0> \n sub . counter = <NUM_LIT:0> \n i += <NUM_LIT:1> \n def clearline ( self , row ) : \n \"\"\"<STR_LIT>\"\"\" \n self . data [ self . current_row + row ] = \"<STR_LIT:\\n>\" \n def generate ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n infile = open ( self . output_filename , '<STR_LIT:w>' ) \n infile . writelines ( self . data ) \n infile . close ( ) \n class FileParser ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , end_of_line_comment_char = None , full_line_comment_char = None ) : \n self . filename = [ ] \n self . data = [ ] \n self . delimiter = \"<STR_LIT>\" \n self . end_of_line_comment_char = end_of_line_comment_char \n self . full_line_comment_char = full_line_comment_char \n self . current_row = <NUM_LIT:0> \n self . anchored = False \n self . set_delimiters ( self . delimiter ) \n def set_file ( self , filename ) : \n \"\"\"<STR_LIT>\"\"\" \n self . filename = filename \n inputfile = open ( filename , '<STR_LIT:r>' ) \n if not self . end_of_line_comment_char and not self . full_line_comment_char : \n self . data = inputfile . readlines ( ) \n else : \n self . data = [ ] \n for line in inputfile : \n if line [ <NUM_LIT:0> ] == self . full_line_comment_char : \n continue \n self . data . append ( line . split ( self . end_of_line_comment_char ) [ <NUM_LIT:0> ] ) \n inputfile . close ( ) \n def set_delimiters ( self , delimiter ) : \n \"\"\"<STR_LIT>\"\"\" \n self . delimiter = delimiter \n if delimiter != \"<STR_LIT>\" : \n ParserElement . setDefaultWhitespaceChars ( str ( delimiter ) ) \n self . _reset_tokens ( ) \n def mark_anchor ( self , anchor , occurrence = <NUM_LIT:1> ) : \n \"\"\"<STR_LIT>\"\"\" \n if not isinstance ( occurrence , int ) : \n raise ValueError ( \"<STR_LIT>\" ) \n instance = <NUM_LIT:0> \n if occurrence > <NUM_LIT:0> : \n count = <NUM_LIT:0> \n max_lines = len ( self . data ) \n for index in xrange ( self . current_row , max_lines ) : \n line = self . data [ index ] \n if count == <NUM_LIT:0> and self . anchored : \n line = line . split ( anchor ) [ - <NUM_LIT:1> ] \n if anchor in line : \n instance += <NUM_LIT:1> \n if instance == occurrence : \n self . current_row += count \n self . anchored = True \n return \n count += <NUM_LIT:1> \n elif occurrence < <NUM_LIT:0> : \n max_lines = len ( self . data ) - <NUM_LIT:1> \n count = max_lines \n for index in xrange ( max_lines , - <NUM_LIT:1> , - <NUM_LIT:1> ) : \n line = self . data [ index ] \n if count == max_lines and self . anchored : \n line = line . split ( anchor ) [ <NUM_LIT:0> ] \n if anchor in line : \n instance += - <NUM_LIT:1> \n if instance == occurrence : \n self . current_row = count \n self . anchored = True \n return \n count -= <NUM_LIT:1> \n else : \n raise ValueError ( \"<STR_LIT>\" ) \n raise RuntimeError ( \"<STR_LIT>\" % ( anchor , self . filename ) ) \n def reset_anchor ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . current_row = <NUM_LIT:0> \n self . anchored = False \n def transfer_line ( self , row ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . data [ self . current_row + row ] . rstrip ( ) \n def transfer_var ( self , row , field , fieldend = None ) : \n \"\"\"<STR_LIT>\"\"\" \n j = self . current_row + row \n line = self . data [ j ] \n if self . delimiter == \"<STR_LIT>\" : \n if not fieldend : \n line = line [ ( field - <NUM_LIT:1> ) : ] \n else : \n line = line [ ( field - <NUM_LIT:1> ) : ( fieldend ) ] \n data = self . _parse_line ( ) . parseString ( line ) \n if len ( data ) > <NUM_LIT:1> : \n return line \n else : \n return data [ <NUM_LIT:0> ] \n else : \n data = self . _parse_line ( ) . parseString ( line ) \n return data [ field - <NUM_LIT:1> ] \n def transfer_keyvar ( self , key , field , occurrence = <NUM_LIT:1> , rowoffset = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n if not isinstance ( occurrence , int ) or occurrence == <NUM_LIT:0> : \n msg = \"<STR_LIT>\" \n raise ValueError ( msg ) \n instance = <NUM_LIT:0> \n if occurrence > <NUM_LIT:0> : \n row = <NUM_LIT:0> \n for line in self . data [ self . current_row : ] : \n if line . find ( key ) > - <NUM_LIT:1> : \n instance += <NUM_LIT:1> \n if instance == occurrence : \n break \n row += <NUM_LIT:1> \n elif occurrence < <NUM_LIT:0> : \n row = - <NUM_LIT:1> \n for line in reversed ( self . data [ self . current_row : ] ) : \n if line . find ( key ) > - <NUM_LIT:1> : \n instance += - <NUM_LIT:1> \n if instance == occurrence : \n break \n row -= <NUM_LIT:1> \n j = self . current_row + row + rowoffset \n line = self . data [ j ] \n fields = self . _parse_line ( ) . parseString ( line . replace ( key , \"<STR_LIT>\" ) ) \n return fields [ field ] \n def transfer_array ( self , rowstart , fieldstart , rowend = None , fieldend = None ) : \n \"\"\"<STR_LIT>\"\"\" \n j1 = self . current_row + rowstart \n if rowend is None : \n j2 = j1 + <NUM_LIT:1> \n else : \n j2 = self . current_row + rowend + <NUM_LIT:1> \n if not fieldend : \n raise ValueError ( \"<STR_LIT>\" ) \n lines = self . data [ j1 : j2 ] \n data = zeros ( shape = ( <NUM_LIT:0> , <NUM_LIT:0> ) ) \n for i , line in enumerate ( lines ) : \n if self . delimiter == \"<STR_LIT>\" : \n line = line [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] \n line = line . strip ( ) \n parsed = self . _parse_line ( ) . parseString ( line ) \n newdata = array ( parsed [ : ] ) \n if '<STR_LIT>' in str ( newdata . dtype ) : \n newdata = array ( line ) \n data = append ( data , newdata ) \n else : \n parsed = self . _parse_line ( ) . parseString ( line ) \n if i == j2 - j1 - <NUM_LIT:1> : \n data = append ( data , array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] ) ) \n else : \n data = append ( data , array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : ] ) ) \n fieldstart = <NUM_LIT:1> \n return data \n def transfer_2Darray ( self , rowstart , fieldstart , rowend , fieldend = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if fieldend and ( fieldstart > fieldend ) : \n msg = \"<STR_LIT>\" \n raise ValueError ( msg ) \n if rowstart > rowend : \n msg = \"<STR_LIT>\" \n raise ValueError ( msg ) \n j1 = self . current_row + rowstart \n j2 = self . current_row + rowend + <NUM_LIT:1> \n lines = list ( self . data [ j1 : j2 ] ) \n if self . delimiter == \"<STR_LIT>\" : \n if fieldend : \n line = lines [ <NUM_LIT:0> ] [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] \n else : \n line = lines [ <NUM_LIT:0> ] [ ( fieldstart - <NUM_LIT:1> ) : ] \n parsed = self . _parse_line ( ) . parseString ( line ) \n row = array ( parsed [ : ] ) \n data = zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n data [ <NUM_LIT:0> , : ] = row \n for i , line in enumerate ( list ( lines [ <NUM_LIT:1> : ] ) ) : \n if fieldend : \n line = line [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] \n else : \n line = line [ ( fieldstart - <NUM_LIT:1> ) : ] \n parsed = self . _parse_line ( ) . parseString ( line ) \n data [ i + <NUM_LIT:1> , : ] = array ( parsed [ : ] ) \n else : \n parsed = self . _parse_line ( ) . parseString ( lines [ <NUM_LIT:0> ] ) \n if fieldend : \n row = array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] ) \n else : \n row = array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : ] ) \n data = zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n data [ <NUM_LIT:0> , : ] = row \n for i , line in enumerate ( list ( lines [ <NUM_LIT:1> : ] ) ) : \n parsed = self . _parse_line ( ) . parseString ( line ) \n if fieldend : \n try : \n data [ i + <NUM_LIT:1> , : ] = array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] ) \n except : \n print data \n else : \n data [ i + <NUM_LIT:1> , : ] = array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : ] ) \n return data \n def _parse_line ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . line_parse_token \n def _reset_tokens ( self ) : \n '''<STR_LIT>''' \n if self . delimiter . isspace ( ) : \n textchars = printables \n else : \n textchars = alphanums \n symbols = [ '<STR_LIT:.>' , '<STR_LIT:/>' , '<STR_LIT:+>' , '<STR_LIT:*>' , '<STR_LIT>' , '<STR_LIT:(>' , '<STR_LIT:)>' , '<STR_LIT:[>' , '<STR_LIT:]>' , '<STR_LIT:=>' , \n '<STR_LIT::>' , '<STR_LIT:;>' , '<STR_LIT:?>' , '<STR_LIT:%>' , '<STR_LIT:&>' , '<STR_LIT:!>' , '<STR_LIT:#>' , '<STR_LIT:|>' , '<STR_LIT:<>' , '<STR_LIT:>>' , \n '<STR_LIT:{>' , '<STR_LIT:}>' , '<STR_LIT:->' , '<STR_LIT:_>' , '<STR_LIT:@>' , '<STR_LIT:$>' , '<STR_LIT>' ] \n for symbol in symbols : \n if symbol not in self . delimiter : \n textchars = textchars + symbol \n digits = Word ( nums ) \n dot = \"<STR_LIT:.>\" \n sign = oneOf ( \"<STR_LIT>\" ) \n ee = CaselessLiteral ( '<STR_LIT:E>' ) | CaselessLiteral ( '<STR_LIT:D>' ) \n num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n num_float = ToFloat ( Combine ( Optional ( sign ) + \n ( ( digits + dot + Optional ( digits ) ) | \n ( dot + digits ) ) + \n Optional ( ee + Optional ( sign ) + digits ) \n ) ) \n mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n nan = ToInf ( oneOf ( \"<STR_LIT>\" ) ) | ToNan ( oneOf ( \"<STR_LIT>\" + \"<STR_LIT>\" ) ) \n string_text = Word ( textchars ) \n self . line_parse_token = ( OneOrMore ( ( nan | num_float | mixed_exp | num_int | \n <mask0> ) ) ) \n", "gt": "string_text"}
{"input": "\n from __future__ import print_function \n from openmdao . api import Component , Group , Problem , Newton , ScipyGMRES \n class Line ( Component ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n super ( Line , self ) . __init__ ( ) \n self . add_param ( '<STR_LIT:x>' , <NUM_LIT:1.0> ) \n self . add_output ( '<STR_LIT:y>' , <NUM_LIT:0.0> ) \n self . slope = - <NUM_LIT> \n self . intercept = <NUM_LIT> \n def solve_nonlinear ( self , params , unknowns , resids ) : \n \"\"\"<STR_LIT>\"\"\" \n x = params [ '<STR_LIT:x>' ] \n m = self . slope \n b = self . intercept \n unknowns [ '<STR_LIT:y>' ] = m * x + b \n def linearize ( self , params , unknowns , resids ) : \n \"\"\"<STR_LIT>\"\"\" \n m = self . slope \n J = { } \n J [ '<STR_LIT:y>' , '<STR_LIT:x>' ] = m \n return J \n class Parabola ( Component ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n super ( Parabola , self ) . __init__ ( ) \n self . add_param ( '<STR_LIT:x>' , <NUM_LIT:1.0> ) \n self . add_output ( '<STR_LIT:y>' , <NUM_LIT:0.0> ) \n self . a = <NUM_LIT> \n self . b = <NUM_LIT:0.0> \n self . c = - <NUM_LIT> \n def solve_nonlinear ( self , params , unknowns , resids ) : \n \"\"\"<STR_LIT>\"\"\" \n x = params [ '<STR_LIT:x>' ] \n a = self . a \n b = self . b \n c = self . c \n unknowns [ '<STR_LIT:y>' ] = a * x ** <NUM_LIT:2> + b * x + c \n def linearize ( self , params , unknowns , resids ) : \n \"\"\"<STR_LIT>\"\"\" \n x = params [ '<STR_LIT:x>' ] \n a = self . a \n b = self . b \n J = { } \n J [ '<STR_LIT:y>' , '<STR_LIT:x>' ] = <NUM_LIT> * a * x + b \n return J \n class Balance ( Component ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n super ( Balance , self ) . __init__ ( ) \n self . add_param ( '<STR_LIT>' , <NUM_LIT:0.0> ) \n self . add_param ( '<STR_LIT>' , <NUM_LIT:0.0> ) \n self . add_state ( '<STR_LIT:x>' , <NUM_LIT> ) \n def solve_nonlinear ( self , params , unknowns , resids ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n def apply_nonlinear ( self , params , unknowns , resids ) : \n \"\"\"<STR_LIT>\"\"\" \n y1 = params [ '<STR_LIT>' ] \n y2 = params [ '<STR_LIT>' ] \n resids [ '<STR_LIT:x>' ] = y1 - y2 \n def linearize ( self , params , unknowns , resids ) : \n \"\"\"<STR_LIT>\"\"\" \n J = { } \n J [ '<STR_LIT:x>' , '<STR_LIT>' ] = <NUM_LIT:1.0> \n J [ '<STR_LIT:x>' , '<STR_LIT>' ] = - <NUM_LIT:1.0> \n return J \n if __name__ == '<STR_LIT:__main__>' : \n top = Problem ( ) \n root = top . root = Group ( ) \n root . add ( '<STR_LIT>' , Line ( ) ) \n root . add ( '<STR_LIT>' , Parabola ( ) ) \n root . add ( '<STR_LIT>' , Balance ( ) ) \n root . connect ( '<STR_LIT>' , '<STR_LIT>' ) \n root . connect ( '<STR_LIT>' , '<STR_LIT>' ) \n root . connect ( '<STR_LIT>' , '<STR_LIT>' ) \n root . connect ( '<STR_LIT>' , '<STR_LIT>' ) \n root . nl_solver = Newton ( ) \n root . ln_solver = ScipyGMRES ( ) \n top . setup ( ) \n top [ '<STR_LIT>' ] = <NUM_LIT> \n root . list_states ( ) \n top . run ( ) \n print ( '<STR_LIT>' % ( top [ '<STR_LIT>' ] , top [ '<STR_LIT>' ] , top [ '<STR_LIT>' ] ) ) \n top [ '<STR_LIT>' ] = - <NUM_LIT> \n root . list_states ( ) \n top . run ( ) \n print ( '<STR_LIT>' % ( top [ '<STR_LIT>' ] , top [ '<STR_LIT>' ] , <mask0> [ '<STR_LIT>' ] ) ) \n", "gt": "top"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import warnings \n from openmdao . components . exec_comp import ExecComp \n class ConstraintComp ( ExecComp ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , expr , out = '<STR_LIT>' ) : \n warnings . simplefilter ( '<STR_LIT>' , DeprecationWarning ) \n warnings . warn ( \"<STR_LIT>\" , \n DeprecationWarning , stacklevel = <NUM_LIT:2> ) \n warnings . simplefilter ( '<STR_LIT:ignore>' , DeprecationWarning ) \n newexpr = _combined_expr ( expr ) \n super ( ConstraintComp , self ) . __init__ ( \"<STR_LIT>\" % ( out , newexpr ) ) \n def _combined_expr ( expr ) : \n \"\"\"<STR_LIT>\"\"\" \n lhs , op , rhs = _parse_constraint ( expr ) \n first , second = ( rhs , lhs ) if op . startswith ( '<STR_LIT:>>' ) else ( lhs , rhs ) \n try : \n if float ( first ) == <NUM_LIT:0> : \n return \"<STR_LIT>\" % second \n except Exception : \n pass \n try : \n if float ( second ) == <NUM_LIT:0.> : \n return first \n except Exception : \n pass \n return '<STR_LIT>' % ( first , second ) \n def _parse_constraint ( expr_string ) : \n \"\"\"<STR_LIT>\"\"\" \n for comparator in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT:>>' , '<STR_LIT:<>' , '<STR_LIT:=>' ] : \n parts = expr_string . split ( comparator ) \n if len ( parts ) == <NUM_LIT:2> : \n if comparator == '<STR_LIT>' : \n break \n return ( parts [ <NUM_LIT:0> ] . strip ( ) , comparator , parts [ <NUM_LIT:1> ] . strip ( ) ) \n msg = \"<STR_LIT>\" \n raise ValueError ( <mask0> ) \n", "gt": "msg"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import print_function \n import os \n import sys \n import json \n import warnings \n import traceback \n from collections import OrderedDict \n from itertools import chain \n from six import iteritems , itervalues \n from six . moves import cStringIO \n import networkx as nx \n import numpy as np \n from openmdao . core . system import System \n from openmdao . core . group import Group \n from openmdao . core . component import Component \n from openmdao . core . parallel_group import ParallelGroup \n from openmdao . core . parallel_fd_group import ParallelFDGroup \n from openmdao . core . basic_impl import BasicImpl \n from openmdao . core . _checks import check_connections , _both_names \n from openmdao . core . driver import Driver \n from openmdao . core . mpi_wrap import MPI , under_mpirun , debug \n from openmdao . core . relevance import Relevance \n from openmdao . components . indep_var_comp import IndepVarComp \n from openmdao . solvers . scipy_gmres import ScipyGMRES \n from openmdao . solvers . ln_direct import DirectSolver \n from openmdao . solvers . ln_gauss_seidel import LinearGaussSeidel \n from openmdao . units . units import get_conversion_tuple \n from openmdao . util . string_util import get_common_ancestor , nearest_child , name_relative_to \n from openmdao . util . graph import plain_bfs \n from openmdao . util . options import OptionsDictionary \n force_check = os . environ . get ( '<STR_LIT>' ) \n trace = os . environ . get ( '<STR_LIT>' ) \n class _ProbData ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n self . top_lin_gs = False \n self . in_complex_step = False \n class Problem ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , root = None , driver = None , impl = None , comm = None ) : \n super ( Problem , self ) . __init__ ( ) \n self . root = root \n self . _probdata = _ProbData ( ) \n if MPI : \n from openmdao . core . petsc_impl import PetscImpl \n if impl != PetscImpl : \n raise ValueError ( \"<STR_LIT>\" ) \n if impl is None : \n self . _impl = BasicImpl \n else : \n self . _impl = impl \n self . comm = comm \n if driver is None : \n self . driver = Driver ( ) \n else : \n self . driver = driver \n self . pathname = '<STR_LIT>' \n def __getitem__ ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n if name in self . root . unknowns : \n return self . root . unknowns [ name ] \n elif name in self . root . params : \n return self . root . params [ name ] \n elif name in self . root . _sysdata . to_abs_pnames : \n for p in self . root . _sysdata . to_abs_pnames [ name ] : \n return self . _rec_get_param ( p ) \n elif name in self . _dangling : \n for p in self . _dangling [ name ] : \n return self . _rec_get_param ( p ) \n else : \n raise KeyError ( \"<STR_LIT>\" % name ) \n def _rec_get_param ( self , absname ) : \n parts = absname . rsplit ( '<STR_LIT:.>' , <NUM_LIT:1> ) \n if len ( parts ) == <NUM_LIT:1> : \n return self . root . params [ absname ] \n else : \n grp = self . root . _subsystem ( parts [ <NUM_LIT:0> ] ) \n return grp . params [ parts [ <NUM_LIT:1> ] ] \n def __setitem__ ( self , name , val ) : \n \"\"\"<STR_LIT>\"\"\" \n if name in self . root . unknowns : \n self . root . unknowns [ name ] = val \n elif name in self . _dangling : \n for p in self . _dangling [ name ] : \n parts = p . rsplit ( '<STR_LIT:.>' , <NUM_LIT:1> ) \n if len ( parts ) == <NUM_LIT:1> : \n self . root . params [ p ] = val \n else : \n grp = self . root . _subsystem ( parts [ <NUM_LIT:0> ] ) \n grp . params [ parts [ <NUM_LIT:1> ] ] = val \n else : \n raise KeyError ( \"<STR_LIT>\" % name ) \n def _setup_connections ( self , params_dict , unknowns_dict ) : \n \"\"\"<STR_LIT>\"\"\" \n to_prom_name = self . _probdata . to_prom_name \n connections = self . root . _get_explicit_connections ( ) \n prom_noconns = self . _add_implicit_connections ( connections ) \n input_graph = nx . DiGraph ( ) \n self . _dangling = { } \n to_abs_pnames = self . root . _sysdata . to_abs_pnames \n usrcs = set ( ) \n for tgt , srcs in iteritems ( connections ) : \n for src , idxs in srcs : \n input_graph . add_edge ( src , tgt , idxs = idxs ) \n if src in unknowns_dict : \n usrcs . add ( src ) \n for prom , plist in iteritems ( to_abs_pnames ) : \n input_graph . add_nodes_from ( plist ) \n if prom in prom_noconns : \n start = plist [ <NUM_LIT:0> ] \n input_graph . add_edges_from ( ( ( start , p ) for p in plist [ <NUM_LIT:1> : ] ) , \n idxs = None ) \n newconns = { } \n for src in usrcs : \n newconns [ src ] = None \n src_idxs = { src : None } \n for s , t in nx . dfs_edges ( input_graph , src ) : \n tidxs = input_graph [ s ] [ t ] [ '<STR_LIT>' ] \n sidxs = src_idxs [ s ] \n if tidxs is None : \n tidxs = sidxs \n elif sidxs is not None : \n tidxs = np . array ( sidxs ) [ tidxs ] \n src_idxs [ t ] = tidxs \n if t in newconns : \n newconns [ t ] . append ( ( src , tidxs ) ) \n else : \n newconns [ t ] = [ ( src , tidxs ) ] \n self . _input_inputs = { } \n for node in input_graph . nodes_iter ( ) : \n if node not in newconns and len ( input_graph . pred [ node ] ) == <NUM_LIT:0> : \n nosrc = [ node ] \n for s , t in nx . dfs_edges ( input_graph , node ) : \n if t in newconns : \n src = newconns [ t ] [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] \n for n in nosrc : \n newconns [ n ] = [ ( src , None ) ] \n break \n else : \n nosrc . append ( t ) \n else : \n set_nosrc = set ( nosrc ) \n for n in nosrc : \n self . _dangling [ to_prom_name [ n ] ] = set_nosrc \n self . _input_inputs [ n ] = nosrc \n connections = OrderedDict ( ) \n for tgt , srcs in sorted ( newconns . items ( ) ) : \n if srcs is not None : \n if len ( srcs ) > <NUM_LIT:1> : \n src_names = ( n for n , idx in srcs ) \n self . _setup_errors . append ( \"<STR_LIT>\" \n \"<STR_LIT>\" % \n ( tgt , sorted ( src_names ) ) ) \n connections [ tgt ] = srcs [ <NUM_LIT:0> ] \n return connections \n def _check_input_diffs ( self , connections , params_dict , unknowns_dict ) : \n \"\"\"<STR_LIT>\"\"\" \n for tgt , connected_inputs in iteritems ( self . _input_inputs ) : \n tgt_idx = connected_inputs . index ( tgt ) \n units = [ params_dict [ n ] . get ( '<STR_LIT>' ) for n in connected_inputs ] \n vals = [ params_dict [ n ] [ '<STR_LIT>' ] for n in connected_inputs ] \n diff_units = [ ] \n for i , u in enumerate ( units ) : \n if i != tgt_idx and u != units [ tgt_idx ] : \n if units [ tgt_idx ] is None : \n sname , s = connected_inputs [ i ] , u \n tname , t = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n else : \n sname , s = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n tname , t = connected_inputs [ i ] , u \n diff_units . append ( ( connected_inputs [ i ] , u ) ) \n if isinstance ( vals [ tgt_idx ] , np . ndarray ) : \n diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n enumerate ( vals ) if not \n ( isinstance ( v , np . ndarray ) and \n v . shape == vals [ tgt_idx ] . shape and \n ( v == vals [ tgt_idx ] ) . all ( ) ) ] \n else : \n vtype = type ( vals [ tgt_idx ] ) \n diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n enumerate ( vals ) if vtype != type ( v ) or \n v != vals [ tgt_idx ] ] \n if diff_units : \n filt = set ( [ u for n , u in diff_units ] ) \n if None in filt : \n filt . remove ( None ) \n if filt : \n proms = set ( [ params_dict [ item ] [ '<STR_LIT>' ] for item in connected_inputs ] ) \n if len ( proms ) == <NUM_LIT:1> : \n msg = \"<STR_LIT>\" + \"<STR_LIT>\" % proms . pop ( ) \n else : \n msg = \"<STR_LIT>\" + \"<STR_LIT>\" \n msg += \"<STR_LIT>\" % sorted ( [ ( tgt , params_dict [ tgt ] . get ( '<STR_LIT>' ) ) ] + diff_units ) \n correct_src = params_dict [ connected_inputs [ <NUM_LIT:0> ] ] [ '<STR_LIT>' ] \n msg += \"<STR_LIT>\" % correct_src + \"<STR_LIT>\" \n self . _setup_errors . append ( msg ) \n if diff_vals : \n msg = ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" % \n ( sorted ( [ ( tgt , params_dict [ tgt ] [ '<STR_LIT>' ] ) ] + \n diff_vals ) ) ) \n self . _setup_errors . append ( msg ) \n for promname , absnames in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n if len ( absnames ) > <NUM_LIT:1> : \n step_sizes , step_types , forms = { } , { } , { } \n for name in absnames : \n meta = self . root . _params_dict [ name ] \n ss = meta . get ( '<STR_LIT>' ) \n if ss is not None : \n step_sizes [ ss ] = name \n st = meta . get ( '<STR_LIT>' ) \n if st is not None : \n step_types [ st ] = name \n f = meta . get ( '<STR_LIT>' ) \n if f is not None : \n forms [ f ] = name \n if len ( step_sizes ) > <NUM_LIT:1> : \n self . _setup_errors . append ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" % ( promname , \n sorted ( [ ( v , k ) for k , v in step_sizes . items ( ) ] ) ) ) \n if len ( step_types ) > <NUM_LIT:1> : \n self . _setup_errors . append ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" % ( promname , \n sorted ( [ ( v , k ) for k , v in step_types . items ( ) ] ) ) ) \n if len ( forms ) > <NUM_LIT:1> : \n self . _setup_errors . append ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" % ( promname , \n sorted ( [ ( v , k ) for k , v in forms . items ( ) ] ) ) ) \n def _get_ubc_vars ( self , connections ) : \n \"\"\"<STR_LIT>\"\"\" \n full_order = { s . pathname : i for i , s in \n enumerate ( self . root . subsystems ( recurse = True ) ) } \n ubcs = [ ] \n for tgt , srcs in iteritems ( connections ) : \n tsys = tgt . rsplit ( '<STR_LIT:.>' , <NUM_LIT:1> ) [ <NUM_LIT:0> ] \n ssys = srcs [ <NUM_LIT:0> ] . rsplit ( '<STR_LIT:.>' , <NUM_LIT:1> ) [ <NUM_LIT:0> ] \n if full_order [ ssys ] > full_order [ tsys ] : \n ubcs . append ( tgt ) \n return ubcs \n def setup ( self , check = True , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _setup_errors = [ ] \n tree_changed = False \n meta_changed = False \n self . _probdata = _ProbData ( ) \n if isinstance ( self . root . ln_solver , LinearGaussSeidel ) : \n self . _probdata . top_lin_gs = True \n self . driver . root = self . root \n self . root . _init_sys_data ( self . pathname , self . _probdata ) \n self . _setup_communicators ( ) \n params_dict , unknowns_dict = self . root . _setup_variables ( ) \n self . _probdata . params_dict = params_dict \n self . _probdata . unknowns_dict = unknowns_dict \n self . _probdata . to_prom_name = self . root . _sysdata . to_prom_name \n connections = self . _setup_connections ( params_dict , unknowns_dict ) \n self . _probdata . connections = connections \n for tgt , ( src , idxs ) in iteritems ( connections ) : \n tmeta = params_dict [ tgt ] \n if '<STR_LIT>' not in tmeta or not tmeta [ '<STR_LIT>' ] : \n if tmeta [ '<STR_LIT>' ] == ( ) : \n smeta = unknowns_dict [ src ] \n if idxs is not None : \n size = len ( idxs ) \n tmeta [ '<STR_LIT>' ] = ( size , ) \n tmeta [ '<STR_LIT:size>' ] = size \n tmeta [ '<STR_LIT>' ] = smeta [ '<STR_LIT>' ] [ np . array ( idxs ) ] \n else : \n tmeta [ '<STR_LIT>' ] = smeta [ '<STR_LIT>' ] \n tmeta [ '<STR_LIT:size>' ] = smeta [ '<STR_LIT:size>' ] \n tmeta [ '<STR_LIT>' ] = smeta [ '<STR_LIT>' ] \n if idxs is not None : \n if isinstance ( idxs , np . ndarray ) : \n tmeta [ '<STR_LIT>' ] = idxs \n else : \n tmeta [ '<STR_LIT>' ] = np . array ( idxs , \n dtype = self . _impl . idx_arr_type ) \n if MPI : \n for s in self . root . components ( recurse = True ) : \n if hasattr ( s , '<STR_LIT>' ) or ( \n hasattr ( s , '<STR_LIT>' ) and ( s . setup_distrib \n is not Component . setup_distrib ) ) : \n meta_changed = True \n if tree_changed : \n return self . setup ( check = check , out_stream = out_stream ) \n elif meta_changed : \n params_dict , unknowns_dict = self . root . _setup_variables ( compute_indices = True ) \n self . _setup_errors . extend ( check_connections ( connections , params_dict , \n unknowns_dict , \n self . root . _sysdata . to_prom_name ) ) \n self . _setup_units ( connections , params_dict , unknowns_dict ) \n to_prom_name = self . root . _sysdata . to_prom_name \n self . _probdata . to_prom_name = to_prom_name \n for path , meta in iteritems ( params_dict ) : \n meta [ '<STR_LIT>' ] = to_prom_name [ path ] \n if path not in connections : \n if '<STR_LIT>' not in meta or not meta [ '<STR_LIT>' ] : \n if meta [ '<STR_LIT>' ] == ( ) : \n self . _setup_errors . append ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( path ) ) \n for path , meta in iteritems ( unknowns_dict ) : \n meta [ '<STR_LIT>' ] = to_prom_name [ path ] \n param_owners = _assign_parameters ( connections ) \n pois = self . driver . desvars_of_interest ( ) \n oois = self . driver . outputs_of_interest ( ) \n self . _driver_vois = set ( ) \n for tup in chain ( pois , oois ) : \n self . _driver_vois . update ( tup ) \n promoted_unknowns = self . root . _sysdata . to_abs_uname \n parallel_p = False \n for vnames in pois : \n if len ( vnames ) > <NUM_LIT:1> : \n parallel_p = True \n for v in vnames : \n if v not in promoted_unknowns : \n raise NameError ( \"<STR_LIT>\" % v ) \n parallel_u = False \n for vnames in oois : \n if len ( vnames ) > <NUM_LIT:1> : \n parallel_u = True \n for v in vnames : \n if v not in promoted_unknowns : \n raise NameError ( \"<STR_LIT>\" % v ) \n mode = self . _check_for_parallel_derivs ( pois , oois , parallel_u , parallel_p ) \n self . _probdata . relevance = Relevance ( self . root , params_dict , \n unknowns_dict , connections , \n pois , oois , mode ) \n for s in self . root . subgroups ( recurse = True , include_self = True ) : \n if not s . _order_set : \n order = None \n broken_edges = None \n if self . comm . rank == <NUM_LIT:0> : \n order , broken_edges = s . list_auto_order ( ) \n if MPI : \n if trace : \n debug ( \"<STR_LIT>\" ) \n order , broken_edges = self . comm . bcast ( ( order , broken_edges ) , root = <NUM_LIT:0> ) \n if trace : \n debug ( \"<STR_LIT>\" ) \n s . set_order ( order ) \n for edge in broken_edges : \n cname = edge [ <NUM_LIT:1> ] \n head_sys = self . root \n for name in cname . split ( '<STR_LIT:.>' ) : \n head_sys = getattr ( head_sys , name ) \n head_sys . _run_apply = True \n self . _check_input_diffs ( connections , params_dict , unknowns_dict ) \n alloc_derivs = not self . root . fd_options [ '<STR_LIT>' ] \n for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n alloc_derivs = alloc_derivs or sub . nl_solver . supports [ '<STR_LIT>' ] \n self . root . _setup_vectors ( param_owners , impl = self . _impl , alloc_derivs = alloc_derivs ) \n self . driver . _setup ( ) \n self . _poi_indices , self . _qoi_indices = self . driver . _map_voi_indices ( ) \n for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n sub . nl_solver . setup ( sub ) \n sub . ln_solver . setup ( sub ) \n self . _check_solvers ( ) \n self . _start_recorders ( ) \n if self . _setup_errors : \n stream = cStringIO ( ) \n stream . write ( \"<STR_LIT>\" ) \n for err in self . _setup_errors : \n stream . write ( \"<STR_LIT>\" % err ) \n raise RuntimeError ( stream . getvalue ( ) ) \n OptionsDictionary . locked = True \n if check or force_check : \n return self . check_setup ( out_stream ) \n return { } \n def cleanup ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . driver . cleanup ( ) \n self . root . cleanup ( ) \n def _check_solvers ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n iterated_states = set ( ) \n group_states = [ ] \n has_iter_solver = { } \n for group in self . root . subgroups ( recurse = True , include_self = True ) : \n try : \n has_iter_solver [ group . pathname ] = ( group . ln_solver . options [ '<STR_LIT>' ] > <NUM_LIT:1> ) \n except KeyError : \n if isinstance ( group . ln_solver , DirectSolver ) : \n has_iter_solver [ group . pathname ] = ( True ) \n opt = group . fd_options \n if opt [ '<STR_LIT>' ] == True and opt [ '<STR_LIT>' ] == '<STR_LIT>' : \n if group . name != '<STR_LIT>' : \n msg = \"<STR_LIT>\" \n msg += \"<STR_LIT>\" \n self . _setup_errors . append ( msg ) \n for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n if hasattr ( sub . nl_solver , '<STR_LIT>' ) : \n msg = \"<STR_LIT>\" \n msg += \"<STR_LIT>\" \n self . _setup_errors . append ( msg . format ( sub . name ) ) \n parts = group . pathname . split ( '<STR_LIT:.>' ) \n for i in range ( len ( parts ) ) : \n if has_iter_solver [ '<STR_LIT:.>' . join ( parts [ : i ] ) ] : \n is_iterated_somewhere = True \n break \n else : \n is_iterated_somewhere = False \n if is_iterated_somewhere : \n continue \n if isinstance ( group . ln_solver , LinearGaussSeidel ) and group . ln_solver . options [ '<STR_LIT>' ] == <NUM_LIT:1> : \n graph = group . _get_sys_graph ( ) \n strong = [ sorted ( s ) for s in nx . strongly_connected_components ( graph ) \n if len ( s ) > <NUM_LIT:1> ] \n if strong : \n self . _setup_errors . append ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n % ( group . pathname , strong ) ) \n states = [ n for n , m in iteritems ( group . _unknowns_dict ) if m . get ( '<STR_LIT:state>' ) ] \n if states : \n group_states . append ( ( group , states ) ) \n if isinstance ( group . ln_solver , DirectSolver ) or group . ln_solver . options [ '<STR_LIT>' ] > <NUM_LIT:1> : \n iterated_states . update ( states ) \n else : \n for s in states : \n if s not in iterated_states : \n cname = s . rsplit ( '<STR_LIT:.>' , <NUM_LIT:1> ) [ <NUM_LIT:0> ] \n comp = self . root \n for name in cname . split ( '<STR_LIT:.>' ) : \n comp = getattr ( comp , name ) \n if not _needs_iteration ( comp ) : \n iterated_states . add ( s ) \n for group , states in group_states : \n uniterated_states = [ s for s in states if s not in iterated_states ] \n if uniterated_states : \n self . _setup_errors . append ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" % \n ( group . pathname , uniterated_states ) ) \n def _check_dangling_params ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n to_prom_name = self . root . _sysdata . to_prom_name \n dangling_params = sorted ( set ( [ \n to_prom_name [ p ] for p , m in iteritems ( self . root . _params_dict ) \n if p not in self . root . connections \n ] ) ) \n if dangling_params : \n print ( \"<STR_LIT>\" , \n file = out_stream ) \n for d in dangling_params : \n print ( d , file = out_stream ) \n return dangling_params \n def _check_mode ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _calculated_mode != self . root . _probdata . relevance . mode : \n print ( \"<STR_LIT>\" \n \"<STR_LIT>\" % ( self . root . _probdata . relevance . mode , \n self . _calculated_mode , \n self . _p_length , \n self . _u_length ) , \n file = out_stream ) \n return ( self . root . _probdata . relevance . mode , self . _calculated_mode ) \n def _check_no_unknown_comps ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n nocomps = sorted ( [ c . pathname for c in self . root . components ( recurse = True , \n local = True ) \n if len ( c . unknowns ) == <NUM_LIT:0> ] ) \n if nocomps : \n print ( \"<STR_LIT>\" , file = out_stream ) \n for n in nocomps : \n print ( n , file = out_stream ) \n return nocomps \n def _check_no_recorders ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n recorders = [ ] \n recorders . extend ( self . driver . recorders ) \n for grp in self . root . subgroups ( recurse = True , local = True , \n include_self = True ) : \n recorders . extend ( grp . nl_solver . recorders ) \n recorders . extend ( grp . ln_solver . recorders ) \n if not recorders : \n print ( \"<STR_LIT>\" , \n file = out_stream ) \n return recorders \n def _check_no_connect_comps ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n conn_comps = set ( [ t . rsplit ( '<STR_LIT:.>' , <NUM_LIT:1> ) [ <NUM_LIT:0> ] \n for t in self . root . connections ] ) \n conn_comps . update ( [ s . rsplit ( '<STR_LIT:.>' , <NUM_LIT:1> ) [ <NUM_LIT:0> ] \n for s , i in itervalues ( self . root . connections ) ] ) \n noconn_comps = sorted ( [ c . pathname \n for c in self . root . components ( recurse = True , local = True ) \n if c . pathname not in conn_comps ] ) \n if noconn_comps : \n print ( \"<STR_LIT>\" , file = out_stream ) \n for comp in noconn_comps : \n print ( comp , file = out_stream ) \n return noconn_comps \n def _check_mpi ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n if under_mpirun ( ) : \n parr = True \n if self . comm . rank == <NUM_LIT:0> : \n for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n if ( isinstance ( grp , ParallelGroup ) or \n isinstance ( grp , ParallelFDGroup ) ) : \n break \n else : \n parr = False \n print ( \"<STR_LIT>\" , \n file = out_stream ) \n mincpu , maxcpu = self . root . get_req_procs ( ) \n if maxcpu is not None and self . comm . size > maxcpu : \n print ( \"<STR_LIT>\" % \n ( self . comm . size , maxcpu ) ) \n return ( self . comm . size , maxcpu , parr ) \n else : \n pargrps = [ ] \n for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n if isinstance ( grp , ParallelGroup ) : \n print ( \"<STR_LIT>\" % \n grp . pathname , file = out_stream ) \n pargrps . append ( grp . pathname ) \n return sorted ( pargrps ) \n def _check_graph ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n cycles = [ ] \n ooo = [ ] \n for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n graph = grp . _get_sys_graph ( ) \n strong = [ s for s in nx . strongly_connected_components ( graph ) \n if len ( s ) > <NUM_LIT:1> ] \n if strong : \n relstrong = [ ] \n for slist in strong : \n relstrong . append ( [ ] ) \n for s in slist : \n relstrong [ - <NUM_LIT:1> ] . append ( nearest_child ( grp . pathname , s ) ) \n subs = [ s for s in grp . _subsystems ] \n tups = sorted ( [ ( subs . index ( s ) , s ) for s in relstrong [ - <NUM_LIT:1> ] ] ) \n relstrong [ - <NUM_LIT:1> ] = [ t [ <NUM_LIT:1> ] for t in tups ] \n print ( \"<STR_LIT>\" % \n ( grp . pathname , relstrong ) , file = out_stream ) \n cycles . append ( relstrong ) \n graph , _ = grp . _break_cycles ( grp . list_order ( ) , graph ) \n visited = set ( ) \n out_of_order = { } \n for sub in itervalues ( grp . _subsystems ) : \n visited . add ( sub . pathname ) \n for u , v in nx . dfs_edges ( graph , sub . pathname ) : \n if v in visited : \n out_of_order . setdefault ( nearest_child ( grp . pathname , v ) , \n set ( ) ) . add ( sub . pathname ) \n if out_of_order : \n for name in out_of_order : \n out_of_order [ name ] = sorted ( [ \n nearest_child ( grp . pathname , n ) for n in out_of_order [ name ] \n ] ) \n print ( \"<STR_LIT>\" % \n grp . pathname , file = out_stream ) \n for n , subs in iteritems ( out_of_order ) : \n print ( \"<STR_LIT>\" % ( n , subs ) , file = out_stream ) \n ooo . append ( ( grp . pathname , list ( iteritems ( out_of_order ) ) ) ) \n print ( \"<STR_LIT>\" % grp . list_auto_order ( ) [ <NUM_LIT:0> ] , \n file = out_stream ) \n return ( cycles , sorted ( ooo ) ) \n def _check_gmres_under_mpi ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n if under_mpirun ( ) : \n has_parallel = False \n for s in self . root . subgroups ( recurse = True , include_self = True ) : \n if isinstance ( s , ParallelGroup ) : \n has_parallel = True \n break \n if has_parallel and isinstance ( self . root . ln_solver , ScipyGMRES ) : \n print ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" , file = out_stream ) \n def _check_ubcs ( self , out_stream = sys . stdout ) : \n ubcs = self . _get_ubc_vars ( self . root . connections ) \n if ubcs : \n print ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" % ubcs , file = out_stream ) \n return ubcs \n def _check_unmarked_pbos ( self , out_stream = sys . stdout ) : \n pbos = [ ] \n for comp in self . root . components ( recurse = True , include_self = True ) : \n if comp . _pbo_warns : \n pbos . append ( ( comp . pathname , comp . _pbo_warns ) ) \n if pbos : \n print ( \"<STR_LIT>\" \n \"<STR_LIT>\" , file = out_stream ) \n for cname , pbo_warns in sorted ( pbos , key = lambda x : x [ <NUM_LIT:0> ] ) : \n for vname , val in pbo_warns : \n print ( \"<STR_LIT>\" % ( '<STR_LIT:.>' . join ( ( cname , vname ) ) , \n type ( val ) . __name__ ) , file = out_stream ) \n return pbos \n def _check_relevant_pbos ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . driver . __class__ is Driver or self . driver . supports [ '<STR_LIT>' ] is False or self . root . fd_options [ '<STR_LIT>' ] : \n return [ ] \n vec = self . root . unknowns \n pbos = [ var for var in vec if vec . metadata ( var ) . get ( '<STR_LIT>' ) ] \n rels = set ( ) \n for key , rel in iteritems ( self . _probdata . relevance . relevant ) : \n rels . update ( rel ) \n rel_pbos = rels . intersection ( pbos ) \n if rel_pbos : \n rel_conns = [ ] \n for src in rel_pbos : \n for tgt , src_tuple in iteritems ( self . root . connections ) : \n if src_tuple [ <NUM_LIT:0> ] == src and tgt in rels : \n rel_conns . append ( ( src , tgt ) ) \n if rel_conns : \n print ( \"<STR_LIT>\" , \n file = out_stream ) \n for src , tgt in rel_conns : \n val = vec [ src ] \n print ( \"<STR_LIT>\" % ( src , tgt , type ( val ) . __name__ ) , \n file = out_stream ) \n else : \n print ( \"<STR_LIT>\" \n \"<STR_LIT>\" , sorted ( rel_pbos ) ) \n print ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" , \n file = out_stream ) \n return list ( rel_pbos ) \n def check_setup ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n print ( \"<STR_LIT>\" , file = out_stream ) \n print ( \"<STR_LIT>\" , file = out_stream ) \n results = { } \n results [ '<STR_LIT>' ] = self . _check_no_recorders ( out_stream ) \n results [ '<STR_LIT>' ] = self . _check_mpi ( out_stream ) \n results [ '<STR_LIT>' ] = self . _check_dangling_params ( out_stream ) \n results [ '<STR_LIT>' ] = self . _check_mode ( out_stream ) \n results [ '<STR_LIT>' ] = self . _check_no_unknown_comps ( out_stream ) \n results [ '<STR_LIT>' ] = self . _check_no_connect_comps ( out_stream ) \n results [ '<STR_LIT>' ] , results [ '<STR_LIT>' ] = self . _check_graph ( out_stream ) \n results [ '<STR_LIT>' ] = self . _check_ubcs ( out_stream ) \n results [ '<STR_LIT>' ] = self . _check_gmres_under_mpi ( out_stream ) \n results [ '<STR_LIT>' ] = self . _check_unmarked_pbos ( out_stream ) \n results [ '<STR_LIT>' ] = self . _check_relevant_pbos ( out_stream ) \n for s in self . root . subsystems ( recurse = True , local = True , include_self = True ) : \n stream = cStringIO ( ) \n s . check_setup ( out_stream = stream ) \n content = stream . getvalue ( ) \n if content : \n print ( \"<STR_LIT>\" % ( s . pathname , content ) , file = out_stream ) \n results [ \"<STR_LIT>\" % s . pathname ] = content \n print ( \"<STR_LIT>\" , file = out_stream ) \n print ( \"<STR_LIT>\" , file = out_stream ) \n return results \n def pre_run_check ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if not self . root . fd_options . locked : \n msg = \"<STR_LIT>\" \n raise RuntimeError ( msg ) \n def run ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . pre_run_check ( ) \n if self . root . is_active ( ) : \n self . driver . run ( self ) \n if MPI : \n if trace : debug ( \"<STR_LIT>\" ) \n self . root . comm . barrier ( ) \n if trace : debug ( \"<STR_LIT>\" ) \n def run_once ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . pre_run_check ( ) \n root = self . root \n driver = self . driver \n if root . is_active ( ) : \n driver . run_once ( self ) \n with root . _dircontext : \n root . apply_nonlinear ( root . params , root . unknowns , root . resids , \n metadata = driver . metadata ) \n if MPI : \n if trace : debug ( \"<STR_LIT>\" ) \n root . comm . barrier ( ) \n if trace : debug ( \"<STR_LIT>\" ) \n def _mode ( self , mode , indep_list , unknown_list ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _p_length = <NUM_LIT:0> \n self . _u_length = <NUM_LIT:0> \n uset = set ( ) \n for unames in unknown_list : \n if isinstance ( unames , tuple ) : \n uset . update ( unames ) \n else : \n uset . add ( unames ) \n pset = set ( ) \n for pnames in indep_list : \n if isinstance ( pnames , tuple ) : \n pset . update ( pnames ) \n else : \n pset . add ( pnames ) \n to_prom_name = self . root . _sysdata . to_prom_name \n for path , meta in chain ( iteritems ( self . root . _unknowns_dict ) , \n iteritems ( self . root . _params_dict ) ) : \n prom_name = to_prom_name [ path ] \n if prom_name in uset : \n self . _u_length += meta [ '<STR_LIT:size>' ] \n uset . remove ( prom_name ) \n if prom_name in pset : \n self . _p_length += meta [ '<STR_LIT:size>' ] \n pset . remove ( prom_name ) \n if uset : \n raise RuntimeError ( \"<STR_LIT>\" % list ( uset ) ) \n if pset : \n raise RuntimeError ( \"<STR_LIT>\" % list ( pset ) ) \n if self . _p_length > self . _u_length : \n self . _calculated_mode = '<STR_LIT>' \n else : \n self . _calculated_mode = '<STR_LIT>' \n if mode == '<STR_LIT>' : \n mode = self . root . ln_solver . options [ '<STR_LIT>' ] \n if mode == '<STR_LIT>' : \n mode = self . _calculated_mode \n return mode \n def calc_gradient ( self , indep_list , unknown_list , mode = '<STR_LIT>' , \n return_format = '<STR_LIT>' , dv_scale = None , cn_scale = None , \n sparsity = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if mode not in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : \n msg = \"<STR_LIT>\" \n raise ValueError ( msg ) \n if return_format not in [ '<STR_LIT>' , '<STR_LIT>' ] : \n msg = \"<STR_LIT>\" \n raise ValueError ( msg ) \n with self . root . _dircontext : \n if mode == '<STR_LIT>' or self . root . fd_options [ '<STR_LIT>' ] : \n return self . _calc_gradient_fd ( indep_list , unknown_list , \n return_format , dv_scale = dv_scale , \n cn_scale = cn_scale , sparsity = sparsity ) \n else : \n return self . _calc_gradient_ln_solver ( indep_list , unknown_list , \n return_format , mode , \n dv_scale = dv_scale , \n cn_scale = cn_scale , \n sparsity = sparsity ) \n def _calc_gradient_fd ( self , indep_list , unknown_list , return_format , \n dv_scale = None , cn_scale = None , sparsity = None ) : \n \"\"\"<STR_LIT>\"\"\" \n root = self . root \n unknowns = root . unknowns \n params = root . params \n to_prom_name = root . _sysdata . to_prom_name \n to_abs_pnames = root . _sysdata . to_abs_pnames \n to_abs_uname = root . _sysdata . to_abs_uname \n if dv_scale is None : \n dv_scale = { } \n if cn_scale is None : \n cn_scale = { } \n abs_params = [ ] \n fd_unknowns = [ var for var in unknown_list if var not in indep_list ] \n pass_unknowns = [ var for var in unknown_list if var in indep_list ] \n for name in indep_list : \n if name in unknowns : \n name = to_abs_uname [ name ] \n for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n if name == src : \n name = tgt \n break \n abs_params . append ( name ) \n Jfd = root . fd_jacobian ( params , unknowns , root . resids , total_derivs = True , \n fd_params = abs_params , fd_unknowns = fd_unknowns , \n pass_unknowns = pass_unknowns , \n poi_indices = self . _poi_indices , \n qoi_indices = self . _qoi_indices ) \n def get_fd_ikey ( ikey ) : \n if isinstance ( ikey , tuple ) : \n ikey = ikey [ <NUM_LIT:0> ] \n fd_ikey = ikey \n if fd_ikey not in params : \n for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n if src == ikey : \n fd_ikey = tgt \n break \n if fd_ikey not in params : \n for key , meta in iteritems ( params ) : \n if to_prom_name [ key ] == fd_ikey : \n fd_ikey = meta [ '<STR_LIT>' ] \n break \n return fd_ikey \n if return_format == '<STR_LIT>' : \n J = OrderedDict ( ) \n for okey in unknown_list : \n J [ okey ] = OrderedDict ( ) \n for j , ikey in enumerate ( indep_list ) : \n if sparsity is not None : \n if ikey not in sparsity [ okey ] : \n continue \n abs_ikey = abs_params [ j ] \n fd_ikey = get_fd_ikey ( abs_ikey ) \n if ( okey , fd_ikey ) not in Jfd : \n fd_ikey = to_abs_pnames [ fd_ikey ] [ <NUM_LIT:0> ] \n J [ okey ] [ ikey ] = Jfd [ ( okey , fd_ikey ) ] \n if ikey in dv_scale : \n J [ okey ] [ ikey ] *= dv_scale [ ikey ] \n if okey in cn_scale : \n J [ okey ] [ ikey ] *= cn_scale [ okey ] \n else : \n usize = <NUM_LIT:0> \n psize = <NUM_LIT:0> \n for u in unknown_list : \n if u in self . _qoi_indices : \n idx = self . _qoi_indices [ u ] \n usize += len ( idx ) \n else : \n usize += self . root . unknowns . metadata ( u ) [ '<STR_LIT:size>' ] \n for p in indep_list : \n if p in self . _poi_indices : \n idx = self . _poi_indices [ p ] \n psize += len ( idx ) \n else : \n psize += self . root . unknowns . metadata ( p ) [ '<STR_LIT:size>' ] \n J = np . zeros ( ( usize , psize ) ) \n ui = <NUM_LIT:0> \n for u in unknown_list : \n pi = <NUM_LIT:0> \n for j , p in enumerate ( indep_list ) : \n abs_ikey = abs_params [ j ] \n fd_ikey = get_fd_ikey ( abs_ikey ) \n if ( u , fd_ikey ) not in Jfd : \n fd_ikey = to_abs_pnames [ fd_ikey ] [ <NUM_LIT:0> ] \n pd = Jfd [ u , fd_ikey ] \n rows , cols = pd . shape \n for row in range ( <NUM_LIT:0> , rows ) : \n for col in range ( <NUM_LIT:0> , cols ) : \n J [ ui + row ] [ pi + col ] = pd [ row ] [ col ] \n if p in dv_scale : \n J [ ui + row ] [ pi + col ] *= dv_scale [ p ] \n if u in cn_scale : \n J [ ui + row ] [ pi + col ] *= cn_scale [ u ] \n pi += cols \n ui += rows \n return J \n def _calc_gradient_ln_solver ( self , indep_list , unknown_list , return_format , mode , \n dv_scale = None , cn_scale = None , sparsity = None ) : \n \"\"\"<STR_LIT>\"\"\" \n root = self . root \n relevance = root . _probdata . relevance \n unknowns = root . unknowns \n unknowns_dict = root . _unknowns_dict \n to_abs_uname = root . _sysdata . to_abs_uname \n comm = root . comm \n iproc = comm . rank \n nproc = comm . size \n owned = root . _owning_ranks \n if dv_scale is None : \n dv_scale = { } \n if cn_scale is None : \n cn_scale = { } \n mode = self . _mode ( mode , indep_list , unknown_list ) \n fwd = mode == '<STR_LIT>' \n root . clear_dparams ( ) \n for names in root . _probdata . relevance . vars_of_interest ( mode ) : \n for name in names : \n if name in root . dumat : \n root . dumat [ name ] . vec [ : ] = <NUM_LIT:0.0> \n root . drmat [ name ] . vec [ : ] = <NUM_LIT:0.0> \n root . dumat [ None ] . vec [ : ] = <NUM_LIT:0.0> \n root . drmat [ None ] . vec [ : ] = <NUM_LIT:0.0> \n root . _sys_linearize ( root . params , unknowns , root . resids ) \n if return_format == '<STR_LIT>' : \n J = OrderedDict ( ) \n for okeys in unknown_list : \n if isinstance ( okeys , str ) : \n okeys = ( okeys , ) \n for okey in okeys : \n J [ okey ] = OrderedDict ( ) \n for ikeys in indep_list : \n if isinstance ( ikeys , str ) : \n ikeys = ( ikeys , ) \n for ikey in ikeys : \n if sparsity is not None : \n if ikey not in sparsity [ okey ] : \n continue \n J [ okey ] [ ikey ] = None \n else : \n usize = <NUM_LIT:0> \n psize = <NUM_LIT:0> \n Jslices = OrderedDict ( ) \n for u in unknown_list : \n start = usize \n if u in self . _qoi_indices : \n idx = self . _qoi_indices [ u ] \n usize += len ( idx ) \n else : \n usize += self . root . unknowns . metadata ( u ) [ '<STR_LIT:size>' ] \n Jslices [ u ] = slice ( start , usize ) \n for p in indep_list : \n start = psize \n if p in self . _poi_indices : \n idx = self . _poi_indices [ p ] \n psize += len ( idx ) \n else : \n psize += unknowns . metadata ( p ) [ '<STR_LIT:size>' ] \n Jslices [ p ] = slice ( start , psize ) \n J = np . zeros ( ( usize , psize ) ) \n if fwd : \n input_list , output_list = indep_list , unknown_list \n poi_indices , qoi_indices = self . _poi_indices , self . _qoi_indices \n in_scale , un_scale = dv_scale , cn_scale \n else : \n input_list , output_list = unknown_list , indep_list \n qoi_indices , poi_indices = self . _poi_indices , self . _qoi_indices \n in_scale , un_scale = cn_scale , dv_scale \n all_vois = self . root . _probdata . relevance . vars_of_interest ( mode ) \n input_set = set ( ) \n for inp in input_list : \n if isinstance ( inp , str ) : \n input_set . add ( inp ) \n else : \n input_set . update ( inp ) \n voi_sets = [ ] \n for voi_set in all_vois : \n for voi in voi_set : \n if voi in input_set : \n voi_sets . append ( voi_set ) \n break \n flat_voi = [ item for sublist in all_vois for item in sublist ] \n for items in input_list : \n if isinstance ( items , str ) : \n items = ( items , ) \n for item in items : \n if item not in flat_voi : \n voi_sets . append ( ( item , ) ) \n voi_srcs = { } \n for params in voi_sets : \n rhs = OrderedDict ( ) \n voi_idxs = { } \n old_size = None \n for voi in params : \n vkey = self . _get_voi_key ( voi , params ) \n duvec = self . root . dumat [ vkey ] \n rhs [ vkey ] = np . empty ( ( len ( duvec . vec ) , ) ) \n voi_srcs [ vkey ] = voi \n if voi in duvec : \n in_idxs = duvec . _get_local_idxs ( voi , poi_indices ) \n else : \n in_idxs = [ ] \n if len ( in_idxs ) == <NUM_LIT:0> : \n if voi in poi_indices : \n in_idxs = duvec . to_idx_array ( poi_indices [ voi ] ) \n else : \n in_idxs = np . arange ( <NUM_LIT:0> , unknowns_dict [ to_abs_uname [ voi ] ] [ '<STR_LIT:size>' ] , dtype = int ) \n if old_size is None : \n old_size = len ( in_idxs ) \n elif old_size != len ( in_idxs ) : \n raise RuntimeError ( \"<STR_LIT>\" \n \"<STR_LIT>\" % ( params , old_size , len ( in_idxs ) ) ) \n voi_idxs [ vkey ] = in_idxs \n for i in range ( len ( in_idxs ) ) : \n for voi in params : \n vkey = self . _get_voi_key ( voi , params ) \n rhs [ vkey ] [ : ] = <NUM_LIT:0.0> \n if self . root . _owning_ranks [ voi_srcs [ vkey ] ] == iproc : \n rhs [ vkey ] [ voi_idxs [ vkey ] [ i ] ] = - <NUM_LIT:1.0> \n dx_mat = root . ln_solver . solve ( rhs , root , mode ) \n for param , dx in iteritems ( dx_mat ) : \n vkey = self . _get_voi_key ( param , params ) \n if param is None : \n param = params [ <NUM_LIT:0> ] \n for item in output_list : \n if sparsity is not None : \n if fwd and param not in sparsity [ item ] : \n continue \n elif not fwd and item not in sparsity [ param ] : \n continue \n if relevance . is_relevant ( vkey , item ) : \n if fwd or owned [ item ] == iproc : \n out_idxs = self . root . dumat [ vkey ] . _get_local_idxs ( item , \n qoi_indices , \n get_slice = True ) \n dxval = dx [ out_idxs ] \n if dxval . size == <NUM_LIT:0> : \n dxval = None \n else : \n dxval = None \n if nproc > <NUM_LIT:1> : \n if trace : \n debug ( \"<STR_LIT>\" % \n ( dxval , owned [ item ] , param , item ) ) \n dxval = comm . bcast ( dxval , root = owned [ item ] ) \n if trace : \n debug ( \"<STR_LIT>\" ) \n else : \n if item in qoi_indices : \n zsize = len ( qoi_indices [ item ] ) \n else : \n zsize = unknowns . metadata ( item ) [ '<STR_LIT:size>' ] \n dxval = np . zeros ( zsize ) \n if dxval is not None : \n nk = len ( dxval ) \n if return_format == '<STR_LIT>' : \n if fwd : \n if J [ item ] [ param ] is None : \n J [ item ] [ param ] = np . zeros ( ( nk , len ( in_idxs ) ) ) \n J [ item ] [ param ] [ : , i ] = dxval \n if param in in_scale : \n J [ item ] [ param ] [ : , i ] *= in_scale [ param ] \n if item in un_scale : \n J [ item ] [ param ] [ : , i ] *= un_scale [ item ] \n else : \n if J [ param ] [ item ] is None : \n J [ param ] [ item ] = np . zeros ( ( len ( in_idxs ) , nk ) ) \n J [ param ] [ item ] [ i , : ] = dxval \n if param in in_scale : \n J [ param ] [ item ] [ i , : ] *= in_scale [ param ] \n if item in un_scale : \n J [ param ] [ item ] [ i , : ] *= un_scale [ item ] \n else : \n if fwd : \n J [ Jslices [ item ] , Jslices [ param ] . start + i ] = dxval \n if param in in_scale : \n J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= in_scale [ param ] \n if item in un_scale : \n J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= un_scale [ item ] \n else : \n J [ Jslices [ param ] . start + i , Jslices [ item ] ] = dxval \n if param in in_scale : \n J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= in_scale [ param ] \n if item in un_scale : \n J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= un_scale [ item ] \n root . clear_dparams ( ) \n return J \n def _get_voi_key ( self , voi , grp ) : \n \"\"\"<STR_LIT>\"\"\" \n if ( voi in self . _driver_vois and \n isinstance ( self . root . ln_solver , LinearGaussSeidel ) ) : \n if ( len ( grp ) > <NUM_LIT:1> or \n self . root . ln_solver . options [ '<STR_LIT>' ] ) : \n return voi \n return None \n def check_partial_derivatives ( self , out_stream = sys . stdout , comps = None , \n compact_print = False ) : \n \"\"\"<STR_LIT>\"\"\" \n root = self . root \n if self . driver . iter_count < <NUM_LIT:1> : \n out_stream . write ( '<STR_LIT>' ) \n self . run_once ( ) \n root . _sys_linearize ( root . params , root . unknowns , root . resids ) \n if out_stream is not None : \n out_stream . write ( '<STR_LIT>' ) \n data = { } \n voi = None \n allcomps = root . components ( recurse = True ) \n if comps is None : \n comps = allcomps \n else : \n allcompnames = set ( [ c . pathname for c in allcomps ] ) \n requested = set ( comps ) \n diff = requested . difference ( allcompnames ) \n if diff : \n sorted_diff = list ( diff ) \n sorted_diff . sort ( ) \n msg = \"<STR_LIT>\" \n msg += str ( sorted_diff ) \n raise RuntimeError ( msg ) \n comps = [ root . _subsystem ( c_name ) for c_name in comps ] \n for comp in comps : \n cname = comp . pathname \n opt = comp . fd_options \n fwd_rev = True \n if opt [ '<STR_LIT>' ] : \n f_d_2 = True \n fd_desc = opt [ '<STR_LIT>' ] \n fd_desc2 = opt [ '<STR_LIT>' ] \n else : \n f_d_2 = False \n fd_desc = None \n fd_desc2 = None \n if opt [ '<STR_LIT>' ] : \n if not f_d_2 : \n continue \n fwd_rev = False \n if isinstance ( comp , IndepVarComp ) : \n continue \n data [ cname ] = { } \n jac_fwd = OrderedDict ( ) \n jac_rev = OrderedDict ( ) \n jac_fd = OrderedDict ( ) \n jac_fd2 = OrderedDict ( ) \n params = comp . params \n unknowns = comp . unknowns \n resids = comp . resids \n dparams = comp . dpmat [ voi ] \n dunknowns = comp . dumat [ voi ] \n dresids = comp . drmat [ voi ] \n states = comp . states \n if len ( dparams ) == <NUM_LIT:0> : \n continue \n param_list = [ item for item in dparams if not dparams . metadata ( item ) . get ( '<STR_LIT>' ) ] \n param_list . extend ( states ) \n unkn_list = [ item for item in dunknowns if not dunknowns . metadata ( item ) . get ( '<STR_LIT>' ) ] \n if out_stream is not None : \n out_stream . write ( '<STR_LIT:->' * ( len ( cname ) + <NUM_LIT:15> ) + '<STR_LIT:\\n>' ) \n out_stream . write ( \"<STR_LIT>\" % cname ) \n out_stream . write ( '<STR_LIT:->' * ( len ( cname ) + <NUM_LIT:15> ) + '<STR_LIT:\\n>' ) \n for p_name in param_list : \n if not fwd_rev : \n break \n dinputs = dunknowns if p_name in states else dparams \n p_size = np . size ( dinputs [ p_name ] ) \n for u_name in unkn_list : \n u_size = np . size ( dunknowns [ u_name ] ) \n if comp . _jacobian_cache : \n if ( u_name , p_name ) in comp . _jacobian_cache : \n user = comp . _jacobian_cache [ ( u_name , p_name ) ] . shape \n if len ( user ) < <NUM_LIT:2> : \n user = ( user [ <NUM_LIT:0> ] , <NUM_LIT:1> ) \n if user [ <NUM_LIT:0> ] != u_size or user [ <NUM_LIT:1> ] != p_size : \n msg = \"<STR_LIT>\" + \"<STR_LIT>\" \n msg = msg . format ( cname , u_name , p_name , ( u_size , p_size ) , user ) \n raise ValueError ( msg ) \n jac_fwd [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n jac_rev [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n if fwd_rev : \n for u_name in unkn_list : \n u_size = np . size ( dunknowns [ u_name ] ) \n for idx in range ( u_size ) : \n dresids . vec [ : ] = <NUM_LIT:0.0> \n root . clear_dparams ( ) \n dunknowns . vec [ : ] = <NUM_LIT:0.0> \n dresids . _dat [ u_name ] . val [ idx ] = <NUM_LIT:1.0> \n try : \n comp . apply_linear ( params , unknowns , dparams , \n dunknowns , dresids , '<STR_LIT>' ) \n finally : \n dparams . _apply_unit_derivatives ( ) \n for p_name in param_list : \n dinputs = dunknowns if p_name in states else dparams \n jac_rev [ ( u_name , p_name ) ] [ idx , : ] = dinputs . _dat [ p_name ] . val \n if fwd_rev : \n for p_name in param_list : \n dinputs = dunknowns if p_name in states else dparams \n p_size = np . size ( dinputs [ p_name ] ) \n for idx in range ( p_size ) : \n dresids . vec [ : ] = <NUM_LIT:0.0> \n root . clear_dparams ( ) \n dunknowns . vec [ : ] = <NUM_LIT:0.0> \n dinputs . _dat [ p_name ] . val [ idx ] = <NUM_LIT:1.0> \n dparams . _apply_unit_derivatives ( ) \n comp . apply_linear ( params , unknowns , dparams , \n dunknowns , dresids , '<STR_LIT>' ) \n for u_name , u_val in dresids . vec_val_iter ( ) : \n jac_fwd [ ( u_name , p_name ) ] [ : , idx ] = u_val \n dresids . vec [ : ] = <NUM_LIT:0.0> \n root . clear_dparams ( ) \n dunknowns . vec [ : ] = <NUM_LIT:0.0> \n if opt [ '<STR_LIT>' ] == '<STR_LIT>' : \n fd_func = comp . complex_step_jacobian \n else : \n fd_func = comp . fd_jacobian \n jac_fd = fd_func ( params , unknowns , resids ) \n if f_d_2 : \n dresids . vec [ : ] = <NUM_LIT:0.0> \n root . clear_dparams ( ) \n dunknowns . vec [ : ] = <NUM_LIT:0.0> \n if opt [ '<STR_LIT>' ] == '<STR_LIT>' : \n fd_func = comp . complex_step_jacobian \n else : \n fd_func = comp . fd_jacobian \n save_form = opt [ '<STR_LIT>' ] \n OptionsDictionary . locked = False \n opt [ '<STR_LIT>' ] = opt [ '<STR_LIT>' ] \n jac_fd2 = fd_func ( params , unknowns , resids ) \n opt [ '<STR_LIT>' ] = save_form \n OptionsDictionary . locked = True \n _assemble_deriv_data ( chain ( dparams , states ) , resids , data [ cname ] , \n jac_fwd , jac_rev , jac_fd , out_stream , \n c_name = cname , jac_fd2 = jac_fd2 , fd_desc = fd_desc , \n fd_desc2 = fd_desc2 , compact_print = compact_print ) \n return data \n def check_total_derivatives ( self , out_stream = sys . stdout ) : \n \"\"\"<STR_LIT>\"\"\" \n root = self . root \n driver = self . driver \n if driver . iter_count < <NUM_LIT:1> : \n out_stream . write ( '<STR_LIT>' ) \n self . run_once ( ) \n if out_stream is not None : \n out_stream . write ( '<STR_LIT>' ) \n if len ( driver . _desvars ) > <NUM_LIT:0> : \n param_srcs = list ( driver . _desvars . keys ( ) ) \n to_abs_name = root . _sysdata . to_abs_uname \n indep_list = [ p for p in param_srcs if not root . _unknowns_dict [ to_abs_name [ p ] ] . get ( '<STR_LIT>' ) ] \n else : \n abs_indep_list = root . _get_fd_params ( ) \n param_srcs = [ root . connections [ p ] for p in abs_indep_list if not root . _params_dict [ p ] . get ( '<STR_LIT>' ) ] \n to_prom_name = self . root . _sysdata . to_prom_name \n indep_list = [ \n to_prom_name [ p ] for p , idxs in param_srcs \n ] \n if len ( driver . _objs ) > <NUM_LIT:0> or len ( driver . _cons ) > <NUM_LIT:0> : \n unknown_list = list ( driver . _objs . keys ( ) ) \n unknown_list . extend ( list ( driver . _cons . keys ( ) ) ) \n unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( '<STR_LIT>' ) ] \n else : \n unknown_list = root . _get_fd_unknowns ( ) \n unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( '<STR_LIT>' ) ] \n if root . ln_solver . options . get ( '<STR_LIT>' ) : \n mode = self . _mode ( '<STR_LIT>' , indep_list , unknown_list ) \n if mode == '<STR_LIT>' : \n fwd , rev = True , False \n Jrev = None \n if out_stream is not None : \n out_stream . write ( '<STR_LIT>' ) \n out_stream . write ( '<STR_LIT>' ) \n else : \n fwd , rev = False , True \n Jfor = None \n if out_stream is not None : \n out_stream . write ( '<STR_LIT>' ) \n out_stream . write ( '<STR_LIT>' ) \n else : \n fwd = rev = True \n if fwd : \n Jfor = self . calc_gradient ( indep_list , unknown_list , mode = '<STR_LIT>' , \n return_format = '<STR_LIT>' ) \n Jfor = _jac_to_flat_dict ( Jfor ) \n if rev : \n Jrev = self . calc_gradient ( indep_list , unknown_list , mode = '<STR_LIT>' , \n return_format = '<STR_LIT>' ) \n Jrev = _jac_to_flat_dict ( Jrev ) \n Jfd = self . calc_gradient ( indep_list , unknown_list , mode = '<STR_LIT>' , \n return_format = '<STR_LIT>' ) \n Jfd = _jac_to_flat_dict ( Jfd ) \n data = { } \n _assemble_deriv_data ( indep_list , unknown_list , data , \n Jfor , Jrev , Jfd , out_stream ) \n return data \n def _start_recorders ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . driver . recorders . startup ( self . root ) \n self . driver . recorders . record_metadata ( self . root ) \n for group in self . root . subgroups ( recurse = True , include_self = True ) : \n for solver in ( group . nl_solver , group . ln_solver ) : \n solver . recorders . startup ( group ) \n solver . recorders . record_metadata ( self . root ) \n def _check_for_parallel_derivs ( self , params , unknowns , par_u , par_p ) : \n \"\"\"<STR_LIT>\"\"\" \n mode = self . _mode ( '<STR_LIT>' , params , unknowns ) \n if mode == '<STR_LIT>' : \n has_parallel_derivs = par_p \n else : \n has_parallel_derivs = par_u \n if ( isinstance ( self . root . ln_solver , LinearGaussSeidel ) and \n self . root . ln_solver . options [ '<STR_LIT>' ] ) and has_parallel_derivs : \n for sub in self . root . subgroups ( recurse = True ) : \n sub_mode = sub . ln_solver . options [ '<STR_LIT>' ] \n if isinstance ( sub . ln_solver , LinearGaussSeidel ) and sub_mode not in ( mode , '<STR_LIT>' ) : \n msg = \"<STR_LIT>\" \"<STR_LIT>\" \n msg = msg . format ( name = sub . name , submode = sub_mode , rootmode = mode ) \n self . _setup_errors . append ( msg ) \n return mode \n def _json_system_tree ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n def _tree_dict ( system ) : \n dct = OrderedDict ( ) \n for s in system . subsystems ( recurse = True ) : \n if isinstance ( s , Group ) : \n dct [ s . name ] = _tree_dict ( s ) \n else : \n dct [ s . name ] = OrderedDict ( ) \n for vname , meta in iteritems ( s . unknowns ) : \n dct [ s . name ] [ vname ] = m = meta . copy ( ) \n for mname in m : \n if isinstance ( m [ mname ] , np . ndarray ) : \n m [ mname ] = m [ mname ] . tolist ( ) \n return dct \n tree = OrderedDict ( ) \n tree [ '<STR_LIT:root>' ] = _tree_dict ( self . root ) \n return json . dumps ( tree ) \n def _setup_communicators ( self ) : \n if self . comm is None : \n self . comm = self . _impl . world_comm ( ) \n minproc , maxproc = self . driver . get_req_procs ( ) \n if MPI : \n if not ( maxproc is None or maxproc >= self . comm . size ) : \n raise RuntimeError ( \"<STR_LIT>\" \n \"<STR_LIT>\" % \n ( self . comm . size , minproc , maxproc ) ) \n elif self . comm . size < minproc : \n if maxproc is None : \n maxproc = '<STR_LIT>' \n raise RuntimeError ( \"<STR_LIT>\" \n \"<STR_LIT>\" % \n ( self . comm . size , minproc , maxproc ) ) \n self . driver . _setup_communicators ( self . comm , os . getcwd ( ) ) \n def _setup_units ( self , connections , params_dict , unknowns_dict ) : \n \"\"\"<STR_LIT>\"\"\" \n to_prom_name = self . root . _sysdata . to_prom_name \n for target , ( source , idxs ) in iteritems ( connections ) : \n tmeta = params_dict [ target ] \n smeta = unknowns_dict [ source ] \n if '<STR_LIT>' not in tmeta or '<STR_LIT>' not in smeta : \n continue \n src_unit = smeta [ '<STR_LIT>' ] \n tgt_unit = tmeta [ '<STR_LIT>' ] \n try : \n scale , offset = get_conversion_tuple ( src_unit , tgt_unit ) \n except TypeError as err : \n if str ( err ) == \"<STR_LIT>\" : \n msg = \"<STR_LIT>\" \"<STR_LIT>\" \"<STR_LIT>\" . format ( src_unit , \n _both_names ( smeta , to_prom_name ) , \n tgt_unit , \n _both_names ( tmeta , to_prom_name ) ) \n self . _setup_errors . append ( msg ) \n continue \n else : \n raise \n if scale != <NUM_LIT:1.0> or offset != <NUM_LIT:0.0> : \n tmeta [ '<STR_LIT>' ] = ( scale , offset ) \n def _add_implicit_connections ( self , connections ) : \n \"\"\"<STR_LIT>\"\"\" \n dangling = set ( ) \n abs_unames = self . root . _sysdata . to_abs_uname \n for prom_name , pabs_list in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n if prom_name in abs_unames : \n for pabs in pabs_list : \n connections . setdefault ( pabs , [ ] ) . append ( ( abs_unames [ prom_name ] , None ) ) \n else : \n dangling . add ( prom_name ) \n return dangling \n def print_all_convergence ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n root = self . root \n root . ln_solver . print_all_convergence ( ) \n root . nl_solver . print_all_convergence ( ) \n for grp in root . subgroups ( recurse = True ) : \n grp . ln_solver . print_all_convergence ( ) \n grp . nl_solver . print_all_convergence ( ) \n def _assign_parameters ( connections ) : \n \"\"\"<STR_LIT>\"\"\" \n param_owners = { } \n for par , ( unk , idxs ) in iteritems ( connections ) : \n param_owners . setdefault ( get_common_ancestor ( par , unk ) , set ( ) ) . add ( par ) \n return param_owners \n def _jac_to_flat_dict ( jac ) : \n \"\"\"<STR_LIT>\"\"\" \n new_jac = OrderedDict ( ) \n for key1 , val1 in iteritems ( jac ) : \n for key2 , val2 in iteritems ( val1 ) : \n new_jac [ ( key1 , key2 ) ] = val2 \n return new_jac \n def _pad_name ( name , pad_num = <NUM_LIT> , quotes = True ) : \n \"\"\"<STR_LIT>\"\"\" \n l_name = len ( name ) \n if l_name < pad_num : \n pad = pad_num - l_name \n if quotes : \n pad_str = \"<STR_LIT>\" \n else : \n pad_str = \"<STR_LIT>\" \n pad_name = pad_str . format ( name = name , sep = '<STR_LIT>' , pad = pad ) \n return pad_name \n else : \n return '<STR_LIT>' . format ( name ) \n def _assemble_deriv_data ( params , resids , cdata , jac_fwd , jac_rev , jac_fd , \n out_stream , c_name = '<STR_LIT:root>' , jac_fd2 = None , fd_desc = None , \n fd_desc2 = None , compact_print = False ) : \n \"\"\"<STR_LIT>\"\"\" \n started = False \n for p_name in params : \n for u_name in resids : \n key = ( u_name , p_name ) \n if key not in jac_fd : \n continue \n ldata = cdata [ key ] = { } \n Jsub_fd = jac_fd [ key ] \n ldata [ '<STR_LIT>' ] = Jsub_fd \n magfd = np . linalg . norm ( Jsub_fd ) \n if jac_fwd : \n Jsub_for = jac_fwd [ key ] \n ldata [ '<STR_LIT>' ] = Jsub_for \n magfor = np . linalg . norm ( Jsub_for ) \n else : \n magfor = None \n if jac_rev : \n Jsub_rev = jac_rev [ key ] \n ldata [ '<STR_LIT>' ] = Jsub_rev \n magrev = np . linalg . norm ( Jsub_rev ) \n else : \n magrev = None \n if jac_fd2 : \n Jsub_fd2 = jac_fd2 [ key ] \n ldata [ '<STR_LIT>' ] = Jsub_fd2 \n magfd2 = np . linalg . norm ( Jsub_fd2 ) \n else : \n magfd2 = None \n ldata [ '<STR_LIT>' ] = ( magfor , magrev , magfd ) \n if jac_fwd : \n abs1 = np . linalg . norm ( Jsub_for - Jsub_fd ) \n else : \n abs1 = None \n if jac_rev : \n abs2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) \n else : \n abs2 = None \n if jac_fwd and jac_rev : \n abs3 = np . linalg . norm ( Jsub_for - Jsub_rev ) \n else : \n abs3 = None \n if jac_fd2 : \n abs4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) \n else : \n abs4 = None \n ldata [ '<STR_LIT>' ] = ( abs1 , abs2 , abs3 ) \n if magfd == <NUM_LIT:0.0> : \n rel1 = rel2 = rel3 = rel4 = float ( '<STR_LIT>' ) \n else : \n if jac_fwd : \n rel1 = np . linalg . norm ( Jsub_for - Jsub_fd ) / magfd \n else : \n rel1 = None \n if jac_rev : \n rel2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) / magfd \n else : \n rel2 = None \n if jac_fwd and jac_rev : \n rel3 = np . linalg . norm ( Jsub_for - Jsub_rev ) / magfd \n else : \n rel3 = None \n if jac_fd2 : \n rel4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) / magfd \n else : \n rel4 = None \n ldata [ '<STR_LIT>' ] = ( rel1 , rel2 , rel3 ) \n if out_stream is None : \n continue \n if compact_print : \n if jac_fwd and jac_rev : \n if not started : \n tmp1 = \"<STR_LIT>\" \n out_str = tmp1 . format ( _pad_name ( '<STR_LIT>' ) , _pad_name ( '<STR_LIT>' ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT:10> , quotes = False ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT:10> , quotes = False ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT:10> , quotes = False ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT:10> , quotes = False ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT:10> , quotes = False ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT:10> , quotes = False ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT:10> , quotes = False ) \n ) \n out_stream . write ( out_str ) \n out_stream . write ( '<STR_LIT:->' * len ( out_str ) + '<STR_LIT:\\n>' ) \n started = True \n tmp1 = \"<STR_LIT>\" \n out_stream . write ( tmp1 . format ( _pad_name ( u_name ) , _pad_name ( p_name ) , \n magfor , magrev , magfd , abs1 , abs2 , \n rel1 , rel2 ) ) \n elif jac_fd and jac_fd2 : \n if not started : \n tmp1 = \"<STR_LIT>\" \n out_str = tmp1 . format ( _pad_name ( '<STR_LIT>' ) , _pad_name ( '<STR_LIT>' ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT> , quotes = False ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT:12> , quotes = False ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT:12> , quotes = False ) , \n _pad_name ( '<STR_LIT>' , <NUM_LIT:12> , quotes = False ) \n ) \n out_stream . write ( out_str ) \n out_stream . write ( '<STR_LIT:->' * len ( out_str ) + '<STR_LIT:\\n>' ) \n started = True \n tmp1 = \"<STR_LIT>\" \n out_stream . write ( tmp1 . format ( _pad_name ( u_name ) , _pad_name ( p_name ) , \n magfd , magfd2 , abs4 , rel4 ) ) \n else : \n if started : \n out_stream . write ( '<STR_LIT>' * <NUM_LIT:30> + '<STR_LIT:\\n>' ) \n else : \n started = True \n out_stream . write ( \"<STR_LIT>\" % ( c_name , u_name , p_name ) ) \n if jac_fwd : \n out_stream . write ( '<STR_LIT>' % magfor ) \n if jac_rev : \n out_stream . write ( '<STR_LIT>' % magrev ) \n if not jac_fwd and not jac_rev : \n out_stream . write ( '<STR_LIT>' ) \n if jac_fd : \n out_stream . write ( '<STR_LIT>' % magfd ) \n if fd_desc : \n out_stream . write ( '<STR_LIT>' % fd_desc ) \n out_stream . write ( '<STR_LIT:\\n>' ) \n if jac_fd2 : \n out_stream . write ( '<STR_LIT>' % magfd2 ) \n if fd_desc2 : \n out_stream . write ( '<STR_LIT>' % fd_desc2 ) \n out_stream . write ( '<STR_LIT:\\n>' ) \n out_stream . write ( '<STR_LIT:\\n>' ) \n if jac_fwd : \n out_stream . write ( '<STR_LIT>' % abs1 ) \n if jac_rev : \n out_stream . write ( '<STR_LIT>' % abs2 ) \n if jac_fwd and jac_rev : \n out_stream . write ( '<STR_LIT>' % abs3 ) \n if jac_fd2 : \n out_stream . write ( '<STR_LIT>' % abs4 ) \n out_stream . write ( '<STR_LIT:\\n>' ) \n if jac_fwd : \n out_stream . write ( '<STR_LIT>' % rel1 ) \n if jac_rev : \n out_stream . write ( '<STR_LIT>' % rel2 ) \n if jac_fwd and jac_rev : \n out_stream . write ( '<STR_LIT>' % rel3 ) \n if jac_fd2 : \n out_stream . write ( '<STR_LIT>' % rel4 ) \n out_stream . write ( '<STR_LIT:\\n>' ) \n if jac_fwd : \n out_stream . write ( '<STR_LIT>' ) \n out_stream . write ( str ( Jsub_for ) ) \n out_stream . write ( '<STR_LIT>' ) \n if jac_rev : \n out_stream . write ( '<STR_LIT>' ) \n out_stream . write ( str ( Jsub_rev ) ) \n out_stream . write ( '<STR_LIT>' ) \n out_stream . write ( '<STR_LIT>' ) \n out_stream . write ( str ( Jsub_fd ) ) \n out_stream . write ( '<STR_LIT>' ) \n if jac_fd2 : \n out_stream . write ( '<STR_LIT>' ) \n out_stream . write ( str ( Jsub_fd2 ) ) \n out_stream . write ( '<STR_LIT>' ) \n def _needs_iteration ( comp ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( comp , Component ) and comp . is_active ( ) and comp . states : \n for klass in comp . __class__ . __mro__ : \n if klass is Component : \n break \n if '<STR_LIT>' in klass . __dict__ : \n return False \n return True \n return False \n def _get_gmres_name ( ) : \n if MPI : \n return '<STR_LIT>' \n else : \n <mask0> '<STR_LIT>' \n", "gt": "return"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from collections import OrderedDict \n from sqlitedict import SqliteDict \n from openmdao . recorders . base_recorder import BaseRecorder \n from openmdao . util . record_util import format_iteration_coordinate \n from openmdao . core . mpi_wrap import MPI \n class SqliteRecorder ( BaseRecorder ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , out , ** sqlite_dict_args ) : \n super ( SqliteRecorder , self ) . __init__ ( ) \n if MPI and MPI . COMM_WORLD . rank > <NUM_LIT:0> : \n self . _open_close_sqlitedict = False \n else : \n self . _open_close_sqlitedict = True \n if self . _open_close_sqlitedict : \n sqlite_dict_args . setdefault ( '<STR_LIT>' , True ) \n sqlite_dict_args . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) \n self . out = SqliteDict ( filename = out , flag = '<STR_LIT:n>' , ** sqlite_dict_args ) \n else : \n self . out = None \n def record_metadata ( self , group ) : \n \"\"\"<STR_LIT>\"\"\" \n params = group . params . iteritems ( ) \n resids = group . resids . iteritems ( ) \n unknowns = group . unknowns . iteritems ( ) \n data = OrderedDict ( [ ( '<STR_LIT>' , dict ( params ) ) , \n ( '<STR_LIT>' , dict ( unknowns ) ) , \n ] ) \n self . out [ '<STR_LIT>' ] = data \n def record_iteration ( self , params , unknowns , resids , metadata ) : \n \"\"\"<STR_LIT>\"\"\" \n data = OrderedDict ( ) \n iteration_coordinate = metadata [ '<STR_LIT>' ] \n timestamp = metadata [ '<STR_LIT>' ] \n group_name = format_iteration_coordinate ( iteration_coordinate ) \n data [ '<STR_LIT>' ] = timestamp \n data [ '<STR_LIT:success>' ] = metadata [ '<STR_LIT:success>' ] \n data [ '<STR_LIT>' ] = metadata [ '<STR_LIT>' ] \n if self . options [ '<STR_LIT>' ] : \n data [ '<STR_LIT>' ] = self . _filter_vector ( params , '<STR_LIT:p>' , iteration_coordinate ) \n if self . options [ '<STR_LIT>' ] : \n data [ '<STR_LIT>' ] = self . _filter_vector ( unknowns , '<STR_LIT:u>' , iteration_coordinate ) \n if self . options [ '<STR_LIT>' ] : \n data [ '<STR_LIT>' ] = self . _filter_vector ( resids , '<STR_LIT:r>' , iteration_coordinate ) \n self . out [ group_name ] = data \n def record_derivatives ( self , derivs , metadata ) : \n \"\"\"<STR_LIT>\"\"\" \n data = OrderedDict ( ) \n iteration_coordinate = metadata [ '<STR_LIT>' ] \n timestamp = metadata [ '<STR_LIT>' ] \n group_name = format_iteration_coordinate ( iteration_coordinate ) \n group_name = '<STR_LIT>' % group_name \n data [ '<STR_LIT>' ] = timestamp \n data [ '<STR_LIT:success>' ] = metadata [ '<STR_LIT:success>' ] \n data [ '<STR_LIT>' ] = metadata [ '<STR_LIT>' ] \n data [ '<STR_LIT>' ] = derivs \n self . out [ group_name ] = data \n def close ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _open_close_sqlitedict : \n if self . out is not None : \n self . out . close ( ) \n self . out = <mask0> \n", "gt": "None"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import numpy as np \n from numpy import atleast_2d as array2d \n from scipy import linalg \n from scipy . optimize import minimize \n from scipy . spatial . distance import squareform \n from openmdao . surrogate_models . surrogate_model import MultiFiSurrogateModel \n import logging \n _logger = logging . getLogger ( ) \n MACHINE_EPSILON = np . finfo ( np . double ) . eps \n NUGGET = <NUM_LIT> * MACHINE_EPSILON \n INITIAL_RANGE_DEFAULT = <NUM_LIT> \n TOLERANCE_DEFAULT = <NUM_LIT> \n THETA0_DEFAULT = <NUM_LIT:0.5> \n THETAL_DEFAULT = <NUM_LIT> \n THETAU_DEFAULT = <NUM_LIT:50> \n if hasattr ( linalg , '<STR_LIT>' ) : \n solve_triangular = linalg . solve_triangular \n else : \n def solve_triangular ( x , y , lower = True ) : \n return linalg . solve ( x , y ) \n def constant_regression ( x ) : \n \"\"\"<STR_LIT>\"\"\" \n x = np . asarray ( x , dtype = np . float ) \n n_eval = x . shape [ <NUM_LIT:0> ] \n f = np . ones ( [ n_eval , <NUM_LIT:1> ] ) \n return f \n def linear_regression ( x ) : \n \"\"\"<STR_LIT>\"\"\" \n x = np . asarray ( x , dtype = np . float ) \n n_eval = x . shape [ <NUM_LIT:0> ] \n f = np . hstack ( [ np . ones ( [ n_eval , <NUM_LIT:1> ] ) , x ] ) \n return f \n def squared_exponential_correlation ( theta , d ) : \n \"\"\"<STR_LIT>\"\"\" \n theta = np . asarray ( theta , dtype = np . float ) \n d = np . asarray ( d , dtype = np . float ) \n if d . ndim > <NUM_LIT:1> : \n n_features = d . shape [ <NUM_LIT:1> ] \n else : \n n_features = <NUM_LIT:1> \n if theta . size == <NUM_LIT:1> : \n return np . exp ( - theta [ <NUM_LIT:0> ] * np . sum ( d ** <NUM_LIT:2> , axis = <NUM_LIT:1> ) ) \n elif theta . size != n_features : \n raise ValueError ( \"<STR_LIT>\" % n_features ) \n else : \n return np . exp ( - np . sum ( theta . reshape ( <NUM_LIT:1> , n_features ) * d ** <NUM_LIT:2> , axis = <NUM_LIT:1> ) ) \n def l1_cross_distances ( X , Y = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if Y is None : \n X = array2d ( X ) \n n_samples , n_features = X . shape \n n_nonzero_cross_dist = n_samples * ( n_samples - <NUM_LIT:1> ) // <NUM_LIT:2> \n D = np . zeros ( ( n_nonzero_cross_dist , n_features ) ) \n ll_1 = <NUM_LIT:0> \n for k in range ( n_samples - <NUM_LIT:1> ) : \n ll_0 = ll_1 \n ll_1 = ll_0 + n_samples - k - <NUM_LIT:1> \n D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - X [ ( k + <NUM_LIT:1> ) : ] ) \n return D \n else : \n X = array2d ( X ) \n Y = array2d ( Y ) \n n_samples_X , n_features_X = X . shape \n n_samples_Y , n_features_Y = Y . shape \n if n_features_X != n_features_Y : \n raise ValueError ( \"<STR_LIT>\" ) \n n_features = n_features_X \n n_nonzero_cross_dist = n_samples_X * n_samples_Y \n D = np . zeros ( ( n_nonzero_cross_dist , n_features ) ) \n ll_1 = <NUM_LIT:0> \n for k in range ( n_samples_X ) : \n ll_0 = ll_1 \n ll_1 = ll_0 + n_samples_Y \n D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - Y ) \n return D \n class MultiFiCoKriging ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n _regression_types = { \n '<STR_LIT>' : constant_regression , \n '<STR_LIT>' : linear_regression } \n def __init__ ( self , regr = '<STR_LIT>' , rho_regr = '<STR_LIT>' , \n theta = None , theta0 = None , thetaL = None , thetaU = None ) : \n self . corr = squared_exponential_correlation \n self . regr = regr \n self . rho_regr = rho_regr \n self . theta = theta \n self . theta0 = theta0 \n self . thetaL = thetaL \n self . thetaU = thetaU \n self . _nfev = <NUM_LIT:0> \n def _build_R ( self , lvl , theta ) : \n \"\"\"<STR_LIT>\"\"\" \n D = self . D [ lvl ] \n n_samples = self . n_samples [ lvl ] \n R = np . eye ( n_samples ) * ( <NUM_LIT:1.> + NUGGET ) \n corr = squareform ( self . corr ( theta , D ) ) \n R = R + corr \n return R \n def fit ( self , X , y , \n initial_range = INITIAL_RANGE_DEFAULT , tol = TOLERANCE_DEFAULT ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _check_list_structure ( X , y ) \n self . _check_params ( ) \n X = self . X \n y = self . y \n nlevel = self . nlevel \n n_samples = self . n_samples \n self . beta = nlevel * [ <NUM_LIT:0> ] \n self . beta_rho = nlevel * [ None ] \n self . beta_regr = nlevel * [ None ] \n self . C = nlevel * [ <NUM_LIT:0> ] \n self . D = nlevel * [ <NUM_LIT:0> ] \n self . F = nlevel * [ <NUM_LIT:0> ] \n self . p = nlevel * [ <NUM_LIT:0> ] \n self . q = nlevel * [ <NUM_LIT:0> ] \n self . G = nlevel * [ <NUM_LIT:0> ] \n self . sigma2 = nlevel * [ <NUM_LIT:0> ] \n self . _R_adj = nlevel * [ None ] \n y_best = y [ nlevel - <NUM_LIT:1> ] \n for i in range ( nlevel - <NUM_LIT:1> ) [ : : - <NUM_LIT:1> ] : \n y_best = np . concatenate ( ( y [ i ] [ : - n_samples [ i + <NUM_LIT:1> ] ] , y_best ) ) \n self . y_best = y_best \n self . y_mean = np . zeros ( <NUM_LIT:1> ) \n self . y_std = np . ones ( <NUM_LIT:1> ) \n self . X_mean = np . zeros ( <NUM_LIT:1> ) \n self . X_std = np . ones ( <NUM_LIT:1> ) \n for lvl in range ( nlevel ) : \n self . D [ lvl ] = l1_cross_distances ( X [ lvl ] ) \n if ( np . min ( np . sum ( self . D [ lvl ] , axis = <NUM_LIT:1> ) ) == <NUM_LIT:0.> ) : \n raise Exception ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n self . F [ lvl ] = self . regr ( X [ lvl ] ) \n self . p [ lvl ] = self . F [ lvl ] . shape [ <NUM_LIT:1> ] \n if lvl > <NUM_LIT:0> : \n F_rho = self . rho_regr ( X [ lvl ] ) \n self . q [ lvl ] = F_rho . shape [ <NUM_LIT:1> ] \n self . F [ lvl ] = np . hstack ( ( F_rho * np . dot ( ( self . y [ lvl - <NUM_LIT:1> ] ) [ - n_samples [ lvl ] : ] , \n np . ones ( ( <NUM_LIT:1> , self . q [ lvl ] ) ) ) , self . F [ lvl ] ) ) \n else : \n self . q [ lvl ] = <NUM_LIT:0> \n n_samples_F_i = self . F [ lvl ] . shape [ <NUM_LIT:0> ] \n if n_samples_F_i != n_samples [ lvl ] : \n raise Exception ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n if int ( self . p [ lvl ] + self . q [ lvl ] ) >= n_samples_F_i : \n raise Exception ( ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n % ( n_samples [ i ] , self . p [ lvl ] + self . q [ lvl ] ) ) \n self . X = X \n self . y = y \n self . rlf_value = np . zeros ( nlevel ) \n for lvl in range ( nlevel ) : \n if self . theta [ lvl ] is None : \n sol = self . _max_rlf ( lvl = lvl , initial_range = initial_range , tol = tol ) \n self . theta [ lvl ] = sol [ '<STR_LIT>' ] \n self . rlf_value [ lvl ] = sol [ '<STR_LIT>' ] \n if np . isinf ( self . rlf_value [ lvl ] ) : \n raise Exception ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n else : \n self . rlf_value [ lvl ] = self . rlf ( lvl = lvl ) \n if np . isinf ( self . rlf_value [ lvl ] ) : \n raise Exception ( \"<STR_LIT>\" ) \n return \n def rlf ( self , lvl , theta = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if theta is None : \n theta = self . theta [ lvl ] \n rlf_value = <NUM_LIT> \n n_samples = self . n_samples [ lvl ] \n y = self . y [ lvl ] \n F = self . F [ lvl ] \n p = self . p [ lvl ] \n q = self . q [ lvl ] \n R = self . _build_R ( lvl , theta ) \n try : \n C = linalg . cholesky ( R , lower = True ) \n except linalg . LinAlgError : \n _logger . warning ( ( '<STR_LIT>' % lvl ) + \n '<STR_LIT>' + str ( theta ) ) \n return rlf_value \n Ft = solve_triangular ( C , F , lower = True ) \n Yt = solve_triangular ( C , y , lower = True ) \n try : \n Q , G = linalg . qr ( Ft , econ = True ) \n except : \n Q , G = linalg . qr ( Ft , mode = '<STR_LIT>' ) \n pass \n beta = solve_triangular ( G , np . dot ( Q . T , Yt ) ) \n err = Yt - np . dot ( Ft , beta ) \n err2 = np . dot ( err . T , err ) [ <NUM_LIT:0> , <NUM_LIT:0> ] \n self . _err = err \n sigma2 = err2 / ( n_samples - p - q ) \n detR = ( ( np . diag ( C ) ) ** ( <NUM_LIT> / n_samples ) ) . prod ( ) \n rlf_value = ( n_samples - p - q ) * np . log10 ( sigma2 ) + n_samples * np . log10 ( detR ) \n self . beta_rho [ lvl ] = beta [ : q ] \n self . beta_regr [ lvl ] = beta [ q : ] \n self . beta [ lvl ] = beta \n self . sigma2 [ lvl ] = sigma2 \n self . C [ lvl ] = C \n self . G [ lvl ] = G \n return rlf_value \n def _max_rlf ( self , lvl , initial_range , tol ) : \n \"\"\"<STR_LIT>\"\"\" \n thetaL = self . thetaL [ lvl ] \n thetaU = self . thetaU [ lvl ] \n def rlf_transform ( x ) : \n return self . rlf ( theta = <NUM_LIT> ** x , lvl = lvl ) \n theta0 = self . theta0 [ lvl ] \n x0 = np . log10 ( theta0 [ <NUM_LIT:0> ] ) \n constraints = [ ] \n for i in range ( theta0 . size ) : \n constraints . append ( { '<STR_LIT:type>' : '<STR_LIT>' , '<STR_LIT>' : lambda log10t , i = i : \n log10t [ i ] - np . log10 ( thetaL [ <NUM_LIT:0> ] [ i ] ) } ) \n constraints . append ( { '<STR_LIT:type>' : '<STR_LIT>' , '<STR_LIT>' : lambda log10t , i = i : \n np . log10 ( thetaU [ <NUM_LIT:0> ] [ i ] ) - log10t [ i ] } ) \n constraints = tuple ( constraints ) \n sol = minimize ( rlf_transform , x0 , method = '<STR_LIT>' , \n constraints = constraints , \n options = { '<STR_LIT>' : initial_range , \n '<STR_LIT>' : tol , '<STR_LIT>' : <NUM_LIT:0> } ) \n log10_optimal_x = sol [ '<STR_LIT:x>' ] \n optimal_rlf_value = sol [ '<STR_LIT>' ] \n self . _nfev += sol [ '<STR_LIT>' ] \n optimal_theta = <NUM_LIT> ** log10_optimal_x \n res = { } \n res [ '<STR_LIT>' ] = optimal_theta \n res [ '<STR_LIT>' ] = optimal_rlf_value \n return res \n def predict ( self , X , eval_MSE = True ) : \n \"\"\"<STR_LIT>\"\"\" \n X = array2d ( X ) \n nlevel = self . nlevel \n n_eval , n_features_X = X . shape \n mu = np . zeros ( ( n_eval , nlevel ) ) \n f = self . regr ( X ) \n f0 = self . regr ( X ) \n dx = l1_cross_distances ( X , Y = self . X [ <NUM_LIT:0> ] ) \n F = self . F [ <NUM_LIT:0> ] \n C = self . C [ <NUM_LIT:0> ] \n beta = self . beta [ <NUM_LIT:0> ] \n Ft = solve_triangular ( C , F , lower = True ) \n yt = solve_triangular ( C , self . y [ <NUM_LIT:0> ] , lower = True ) \n r_ = self . corr ( self . theta [ <NUM_LIT:0> ] , dx ) . reshape ( n_eval , self . n_samples [ <NUM_LIT:0> ] ) \n gamma = solve_triangular ( C . T , yt - np . dot ( Ft , beta ) , lower = False ) \n mu [ : , <NUM_LIT:0> ] = ( np . dot ( f , beta ) + np . dot ( r_ , gamma ) ) . ravel ( ) \n if eval_MSE : \n self . sigma2_rho = nlevel * [ None ] \n MSE = np . zeros ( ( n_eval , nlevel ) ) \n r_t = solve_triangular ( C , r_ . T , lower = True ) \n G = self . G [ <NUM_LIT:0> ] \n u_ = solve_triangular ( G . T , f . T - np . dot ( Ft . T , r_t ) , lower = True ) \n MSE [ : , <NUM_LIT:0> ] = self . sigma2 [ <NUM_LIT:0> ] * ( <NUM_LIT:1> - ( r_t ** <NUM_LIT:2> ) . sum ( axis = <NUM_LIT:0> ) + ( u_ ** <NUM_LIT:2> ) . sum ( axis = <NUM_LIT:0> ) ) \n for i in range ( <NUM_LIT:1> , nlevel ) : \n C = self . C [ i ] \n F = self . F [ i ] \n g = self . rho_regr ( X ) \n dx = l1_cross_distances ( X , Y = self . X [ i ] ) \n r_ = self . corr ( self . theta [ i ] , dx ) . reshape ( n_eval , self . n_samples [ i ] ) \n f = np . vstack ( ( g . T * mu [ : , i - <NUM_LIT:1> ] , f0 . T ) ) \n Ft = solve_triangular ( C , F , lower = True ) \n yt = solve_triangular ( C , self . y [ i ] , lower = True ) \n r_t = solve_triangular ( C , r_ . T , lower = True ) \n G = self . G [ i ] \n beta = self . beta [ i ] \n mu [ : , i ] = ( np . dot ( f . T , beta ) + np . dot ( r_t . T , yt - np . dot ( Ft , beta ) ) ) . ravel ( ) \n if eval_MSE : \n Q_ = ( np . dot ( ( yt - np . dot ( Ft , beta ) ) . T , yt - np . dot ( Ft , beta ) ) ) [ <NUM_LIT:0> , <NUM_LIT:0> ] \n u_ = solve_triangular ( G . T , f - np . dot ( Ft . T , r_t ) , lower = True ) \n sigma2_rho = np . dot ( g , self . sigma2 [ i ] * linalg . inv ( np . dot ( G . T , G ) ) [ : self . q [ i ] , : self . q [ i ] ] + np . dot ( beta [ : self . q [ i ] ] , beta [ : self . q [ i ] ] . T ) ) \n sigma2_rho = ( sigma2_rho * g ) . sum ( axis = <NUM_LIT:1> ) \n MSE [ : , i ] = sigma2_rho * MSE [ : , i - <NUM_LIT:1> ] + Q_ / ( <NUM_LIT:2> * ( self . n_samples [ i ] - self . p [ i ] - self . q [ i ] ) ) * ( <NUM_LIT:1> - ( r_t ** <NUM_LIT:2> ) . sum ( axis = <NUM_LIT:0> ) ) + self . sigma2 [ i ] * ( u_ ** <NUM_LIT:2> ) . sum ( axis = <NUM_LIT:0> ) \n for i in range ( nlevel ) : \n mu [ : , i ] = self . y_mean + self . y_std * mu [ : , i ] \n if eval_MSE : \n MSE [ : , i ] = self . y_std ** <NUM_LIT:2> * MSE [ : , i ] \n if eval_MSE : \n return mu [ : , - <NUM_LIT:1> ] . reshape ( ( n_eval , <NUM_LIT:1> ) ) , MSE [ : , - <NUM_LIT:1> ] . reshape ( ( n_eval , <NUM_LIT:1> ) ) \n else : \n return mu [ : , - <NUM_LIT:1> ] . reshape ( ( n_eval , <NUM_LIT:1> ) ) \n def _check_list_structure ( self , X , y ) : \n if type ( X ) is not list : \n nlevel = <NUM_LIT:1> \n X = [ X ] \n else : \n nlevel = len ( X ) \n if type ( y ) is not list : \n y = [ y ] \n if len ( X ) != len ( y ) : \n raise ValueError ( \"<STR_LIT>\" ) \n n_samples = np . zeros ( nlevel , dtype = int ) \n n_features = np . zeros ( nlevel , dtype = int ) \n n_samples_y = np . zeros ( nlevel , dtype = int ) \n for i in range ( nlevel ) : \n n_samples [ i ] , n_features [ i ] = X [ i ] . shape \n if i > <NUM_LIT:1> and n_features [ i ] != n_features [ i - <NUM_LIT:1> ] : \n raise ValueError ( \"<STR_LIT>\" ) \n y [ i ] = np . asarray ( y [ i ] ) . ravel ( ) [ : , np . newaxis ] \n n_samples_y [ i ] = y [ i ] . shape [ <NUM_LIT:0> ] \n if n_samples [ i ] != n_samples_y [ i ] : \n raise ValueError ( \"<STR_LIT>\" ) \n self . n_features = n_features [ <NUM_LIT:0> ] \n if type ( self . theta ) is not list : \n self . theta = nlevel * [ self . theta ] \n elif len ( self . theta ) != nlevel : \n raise ValueError ( \"<STR_LIT>\" % nlevel ) \n if type ( self . theta0 ) is not list : \n self . theta0 = nlevel * [ self . theta0 ] \n elif len ( self . theta0 ) != nlevel : \n raise ValueError ( \"<STR_LIT>\" % nlevel ) \n if type ( self . thetaL ) is not list : \n self . thetaL = nlevel * [ self . thetaL ] \n elif len ( self . thetaL ) != nlevel : \n raise ValueError ( \"<STR_LIT>\" % nlevel ) \n if type ( self . thetaU ) is not list : \n self . thetaU = nlevel * [ self . thetaU ] \n elif len ( self . thetaU ) != nlevel : \n raise ValueError ( \"<STR_LIT>\" % nlevel ) \n self . nlevel = nlevel \n self . X = X [ : ] \n self . y = y [ : ] \n self . n_samples = n_samples \n return \n def _check_params ( self ) : \n if not callable ( self . regr ) : \n if self . regr in self . _regression_types : \n self . regr = self . _regression_types [ self . regr ] \n else : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n % ( self . _regression_types . keys ( ) , self . regr ) ) \n if not callable ( self . rho_regr ) : \n if self . rho_regr in self . _regression_types : \n self . rho_regr = self . _regression_types [ self . rho_regr ] \n else : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n % ( self . _regression_types . keys ( ) , self . rho_regr ) ) \n for i in range ( self . nlevel ) : \n if self . theta [ i ] is not None : \n self . theta [ i ] = array2d ( self . theta [ i ] ) \n if np . any ( self . theta [ i ] <= <NUM_LIT:0> ) : \n raise ValueError ( \"<STR_LIT>\" ) \n if self . theta0 [ i ] is not None : \n self . theta0 [ i ] = array2d ( self . theta0 [ i ] ) \n if np . any ( self . theta0 [ i ] <= <NUM_LIT:0> ) : \n raise ValueError ( \"<STR_LIT>\" ) \n else : \n self . theta0 [ i ] = array2d ( self . n_features * [ THETA0_DEFAULT ] ) \n lth = self . theta0 [ i ] . size \n if self . thetaL [ i ] is not None : \n self . thetaL [ i ] = array2d ( self . thetaL [ i ] ) \n if self . thetaL [ i ] . size != lth : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n else : \n self . thetaL [ i ] = array2d ( self . n_features * [ THETAL_DEFAULT ] ) \n if self . thetaU [ i ] is not None : \n self . thetaU [ i ] = array2d ( self . thetaU [ i ] ) \n if self . thetaU [ i ] . size != lth : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n else : \n self . thetaU [ i ] = array2d ( self . n_features * [ THETAU_DEFAULT ] ) \n if np . any ( self . thetaL [ i ] <= <NUM_LIT:0> ) or np . any ( self . thetaU [ i ] < self . thetaL [ i ] ) : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n return \n class MultiFiCoKrigingSurrogate ( MultiFiSurrogateModel ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , regr = '<STR_LIT>' , rho_regr = '<STR_LIT>' , \n theta = None , theta0 = None , thetaL = None , thetaU = None , \n tolerance = TOLERANCE_DEFAULT , initial_range = INITIAL_RANGE_DEFAULT ) : \n super ( MultiFiCoKrigingSurrogate , self ) . __init__ ( ) \n self . tolerance = tolerance \n self . initial_range = initial_range \n self . model = MultiFiCoKriging ( regr = regr , rho_regr = rho_regr , theta = theta , \n theta0 = theta0 , thetaL = thetaL , thetaU = thetaU ) \n def predict ( self , new_x ) : \n \"\"\"<STR_LIT>\"\"\" \n Y_pred , MSE = self . model . predict ( [ new_x ] ) \n return Y_pred , np . sqrt ( np . abs ( MSE ) ) \n def train_multifi ( self , X , Y ) : \n \"\"\"<STR_LIT>\"\"\" \n X , Y = self . _fit_adapter ( X , Y ) \n self . model . fit ( X , Y , tol = self . tolerance , initial_range = self . initial_range ) \n def _fit_adapter ( self , X , Y ) : \n if len ( np . shape ( np . array ( X [ <NUM_LIT:0> ] ) ) ) == <NUM_LIT:1> : \n X = [ X ] \n Y = [ Y ] \n X = [ np . array ( x ) for x in reversed ( X ) ] \n Y = [ np . array ( y ) for y in reversed ( Y ) ] \n return ( X , Y ) \n class FloatMultiFiCoKrigingSurrogate ( MultiFiCoKrigingSurrogate ) : \n \"\"\"<STR_LIT>\"\"\" \n def predict ( self , new_x ) : \n dist = super ( FloatMultiFiCoKrigingSurrogate , self ) . predict ( new_x ) \n return dist . mu \n if __name__ == \"<STR_LIT:__main__>\" : \n import doctest \n doctest . <mask0> ( ) \n", "gt": "testmod"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import print_function \n import re \n from six . moves import range \n from pyparsing import CaselessLiteral , Combine , OneOrMore , Optional , TokenConverter , Word , nums , oneOf , printables , ParserElement , alphanums \n import numpy as np \n __all__ = [ '<STR_LIT>' , '<STR_LIT>' ] \n def _getformat ( val ) : \n if int ( val ) == val : \n return \"<STR_LIT>\" \n else : \n return \"<STR_LIT>\" \n class _SubHelper ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n self . newtext = \"<STR_LIT>\" \n self . replace_location = <NUM_LIT:0> \n self . current_location = <NUM_LIT:0> \n self . counter = <NUM_LIT:0> \n self . start_location = <NUM_LIT:0> \n self . end_location = <NUM_LIT:0> \n def set ( self , newtext , location ) : \n \"\"\"<STR_LIT>\"\"\" \n self . newtext = newtext \n self . replace_location = location \n self . current_location = <NUM_LIT:0> \n def set_array ( self , newtext , start_location , end_location ) : \n \"\"\"<STR_LIT>\"\"\" \n self . newtext = newtext \n self . start_location = start_location \n self . end_location = end_location \n self . current_location = <NUM_LIT:0> \n def replace ( self , text ) : \n \"\"\"<STR_LIT>\"\"\" \n self . current_location += <NUM_LIT:1> \n if self . current_location == self . replace_location : \n if isinstance ( self . newtext , float ) : \n return _getformat ( self . newtext ) % self . newtext \n else : \n return str ( self . newtext ) \n else : \n return text . group ( ) \n def replace_array ( self , text ) : \n \"\"\"<STR_LIT>\"\"\" \n self . current_location += <NUM_LIT:1> \n end = len ( self . newtext ) \n if self . current_location >= self . start_location and self . current_location <= self . end_location and self . counter < end : \n if isinstance ( self . newtext [ self . counter ] , float ) : \n val = self . newtext [ self . counter ] \n newval = _getformat ( val ) % val \n else : \n newval = str ( self . newtext [ self . counter ] ) \n self . counter += <NUM_LIT:1> \n return newval \n else : \n return text . group ( ) \n class ToInteger ( TokenConverter ) : \n \"\"\"<STR_LIT>\"\"\" \n def postParse ( self , instring , loc , tokenlist ) : \n \"\"\"<STR_LIT>\"\"\" \n return int ( tokenlist [ <NUM_LIT:0> ] ) \n class ToFloat ( TokenConverter ) : \n \"\"\"<STR_LIT>\"\"\" \n def postParse ( self , instring , loc , tokenlist ) : \n \"\"\"<STR_LIT>\"\"\" \n return float ( tokenlist [ <NUM_LIT:0> ] . replace ( '<STR_LIT:D>' , '<STR_LIT:E>' ) ) \n class ToNan ( TokenConverter ) : \n \"\"\"<STR_LIT>\"\"\" \n def postParse ( self , instring , loc , tokenlist ) : \n \"\"\"<STR_LIT>\"\"\" \n return float ( '<STR_LIT>' ) \n class ToInf ( TokenConverter ) : \n \"\"\"<STR_LIT>\"\"\" \n def postParse ( self , instring , loc , tokenlist ) : \n \"\"\"<STR_LIT>\"\"\" \n return float ( '<STR_LIT>' ) \n class InputFileGenerator ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self ) : \n self . template_filename = [ ] \n self . output_filename = [ ] \n self . delimiter = \"<STR_LIT:U+0020>\" \n self . reg = re . compile ( '<STR_LIT>' ) \n self . data = [ ] \n self . current_row = <NUM_LIT:0> \n self . anchored = False \n def set_template_file ( self , filename ) : \n \"\"\"<STR_LIT>\"\"\" \n self . template_filename = filename \n templatefile = open ( filename , '<STR_LIT:r>' ) \n self . data = templatefile . readlines ( ) \n templatefile . close ( ) \n def set_generated_file ( self , filename ) : \n \"\"\"<STR_LIT>\"\"\" \n self . output_filename = filename \n def set_delimiters ( self , delimiter ) : \n \"\"\"<STR_LIT>\"\"\" \n self . delimiter = delimiter \n self . reg = re . compile ( '<STR_LIT>' + delimiter + '<STR_LIT>' ) \n def mark_anchor ( self , anchor , occurrence = <NUM_LIT:1> ) : \n \"\"\"<STR_LIT>\"\"\" \n if not isinstance ( occurrence , int ) : \n raise ValueError ( \"<STR_LIT>\" ) \n instance = <NUM_LIT:0> \n if occurrence > <NUM_LIT:0> : \n count = <NUM_LIT:0> \n max_lines = len ( self . data ) \n for index in range ( self . current_row , max_lines ) : \n line = self . data [ index ] \n if count == <NUM_LIT:0> and self . anchored : \n line = line . split ( anchor ) [ - <NUM_LIT:1> ] \n if line . find ( anchor ) > - <NUM_LIT:1> : \n instance += <NUM_LIT:1> \n if instance == occurrence : \n self . current_row += count \n self . anchored = True \n return \n count += <NUM_LIT:1> \n elif occurrence < <NUM_LIT:0> : \n max_lines = len ( self . data ) - <NUM_LIT:1> \n count = max_lines \n for index in range ( max_lines , - <NUM_LIT:1> , - <NUM_LIT:1> ) : \n line = self . data [ index ] \n if count == max_lines and self . anchored : \n line = line . split ( anchor ) [ <NUM_LIT:0> ] \n if line . find ( anchor ) > - <NUM_LIT:1> : \n instance += - <NUM_LIT:1> \n if instance == occurrence : \n self . current_row = count \n self . anchored = True \n return \n count -= <NUM_LIT:1> \n else : \n raise ValueError ( \"<STR_LIT>\" ) \n raise RuntimeError ( \"<STR_LIT>\" % ( anchor , self . template_filename ) ) \n def reset_anchor ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . current_row = <NUM_LIT:0> \n self . anchored = False \n def transfer_var ( self , value , row , field ) : \n \"\"\"<STR_LIT>\"\"\" \n j = self . current_row + row \n line = self . data [ j ] \n sub = _SubHelper ( ) \n sub . set ( value , field ) \n newline = re . sub ( self . reg , sub . replace , line ) \n self . data [ j ] = newline \n def transfer_array ( self , value , row_start , field_start , field_end , \n row_end = None , sep = \"<STR_LIT:U+002CU+0020>\" ) : \n \"\"\"<STR_LIT>\"\"\" \n if row_end is None : \n row_end = row_start \n sub = _SubHelper ( ) \n for row in range ( row_start , row_end + <NUM_LIT:1> ) : \n j = self . current_row + row \n line = self . data [ j ] \n if row == row_end : \n f_end = field_end \n else : \n f_end = <NUM_LIT> \n sub . set_array ( value , field_start , f_end ) \n field_start = <NUM_LIT:0> \n newline = re . sub ( self . reg , sub . replace_array , line ) \n self . data [ j ] = newline \n if sub . counter < len ( value ) : \n for val in value [ sub . counter : ] : \n newline = newline . rstrip ( ) + sep + str ( val ) \n self . data [ j ] = newline \n elif sub . counter > len ( value ) : \n raise ValueError ( \"<STR_LIT>\" ) \n self . data [ j ] += \"<STR_LIT:\\n>\" \n def transfer_2Darray ( self , value , row_start , row_end , field_start , \n field_end ) : \n \"\"\"<STR_LIT>\"\"\" \n sub = _SubHelper ( ) \n i = <NUM_LIT:0> \n for row in range ( row_start , row_end + <NUM_LIT:1> ) : \n j = self . current_row + row \n line = self . data [ j ] \n sub . set_array ( value [ i , : ] , field_start , field_end ) \n newline = re . sub ( self . reg , sub . replace_array , line ) \n self . data [ j ] = newline \n sub . current_location = <NUM_LIT:0> \n sub . counter = <NUM_LIT:0> \n i += <NUM_LIT:1> \n def clearline ( self , row ) : \n \"\"\"<STR_LIT>\"\"\" \n self . data [ self . current_row + row ] = \"<STR_LIT:\\n>\" \n def generate ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n infile = open ( self . output_filename , '<STR_LIT:w>' ) \n infile . writelines ( self . data ) \n infile . close ( ) \n class FileParser ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , end_of_line_comment_char = None , full_line_comment_char = None ) : \n self . filename = [ ] \n self . data = [ ] \n self . delimiter = \"<STR_LIT>\" \n self . end_of_line_comment_char = end_of_line_comment_char \n self . full_line_comment_char = full_line_comment_char \n self . current_row = <NUM_LIT:0> \n self . anchored = False \n self . set_delimiters ( self . delimiter ) \n def set_file ( self , filename ) : \n \"\"\"<STR_LIT>\"\"\" \n self . filename = filename \n inputfile = open ( filename , '<STR_LIT:r>' ) \n if not self . end_of_line_comment_char and not self . full_line_comment_char : \n self . data = inputfile . readlines ( ) \n else : \n self . data = [ ] \n for line in inputfile : \n if line [ <NUM_LIT:0> ] == self . full_line_comment_char : \n continue \n self . data . append ( line . split ( self . end_of_line_comment_char ) [ <NUM_LIT:0> ] ) \n inputfile . close ( ) \n def set_delimiters ( self , delimiter ) : \n \"\"\"<STR_LIT>\"\"\" \n self . delimiter = delimiter \n if delimiter != \"<STR_LIT>\" : \n ParserElement . setDefaultWhitespaceChars ( str ( delimiter ) ) \n self . _reset_tokens ( ) \n def mark_anchor ( self , anchor , occurrence = <NUM_LIT:1> ) : \n \"\"\"<STR_LIT>\"\"\" \n if not isinstance ( occurrence , int ) : \n raise ValueError ( \"<STR_LIT>\" ) \n instance = <NUM_LIT:0> \n if occurrence > <NUM_LIT:0> : \n count = <NUM_LIT:0> \n max_lines = len ( self . data ) \n for index in range ( self . current_row , max_lines ) : \n line = self . data [ index ] \n if count == <NUM_LIT:0> and self . anchored : \n line = line . split ( anchor ) [ - <NUM_LIT:1> ] \n if anchor in line : \n instance += <NUM_LIT:1> \n if instance == occurrence : \n self . current_row += count \n self . anchored = True \n return \n count += <NUM_LIT:1> \n elif occurrence < <NUM_LIT:0> : \n max_lines = len ( self . data ) - <NUM_LIT:1> \n count = max_lines \n for index in range ( max_lines , - <NUM_LIT:1> , - <NUM_LIT:1> ) : \n line = self . data [ index ] \n if count == max_lines and self . anchored : \n line = line . split ( anchor ) [ <NUM_LIT:0> ] \n if anchor in line : \n instance += - <NUM_LIT:1> \n if instance == occurrence : \n self . current_row = count \n self . anchored = True \n return \n count -= <NUM_LIT:1> \n else : \n raise ValueError ( \"<STR_LIT>\" ) \n raise RuntimeError ( \"<STR_LIT>\" % ( anchor , self . filename ) ) \n def reset_anchor ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . current_row = <NUM_LIT:0> \n self . anchored = False \n def transfer_line ( self , row ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . data [ self . current_row + row ] . rstrip ( ) \n def transfer_var ( self , row , field , fieldend = None ) : \n \"\"\"<STR_LIT>\"\"\" \n j = self . current_row + row \n line = self . data [ j ] \n if self . delimiter == \"<STR_LIT>\" : \n if not fieldend : \n line = line [ ( field - <NUM_LIT:1> ) : ] \n else : \n line = line [ ( field - <NUM_LIT:1> ) : ( fieldend ) ] \n data = self . _parse_line ( ) . parseString ( line ) \n if len ( data ) > <NUM_LIT:1> : \n return line \n else : \n return data [ <NUM_LIT:0> ] \n else : \n data = self . _parse_line ( ) . parseString ( line ) \n return data [ field - <NUM_LIT:1> ] \n def transfer_keyvar ( self , key , field , occurrence = <NUM_LIT:1> , rowoffset = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n if not isinstance ( occurrence , int ) or occurrence == <NUM_LIT:0> : \n msg = \"<STR_LIT>\" \n raise ValueError ( msg ) \n instance = <NUM_LIT:0> \n if occurrence > <NUM_LIT:0> : \n row = <NUM_LIT:0> \n for line in self . data [ self . current_row : ] : \n if line . find ( key ) > - <NUM_LIT:1> : \n instance += <NUM_LIT:1> \n if instance == occurrence : \n break \n row += <NUM_LIT:1> \n elif occurrence < <NUM_LIT:0> : \n row = - <NUM_LIT:1> \n for line in reversed ( self . data [ self . current_row : ] ) : \n if line . find ( key ) > - <NUM_LIT:1> : \n instance += - <NUM_LIT:1> \n if instance == occurrence : \n break \n row -= <NUM_LIT:1> \n j = self . current_row + row + rowoffset \n line = self . data [ j ] \n fields = self . _parse_line ( ) . parseString ( line . replace ( key , \"<STR_LIT>\" ) ) \n return fields [ field ] \n def transfer_array ( self , rowstart , fieldstart , rowend = None , fieldend = None ) : \n \"\"\"<STR_LIT>\"\"\" \n j1 = self . current_row + rowstart \n if rowend is None : \n j2 = j1 + <NUM_LIT:1> \n else : \n j2 = self . current_row + rowend + <NUM_LIT:1> \n if not fieldend : \n raise ValueError ( \"<STR_LIT>\" ) \n lines = self . data [ j1 : j2 ] \n data = np . zeros ( shape = ( <NUM_LIT:0> , <NUM_LIT:0> ) ) \n for i , line in enumerate ( lines ) : \n if self . delimiter == \"<STR_LIT>\" : \n line = line [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] \n line = line . strip ( ) \n parsed = self . _parse_line ( ) . parseString ( line ) \n newdata = np . array ( parsed [ : ] ) \n if newdata . dtype . type is np . str_ : \n newdata = np . array ( line ) \n data = np . append ( data , newdata ) \n else : \n parsed = self . _parse_line ( ) . parseString ( line ) \n if i == j2 - j1 - <NUM_LIT:1> : \n data = np . append ( data , np . array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] ) ) \n else : \n data = np . append ( data , np . array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : ] ) ) \n fieldstart = <NUM_LIT:1> \n return data \n def transfer_2Darray ( self , rowstart , fieldstart , rowend , fieldend = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if fieldend and ( fieldstart > fieldend ) : \n msg = \"<STR_LIT>\" \n raise ValueError ( msg ) \n if rowstart > rowend : \n msg = \"<STR_LIT>\" \n raise ValueError ( msg ) \n j1 = self . current_row + rowstart \n j2 = self . current_row + rowend + <NUM_LIT:1> \n lines = list ( self . data [ j1 : j2 ] ) \n if self . delimiter == \"<STR_LIT>\" : \n if fieldend : \n line = lines [ <NUM_LIT:0> ] [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] \n else : \n line = lines [ <NUM_LIT:0> ] [ ( fieldstart - <NUM_LIT:1> ) : ] \n parsed = self . _parse_line ( ) . parseString ( line ) \n row = np . array ( parsed [ : ] ) \n data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n data [ <NUM_LIT:0> , : ] = row \n for i , line in enumerate ( list ( lines [ <NUM_LIT:1> : ] ) ) : \n if fieldend : \n line = line [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] \n else : \n line = line [ ( fieldstart - <NUM_LIT:1> ) : ] \n parsed = self . _parse_line ( ) . parseString ( line ) \n data [ i + <NUM_LIT:1> , : ] = np . array ( parsed [ : ] ) \n else : \n parsed = self . _parse_line ( ) . parseString ( lines [ <NUM_LIT:0> ] ) \n if fieldend : \n row = np . array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] ) \n else : \n row = np . array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : ] ) \n data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n data [ <NUM_LIT:0> , : ] = row \n for i , line in enumerate ( list ( lines [ <NUM_LIT:1> : ] ) ) : \n parsed = self . _parse_line ( ) . parseString ( line ) \n if fieldend : \n try : \n data [ i + <NUM_LIT:1> , : ] = np . array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : fieldend ] ) \n except : \n print ( data ) \n else : \n data [ i + <NUM_LIT:1> , : ] = np . array ( parsed [ ( fieldstart - <NUM_LIT:1> ) : ] ) \n return data \n def _parse_line ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . line_parse_token \n def _reset_tokens ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . delimiter . isspace ( ) : \n textchars = printables \n else : \n textchars = alphanums \n symbols = [ '<STR_LIT:.>' , '<STR_LIT:/>' , '<STR_LIT:+>' , '<STR_LIT:*>' , '<STR_LIT>' , '<STR_LIT:(>' , '<STR_LIT:)>' , '<STR_LIT:[>' , '<STR_LIT:]>' , '<STR_LIT:=>' , \n '<STR_LIT::>' , '<STR_LIT:;>' , '<STR_LIT:?>' , '<STR_LIT:%>' , '<STR_LIT:&>' , '<STR_LIT:!>' , '<STR_LIT:#>' , '<STR_LIT:|>' , '<STR_LIT:<>' , '<STR_LIT:>>' , \n '<STR_LIT:{>' , '<STR_LIT:}>' , '<STR_LIT:->' , '<STR_LIT:_>' , '<STR_LIT:@>' , '<STR_LIT:$>' , '<STR_LIT>' ] \n for symbol in symbols : \n if symbol not in self . delimiter : \n textchars = textchars + symbol \n digits = Word ( nums ) \n dot = \"<STR_LIT:.>\" \n sign = oneOf ( \"<STR_LIT>\" ) \n ee = CaselessLiteral ( '<STR_LIT:E>' ) | CaselessLiteral ( '<STR_LIT:D>' ) \n num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n num_float = ToFloat ( Combine ( Optional ( sign ) + \n ( ( digits + dot + Optional ( digits ) ) | \n ( dot + digits ) ) + \n Optional ( ee + Optional ( sign ) + digits ) \n ) ) \n mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n nan = ToInf ( oneOf ( \"<STR_LIT>\" ) ) | ToNan ( oneOf ( \"<STR_LIT>\" + \"<STR_LIT>\" ) ) \n string_text = Word ( textchars ) \n self . line_parse_token = ( OneOrMore ( ( nan | num_float | mixed_exp | num_int | \n <mask0> ) ) ) \n", "gt": "string_text"}
{"input": "\n from django import template \n register = template . Library ( ) \n @ register . filter ( name = '<STR_LIT>' ) \n def get_item ( dictionary , key ) : \n return getattr ( dictionary , <mask0> ) \n", "gt": "key"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n import os \n import time \n if sys . version_info < ( <NUM_LIT:2> , <NUM_LIT:6> , <NUM_LIT:0> ) : \n print ( '<STR_LIT>' \n '<STR_LIT>' ) \n elif sys . version_info >= ( <NUM_LIT:3> , <NUM_LIT:0> , <NUM_LIT:0> ) : \n sys . stderr . write ( '<STR_LIT>' \n '<STR_LIT>' ) \n sys . exit ( <NUM_LIT:2> ) \n sys . path . insert ( <NUM_LIT:0> , '<STR_LIT>' ) \n sys . path . insert ( <NUM_LIT:0> , os . getcwd ( ) ) \n import shutil \n import optparse \n import requests \n import subprocess \n try : \n import instances_creator_conf as icc \n import psycopg2 \n except ImportError : \n sys . stderr . write ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n sys . exit ( <NUM_LIT:3> ) \n os . environ [ '<STR_LIT>' ] = icc . DB_PASSWORD \n class AskbotInstance ( ) : \n def __init__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if not os . environ [ '<STR_LIT>' ] == '<STR_LIT:root>' : \n self . abort ( '<STR_LIT>' ) \n sys . path . insert ( <NUM_LIT:0> , icc . DEFAULT_INSTANCE_DIR ) \n def _populate_file ( self , original_file , values ) : \n \"\"\"<STR_LIT>\"\"\" \n f = open ( original_file , '<STR_LIT:r>' ) \n file_content = f . read ( ) \n f . close ( ) \n populated_settings = file_content . format ( ** values ) \n f = open ( original_file , '<STR_LIT:w>' ) \n f . write ( populated_settings ) \n f . close ( ) \n def create_instance ( self , instance_name , instance_db_name ) : \n \"\"\"<STR_LIT>\"\"\" \n INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n try : \n shutil . copytree ( icc . SKEL_DIR , INSTANCE_DIR ) \n os . chdir ( INSTANCE_DIR ) \n template = os . path . join ( INSTANCE_DIR , '<STR_LIT>' ) \n os . symlink ( \n os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , \n os . path . join ( INSTANCE_DIR , '<STR_LIT>' ) ) \n values = { \n '<STR_LIT>' : instance_name , \n '<STR_LIT>' : instance_db_name , \n '<STR_LIT>' : icc . DB_HOST , \n '<STR_LIT>' : icc . BASE_URL \n } \n self . _populate_file ( template , values ) \n print ( '<STR_LIT>' . format ( instance_name ) ) \n except : \n self . abort ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n def create_db ( self , instance_db_name ) : \n \"\"\"<STR_LIT>\"\"\" \n createdb = subprocess . Popen ( ( '<STR_LIT>' \n '<STR_LIT>' ) % ( instance_db_name , \n icc . DB_USER ) , shell = True ) \n createdb . wait ( ) \n try : \n psycopg2 . connect ( \n database = instance_db_name , \n user = icc . DB_USER , \n password = icc . DB_PASSWORD , \n host = icc . DB_HOST \n ) \n print ( '<STR_LIT>' \n '<STR_LIT>' . format ( instance_db_name ) ) \n except : \n self . abort ( '<STR_LIT>' \n '<STR_LIT>' ) \n def syncdb_and_migrate ( self , instance_name ) : \n \"\"\"<STR_LIT>\"\"\" \n working_dir = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n os . chdir ( working_dir ) \n syncdb = subprocess . Popen ( ( '<STR_LIT>' \n '<STR_LIT>' ) , shell = True ) \n syncdb . wait ( ) \n def collect_static ( seld , instance_name ) : \n \"\"\"<STR_LIT>\"\"\" \n working_dir = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n os . chdir ( working_dir ) \n collectstatic = subprocess . Popen ( ( '<STR_LIT>' \n '<STR_LIT>' ) , \n shell = True ) \n collectstatic . wait ( ) \n def add_instance_to_supervisor ( self , instance_name ) : \n \"\"\"<STR_LIT>\"\"\" \n INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n try : \n template = os . path . join ( INSTANCE_DIR , '<STR_LIT>' ) \n values = { \n '<STR_LIT>' : instance_name , \n '<STR_LIT>' : INSTANCE_DIR \n } \n self . _populate_file ( template , values ) \n os . symlink ( template , os . path . join ( \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' % instance_name ) ) \n print ( '<STR_LIT>' ) \n except : \n self . abort ( '<STR_LIT>' \n '<STR_LIT>' ) \n def add_instance_to_nginx ( self , instance_name ) : \n \"\"\"<STR_LIT>\"\"\" \n INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n try : \n template = os . path . join ( INSTANCE_DIR , '<STR_LIT>' ) \n values = { '<STR_LIT>' : instance_name } \n self . _populate_file ( template , values ) \n template = os . path . join ( INSTANCE_DIR , '<STR_LIT>' ) \n values = { '<STR_LIT>' : instance_name } \n self . _populate_file ( template , values ) \n print ( '<STR_LIT>' ) \n except : \n self . abort ( '<STR_LIT>' \n '<STR_LIT>' ) \n def restart_server ( self ) : \n supervisord = subprocess . Popen ( ( '<STR_LIT>' \n '<STR_LIT>' ) , shell = True ) \n supervisord . wait ( ) \n nginx = subprocess . Popen ( ( '<STR_LIT>' \n '<STR_LIT>' ) , shell = True ) \n nginx . wait ( ) \n time . sleep ( <NUM_LIT:5> ) \n def update_entries_metadata ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n update_metadata = subprocess . Popen ( ( '<STR_LIT>' \n '<STR_LIT>' ) , \n shell = True ) \n update_metadata . wait ( ) \n url = ( '<STR_LIT>' \n '<STR_LIT>' ) % icc . META_REFRESH_KEY \n requests . get ( url ) \n except : \n self . abort ( '<STR_LIT>' \n '<STR_LIT>' ) \n def disable_instance ( self , instance_name ) : \n \"\"\"<STR_LIT>\"\"\" \n INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n try : \n if not os . path . isdir ( icc . DEFAULT_DISABLED_INSTANCES_DIR ) : \n os . makedirs ( icc . DEFAULT_DISABLED_INSTANCES_DIR ) \n shutil . copy ( INSTANCE_DIR , icc . DEFAULT_DISABLED_INSTANCES_DIR ) \n shutil . rmtree ( INSTANCE_DIR ) \n print ( '<STR_LIT>' . format ( instance_name ) ) \n except : \n self . abort ( '<STR_LIT>' \n '<STR_LIT>' ) \n def destroy_instance ( self , instance_name ) : \n \"\"\"<STR_LIT>\"\"\" \n INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n sys . path . insert ( <NUM_LIT:0> , INSTANCE_DIR ) \n try : \n import instance_settings \n except : \n self . abort ( '<STR_LIT>' \n '<STR_LIT>' ) \n try : \n instance_db_name = instance_settings . DATABASE_NAME \n dropdb = subprocess . Popen ( '<STR_LIT>' % \n instance_db_name , shell = True ) \n dropdb . wait ( ) \n shutil . rmtree ( INSTANCE_DIR ) \n os . remove ( os . path . join ( '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' % instance_name ) ) \n print ( '<STR_LIT>' . format ( instance_name ) ) \n except : \n self . abort ( '<STR_LIT>' \n '<STR_LIT>' ) \n def abort ( self , msg , status = <NUM_LIT:1> ) : \n sys . stderr . write ( msg ) \n sys . exit ( status ) \n parser = optparse . OptionParser ( \n description = ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) , \n version = '<STR_LIT>' \n ) \n parser . add_option ( \n '<STR_LIT:-c>' , \n '<STR_LIT>' , \n help = '<STR_LIT>' , \n dest = '<STR_LIT>' , \n action = '<STR_LIT:store>' , \n nargs = <NUM_LIT:2> \n ) \n parser . add_option ( \n '<STR_LIT>' , \n '<STR_LIT>' , \n help = '<STR_LIT>' , \n dest = '<STR_LIT>' \n ) \n parser . add_option ( \n '<STR_LIT>' , \n '<STR_LIT>' , \n help = '<STR_LIT>' , \n dest = '<STR_LIT>' \n ) \n parser . add_option ( \n '<STR_LIT>' , \n help = '<STR_LIT>' , \n action = '<STR_LIT:store_true>' , \n dest = '<STR_LIT>' \n ) \n ( opts , args ) = parser . parse_args ( ) \n inst = AskbotInstance ( ) \n if opts . instance_data : \n INSTANCE_NAME = opts . instance_data [ <NUM_LIT:0> ] \n INSTANCE_DB_NAME = opts . instance_data [ <NUM_LIT:1> ] \n inst . create_instance ( INSTANCE_NAME , INSTANCE_DB_NAME ) \n inst . add_instance_to_supervisor ( INSTANCE_NAME ) \n inst . add_instance_to_nginx ( INSTANCE_NAME ) \n inst . create_db ( INSTANCE_DB_NAME ) \n inst . syncdb_and_migrate ( INSTANCE_NAME ) \n inst . collect_static ( INSTANCE_NAME ) \n inst . restart_server ( ) \n if not opts . no_metadata : \n inst . update_entries_metadata ( ) \n elif opts . disable_instance_name : \n INSTANCE_NAME = opts . disable_instance_name \n inst . disable_instance ( INSTANCE_NAME ) \n elif opts . destroy_instance_name : \n INSTANCE_NAME = opts . destroy_instance_name \n inst . <mask0> ( INSTANCE_NAME ) \n", "gt": "destroy_instance"}
{"input": "\n <mask0> = '<STR_LIT>' \n", "gt": "default_app_config"}
{"input": "\n from . . utils . access_permissions import BaseAccessPermissions \n class MediafileAccessPermissions ( BaseAccessPermissions ) : \n \"\"\"<STR_LIT>\"\"\" \n def can_retrieve ( self , user ) : \n \"\"\"<STR_LIT>\"\"\" \n return user . has_perm ( '<STR_LIT>' ) \n def get_serializer_class ( self , user = None ) : \n \"\"\"<STR_LIT>\"\"\" \n from . serializers import MediafileSerializer \n return <mask0> \n", "gt": "MediafileSerializer"}
{"input": "\n from __future__ import unicode_literals \n from django . db import migrations , models \n import openslides . utils . models \n class Migration ( migrations . Migration ) : \n initial = True \n dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ] \n operations = [ \n migrations . CreateModel ( \n name = '<STR_LIT>' , \n fields = [ \n ( '<STR_LIT:id>' , models . AutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name = '<STR_LIT>' ) ) , \n ( '<STR_LIT:password>' , models . CharField ( max_length = <NUM_LIT> , verbose_name = '<STR_LIT:password>' ) ) , \n ( '<STR_LIT>' , models . DateTimeField ( blank = True , null = True , verbose_name = '<STR_LIT>' ) ) , \n ( '<STR_LIT>' , models . BooleanField ( \n default = False , \n help_text = '<STR_LIT>' , \n verbose_name = '<STR_LIT>' ) ) , \n ( '<STR_LIT:username>' , models . CharField ( blank = True , max_length = <NUM_LIT:255> , unique = True ) ) , \n ( '<STR_LIT>' , models . CharField ( blank = True , max_length = <NUM_LIT:255> ) ) , \n ( '<STR_LIT>' , models . CharField ( blank = True , max_length = <NUM_LIT:255> ) ) , \n ( '<STR_LIT>' , models . CharField ( blank = True , default = '<STR_LIT>' , max_length = <NUM_LIT:255> ) ) , \n ( '<STR_LIT:title>' , models . CharField ( blank = True , default = '<STR_LIT>' , max_length = <NUM_LIT:50> ) ) , \n ( '<STR_LIT>' , models . TextField ( blank = True , default = '<STR_LIT>' ) ) , \n ( '<STR_LIT>' , models . TextField ( blank = True , default = '<STR_LIT>' ) ) , \n ( '<STR_LIT>' , models . CharField ( blank = True , default = '<STR_LIT>' , max_length = <NUM_LIT:100> ) ) , \n ( '<STR_LIT>' , models . BooleanField ( default = True ) ) , \n ( '<STR_LIT>' , models . BooleanField ( default = False ) ) , \n ( '<STR_LIT>' , models . ManyToManyField ( \n blank = True , \n help_text = '<STR_LIT>' , \n related_name = '<STR_LIT>' , \n related_query_name = '<STR_LIT:user>' , \n to = '<STR_LIT>' , \n verbose_name = '<STR_LIT>' ) ) , \n ( '<STR_LIT>' , models . ManyToManyField ( \n blank = True , \n help_text = '<STR_LIT>' , \n related_name = '<STR_LIT>' , \n related_query_name = '<STR_LIT:user>' , \n to = '<STR_LIT>' , \n verbose_name = '<STR_LIT>' ) ) , \n ] , \n options = { \n '<STR_LIT>' : ( \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) ) , \n '<STR_LIT>' : ( ) , \n '<STR_LIT>' : ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT:username>' ) , \n } , \n bases = ( openslides . utils . models . RESTModelMixin , models . <mask0> ) , \n ) , \n ] \n", "gt": "Model"}
{"input": "\n import json \n from django . core . urlresolvers import reverse \n from django . dispatch import receiver \n from rest_framework import status \n from rest_framework . test import APIClient \n from openslides import __version__ as version \n from openslides . core . config import ConfigVariable , config \n from openslides . core . models import CustomSlide , Projector \n from openslides . core . signals import config_signal \n from openslides . utils . rest_api import ValidationError \n from openslides . utils . test import TestCase \n class ProjectorAPI ( TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def test_slide_on_default_projector ( self ) : \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n customslide = CustomSlide . objects . create ( title = '<STR_LIT>' , text = '<STR_LIT>' ) \n default_projector = Projector . objects . get ( pk = <NUM_LIT:1> ) \n default_projector . config = { \n '<STR_LIT>' : { '<STR_LIT:name>' : '<STR_LIT>' , '<STR_LIT:id>' : customslide . id } } \n default_projector . save ( ) \n response = self . client . get ( reverse ( '<STR_LIT>' , args = [ '<STR_LIT:1>' ] ) ) \n self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n '<STR_LIT:id>' : <NUM_LIT:1> , \n '<STR_LIT>' : { \n '<STR_LIT>' : \n { '<STR_LIT:id>' : customslide . id , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:name>' : '<STR_LIT>' } } , \n '<STR_LIT>' : <NUM_LIT:0> , \n '<STR_LIT>' : <NUM_LIT:0> } ) \n def test_invalid_slide_on_default_projector ( self ) : \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n default_projector = Projector . objects . get ( pk = <NUM_LIT:1> ) \n default_projector . config = { \n '<STR_LIT>' : { '<STR_LIT:name>' : '<STR_LIT>' } } \n default_projector . save ( ) \n response = self . client . get ( reverse ( '<STR_LIT>' , args = [ '<STR_LIT:1>' ] ) ) \n self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n '<STR_LIT:id>' : <NUM_LIT:1> , \n '<STR_LIT>' : { \n '<STR_LIT>' : \n { '<STR_LIT:name>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:error>' : '<STR_LIT>' } } , \n '<STR_LIT>' : <NUM_LIT:0> , \n '<STR_LIT>' : <NUM_LIT:0> } ) \n class VersionView ( TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def test_get ( self ) : \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n response = self . client . get ( reverse ( '<STR_LIT>' ) ) \n self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n '<STR_LIT>' : version , \n '<STR_LIT>' : [ \n { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:description>' : '<STR_LIT>' , \n '<STR_LIT:version>' : '<STR_LIT>' } ] } ) \n class ConfigViewSet ( TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def test_retrieve ( self ) : \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n config [ '<STR_LIT>' ] = '<STR_LIT>' \n response = self . client . get ( reverse ( '<STR_LIT>' , args = [ '<STR_LIT>' ] ) ) \n self . assertEqual ( \n response . data , \n { '<STR_LIT:key>' : '<STR_LIT>' , \n '<STR_LIT:value>' : '<STR_LIT>' } ) \n def test_update ( self ) : \n self . client = APIClient ( ) \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n response = self . client . put ( \n reverse ( '<STR_LIT>' , args = [ '<STR_LIT>' ] ) , \n { '<STR_LIT:value>' : '<STR_LIT>' } ) \n self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n self . assertEqual ( config [ '<STR_LIT>' ] , '<STR_LIT>' ) \n def test_update_wrong_datatype ( self ) : \n self . client = APIClient ( ) \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n response = self . client . put ( \n reverse ( '<STR_LIT>' , args = [ '<STR_LIT>' ] ) , \n { '<STR_LIT:value>' : '<STR_LIT>' } ) \n self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n self . assertEqual ( response . data , { '<STR_LIT>' : \"<STR_LIT>\" } ) \n def test_update_wrong_datatype_that_can_be_converted ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . client = APIClient ( ) \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n response = self . client . put ( \n reverse ( '<STR_LIT>' , args = [ '<STR_LIT>' ] ) , \n { '<STR_LIT:value>' : '<STR_LIT>' } ) \n self . assertEqual ( response . status_code , <NUM_LIT:200> ) \n def test_update_good_choice ( self ) : \n self . client = APIClient ( ) \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n response = self . client . put ( \n reverse ( '<STR_LIT>' , args = [ '<STR_LIT>' ] ) , \n { '<STR_LIT:value>' : '<STR_LIT>' } ) \n self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n self . assertEqual ( config [ '<STR_LIT>' ] , '<STR_LIT>' ) \n def test_update_bad_choice ( self ) : \n self . client = APIClient ( ) \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n response = self . client . put ( \n reverse ( '<STR_LIT>' , args = [ '<STR_LIT>' ] ) , \n { '<STR_LIT:value>' : '<STR_LIT>' } ) \n self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n self . assertEqual ( response . data , { '<STR_LIT>' : '<STR_LIT>' } ) \n def test_update_validator_ok ( self ) : \n self . client = APIClient ( ) \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n response = self . client . put ( \n reverse ( '<STR_LIT>' , args = [ '<STR_LIT>' ] ) , \n { '<STR_LIT:value>' : '<STR_LIT>' } ) \n self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n self . assertEqual ( config [ '<STR_LIT>' ] , '<STR_LIT>' ) \n def test_update_validator_invalid ( self ) : \n self . client = APIClient ( ) \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n response = self . client . put ( \n reverse ( '<STR_LIT>' , args = [ '<STR_LIT>' ] ) , \n { '<STR_LIT:value>' : '<STR_LIT>' } ) \n self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n self . assertEqual ( response . data , { '<STR_LIT>' : '<STR_LIT>' } ) \n def test_update_only_with_key ( self ) : \n self . client = APIClient ( ) \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n response = self . client . put ( \n reverse ( '<STR_LIT>' , args = [ '<STR_LIT>' ] ) ) \n self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n self . assertEqual ( response . data , { '<STR_LIT>' : '<STR_LIT>' } ) \n def test_metadata_with_hidden ( self ) : \n self . client . login ( username = '<STR_LIT>' , password = '<STR_LIT>' ) \n response = self . client . options ( reverse ( '<STR_LIT>' ) ) \n filter_obj = filter ( \n lambda item : item [ '<STR_LIT:key>' ] == '<STR_LIT>' , \n response . data [ '<STR_LIT>' ] [ <NUM_LIT:0> ] [ '<STR_LIT>' ] [ <NUM_LIT:0> ] [ '<STR_LIT>' ] ) \n self . assertEqual ( len ( list ( filter_obj ) ) , <NUM_LIT:0> ) \n def validator_for_testing ( value ) : \n \"\"\"<STR_LIT>\"\"\" \n if value == '<STR_LIT>' : \n raise ValidationError ( { '<STR_LIT>' : '<STR_LIT>' } ) \n @ receiver ( config_signal , dispatch_uid = '<STR_LIT>' ) \n def set_simple_config_view_integration_config_test ( sender , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n yield ConfigVariable ( \n name = '<STR_LIT>' , \n default_value = None , \n label = '<STR_LIT>' ) \n yield ConfigVariable ( \n name = '<STR_LIT>' , \n default_value = '<STR_LIT>' ) \n yield ConfigVariable ( \n name = '<STR_LIT>' , \n default_value = <NUM_LIT:0> , \n input_type = '<STR_LIT>' ) \n yield ConfigVariable ( \n name = '<STR_LIT>' , \n default_value = '<STR_LIT>' , \n input_type = '<STR_LIT>' , \n choices = ( \n { '<STR_LIT:value>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT:value>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ) ) \n yield ConfigVariable ( \n name = '<STR_LIT>' , \n default_value = '<STR_LIT>' , \n validators = ( validator_for_testing , ) ) \n yield ConfigVariable ( \n name = '<STR_LIT>' , \n default_value = None , \n label = '<STR_LIT>' , \n hidden = <mask0> ) \n", "gt": "True"}
{"input": "\n from unittest import TestCase \n from unittest . mock import MagicMock , patch \n from openslides . users . serializers import UserFullSerializer \n from openslides . utils . rest_api import ValidationError \n class UserCreateUpdateSerializerTest ( TestCase ) : \n def test_validate_no_data ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n serializer = UserFullSerializer ( ) \n data = { } \n with self . assertRaises ( ValidationError ) : \n serializer . validate ( data ) \n @ patch ( '<STR_LIT>' ) \n def test_validate_no_username ( self , generate_username ) : \n \"\"\"<STR_LIT>\"\"\" \n generate_username . return_value = '<STR_LIT>' \n serializer = UserFullSerializer ( ) \n data = { '<STR_LIT>' : '<STR_LIT>' } \n new_data = serializer . validate ( data ) \n self . assertEqual ( new_data [ '<STR_LIT:username>' ] , '<STR_LIT>' ) \n def test_validate_no_username_in_patch_request ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n view = MagicMock ( action = '<STR_LIT>' ) \n serializer = UserFullSerializer ( context = { '<STR_LIT>' : view } ) \n data = { '<STR_LIT>' : '<STR_LIT>' } \n new_data = serializer . validate ( data ) \n self . assertIsNone ( new_data . get ( <mask0> ) ) \n", "gt": "'<STR_LIT:username>'"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import re \n URL_REGEX = re . compile ( \n r'<STR_LIT>' \n r'<STR_LIT>' \n r'<STR_LIT>' \n r'<STR_LIT>' \n r'<STR_LIT>' \n r'<STR_LIT>' , re . IGNORECASE ) \n USERNAME_REGEX = re . compile ( r'<STR_LIT>' , re . I ) \n FULLNAME_REGEX = re . compile ( r'<STR_LIT>' , re . U ) \n EMAIL_REGEX = re . compile ( r'<STR_LIT>' , re . IGNORECASE ) \n class CheckValue ( object ) : \n def __init__ ( self ) : \n pass \n def length ( self , data , minimum = - <NUM_LIT:1> , maximum = - <NUM_LIT:1> ) : \n \"\"\"<STR_LIT>\"\"\" \n len_input = len ( data ) \n if len_input < minimum or maximum != - <NUM_LIT:1> and len_input > maximum : \n return False \n return True \n def regexp ( self , data , regex , flags = <NUM_LIT:0> ) : \n \"\"\"<STR_LIT>\"\"\" \n regex = re . compile ( regex , flags ) \n if regex . match ( data ) : \n return True \n return False \n def username ( self , data ) : \n \"\"\"<STR_LIT>\"\"\" \n if USERNAME_REGEX . match ( data ) : \n return True \n return False \n def full_name ( self , data ) : \n \"\"\"<STR_LIT>\"\"\" \n if FULLNAME_REGEX . match ( data ) : \n return True \n return False \n def email ( self , data ) : \n \"\"\"<STR_LIT>\"\"\" \n if EMAIL_REGEX . match ( data ) : \n return True \n return False \n def url ( self , data ) : \n \"\"\"<STR_LIT>\"\"\" \n if URL_REGEX . match ( data ) : \n return True \n return False \n def url_two ( self , data ) : \n \"\"\"<STR_LIT>\"\"\" \n regex = re . compile ( r'<STR_LIT>' , re . IGNORECASE ) \n if regex . match ( data ) : \n return True \n return False \n def is_integer ( self , data ) : \n \"\"\"<STR_LIT:U+0020>\"\"\" \n try : \n tmp = int ( data ) \n return True \n except : \n return False \n def float ( self , data ) : \n \"\"\"<STR_LIT:U+0020>\"\"\" \n try : \n tmp = float ( data ) \n return True \n except : \n return <mask0> \n", "gt": "False"}
{"input": "\n import logging \n import socket \n from bagpipe . bgp . common import utils \n from bagpipe . bgp . common import logDecorator \n from bagpipe . bgp . vpn . vpn_instance import VPNInstance \n from bagpipe . bgp . engine import RouteEvent \n from bagpipe . bgp . vpn . dataplane_drivers import DummyDataplaneDriver as _DummyDataplaneDriver \n from bagpipe . bgp . common . looking_glass import LookingGlass , LGMap \n from bagpipe . exabgp . structure . vpn import RouteDistinguisher , VPNLabelledPrefix \n from bagpipe . exabgp . structure . mpls import LabelStackEntry \n from bagpipe . exabgp . structure . address import AFI , SAFI \n from bagpipe . exabgp . structure . ip import Inet , Prefix \n from bagpipe . exabgp . message . update . route import Route \n from bagpipe . exabgp . message . update . attribute . nexthop import NextHop \n from bagpipe . exabgp . message . update . attribute . communities import ECommunities \n class DummyDataplaneDriver ( _DummyDataplaneDriver ) : \n pass \n class VRF ( VPNInstance , LookingGlass ) : \n type = \"<STR_LIT>\" \n afi = AFI ( AFI . ipv4 ) \n safi = SAFI ( SAFI . mpls_vpn ) \n @ logDecorator . log \n def __init__ ( self , * args , ** kwargs ) : \n VPNInstance . __init__ ( self , * args , ** kwargs ) \n self . readvertised = set ( ) \n def _routeFrom ( self , prefix , label , rd ) : \n return Route ( VPNLabelledPrefix ( self . afi , self . safi , prefix , rd , \n [ LabelStackEntry ( label , True ) ] \n ) ) \n def generateVifBGPRoute ( self , macAdress , ipPrefix , prefixLen , label ) : \n route = self . _routeFrom ( Prefix ( self . afi , ipPrefix , prefixLen ) , label , \n RouteDistinguisher ( \n RouteDistinguisher . TYPE_IP_LOC , None , \n self . bgpManager . getLocalAddress ( ) , \n self . instanceId ) \n ) \n self . log . debug ( \"<STR_LIT>\" , route . attributes ) \n return self . _newRouteEntry ( self . afi , self . safi , self . exportRTs , \n route . nlri , route . attributes ) \n def _getLocalLabels ( self ) : \n for portData in self . macAddress2LocalPortData . itervalues ( ) : \n yield portData [ '<STR_LIT:label>' ] \n def _getRDFromLabel ( self , label ) : \n return RouteDistinguisher ( RouteDistinguisher . TYPE_IP_LOC , None , \n self . bgpManager . getLocalAddress ( ) , \n <NUM_LIT> + label ) \n def _routeForReAdvertisement ( self , prefix , label ) : \n route = self . _routeFrom ( prefix , label , \n self . _getRDFromLabel ( label ) ) \n nh = Inet ( <NUM_LIT:1> , socket . inet_pton ( socket . AF_INET , \n self . dataplane . driver . getLocalAddress ( ) ) ) \n route . attributes . add ( NextHop ( nh ) ) \n route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n routeEntry = self . _newRouteEntry ( self . afi , self . safi , \n self . readvertiseToRTs , \n route . nlri , route . attributes ) \n return routeEntry \n @ logDecorator . log \n def _readvertise ( self , nlri ) : \n self . log . debug ( \"<STR_LIT>\" , nlri . prefix ) \n for label in self . _getLocalLabels ( ) : \n self . log . debug ( \"<STR_LIT>\" , \n nlri . prefix , label ) \n routeEntry = self . _routeForReAdvertisement ( nlri . prefix , label ) \n self . _pushEvent ( RouteEvent ( RouteEvent . ADVERTISE , routeEntry ) ) \n self . readvertised . add ( nlri . prefix ) \n @ logDecorator . log \n def _readvertiseStop ( self , nlri ) : \n self . log . debug ( \"<STR_LIT>\" , nlri . prefix ) \n for label in self . _getLocalLabels ( ) : \n self . log . debug ( \"<STR_LIT>\" , \n nlri . prefix , label ) \n routeEntry = self . _routeForReAdvertisement ( nlri . prefix , label ) \n self . _pushEvent ( RouteEvent ( RouteEvent . WITHDRAW , routeEntry ) ) \n self . readvertised . remove ( nlri . prefix ) \n def vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n advertiseSubnet ) : \n VPNInstance . vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n advertiseSubnet ) \n label = self . macAddress2LocalPortData [ macAddress ] [ '<STR_LIT:label>' ] \n for prefix in self . readvertised : \n self . log . debug ( \"<STR_LIT>\" , \n prefix ) \n routeEntry = self . _routeForReAdvertisement ( prefix , label ) \n self . _pushEvent ( RouteEvent ( RouteEvent . ADVERTISE , routeEntry ) ) \n def vifUnplugged ( self , macAddress , ipAddressPrefix , advertiseSubnet ) : \n label = self . macAddress2LocalPortData [ macAddress ] [ '<STR_LIT:label>' ] \n for prefix in self . readvertised : \n self . log . debug ( \"<STR_LIT>\" , \n prefix ) \n routeEntry = self . _routeForReAdvertisement ( prefix , label ) \n self . _pushEvent ( RouteEvent ( RouteEvent . WITHDRAW , routeEntry ) ) \n VPNInstance . vifUnplugged ( self , macAddress , ipAddressPrefix , \n advertiseSubnet ) \n def _route2trackedEntry ( self , route ) : \n if isinstance ( route . nlri , VPNLabelledPrefix ) : \n return route . nlri . prefix \n else : \n self . log . error ( \"<STR_LIT>\" , \n type ( route . nlri ) ) \n return None \n def _toReadvertise ( self , route ) : \n return ( len ( set ( route . routeTargets ) . intersection ( \n set ( self . readvertiseFromRTs ) ) ) > <NUM_LIT:0> ) \n def _imported ( self , route ) : \n return ( len ( set ( route . routeTargets ) . intersection ( \n set ( self . importRTs ) ) ) > <NUM_LIT:0> ) \n @ utils . synchronized \n @ logDecorator . log \n def _newBestRoute ( self , entry , newRoute ) : \n prefix = entry \n if self . readvertise : \n self . log . debug ( \"<STR_LIT>\" , newRoute . routeTargets ) \n self . log . debug ( \"<STR_LIT>\" , self . readvertiseFromRTs ) \n if self . _toReadvertise ( newRoute ) : \n self . log . debug ( \"<STR_LIT>\" , prefix ) \n self . _readvertise ( newRoute . nlri ) \n if not self . _imported ( newRoute ) : \n self . log . debug ( \"<STR_LIT>\" , prefix ) \n return \n encaps = self . _checkEncaps ( newRoute ) \n if not encaps : \n return \n self . dataplane . setupDataplaneForRemoteEndpoint ( \n prefix , newRoute . attributes . get ( NextHop . ID ) . next_hop , \n newRoute . nlri . labelStack [ <NUM_LIT:0> ] . labelValue , newRoute . nlri , encaps ) \n @ utils . synchronized \n @ logDecorator . log \n def _bestRouteRemoved ( self , entry , oldRoute , last ) : \n prefix = entry \n if self . readvertise and last : \n if self . _toReadvertise ( oldRoute ) : \n self . log . debug ( \"<STR_LIT>\" , prefix ) \n self . _readvertiseStop ( oldRoute . nlri ) \n if not self . _imported ( oldRoute ) : \n self . log . debug ( \"<STR_LIT>\" , prefix ) \n return \n if self . _skipRouteRemoval ( last ) : \n self . log . debug ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n return \n self . dataplane . removeDataplaneForRemoteEndpoint ( \n prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n oldRoute . nlri . labelStack [ <NUM_LIT:0> ] . labelValue , oldRoute . nlri ) \n def getLGMap ( self ) : \n return { \n \"<STR_LIT>\" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n self . <mask0> ] ) \n } \n", "gt": "readvertised"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from bagpipe . exabgp . message . update . attribute import AttributeID , Flag , Attribute \n class Origin ( Attribute ) : \n ID = AttributeID . ORIGIN \n FLAG = Flag . TRANSITIVE \n MULTIPLE = False \n IGP = <NUM_LIT> \n EGP = <NUM_LIT> \n INCOMPLETE = <NUM_LIT> \n def __init__ ( self , origin ) : \n self . origin = origin \n def pack ( self ) : \n return self . _attribute ( chr ( self . origin ) ) \n def __len__ ( self ) : \n return len ( self . pack ( ) ) \n def __str__ ( self ) : \n if self . origin == <NUM_LIT> : return '<STR_LIT>' \n if self . origin == <NUM_LIT> : return '<STR_LIT>' \n if self . origin == <NUM_LIT> : return '<STR_LIT>' \n return '<STR_LIT>' \n def __repr__ ( self ) : \n return str ( self ) \n def __cmp__ ( self , other ) : \n if ( not isinstance ( other , Origin ) \n or ( self . origin != other . origin ) \n ) : \n return - <NUM_LIT:1> \n else : \n <mask0> <NUM_LIT:0> \n", "gt": "return"}
{"input": "\n import setuptools \n setuptools . setup ( \n setup_requires = [ '<STR_LIT>' ] , \n pbr = <mask0> ) \n", "gt": "True"}
{"input": "\n __doc__ = \"\"\"<STR_LIT>\"\"\" \n __author__ = \"\"\"<STR_LIT>\"\"\" \n if __name__ != '<STR_LIT:__main__>' : raise ImportError ( '<STR_LIT>' ) \n import os \n import sys \n DIRECTORY = os . path . abspath ( os . path . dirname ( __file__ ) ) \n sys . path . insert ( <NUM_LIT:0> , os . path . abspath ( os . path . join ( DIRECTORY , '<STR_LIT:..>' , '<STR_LIT:..>' , '<STR_LIT:..>' ) ) ) \n unforkedPid = os . getpid ( ) \n childProcessPid = <NUM_LIT:0> \n from twisted . internet import protocol , defer \n from twisted . python . log import FileLogObserver , textFromEventDict \n from twisted . python . util import untilConcludes \n import signal \n DEFAULT_REACTORS = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n } \n def set_reactor ( ) : \n import platform \n REACTORNAME = DEFAULT_REACTORS . get ( platform . system ( ) , '<STR_LIT>' ) \n if REACTORNAME == '<STR_LIT>' : \n from twisted . internet import kqreactor \n kqreactor . install ( ) \n elif REACTORNAME == '<STR_LIT>' : \n from twisted . internet import epollreactor \n epollreactor . install ( ) \n elif REACTORNAME == '<STR_LIT>' : \n from twisted . internet import pollreactor \n pollreactor . install ( ) \n else : \n from twisted . internet import selectreactor \n selectreactor . install ( ) \n from twisted . internet import reactor \n set_reactor = lambda : reactor \n return reactor \n class ManagedLogger ( FileLogObserver ) : \n \"\"\"<STR_LIT>\"\"\" \n timeFormat = \"<STR_LIT>\" \n def emit ( self , eventDict ) : \n \"\"\"<STR_LIT>\"\"\" \n text = textFromEventDict ( eventDict ) \n if text is None : return \n untilConcludes ( self . write , text ) \n untilConcludes ( self . flush ) \n class DaemonProtocol ( protocol . ProcessProtocol ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , name , label , r , deferred , ** kwargs ) : \n self . deferred = deferred \n self . reactor = r \n out = { \n '<STR_LIT:type>' : '<STR_LIT>' % ( name , label ) \n } \n err = { \n '<STR_LIT:type>' : '<STR_LIT>' % ( name , label ) \n } \n self . name = name \n self . label = label \n import droned . logging \n self . log_stdout = droned . logging . logWithContext ( ** out ) \n self . log_stderr = droned . logging . logWithContext ( ** err ) \n def inConnectionLost ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n def errReceived ( self , data ) : \n \"\"\"<STR_LIT>\"\"\" \n self . log_stderr ( str ( data ) ) \n def outReceived ( self , data ) : \n \"\"\"<STR_LIT>\"\"\" \n self . log_stdout ( str ( data ) ) \n def outConnectionLost ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n def errConnectionLost ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n def connectionMade ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n global closestdin \n if closestdin : \n self . transport . closeStdin ( ) \n global childProcessPid \n global unforkedPid \n x = unforkedPid \n unforkedPid = <NUM_LIT:0> \n if x : self . reactor . callLater ( <NUM_LIT> , os . kill , x , signal . SIGTERM ) \n childProcessPid = self . transport . pid \n sys . stdout . write ( '<STR_LIT>' % ( self . name , self . label , childProcessPid ) ) \n def processExited ( self , reason ) : \n \"\"\"<STR_LIT>\"\"\" \n sys . stdout . write ( '<STR_LIT>' % ( self . name , ) ) \n if not self . deferred . called : \n self . deferred . errback ( reason ) \n global unforkedPid \n global childProcessPid \n childProcessPid = <NUM_LIT:0> \n if unforkedPid : os . kill ( unforkedPid , signal . SIGTERM ) \n processEnded = processExited \n class DaemonWrapper ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n SIGNALS = dict ( ( k , v ) for v , k in signal . __dict__ . iteritems ( ) if v . startswith ( '<STR_LIT>' ) and not v . startswith ( '<STR_LIT>' ) ) \n def __init__ ( self , r , name , label , cmd , args , env ) : \n self . reactor = r \n self . name = name \n self . label = label \n self . fqcmd = cmd \n self . args = args \n self . env = env \n self . exitCode = <NUM_LIT:0> \n self . deferred = defer . succeed ( None ) \n import droned . logging \n self . log = droned . logging . logWithContext ( type = '<STR_LIT>' ) \n def routeSignal ( self , signum , frame ) : \n \"\"\"<STR_LIT>\"\"\" \n if signum == signal . SIGTERM : \n signal . signal ( signal . SIGTERM , signal . SIG_IGN ) \n self . reactor . callLater ( <NUM_LIT> , self . reactor . stop ) \n global childProcessPid \n if childProcessPid : \n self . log ( '<STR_LIT>' % ( self . SIGNALS [ signum ] , childProcessPid ) ) \n try : os . kill ( childProcessPid , signum ) \n except : \n droned . logging . err ( '<STR_LIT>' % ( self . SIGNALS [ signum ] , childProcessPid ) ) \n def processResult ( self , result ) : \n \"\"\"<STR_LIT>\"\"\" \n self . reactor . callLater ( <NUM_LIT> , self . reactor . stop ) \n return result \n def running ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n global masksignals \n if masksignals : \n for signum , signame in self . SIGNALS . items ( ) : \n if signame in ( '<STR_LIT>' , ) : continue \n try : signal . signal ( signum , self . routeSignal ) \n except RuntimeError : pass \n from droned . clients import command \n self . log ( '<STR_LIT>' % ( self . name , self . label ) ) \n pargs = ( self . name , self . label , self . reactor ) \n pkwargs = { } \n global usetty \n global path \n self . deferred = command ( self . fqcmd , self . args , self . env , \n path , usetty , { } , DaemonProtocol , \n * pargs , ** pkwargs \n ) \n self . deferred . addBoth ( self . processResult ) \n return self . deferred \n env = os . environ . copy ( ) \n args = tuple ( sys . argv [ <NUM_LIT:1> : ] ) \n logdir = env . pop ( '<STR_LIT>' , os . path . join ( os . path . sep , '<STR_LIT>' ) ) \n masksignals = bool ( env . pop ( '<STR_LIT>' , True ) ) \n closestdin = bool ( env . pop ( '<STR_LIT>' , True ) ) \n name = env . pop ( '<STR_LIT>' , '<STR_LIT>' ) \n label = env . pop ( '<STR_LIT>' , '<STR_LIT:0>' ) \n usetty = bool ( env . pop ( '<STR_LIT>' , '<STR_LIT:0>' ) ) \n path = env . pop ( '<STR_LIT>' , os . path . sep ) \n if name not in logdir : \n t = os . path . join ( logdir , name , label ) \n try : \n if not os . path . exists ( t ) : \n os . makedirs ( t , mode = <NUM_LIT:0> <NUM_LIT> ) \n logdir = t \n except : pass \n if args and os . path . exists ( args [ <NUM_LIT:0> ] ) : \n try : os . setsid ( ) \n except : pass \n if os . fork ( ) == <NUM_LIT:0> : \n os . chdir ( os . path . sep ) \n os . umask ( <NUM_LIT:0> ) \n sys . stdout . write ( '<STR_LIT>' % ( os . getpid ( ) , ) ) \n sys . stderr . flush ( ) \n sys . stdout . flush ( ) \n import droned . logging \n sys . stdout = droned . logging . StdioKabob ( <NUM_LIT:0> ) \n sys . stderr = droned . logging . StdioKabob ( <NUM_LIT:1> ) \n maxfd = <NUM_LIT> \n try : \n import resource \n maxfd = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ <NUM_LIT:1> ] \n if ( maxfd == resource . RLIM_INFINITY ) : \n maxfd = <NUM_LIT> \n except : pass \n for fd in range ( <NUM_LIT:0> , maxfd ) : \n try : os . close ( fd ) \n except OSError : pass \n os . open ( \n hasattr ( os , \"<STR_LIT>\" ) and os . devnull or \"<STR_LIT>\" , \n os . O_RDWR \n ) \n os . dup2 ( <NUM_LIT:0> , <NUM_LIT:1> ) \n os . dup2 ( <NUM_LIT:0> , <NUM_LIT:2> ) \n loggers = [ \n '<STR_LIT>' % ( name , label ) , \n '<STR_LIT>' % ( name , label ) , \n ] \n droned . logging . logToDir ( directory = logdir ) \n reactor = set_reactor ( ) \n droned . logging . logToDir ( \n directory = logdir , \n LOG_TYPE = tuple ( loggers ) , \n OBSERVER = ManagedLogger \n ) \n dmx = DaemonWrapper ( reactor , name , label , args [ <NUM_LIT:0> ] , args [ <NUM_LIT:1> : ] , env ) \n def killGroup ( ) : \n \"\"\"<STR_LIT>\"\"\" \n dmx . log ( '<STR_LIT>' ) \n signal . signal ( signal . SIGTERM , signal . SIG_IGN ) \n os . kill ( - os . getpgid ( os . getpid ( ) ) , signal . SIGTERM ) \n reactor . addSystemEventTrigger ( '<STR_LIT>' , '<STR_LIT>' , killGroup ) \n reactor . callWhenRunning ( dmx . running ) \n reactor . run ( ) \n sys . exit ( dmx . exitCode ) \n else : \n reactor = set_reactor ( ) \n reactor . callLater ( <NUM_LIT> , sys . exit , <NUM_LIT:1> ) \n reactor . run ( ) \n sys . exit ( <NUM_LIT:0> ) \n sys . <mask0> ( <NUM_LIT:255> ) \n", "gt": "exit"}
{"input": "\n from droned . models . team import Team , SupportAgent \n from droned . models . issue import Issue \n from droned . responders import responder \n @ responder ( pattern = \"<STR_LIT>\" , form = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n def teams ( conversation ) : \n teamlist = '<STR_LIT:\\n>' . join ( \"<STR_LIT>\" % ( team . name , len ( team . agents ) ) for team in Team . objects ) \n conversation . say ( '<STR_LIT:\\n>' + teamlist , useHTML = False ) \n @ responder ( pattern = \"<STR_LIT>\" , form = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n def members ( conversation , name ) : \n if not Team . exists ( name ) : \n conversation . say ( \"<STR_LIT>\" % name ) \n else : \n team = Team ( name ) \n if team . agents : \n members = '<STR_LIT:U+002C>' . join ( agent . jid for agent in team . agents ) \n heading = \"<STR_LIT>\" % name \n conversation . say ( heading + members , useHTML = False ) \n else : \n conversation . say ( \"<STR_LIT>\" ) \n @ responder ( pattern = \"<STR_LIT>\" , form = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n def join ( conversation , name ) : \n if not Team . exists ( name ) : \n conversation . say ( \"<STR_LIT>\" % name ) \n else : \n team = Team ( name ) \n team . addMember ( conversation . buddy ) \n conversation . say ( \"<STR_LIT>\" % name ) \n @ responder ( pattern = \"<STR_LIT>\" , form = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n def leave ( conversation , name ) : \n if not Team . exists ( name ) : \n conversation . say ( \"<STR_LIT>\" % name ) \n else : \n team = Team ( name ) \n team . removeMember ( conversation . buddy ) \n conversation . say ( \"<STR_LIT>\" % name ) \n @ responder ( pattern = \"<STR_LIT>\" , form = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n def busy ( conversation ) : \n agent = SupportAgent ( conversation . buddy ) \n agent . ready = False \n conversation . say ( \"<STR_LIT>\" ) \n @ responder ( pattern = \"<STR_LIT>\" , form = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n def free ( conversation ) : \n agent = SupportAgent ( conversation . buddy ) \n agent . ready = True \n conversation . say ( \"<STR_LIT>\" ) \n @ responder ( pattern = \"<STR_LIT>\" , form = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n def announce ( conversation , message ) : \n agent = SupportAgent ( conversation . buddy ) \n message = \"<STR_LIT>\" % ( agent . name , message ) \n told = <NUM_LIT:0> \n for team in agent . teams : \n for otherAgent in team . agents : \n if otherAgent is agent : continue \n otherAgent . tell ( message ) \n told += <NUM_LIT:1> \n if told : \n agent . tell ( '<STR_LIT>' % told ) \n else : \n agent . tell ( '<STR_LIT>' ) \n @ responder ( pattern = \"<STR_LIT>\" , form = '<STR_LIT>' , help = '<STR_LIT>' ) \n def issues ( conversation ) : \n issues = sorted ( [ i for i in Issue . objects if not i . resolved ] , key = lambda i : i . id ) \n summaries = [ ] \n for issue in issues : \n summary = \"<STR_LIT>\" % ( issue . id , issue . description ) \n if issue . context [ '<STR_LIT>' ] : \n summary += \"<STR_LIT>\" % issue . context [ '<STR_LIT>' ] . name \n elif isinstance ( issue . context [ '<STR_LIT>' ] , SupportAgent ) : \n summary += \"<STR_LIT>\" % issue . context [ '<STR_LIT>' ] . name \n elif issue . context [ '<STR_LIT>' ] is None : \n summary += \"<STR_LIT>\" \n summaries . append ( summary ) \n heading = '<STR_LIT>' % len ( issues ) \n listing = '<STR_LIT:\\n>' . join ( summaries ) \n conversation . say ( heading + listing , useHTML = False ) \n @ responder ( pattern = \"<STR_LIT>\" , form = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n def grab ( conversation , id ) : \n id = int ( id ) \n issue = Issue . byID ( id ) \n if not issue : \n conversation . say ( \"<STR_LIT>\" % id ) \n elif not issue . hasSOP : \n conversation . say ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n conversation . say ( issue . description ) \n contextSummary = '<STR_LIT:\\n>' . join ( \"<STR_LIT>\" % info for info in issue . context . data . items ( ) ) \n conversation . say ( contextSummary , useHTML = False ) \n else : \n existingAgent = issue . context [ '<STR_LIT>' ] \n if existingAgent : \n if existingAgent . conversation is conversation : \n conversation . say ( \"<STR_LIT>\" ) \n return \n existingAgent . conversation . say ( \"<STR_LIT>\" % conversation . buddyName ) \n existingAgent . currentIssue = None \n existingAgent . conversation . nevermind ( ) \n conversation . say ( \"<STR_LIT>\" % id ) \n agent = SupportAgent ( conversation . buddy ) \n agent . ready = True \n agent . currentIssue = None \n agent . conversation . nevermind ( ) \n agent . engage ( issue ) \n @ responder ( pattern = '<STR_LIT>' , form = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n def resolve ( conversation , id , resolution ) : \n id = int ( id ) \n issue = Issue . byID ( id ) \n if not issue : \n conversation . say ( \"<STR_LIT>\" % id ) \n elif issue . hasSOP : \n conversation . say ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n else : \n conversation . say ( \"<STR_LIT>\" % id ) \n issue . resolve ( <mask0> ) \n", "gt": "resolution"}
{"input": "\n from kitt . interfaces import moduleProvides , IDroneDService \n moduleProvides ( IDroneDService ) \n from kitt . util import dictwrapper \n import config \n SERVICENAME = '<STR_LIT>' \n SERVICECONFIG = dictwrapper ( { \n '<STR_LIT>' : { \n config . LOG_DIR : [ \n ( '<STR_LIT>' , int ( <NUM_LIT:7> * len ( config . AUTOSTART_SERVICES ) ) ) , \n ] , \n } \n } ) \n import os , re , time \n from twisted . application . service import Service \n from twisted . internet import defer , task \n from droned . logging import logWithContext \n from kitt . decorators import synchronizedDeferred , deferredAsThread \n import copy \n __doc__ = \"\"\"<STR_LIT>\"\"\" \n log = logWithContext ( type = SERVICENAME ) \n def ageCompare ( f1 , f2 ) : \n t1 = os . path . getmtime ( f1 ) \n t2 = os . path . getmtime ( f2 ) \n if t1 > t2 : return <NUM_LIT:1> \n if t2 == t2 : return <NUM_LIT:0> \n if t2 < t2 : return - <NUM_LIT:1> \n class Janitizer ( Service ) : \n minute = property ( lambda foo : <NUM_LIT> ) \n hour = property ( lambda foo : <NUM_LIT> ) \n day = property ( lambda foo : <NUM_LIT> ) \n week = property ( lambda f : <NUM_LIT> ) \n oldfiles = { } \n watchDict = property ( lambda s : SERVICECONFIG . wrapped . get ( '<STR_LIT>' , { } ) ) \n busy = defer . DeferredLock ( ) \n def update ( self , watchDict ) : \n \"\"\"<STR_LIT>\"\"\" \n tmp = copy . deepcopy ( self . watchDict ) \n tmp . update ( watchDict ) \n SERVICECONFIG . JANITIZE = tmp \n @ synchronizedDeferred ( busy ) \n @ deferredAsThread \n def garbageCheck ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n watchDict = copy . deepcopy ( self . watchDict ) \n for directory , garbageList in watchDict . iteritems ( ) : \n if not os . path . exists ( directory ) : continue \n for pattern , limit in garbageList : \n self . cleanupLinks ( directory ) \n files = [ os . path . join ( directory , f ) for f in os . listdir ( directory ) if re . search ( pattern , f ) ] \n files = sorted ( files ) \n if len ( files ) > int ( limit ) : \n log ( '<STR_LIT>' % '<STR_LIT>' . join ( files ) ) \n while len ( files ) > int ( limit ) : \n oldfile = files . pop ( <NUM_LIT:0> ) \n log ( '<STR_LIT>' % oldfile ) \n if os . path . islink ( oldfile ) : continue \n if os . path . isdir ( oldfile ) : \n for base , dirs , myfiles in os . walk ( oldfile , topdown = False ) : \n for name in myfiles : \n os . remove ( os . path . join ( base , name ) ) \n for name in dirs : \n os . rmdir ( os . path . join ( base , name ) ) \n os . rmdir ( oldfile ) \n else : os . unlink ( oldfile ) \n self . cleanupLinks ( directory ) \n def cleanupLinks ( self , directory ) : \n \"\"\"<STR_LIT>\"\"\" \n files = [ os . path . join ( directory , f ) for f in os . listdir ( directory ) ] \n for f in files [ : ] : \n if not os . path . exists ( f ) : \n log ( '<STR_LIT>' % f ) \n os . unlink ( f ) \n files . remove ( f ) \n return files \n def clean_old_files ( self , directory , age , recurse = True ) : \n \"\"\"<STR_LIT>\"\"\" \n self . oldfiles [ directory ] = ( age , recurse ) \n @ synchronizedDeferred ( busy ) \n @ deferredAsThread \n def clean_elderly ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for directory in self . oldfiles : \n self . recursive_clean ( directory , * self . oldfiles [ directory ] ) \n def recursive_clean ( self , directory , age , recurse ) : \n \"\"\"<STR_LIT>\"\"\" \n try : data = map ( lambda n : os . path . join ( directory , n ) , os . listdir ( directory ) ) \n except : \n log ( '<STR_LIT>' % directory ) \n return \n for node in data : \n if os . path . isdir ( node ) and recurse : \n empty = self . recursive_clean ( node , age , recurse ) \n if empty : \n try : os . rmdir ( node ) \n except : log ( '<STR_LIT>' % node ) \n continue \n if os . path . isdir ( node ) : continue \n if ( time . time ( ) - os . stat ( node ) . st_mtime ) > age : \n try : os . remove ( node ) \n except : log ( '<STR_LIT>' % node ) \n return bool ( os . listdir ( directory ) ) \n def startService ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . GARBAGE_CHECK = task . LoopingCall ( self . garbageCheck ) \n self . ELDERLY_CHECK = task . LoopingCall ( self . clean_elderly ) \n Service . startService ( self ) \n self . GARBAGE_CHECK . start ( self . minute * <NUM_LIT:20> ) \n self . ELDERLY_CHECK . start ( self . minute ) \n def stopService ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n if self . GARBAGE_CHECK . running : \n self . GARBAGE_CHECK . stop ( ) \n if self . ELDERLY_CHECK . running : \n self . ELDERLY_CHECK . stop ( ) \n except : pass \n Service . stopService ( self ) \n parentService = None \n service = None \n def update ( watchDict ) : \n global service \n if not running ( ) : \n raise AssertionError ( '<STR_LIT>' ) \n return service . update ( watchDict ) \n def install ( _parentService ) : \n global parentService \n parentService = _parentService \n def start ( ) : \n global service \n if not running ( ) : \n service = Janitizer ( ) \n service . setName ( SERVICENAME ) \n service . setServiceParent ( parentService ) \n def stop ( ) : \n global service \n if running ( ) : \n service . disownServiceParent ( ) \n service . stopService ( ) \n service = None \n def running ( ) : \n return bool ( service ) and service . running \n __all__ = [ '<STR_LIT>' , <mask0> , '<STR_LIT>' , '<STR_LIT>' ] \n", "gt": "'<STR_LIT:start>'"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from OpenPNM . Geometry import models as gm \n from OpenPNM . Geometry import GenericGeometry \n class SGL10 ( GenericGeometry ) : \n r\"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , ** kwargs ) : \n super ( ) . __init__ ( ** kwargs ) \n self . _generate ( ) \n def _generate ( self ) : \n self . models . add ( propname = '<STR_LIT>' , \n model = gm . pore_misc . random , \n num_range = [ <NUM_LIT:0> , <NUM_LIT> ] , \n regen_mode = '<STR_LIT>' ) \n self . models . add ( propname = '<STR_LIT>' , \n model = gm . throat_misc . neighbor , \n pore_prop = '<STR_LIT>' , \n mode = '<STR_LIT>' ) \n self . models . add ( propname = '<STR_LIT>' , \n model = gm . pore_diameter . sphere , \n psd_name = '<STR_LIT>' , \n psd_shape = <NUM_LIT> , \n psd_loc = <NUM_LIT> , \n psd_scale = <NUM_LIT> , \n psd_offset = <NUM_LIT> ) \n self . models . add ( propname = '<STR_LIT>' , \n model = gm . pore_area . spherical ) \n self . models . add ( propname = '<STR_LIT>' , \n model = gm . pore_volume . sphere ) \n self . models . add ( propname = '<STR_LIT>' , \n model = gm . throat_diameter . cylinder , \n tsd_name = '<STR_LIT>' , \n tsd_shape = <NUM_LIT> , \n tsd_loc = <NUM_LIT> , \n tsd_scale = <NUM_LIT> , \n tsd_offset = <NUM_LIT> ) \n self . models . add ( propname = '<STR_LIT>' , \n model = gm . throat_length . straight ) \n self . models . add ( propname = '<STR_LIT>' , \n model = gm . throat_volume . cylinder ) \n self . models . add ( propname = '<STR_LIT>' , \n model = gm . throat_area . cylinder ) \n self . models . add ( propname = '<STR_LIT>' , \n model = gm . throat_surface_area . <mask0> ) \n", "gt": "cylinder"}
{"input": "\n r\"\"\"<STR_LIT>\"\"\" \n import scipy as _sp \n def pore_to_pore ( geometry , ** kwargs ) : \n r\"\"\"<STR_LIT>\"\"\" \n network = geometry . _net \n throats = network . throats ( geometry . name ) \n pores = network . find_connected_pores ( throats , flatten = False ) \n C0 = network [ '<STR_LIT>' ] [ pores , <NUM_LIT:0> ] \n C1 = network [ '<STR_LIT>' ] [ pores , <NUM_LIT:1> ] \n V = C1 - C0 \n L = _sp . array ( _sp . sqrt ( _sp . sum ( V [ : , : ] ** <NUM_LIT:2> , axis = <NUM_LIT:1> ) ) , ndmin = <NUM_LIT:1> ) \n value = V / _sp . array ( L , ndmin = <NUM_LIT:2> ) . T \n return <mask0> \n", "gt": "value"}
{"input": "\n r\"\"\"<STR_LIT>\"\"\" \n import scipy as sp \n def standard ( phase , \n pore_MW = '<STR_LIT>' , \n pore_density = '<STR_LIT>' , \n ** kwargs ) : \n r\"\"\"<STR_LIT>\"\"\" \n MW = phase [ pore_MW ] \n rho = phase [ pore_density ] \n value = rho / MW \n return value \n def ideal_gas ( phase , \n pore_pressure = '<STR_LIT>' , \n pore_temperature = '<STR_LIT>' , \n ** kwargs ) : \n r\"\"\"<STR_LIT>\"\"\" \n R = <NUM_LIT> \n P = phase [ pore_pressure ] \n T = phase [ pore_temperature ] \n value = P / ( R * T ) \n return value \n def vanderwaals ( phase , \n pore_P = '<STR_LIT>' , \n pore_T = '<STR_LIT>' , \n pore_Pc = '<STR_LIT>' , \n pore_Tc = '<STR_LIT>' , \n ** kwargs ) : \n r\"\"\"<STR_LIT>\"\"\" \n P = phase [ pore_P ] / <NUM_LIT> \n T = phase [ pore_T ] \n Pc = phase [ pore_Pc ] / <NUM_LIT> \n Tc = phase [ pore_Tc ] \n R = <NUM_LIT> \n a = <NUM_LIT> * ( R ** <NUM_LIT:2> ) * ( Tc ** <NUM_LIT:2> ) / ( <NUM_LIT:64> * Pc ) \n b = R * Tc / ( <NUM_LIT:8> * Pc ) \n a1 = - <NUM_LIT:1> / b \n a2 = ( R * T + b * P ) / ( a * b ) \n a3 = - P / ( a * b ) \n a0 = sp . ones ( sp . shape ( a1 ) ) \n coeffs = sp . vstack ( ( a0 , a1 , a2 , a3 ) ) . T \n density = sp . array ( [ sp . roots ( C ) for C in coeffs ] ) \n value = sp . real ( density [ : , <NUM_LIT:2> ] ) * <NUM_LIT> \n return <mask0> \n", "gt": "value"}
{"input": "\n import os \n import sys \n from distutils . util import convert_path \n try : \n from setuptools import setup \n except ImportError : \n from distutils . core import setup \n sys . path . append ( os . getcwd ( ) ) \n main_ = { } \n ver_path = convert_path ( '<STR_LIT>' ) \n with open ( ver_path ) as f : \n for line in f : \n if line . startswith ( '<STR_LIT>' ) : \n exec ( line , main_ ) \n setup ( \n name = '<STR_LIT>' , \n description = '<STR_LIT>' , \n version = main_ [ '<STR_LIT>' ] , \n classifiers = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] , \n packages = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] , \n install_requires = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] , \n author = '<STR_LIT>' , \n author_email = '<STR_LIT>' , \n download_url = '<STR_LIT>' , \n <mask0> = '<STR_LIT>' \n ) \n", "gt": "url"}
{"input": "\n class PoreCentroidTest : \n def test_voronoi ( self ) : \n <mask0> \n", "gt": "pass"}
{"input": "\n import pytest \n import OpenPNM \n class GenericPhaseTest : \n def setup_class ( self ) : \n self . net = OpenPNM . Network . Cubic ( shape = [ <NUM_LIT:5> , <NUM_LIT:5> , <NUM_LIT:5> ] ) \n def test_init_w_no_network ( self ) : \n OpenPNM . Phases . GenericPhase ( ) \n def test_init_w_components ( self ) : \n comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n OpenPNM . Phases . GenericPhase ( network = self . net , \n components = [ comp1 , comp2 ] ) \n def test_set_component_add ( self ) : \n comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n phase = OpenPNM . Phases . GenericPhase ( network = self . net ) \n phase . set_component ( comp1 ) \n phase . set_component ( comp2 ) \n def test_set_component_add_twice ( self ) : \n comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n phase = OpenPNM . Phases . GenericPhase ( network = self . net ) \n phase . set_component ( comp1 ) \n with pytest . raises ( Exception ) : \n phase . set_components ( comp1 ) \n def test_set_component_remove ( self ) : \n comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n phase = OpenPNM . Phases . GenericPhase ( network = self . net , \n components = [ comp1 , comp2 ] ) \n phase . set_component ( comp1 , mode = '<STR_LIT>' ) \n phase . set_component ( comp2 , mode = '<STR_LIT>' ) \n def test_set_component_remove_twice ( self ) : \n comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n phase = OpenPNM . Phases . GenericPhase ( network = self . net , \n components = [ comp1 , comp2 ] ) \n phase . set_component ( comp1 , mode = '<STR_LIT>' ) \n with pytest . raises ( Exception ) : \n phase . set_component ( comp1 , <mask0> = '<STR_LIT>' ) \n", "gt": "mode"}
{"input": "\n import pdb \n import os , sys \n import itertools \n from cPickle import * \n from collections import defaultdict , namedtuple \n from pbtools . pbtranscript . Utils import check_ids_unique \n import pbtools . pbtranscript . tofu_wrap as tofu_wrap \n import pbtools . pbtranscript . BioReaders as BioReaders \n import pbtools . pbtranscript . branch . branch_simple2 as branch_simple2 \n import pbtools . pbtranscript . counting . compare_junctions as compare_junctions \n from pbtools . pbtranscript . io . SeqReaders import LazyFastaReader , LazyFastqReader \n from pbcore . io . FastaIO import FastaWriter \n from pbcore . io . FastqIO import FastqWriter \n from bx . intervals . cluster import ClusterTree \n def pick_rep ( fa_fq_filename , sam_filename , gff_filename , group_filename , output_filename , is_fq = False , pick_least_err_instead = False ) : \n \"\"\"<STR_LIT>\"\"\" \n if is_fq : \n fd = LazyFastqReader ( fa_fq_filename ) \n fout = FastqWriter ( output_filename ) \n else : \n fd = LazyFastaReader ( fa_fq_filename ) \n fout = FastaWriter ( output_filename ) \n rep_info = { } \n id_to_rep = { } \n for line in open ( group_filename ) : \n pb_id , members = line . strip ( ) . split ( '<STR_LIT:\\t>' ) \n print >> sys . stderr , \"<STR_LIT>\" , pb_id \n best_id = None \n best_seq = None \n best_qual = None \n best_err = <NUM_LIT> \n err = <NUM_LIT> \n max_len = <NUM_LIT:0> \n for x in members . split ( '<STR_LIT:U+002C>' ) : \n if is_fq and pick_least_err_instead : \n err = sum ( i ** - ( i / <NUM_LIT> ) for i in fd [ x ] . quality ) \n if ( is_fq and pick_least_err_instead and err < best_err ) or ( ( not is_fq or not pick_least_err_instead ) and len ( fd [ x ] . sequence ) >= max_len ) : \n best_id = x \n best_seq = fd [ x ] . sequence \n if is_fq : \n best_qual = fd [ x ] . quality \n best_err = err \n max_len = len ( fd [ x ] . sequence ) \n rep_info [ pb_id ] = ( best_id , best_seq , best_qual ) \n id_to_rep [ best_id ] = pb_id \n f_gff = open ( gff_filename , '<STR_LIT:w>' ) \n coords = { } \n record_storage = { } \n for r in BioReaders . GMAPSAMReader ( sam_filename , True ) : \n if r . qID in id_to_rep : \n pb_id = id_to_rep [ r . qID ] \n best_id , best_seq , best_qual = rep_info [ pb_id ] \n if r . qID not in coords : \n coords [ r . qID ] = \"<STR_LIT>\" . format ( r . sID , r . sStart , r . sEnd , r . flag . strand ) \n isoform_index = <NUM_LIT:1> \n record_storage [ pb_id ] = r \n else : \n coords [ r . qID ] += \"<STR_LIT>\" . format ( r . sID , r . sStart , r . sEnd , r . flag . strand ) \n isoform_index = <NUM_LIT:1> \n old_r = record_storage [ pb_id ] \n f_gff . write ( \"<STR_LIT>\" . format ( chr = old_r . sID , s = old_r . segments [ <NUM_LIT:0> ] . start + <NUM_LIT:1> , e = old_r . segments [ - <NUM_LIT:1> ] . end , pi = pb_id , j = isoform_index , strand = old_r . flag . strand ) ) \n for s in old_r . segments : \n f_gff . write ( \"<STR_LIT>\" . format ( chr = old_r . sID , s = s . start + <NUM_LIT:1> , e = s . end , pi = pb_id , j = isoform_index , strand = old_r . flag . strand ) ) \n isoform_index = <NUM_LIT:2> \n f_gff . write ( \"<STR_LIT>\" . format ( chr = r . sID , s = r . segments [ <NUM_LIT:0> ] . start + <NUM_LIT:1> , e = r . segments [ - <NUM_LIT:1> ] . end , pi = pb_id , j = isoform_index , strand = r . flag . strand ) ) \n for s in r . segments : \n f_gff . write ( \"<STR_LIT>\" . format ( chr = r . sID , s = s . start + <NUM_LIT:1> , e = s . end , pi = pb_id , j = isoform_index , strand = r . flag . strand ) ) \n f_gff . close ( ) \n for pb_id in rep_info : \n best_id , best_seq , best_qual = rep_info [ pb_id ] \n _id_ = \"<STR_LIT>\" . format ( pb_id , coords [ best_id ] , best_id ) \n _seq_ = best_seq \n if is_fq : \n fout . writeRecord ( _id_ , _seq_ , best_qual ) \n else : \n fout . writeRecord ( _id_ , _seq_ ) \n def sep_by_strand ( records ) : \n output = { '<STR_LIT:+>' : [ ] , '<STR_LIT:->' : [ ] } \n for r in records : \n output [ r . flag . strand ] . append ( r ) \n return output \n def is_fusion_compatible ( r1 , r2 , max_fusion_point_dist , max_exon_end_dist , allow_extra_5_exons ) : \n \"\"\"<STR_LIT>\"\"\" \n assert r1 . flag . strand == r2 . flag . strand \n if r1 . qStart <= <NUM_LIT> * r1 . qLen : \n if r2 . qStart > <NUM_LIT> * r2 . qLen : \n return False \n in_5_portion = True \n else : \n if r2 . qStart <= <NUM_LIT> * r2 . qLen : \n return False \n in_5_portion = False \n plus_is_5end = ( r1 . flag . strand == '<STR_LIT:+>' ) \n type = compare_junctions . compare_junctions ( r1 , r2 ) \n if type == '<STR_LIT>' : \n if len ( r1 . segments ) == <NUM_LIT:1> : \n if len ( r2 . segments ) == <NUM_LIT:1> : \n if in_5_portion and plus_is_5end : dist = abs ( r1 . sStart - r2 . sStart ) \n else : dist = abs ( r1 . sEnd - r2 . sEnd ) \n return dist <= max_fusion_point_dist \n else : \n raise Exception , \"<STR_LIT>\" + \"<STR_LIT>\" \n else : \n return True \n elif type == '<STR_LIT>' or type == '<STR_LIT>' : \n if allow_extra_5_exons : \n if in_5_portion and plus_is_5end : \n if abs ( r1 . segments [ - <NUM_LIT:1> ] . start - r2 . segments [ - <NUM_LIT:1> ] . start ) > max_exon_end_dist : return False \n if abs ( r1 . segments [ - <NUM_LIT:1> ] . end - r2 . segments [ - <NUM_LIT:1> ] . end ) > max_fusion_point_dist : return False \n return True \n elif in_5_portion and ( not plus_is_5end ) : \n if abs ( r1 . segments [ <NUM_LIT:0> ] . end - r2 . segments [ <NUM_LIT:0> ] . end ) > max_exon_end_dist : return False \n if abs ( r1 . segments [ <NUM_LIT:0> ] . start - r2 . segments [ <NUM_LIT:0> ] . start ) > max_fusion_point_dist : return False \n return True \n else : \n return False \n else : \n return False \n else : \n return False \n def merge_fusion_exons ( records , max_fusion_point_dist , max_exon_end_dist , allow_extra_5_exons ) : \n \"\"\"<STR_LIT>\"\"\" \n output = [ [ records [ <NUM_LIT:0> ] ] ] \n for r1 in records [ <NUM_LIT:1> : ] : \n merged = False \n for i , r2s in enumerate ( output ) : \n if all ( is_fusion_compatible ( r1 , r2 , max_fusion_point_dist , max_exon_end_dist , allow_extra_5_exons ) for r2 in r2s ) : \n output [ i ] . append ( r1 ) \n merged = True \n break \n if not merged : \n output . append ( [ r1 ] ) \n return output \n def iter_gmap_sam_for_fusion ( gmap_sam_filename , fusion_candidates , transfrag_len_dict ) : \n \"\"\"<STR_LIT>\"\"\" \n records = [ ] \n iter = BioReaders . GMAPSAMReader ( gmap_sam_filename , True , query_len_dict = transfrag_len_dict ) \n for r in iter : \n if r . qID in fusion_candidates : \n records = [ r ] \n break \n for r in iter : \n if len ( records ) >= <NUM_LIT:1> and ( r . sID == records [ - <NUM_LIT:1> ] . sID and r . sStart < records [ - <NUM_LIT:1> ] . sStart ) : \n print >> sys . stderr , \"<STR_LIT>\" \n sys . exit ( - <NUM_LIT:1> ) \n if len ( records ) >= <NUM_LIT:1> and ( r . sID != records [ <NUM_LIT:0> ] . sID or r . sStart > records [ - <NUM_LIT:1> ] . sEnd ) : \n yield ( sep_by_strand ( records ) ) \n records = [ ] \n if r . qID in fusion_candidates : \n records . append ( r ) \n if len ( records ) > <NUM_LIT:0> : \n yield ( sep_by_strand ( records ) ) \n def find_fusion_candidates ( sam_filename , query_len_dict , min_locus_coverage = <NUM_LIT> , min_locus_coverage_bp = <NUM_LIT:1> , min_total_coverage = <NUM_LIT> , min_dist_between_loci = <NUM_LIT> ) : \n \"\"\"<STR_LIT>\"\"\" \n TmpRec = namedtuple ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n def total_coverage ( tmprecs ) : \n tree = ClusterTree ( <NUM_LIT:0> , <NUM_LIT:0> ) \n for r in tmprecs : tree . insert ( r . qStart , r . qEnd , - <NUM_LIT:1> ) \n return sum ( reg [ <NUM_LIT:1> ] - reg [ <NUM_LIT:0> ] for reg in tree . getregions ( ) ) \n d = defaultdict ( lambda : [ ] ) \n reader = BioReaders . GMAPSAMReader ( sam_filename , True , query_len_dict = query_len_dict ) \n for r in reader : \n if r . sID == '<STR_LIT:*>' : continue \n if r . flag . strand == '<STR_LIT:+>' : \n d [ r . qID ] . append ( TmpRec ( qCov = r . qCoverage , qLen = r . qLen , qStart = r . qStart , qEnd = r . qEnd , sStart = r . sStart , sEnd = r . sEnd , iden = r . identity ) ) \n else : \n d [ r . qID ] . append ( TmpRec ( qCov = r . qCoverage , qLen = r . qLen , qStart = r . qLen - r . qEnd , qEnd = r . qLen - r . qStart , sStart = r . sStart , sEnd = r . sEnd , iden = r . identity ) ) \n fusion_candidates = [ ] \n for k , data in d . iteritems ( ) : \n if len ( data ) > <NUM_LIT:1> and all ( a . iden >= <NUM_LIT> for a in data ) and all ( a . qCov >= min_locus_coverage for a in data ) and all ( a . qCov * a . qLen >= min_locus_coverage_bp for a in data ) and total_coverage ( data ) * <NUM_LIT:1.> / data [ <NUM_LIT:0> ] . qLen >= min_total_coverage and all ( max ( a . sStart , b . sStart ) - min ( a . sEnd , b . sEnd ) >= min_dist_between_loci for a , b in itertools . combinations ( data , <NUM_LIT:2> ) ) : \n fusion_candidates . append ( k ) \n return fusion_candidates \n def fusion_main ( fa_or_fq_filename , sam_filename , output_prefix , is_fq = False , allow_extra_5_exons = True , skip_5_exon_alt = True , prefix_dict_pickle_filename = None , min_locus_coverage = <NUM_LIT> , min_total_coverage = <NUM_LIT> , min_locus_coverage_bp = <NUM_LIT:1> , min_dist_between_loci = <NUM_LIT> ) : \n \"\"\"<STR_LIT>\"\"\" \n compressed_records_pointer_dict = defaultdict ( lambda : [ ] ) \n merged_exons = [ ] \n merged_i = <NUM_LIT:0> \n check_ids_unique ( fa_or_fq_filename , is_fq = is_fq ) \n bs = branch_simple2 . BranchSimple ( fa_or_fq_filename , is_fq = is_fq ) \n fusion_candidates = find_fusion_candidates ( sam_filename , bs . transfrag_len_dict , min_locus_coverage , min_locus_coverage_bp , min_total_coverage , min_dist_between_loci ) \n for recs in iter_gmap_sam_for_fusion ( sam_filename , fusion_candidates , bs . transfrag_len_dict ) : \n for v in recs . itervalues ( ) : \n if len ( v ) > <NUM_LIT:0> : \n o = merge_fusion_exons ( v , max_fusion_point_dist = <NUM_LIT:100> , max_exon_end_dist = <NUM_LIT:0> , allow_extra_5_exons = allow_extra_5_exons ) \n for group in o : \n merged_exons . append ( group ) \n for r in group : compressed_records_pointer_dict [ r . qID ] . append ( merged_i ) \n merged_i += <NUM_LIT:1> \n f_group = open ( '<STR_LIT>' , '<STR_LIT:w>' ) \n gene_index = <NUM_LIT:1> \n already_seen = set ( ) \n for qid , indices in compressed_records_pointer_dict . iteritems ( ) : \n combo = tuple ( indices ) \n if combo in already_seen : \n print \"<STR_LIT>\" , combo \n continue \n already_seen . add ( combo ) \n for isoform_index , i in enumerate ( indices ) : \n bs . cuff_index = gene_index \n records = merged_exons [ i ] \n f_group . write ( \"<STR_LIT>\" . format ( p = \"<STR_LIT>\" , i = gene_index , j = isoform_index , ids = \"<STR_LIT:U+002C>\" . join ( r . qID for r in records ) ) ) \n gene_index += <NUM_LIT:1> \n f_group . close ( ) \n f_group = open ( output_prefix + '<STR_LIT>' , '<STR_LIT:w>' ) \n group_info = { } \n count = <NUM_LIT:0> \n with open ( '<STR_LIT>' ) as f : \n while True : \n line = f . readline ( ) . strip ( ) \n if len ( line ) == <NUM_LIT:0> : break \n pbid1 , groups1 = line . strip ( ) . split ( '<STR_LIT:\\t>' ) \n pbid2 , groups2 = f . readline ( ) . strip ( ) . split ( '<STR_LIT:\\t>' ) \n assert pbid1 . split ( '<STR_LIT:.>' ) [ <NUM_LIT:1> ] == pbid2 . split ( '<STR_LIT:.>' ) [ <NUM_LIT:1> ] \n group = set ( groups1 . split ( '<STR_LIT:U+002C>' ) ) . intersection ( groups2 . split ( '<STR_LIT:U+002C>' ) ) \n f_group . write ( \"<STR_LIT>\" . format ( pbid1 [ : pbid1 . rfind ( '<STR_LIT:.>' ) ] , \"<STR_LIT:U+002C>\" . join ( group ) ) ) \n group_info [ pbid1 [ : pbid1 . rfind ( '<STR_LIT:.>' ) ] ] = list ( group ) \n count += <NUM_LIT:1> \n f_group . close ( ) \n gff_filename = output_prefix + '<STR_LIT>' \n group_filename = output_prefix + '<STR_LIT>' \n if is_fq : \n output_filename = output_prefix + '<STR_LIT>' \n else : \n output_filename = output_prefix + '<STR_LIT>' \n pick_rep ( fa_or_fq_filename , sam_filename , gff_filename , group_filename , output_filename , is_fq = is_fq , pick_least_err_instead = False ) \n print >> sys . stderr , \"<STR_LIT>\" . format ( count ) \n print >> sys . stderr , \"<STR_LIT>\" . format ( output_prefix , output_filename ) \n if prefix_dict_pickle_filename is not None : \n with open ( prefix_dict_pickle_filename ) as f : \n d = load ( f ) \n d1 = d [ '<STR_LIT>' ] \n d1 . update ( d [ '<STR_LIT>' ] ) \n tofu_wrap . get_abundance ( output_prefix , d1 , output_prefix ) \n print >> sys . stderr , \"<STR_LIT>\" . format ( output_prefix ) \n if __name__ == \"<STR_LIT:__main__>\" : \n from argparse import ArgumentParser \n parser = ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , default = False , action = \"<STR_LIT:store_true>\" , help = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , required = True , help = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , required = True , help = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , action = \"<STR_LIT>\" , dest = \"<STR_LIT>\" , default = True , help = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , default = None , help = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT:-c>\" , \"<STR_LIT>\" , type = float , default = <NUM_LIT> , help = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , type = int , default = <NUM_LIT:1> , help = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = float , default = <NUM_LIT> , help = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> , help = \"<STR_LIT>\" ) \n args = parser . parse_args ( ) \n fusion_main ( args . input , args . sam , args . prefix , \n is_fq = args . fq , allow_extra_5_exons = args . allow_extra_5exon , \n skip_5_exon_alt = False , prefix_dict_pickle_filename = args . prefix_dict_pickle_filename , \n min_locus_coverage = args . min_locus_coverage , min_locus_coverage_bp = args . min_locus_coverage_bp , \n min_total_coverage = args . min_total_coverage , \n min_dist_between_loci = args . <mask0> ) \n", "gt": "min_dist_between_loci"}
{"input": "\n __author__ = '<STR_LIT>' \n import pdb \n import os , sys , subprocess \n import numpy \n from pbtools . pbtranscript . io . BLASRRecord import BLASRRecord \n from pbtools . pbtranscript . ice . IceUtils import HitItem , eval_blasr_alignment , alignment_has_large_nonmatch \n from pbtools . pbtranscript . ice . c_IceAlign import get_ece_arr_from_alignment \n class LAshowAlignReader : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , las_out_filename ) : \n self . las_out_filename = las_out_filename \n self . f = open ( las_out_filename ) \n def __iter__ ( self ) : \n return self \n def next ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n raw = self . f . readline ( ) . strip ( ) . split ( ) \n if raw [ <NUM_LIT:0> ] == '<STR_LIT:+>' and raw [ <NUM_LIT:1> ] == '<STR_LIT:+>' : \n raise StopIteration \n qID = int ( raw [ <NUM_LIT:0> ] ) + <NUM_LIT:1> \n sID = int ( raw [ <NUM_LIT:1> ] ) + <NUM_LIT:1> \n score = int ( raw [ <NUM_LIT:2> ] ) \n iden = float ( raw [ <NUM_LIT:3> ] ) \n qStrand = int ( raw [ <NUM_LIT:4> ] ) \n qStart = int ( raw [ <NUM_LIT:5> ] ) \n qEnd = int ( raw [ <NUM_LIT:6> ] ) \n qLen = int ( raw [ <NUM_LIT:7> ] ) \n sStrand = int ( raw [ <NUM_LIT:8> ] ) \n sStart = int ( raw [ <NUM_LIT:9> ] ) \n sEnd = int ( raw [ <NUM_LIT:10> ] ) \n sLen = int ( raw [ <NUM_LIT:11> ] ) \n self . f . readline ( ) \n _qStart , qAln = self . f . readline ( ) . strip ( ) . split ( ) \n assert ( qStrand == <NUM_LIT:0> and int ( _qStart ) - <NUM_LIT:1> == qStart ) or ( qStrand == <NUM_LIT:1> and int ( _qStart ) - <NUM_LIT:1> == qLen - qEnd ) \n alnStr = self . f . readline ( ) . strip ( ) \n _sStart , sAln = self . f . readline ( ) . strip ( ) . split ( ) [ : <NUM_LIT:2> ] \n assert ( sStrand == <NUM_LIT:0> and int ( _sStart ) - <NUM_LIT:1> == sStart ) or ( sStrand == <NUM_LIT:1> and int ( _sStart ) - <NUM_LIT:1> == sLen - sEnd ) \n return BLASRRecord ( qID , qLen , qStart , qEnd , qStrand , sID , sLen , sStart , sEnd , sStrand , score , None , qAln = qAln , alnStr = alnStr , sAln = sAln , identity = iden , strand = '<STR_LIT:+>' if qStrand == sStrand else '<STR_LIT:->' ) \n def dalign_against_ref ( dazz_query_obj , dazz_db_obj , las_out_filename , is_FL , sID_starts_with_c , \n qver_get_func , qvmean_get_func , qv_prob_threshold = <NUM_LIT> , \n ece_penalty = <NUM_LIT:1> , ece_min_len = <NUM_LIT:20> , same_strand_only = True , no_qv_or_aln_checking = False , \n max_missed_start = <NUM_LIT:200> , max_missed_end = <NUM_LIT:50> ) : \n \"\"\"<STR_LIT>\"\"\" \n for r in LAshowAlignReader ( las_out_filename ) : \n missed_q = r . qStart + r . qLength - r . qEnd \n missed_t = r . sStart + r . sLength - r . sEnd \n r . qID = dazz_query_obj [ r . qID ] \n r . sID = dazz_db_obj [ r . sID ] \n if sID_starts_with_c : \n assert r . sID . startswith ( '<STR_LIT:c>' ) \n if r . sID . find ( '<STR_LIT:/>' ) > <NUM_LIT:0> : \n r . sID = r . sID . split ( '<STR_LIT:/>' ) [ <NUM_LIT:0> ] \n if r . sID . endswith ( '<STR_LIT>' ) : \n cID = int ( r . sID [ <NUM_LIT:1> : - <NUM_LIT:4> ] ) \n else : \n cID = int ( r . sID [ <NUM_LIT:1> : ] ) \n else : \n cID = r . sID \n if ( cID == r . qID or \n ( r . strand == '<STR_LIT:->' and same_strand_only ) ) : \n yield HitItem ( qID = r . qID , cID = cID ) \n continue \n if no_qv_or_aln_checking : \n yield HitItem ( qID = r . qID , cID = cID , \n qStart = r . qStart , qEnd = r . qEnd , \n missed_q = missed_q * <NUM_LIT:1.> / r . qLength , \n missed_t = missed_t * <NUM_LIT:1.> / r . sLength , \n fakecigar = <NUM_LIT:1> , \n ece_arr = <NUM_LIT:1> ) \n continue \n if ( is_FL and ( r . sStart > max_missed_start or r . qStart > max_missed_start or \n ( r . sLength - r . sEnd > max_missed_end ) or \n ( r . qLength - r . qEnd > max_missed_end ) ) ) : \n yield HitItem ( qID = r . qID , cID = cID ) \n else : \n cigar_str , ece_arr = eval_blasr_alignment ( \n record = r , \n qver_get_func = qver_get_func , \n sID_starts_with_c = sID_starts_with_c , \n qv_prob_threshold = qv_prob_threshold , \n qvmean_get_func = qvmean_get_func ) \n if alignment_has_large_nonmatch ( ece_arr , \n ece_penalty , ece_min_len ) : \n yield HitItem ( qID = r . qID , cID = cID ) \n else : \n yield HitItem ( qID = r . qID , cID = cID , \n qStart = r . qStart , qEnd = r . qEnd , \n missed_q = missed_q * <NUM_LIT:1.> / r . qLength , \n missed_t = missed_t * <NUM_LIT:1.> / r . sLength , \n fakecigar = cigar_str , \n ece_arr = <mask0> ) \n", "gt": "ece_arr"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import unittest \n class TestInitICE ( unittest . TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . rootDir = op . dirname ( op . dirname ( op . abspath ( __file__ ) ) ) \n self . testDir = op . join ( self . <mask0> , \"<STR_LIT>\" ) \n", "gt": "rootDir"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n __version__ = '<STR_LIT>' \n __all__ = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n import copy as _copy \n import os as _os \n import re as _re \n import sys as _sys \n import textwrap as _textwrap \n from gettext import gettext as _ \n try : \n set \n except NameError : \n from sets import Set as set \n try : \n basestring \n except NameError : \n basestring = str \n try : \n sorted \n except NameError : \n def sorted ( iterable , reverse = False ) : \n result = list ( iterable ) \n result . sort ( ) \n if reverse : \n result . reverse ( ) \n return result \n def _callable ( obj ) : \n return hasattr ( obj , '<STR_LIT>' ) or hasattr ( obj , '<STR_LIT>' ) \n SUPPRESS = '<STR_LIT>' \n OPTIONAL = '<STR_LIT:?>' \n ZERO_OR_MORE = '<STR_LIT:*>' \n ONE_OR_MORE = '<STR_LIT:+>' \n PARSER = '<STR_LIT>' \n REMAINDER = '<STR_LIT>' \n _UNRECOGNIZED_ARGS_ATTR = '<STR_LIT>' \n class _AttributeHolder ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __repr__ ( self ) : \n type_name = type ( self ) . __name__ \n arg_strings = [ ] \n for arg in self . _get_args ( ) : \n arg_strings . append ( repr ( arg ) ) \n for name , value in self . _get_kwargs ( ) : \n arg_strings . append ( '<STR_LIT>' % ( name , value ) ) \n return '<STR_LIT>' % ( type_name , '<STR_LIT:U+002CU+0020>' . join ( arg_strings ) ) \n def _get_kwargs ( self ) : \n return sorted ( self . __dict__ . items ( ) ) \n def _get_args ( self ) : \n return [ ] \n def _ensure_value ( namespace , name , value ) : \n if getattr ( namespace , name , None ) is None : \n setattr ( namespace , name , value ) \n return getattr ( namespace , name ) \n class HelpFormatter ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , \n prog , \n indent_increment = <NUM_LIT:2> , \n max_help_position = <NUM_LIT> , \n width = None ) : \n if width is None : \n try : \n width = int ( _os . environ [ '<STR_LIT>' ] ) \n except ( KeyError , ValueError ) : \n width = <NUM_LIT> \n width -= <NUM_LIT:2> \n self . _prog = prog \n self . _indent_increment = indent_increment \n self . _max_help_position = max_help_position \n self . _width = width \n self . _current_indent = <NUM_LIT:0> \n self . _level = <NUM_LIT:0> \n self . _action_max_length = <NUM_LIT:0> \n self . _root_section = self . _Section ( self , None ) \n self . _current_section = self . _root_section \n self . _whitespace_matcher = _re . compile ( r'<STR_LIT>' ) \n self . _long_break_matcher = _re . compile ( r'<STR_LIT>' ) \n def _indent ( self ) : \n self . _current_indent += self . _indent_increment \n self . _level += <NUM_LIT:1> \n def _dedent ( self ) : \n self . _current_indent -= self . _indent_increment \n assert self . _current_indent >= <NUM_LIT:0> , '<STR_LIT>' \n self . _level -= <NUM_LIT:1> \n class _Section ( object ) : \n def __init__ ( self , formatter , parent , heading = None ) : \n self . formatter = formatter \n self . parent = parent \n self . heading = heading \n self . items = [ ] \n def format_help ( self ) : \n if self . parent is not None : \n self . formatter . _indent ( ) \n join = self . formatter . _join_parts \n for func , args in self . items : \n func ( * args ) \n item_help = join ( [ func ( * args ) for func , args in self . items ] ) \n if self . parent is not None : \n self . formatter . _dedent ( ) \n if not item_help : \n return '<STR_LIT>' \n if self . heading is not SUPPRESS and self . heading is not None : \n current_indent = self . formatter . _current_indent \n heading = '<STR_LIT>' % ( current_indent , '<STR_LIT>' , self . heading ) \n else : \n heading = '<STR_LIT>' \n return join ( [ '<STR_LIT:\\n>' , heading , item_help , '<STR_LIT:\\n>' ] ) \n def _add_item ( self , func , args ) : \n self . _current_section . items . append ( ( func , args ) ) \n def start_section ( self , heading ) : \n self . _indent ( ) \n section = self . _Section ( self , self . _current_section , heading ) \n self . _add_item ( section . format_help , [ ] ) \n self . _current_section = section \n def end_section ( self ) : \n self . _current_section = self . _current_section . parent \n self . _dedent ( ) \n def add_text ( self , text ) : \n if text is not SUPPRESS and text is not None : \n self . _add_item ( self . _format_text , [ text ] ) \n def add_usage ( self , usage , actions , groups , prefix = None ) : \n if usage is not SUPPRESS : \n args = usage , actions , groups , prefix \n self . _add_item ( self . _format_usage , args ) \n def add_argument ( self , action ) : \n if action . help is not SUPPRESS : \n get_invocation = self . _format_action_invocation \n invocations = [ get_invocation ( action ) ] \n for subaction in self . _iter_indented_subactions ( action ) : \n invocations . append ( get_invocation ( subaction ) ) \n invocation_length = max ( [ len ( s ) for s in invocations ] ) \n action_length = invocation_length + self . _current_indent \n self . _action_max_length = max ( self . _action_max_length , \n action_length ) \n self . _add_item ( self . _format_action , [ action ] ) \n def add_arguments ( self , actions ) : \n for action in actions : \n self . add_argument ( action ) \n def format_help ( self ) : \n help = self . _root_section . format_help ( ) \n if help : \n help = self . _long_break_matcher . sub ( '<STR_LIT>' , help ) \n help = help . strip ( '<STR_LIT:\\n>' ) + '<STR_LIT:\\n>' \n return help \n def _join_parts ( self , part_strings ) : \n return '<STR_LIT>' . join ( [ part \n for part in part_strings \n if part and part is not SUPPRESS ] ) \n def _format_usage ( self , usage , actions , groups , prefix ) : \n if prefix is None : \n prefix = _ ( '<STR_LIT>' ) \n if usage is not None : \n usage = usage % dict ( prog = self . _prog ) \n elif usage is None and not actions : \n usage = '<STR_LIT>' % dict ( prog = self . _prog ) \n elif usage is None : \n prog = '<STR_LIT>' % dict ( prog = self . _prog ) \n optionals = [ ] \n positionals = [ ] \n for action in actions : \n if action . option_strings : \n optionals . append ( action ) \n else : \n positionals . append ( action ) \n format = self . _format_actions_usage \n action_usage = format ( optionals + positionals , groups ) \n usage = '<STR_LIT:U+0020>' . join ( [ s for s in [ prog , action_usage ] if s ] ) \n text_width = self . _width - self . _current_indent \n if len ( prefix ) + len ( usage ) > text_width : \n part_regexp = r'<STR_LIT>' \n opt_usage = format ( optionals , groups ) \n pos_usage = format ( positionals , groups ) \n opt_parts = _re . findall ( part_regexp , opt_usage ) \n pos_parts = _re . findall ( part_regexp , pos_usage ) \n assert '<STR_LIT:U+0020>' . join ( opt_parts ) == opt_usage \n assert '<STR_LIT:U+0020>' . join ( pos_parts ) == pos_usage \n def get_lines ( parts , indent , prefix = None ) : \n lines = [ ] \n line = [ ] \n if prefix is not None : \n line_len = len ( prefix ) - <NUM_LIT:1> \n else : \n line_len = len ( indent ) - <NUM_LIT:1> \n for part in parts : \n if line_len + <NUM_LIT:1> + len ( part ) > text_width : \n lines . append ( indent + '<STR_LIT:U+0020>' . join ( line ) ) \n line = [ ] \n line_len = len ( indent ) - <NUM_LIT:1> \n line . append ( part ) \n line_len += len ( part ) + <NUM_LIT:1> \n if line : \n lines . append ( indent + '<STR_LIT:U+0020>' . join ( line ) ) \n if prefix is not None : \n lines [ <NUM_LIT:0> ] = lines [ <NUM_LIT:0> ] [ len ( indent ) : ] \n return lines \n if len ( prefix ) + len ( prog ) <= <NUM_LIT> * text_width : \n indent = '<STR_LIT:U+0020>' * ( len ( prefix ) + len ( prog ) + <NUM_LIT:1> ) \n if opt_parts : \n lines = get_lines ( [ prog ] + opt_parts , indent , prefix ) \n lines . extend ( get_lines ( pos_parts , indent ) ) \n elif pos_parts : \n lines = get_lines ( [ prog ] + pos_parts , indent , prefix ) \n else : \n lines = [ prog ] \n else : \n indent = '<STR_LIT:U+0020>' * len ( prefix ) \n parts = opt_parts + pos_parts \n lines = get_lines ( parts , indent ) \n if len ( lines ) > <NUM_LIT:1> : \n lines = [ ] \n lines . extend ( get_lines ( opt_parts , indent ) ) \n lines . extend ( get_lines ( pos_parts , indent ) ) \n lines = [ prog ] + lines \n usage = '<STR_LIT:\\n>' . join ( lines ) \n return '<STR_LIT>' % ( prefix , usage ) \n def _format_actions_usage ( self , actions , groups ) : \n group_actions = set ( ) \n inserts = { } \n for group in groups : \n try : \n start = actions . index ( group . _group_actions [ <NUM_LIT:0> ] ) \n except ValueError : \n continue \n else : \n end = start + len ( group . _group_actions ) \n if actions [ start : end ] == group . _group_actions : \n for action in group . _group_actions : \n group_actions . add ( action ) \n if not group . required : \n if start in inserts : \n inserts [ start ] += '<STR_LIT>' \n else : \n inserts [ start ] = '<STR_LIT:[>' \n inserts [ end ] = '<STR_LIT:]>' \n else : \n if start in inserts : \n inserts [ start ] += '<STR_LIT>' \n else : \n inserts [ start ] = '<STR_LIT:(>' \n inserts [ end ] = '<STR_LIT:)>' \n for i in range ( start + <NUM_LIT:1> , end ) : \n inserts [ i ] = '<STR_LIT:|>' \n parts = [ ] \n for i , action in enumerate ( actions ) : \n if action . help is SUPPRESS : \n parts . append ( None ) \n if inserts . get ( i ) == '<STR_LIT:|>' : \n inserts . pop ( i ) \n elif inserts . get ( i + <NUM_LIT:1> ) == '<STR_LIT:|>' : \n inserts . pop ( i + <NUM_LIT:1> ) \n elif not action . option_strings : \n part = self . _format_args ( action , action . dest ) \n if action in group_actions : \n if part [ <NUM_LIT:0> ] == '<STR_LIT:[>' and part [ - <NUM_LIT:1> ] == '<STR_LIT:]>' : \n part = part [ <NUM_LIT:1> : - <NUM_LIT:1> ] \n parts . append ( part ) \n else : \n option_string = action . option_strings [ <NUM_LIT:0> ] \n if action . nargs == <NUM_LIT:0> : \n part = '<STR_LIT:%s>' % option_string \n else : \n default = action . dest . upper ( ) \n args_string = self . _format_args ( action , default ) \n part = '<STR_LIT>' % ( option_string , args_string ) \n if not action . required and action not in group_actions : \n part = '<STR_LIT>' % part \n parts . append ( part ) \n for i in sorted ( inserts , reverse = True ) : \n parts [ i : i ] = [ inserts [ i ] ] \n text = '<STR_LIT:U+0020>' . join ( [ item for item in parts if item is not None ] ) \n open = r'<STR_LIT>' \n close = r'<STR_LIT>' \n text = _re . sub ( r'<STR_LIT>' % open , r'<STR_LIT>' , text ) \n text = _re . sub ( r'<STR_LIT>' % close , r'<STR_LIT>' , text ) \n text = _re . sub ( r'<STR_LIT>' % ( open , close ) , r'<STR_LIT>' , text ) \n text = _re . sub ( r'<STR_LIT>' , r'<STR_LIT>' , text ) \n text = text . strip ( ) \n return text \n def _format_text ( self , text ) : \n if '<STR_LIT>' in text : \n text = text % dict ( prog = self . _prog ) \n text_width = self . _width - self . _current_indent \n indent = '<STR_LIT:U+0020>' * self . _current_indent \n return self . _fill_text ( text , text_width , indent ) + '<STR_LIT>' \n def _format_action ( self , action ) : \n help_position = min ( self . _action_max_length + <NUM_LIT:2> , \n self . _max_help_position ) \n help_width = self . _width - help_position \n action_width = help_position - self . _current_indent - <NUM_LIT:2> \n action_header = self . _format_action_invocation ( action ) \n if not action . help : \n tup = self . _current_indent , '<STR_LIT>' , action_header \n action_header = '<STR_LIT>' % tup \n elif len ( action_header ) <= action_width : \n tup = self . _current_indent , '<STR_LIT>' , action_width , action_header \n action_header = '<STR_LIT>' % tup \n indent_first = <NUM_LIT:0> \n else : \n tup = self . _current_indent , '<STR_LIT>' , action_header \n action_header = '<STR_LIT>' % tup \n indent_first = help_position \n parts = [ action_header ] \n if action . help : \n help_text = self . _expand_help ( action ) \n help_lines = self . _split_lines ( help_text , help_width ) \n parts . append ( '<STR_LIT>' % ( indent_first , '<STR_LIT>' , help_lines [ <NUM_LIT:0> ] ) ) \n for line in help_lines [ <NUM_LIT:1> : ] : \n parts . append ( '<STR_LIT>' % ( help_position , '<STR_LIT>' , line ) ) \n elif not action_header . endswith ( '<STR_LIT:\\n>' ) : \n parts . append ( '<STR_LIT:\\n>' ) \n for subaction in self . _iter_indented_subactions ( action ) : \n parts . append ( self . _format_action ( subaction ) ) \n return self . _join_parts ( parts ) \n def _format_action_invocation ( self , action ) : \n if not action . option_strings : \n metavar , = self . _metavar_formatter ( action , action . dest ) ( <NUM_LIT:1> ) \n return metavar \n else : \n parts = [ ] \n if action . nargs == <NUM_LIT:0> : \n parts . extend ( action . option_strings ) \n else : \n default = action . dest . upper ( ) \n args_string = self . _format_args ( action , default ) \n for option_string in action . option_strings : \n parts . append ( '<STR_LIT>' % ( option_string , args_string ) ) \n return '<STR_LIT:U+002CU+0020>' . join ( parts ) \n def _metavar_formatter ( self , action , default_metavar ) : \n if action . metavar is not None : \n result = action . metavar \n elif action . choices is not None : \n choice_strs = [ str ( choice ) for choice in action . choices ] \n result = '<STR_LIT>' % '<STR_LIT:U+002C>' . join ( choice_strs ) \n else : \n result = default_metavar \n def format ( tuple_size ) : \n if isinstance ( result , tuple ) : \n return result \n else : \n return ( result , ) * tuple_size \n return format \n def _format_args ( self , action , default_metavar ) : \n get_metavar = self . _metavar_formatter ( action , default_metavar ) \n if action . nargs is None : \n result = '<STR_LIT:%s>' % get_metavar ( <NUM_LIT:1> ) \n elif action . nargs == OPTIONAL : \n result = '<STR_LIT>' % get_metavar ( <NUM_LIT:1> ) \n elif action . nargs == ZERO_OR_MORE : \n result = '<STR_LIT>' % get_metavar ( <NUM_LIT:2> ) \n elif action . nargs == ONE_OR_MORE : \n result = '<STR_LIT>' % get_metavar ( <NUM_LIT:2> ) \n elif action . nargs == REMAINDER : \n result = '<STR_LIT>' \n elif action . nargs == PARSER : \n result = '<STR_LIT>' % get_metavar ( <NUM_LIT:1> ) \n else : \n formats = [ '<STR_LIT:%s>' for _ in range ( action . nargs ) ] \n result = '<STR_LIT:U+0020>' . join ( formats ) % get_metavar ( action . nargs ) \n return result \n def _expand_help ( self , action ) : \n params = dict ( vars ( action ) , prog = self . _prog ) \n for name in list ( params ) : \n if params [ name ] is SUPPRESS : \n del params [ name ] \n for name in list ( params ) : \n if hasattr ( params [ name ] , '<STR_LIT>' ) : \n params [ name ] = params [ name ] . __name__ \n if params . get ( '<STR_LIT>' ) is not None : \n choices_str = '<STR_LIT:U+002CU+0020>' . join ( [ str ( c ) for c in params [ '<STR_LIT>' ] ] ) \n params [ '<STR_LIT>' ] = choices_str \n return self . _get_help_string ( action ) % params \n def _iter_indented_subactions ( self , action ) : \n try : \n get_subactions = action . _get_subactions \n except AttributeError : \n pass \n else : \n self . _indent ( ) \n for subaction in get_subactions ( ) : \n yield subaction \n self . _dedent ( ) \n def _split_lines ( self , text , width ) : \n text = self . _whitespace_matcher . sub ( '<STR_LIT:U+0020>' , text ) . strip ( ) \n return _textwrap . wrap ( text , width ) \n def _fill_text ( self , text , width , indent ) : \n text = self . _whitespace_matcher . sub ( '<STR_LIT:U+0020>' , text ) . strip ( ) \n return _textwrap . fill ( text , width , initial_indent = indent , \n subsequent_indent = indent ) \n def _get_help_string ( self , action ) : \n return action . help \n class RawDescriptionHelpFormatter ( HelpFormatter ) : \n \"\"\"<STR_LIT>\"\"\" \n def _fill_text ( self , text , width , indent ) : \n return '<STR_LIT>' . join ( [ indent + line for line in text . splitlines ( True ) ] ) \n class RawTextHelpFormatter ( RawDescriptionHelpFormatter ) : \n \"\"\"<STR_LIT>\"\"\" \n def _split_lines ( self , text , width ) : \n return text . splitlines ( ) \n class ArgumentDefaultsHelpFormatter ( HelpFormatter ) : \n \"\"\"<STR_LIT>\"\"\" \n def _get_help_string ( self , action ) : \n help = action . help \n if '<STR_LIT>' not in action . help : \n if action . default is not SUPPRESS : \n defaulting_nargs = [ OPTIONAL , ZERO_OR_MORE ] \n if action . option_strings or action . nargs in defaulting_nargs : \n help += '<STR_LIT>' \n return help \n def _get_action_name ( argument ) : \n if argument is None : \n return None \n elif argument . option_strings : \n return '<STR_LIT:/>' . join ( argument . option_strings ) \n elif argument . metavar not in ( None , SUPPRESS ) : \n return argument . metavar \n elif argument . dest not in ( None , SUPPRESS ) : \n return argument . dest \n else : \n return None \n class ArgumentError ( Exception ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , argument , message ) : \n self . argument_name = _get_action_name ( argument ) \n self . message = message \n def __str__ ( self ) : \n if self . argument_name is None : \n format = '<STR_LIT>' \n else : \n format = '<STR_LIT>' \n return format % dict ( message = self . message , \n argument_name = self . argument_name ) \n class ArgumentTypeError ( Exception ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n class Action ( _AttributeHolder ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , \n option_strings , \n dest , \n nargs = None , \n const = None , \n default = None , \n type = None , \n choices = None , \n required = False , \n help = None , \n metavar = None ) : \n self . option_strings = option_strings \n self . dest = dest \n self . nargs = nargs \n self . const = const \n self . default = default \n self . type = type \n self . choices = choices \n self . required = required \n self . help = help \n self . metavar = metavar \n def _get_kwargs ( self ) : \n names = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT:default>' , \n '<STR_LIT:type>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n return [ ( name , getattr ( self , name ) ) for name in names ] \n def __call__ ( self , parser , namespace , values , option_string = None ) : \n raise NotImplementedError ( _ ( '<STR_LIT>' ) ) \n class _StoreAction ( Action ) : \n def __init__ ( self , \n option_strings , \n dest , \n nargs = None , \n const = None , \n default = None , \n type = None , \n choices = None , \n required = False , \n help = None , \n metavar = None ) : \n if nargs == <NUM_LIT:0> : \n raise ValueError ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n if const is not None and nargs != OPTIONAL : \n raise ValueError ( '<STR_LIT>' % OPTIONAL ) \n super ( _StoreAction , self ) . __init__ ( \n option_strings = option_strings , \n dest = dest , \n nargs = nargs , \n const = const , \n default = default , \n type = type , \n choices = choices , \n required = required , \n help = help , \n metavar = metavar ) \n def __call__ ( self , parser , namespace , values , option_string = None ) : \n setattr ( namespace , self . dest , values ) \n class _StoreConstAction ( Action ) : \n def __init__ ( self , \n option_strings , \n dest , \n const , \n default = None , \n required = False , \n help = None , \n metavar = None ) : \n super ( _StoreConstAction , self ) . __init__ ( \n option_strings = option_strings , \n dest = dest , \n nargs = <NUM_LIT:0> , \n const = const , \n default = default , \n required = required , \n help = help ) \n def __call__ ( self , parser , namespace , values , option_string = None ) : \n setattr ( namespace , self . dest , self . const ) \n class _StoreTrueAction ( _StoreConstAction ) : \n def __init__ ( self , \n option_strings , \n dest , \n default = False , \n required = False , \n help = None ) : \n super ( _StoreTrueAction , self ) . __init__ ( \n option_strings = option_strings , \n dest = dest , \n const = True , \n default = default , \n required = required , \n help = help ) \n class _StoreFalseAction ( _StoreConstAction ) : \n def __init__ ( self , \n option_strings , \n dest , \n default = True , \n required = False , \n help = None ) : \n super ( _StoreFalseAction , self ) . __init__ ( \n option_strings = option_strings , \n dest = dest , \n const = False , \n default = default , \n required = required , \n help = help ) \n class _AppendAction ( Action ) : \n def __init__ ( self , \n option_strings , \n dest , \n nargs = None , \n const = None , \n default = None , \n type = None , \n choices = None , \n required = False , \n help = None , \n metavar = None ) : \n if nargs == <NUM_LIT:0> : \n raise ValueError ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n if const is not None and nargs != OPTIONAL : \n raise ValueError ( '<STR_LIT>' % OPTIONAL ) \n super ( _AppendAction , self ) . __init__ ( \n option_strings = option_strings , \n dest = dest , \n nargs = nargs , \n const = const , \n default = default , \n type = type , \n choices = choices , \n required = required , \n help = help , \n metavar = metavar ) \n def __call__ ( self , parser , namespace , values , option_string = None ) : \n items = _copy . copy ( _ensure_value ( namespace , self . dest , [ ] ) ) \n items . append ( values ) \n setattr ( namespace , self . dest , items ) \n class _AppendConstAction ( Action ) : \n def __init__ ( self , \n option_strings , \n dest , \n const , \n default = None , \n required = False , \n help = None , \n metavar = None ) : \n super ( _AppendConstAction , self ) . __init__ ( \n option_strings = option_strings , \n dest = dest , \n nargs = <NUM_LIT:0> , \n const = const , \n default = default , \n required = required , \n help = help , \n metavar = metavar ) \n def __call__ ( self , parser , namespace , values , option_string = None ) : \n items = _copy . copy ( _ensure_value ( namespace , self . dest , [ ] ) ) \n items . append ( self . const ) \n setattr ( namespace , self . dest , items ) \n class _CountAction ( Action ) : \n def __init__ ( self , \n option_strings , \n dest , \n default = None , \n required = False , \n help = None ) : \n super ( _CountAction , self ) . __init__ ( \n option_strings = option_strings , \n dest = dest , \n nargs = <NUM_LIT:0> , \n default = default , \n required = required , \n help = help ) \n def __call__ ( self , parser , namespace , values , option_string = None ) : \n new_count = _ensure_value ( namespace , self . dest , <NUM_LIT:0> ) + <NUM_LIT:1> \n setattr ( namespace , self . dest , new_count ) \n class _HelpAction ( Action ) : \n def __init__ ( self , \n option_strings , \n dest = SUPPRESS , \n default = SUPPRESS , \n help = None ) : \n super ( _HelpAction , self ) . __init__ ( \n option_strings = option_strings , \n dest = dest , \n default = default , \n nargs = <NUM_LIT:0> , \n help = help ) \n def __call__ ( self , parser , namespace , values , option_string = None ) : \n parser . print_help ( ) \n parser . exit ( ) \n class _VersionAction ( Action ) : \n def __init__ ( self , \n option_strings , \n version = None , \n dest = SUPPRESS , \n default = SUPPRESS , \n help = \"<STR_LIT>\" ) : \n super ( _VersionAction , self ) . __init__ ( \n option_strings = option_strings , \n dest = dest , \n default = default , \n nargs = <NUM_LIT:0> , \n help = help ) \n self . version = version \n def __call__ ( self , parser , namespace , values , option_string = None ) : \n version = self . version \n if version is None : \n version = parser . version \n formatter = parser . _get_formatter ( ) \n formatter . add_text ( version ) \n parser . exit ( message = formatter . format_help ( ) ) \n class _SubParsersAction ( Action ) : \n class _ChoicesPseudoAction ( Action ) : \n def __init__ ( self , name , help ) : \n sup = super ( _SubParsersAction . _ChoicesPseudoAction , self ) \n sup . __init__ ( option_strings = [ ] , dest = name , help = help ) \n def __init__ ( self , \n option_strings , \n prog , \n parser_class , \n dest = SUPPRESS , \n help = None , \n metavar = None ) : \n self . _prog_prefix = prog \n self . _parser_class = parser_class \n self . _name_parser_map = { } \n self . _choices_actions = [ ] \n super ( _SubParsersAction , self ) . __init__ ( \n option_strings = option_strings , \n dest = dest , \n nargs = PARSER , \n choices = self . _name_parser_map , \n help = help , \n metavar = metavar ) \n def add_parser ( self , name , ** kwargs ) : \n if kwargs . get ( '<STR_LIT>' ) is None : \n kwargs [ '<STR_LIT>' ] = '<STR_LIT>' % ( self . _prog_prefix , name ) \n if '<STR_LIT>' in kwargs : \n help = kwargs . pop ( '<STR_LIT>' ) \n choice_action = self . _ChoicesPseudoAction ( name , help ) \n self . _choices_actions . append ( choice_action ) \n parser = self . _parser_class ( ** kwargs ) \n self . _name_parser_map [ name ] = parser \n return parser \n def _get_subactions ( self ) : \n return self . _choices_actions \n def __call__ ( self , parser , namespace , values , option_string = None ) : \n parser_name = values [ <NUM_LIT:0> ] \n arg_strings = values [ <NUM_LIT:1> : ] \n if self . dest is not SUPPRESS : \n setattr ( namespace , self . dest , parser_name ) \n try : \n parser = self . _name_parser_map [ parser_name ] \n except KeyError : \n tup = parser_name , '<STR_LIT:U+002CU+0020>' . join ( self . _name_parser_map ) \n msg = _ ( '<STR_LIT>' % tup ) \n raise ArgumentError ( self , msg ) \n namespace , arg_strings = parser . parse_known_args ( arg_strings , namespace ) \n if arg_strings : \n vars ( namespace ) . setdefault ( _UNRECOGNIZED_ARGS_ATTR , [ ] ) \n getattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) . extend ( arg_strings ) \n class FileType ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , mode = '<STR_LIT:r>' , bufsize = None ) : \n self . _mode = mode \n self . _bufsize = bufsize \n def __call__ ( self , string ) : \n if string == '<STR_LIT:->' : \n if '<STR_LIT:r>' in self . _mode : \n return _sys . stdin \n elif '<STR_LIT:w>' in self . _mode : \n return _sys . stdout \n else : \n msg = _ ( '<STR_LIT>' % self . _mode ) \n raise ValueError ( msg ) \n if self . _bufsize : \n return open ( string , self . _mode , self . _bufsize ) \n else : \n return open ( string , self . _mode ) \n def __repr__ ( self ) : \n args = [ self . _mode , self . _bufsize ] \n args_str = '<STR_LIT:U+002CU+0020>' . join ( [ repr ( arg ) for arg in args if arg is not None ] ) \n return '<STR_LIT>' % ( type ( self ) . __name__ , args_str ) \n class Namespace ( _AttributeHolder ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , ** kwargs ) : \n for name in kwargs : \n setattr ( self , name , kwargs [ name ] ) \n __hash__ = None \n def __eq__ ( self , other ) : \n return vars ( self ) == vars ( other ) \n def __ne__ ( self , other ) : \n return not ( self == other ) \n def __contains__ ( self , key ) : \n return key in self . __dict__ \n class _ActionsContainer ( object ) : \n def __init__ ( self , \n description , \n prefix_chars , \n argument_default , \n conflict_handler ) : \n super ( _ActionsContainer , self ) . __init__ ( ) \n self . description = description \n self . argument_default = argument_default \n self . prefix_chars = prefix_chars \n self . conflict_handler = conflict_handler \n self . _registries = { } \n self . register ( '<STR_LIT:action>' , None , _StoreAction ) \n self . register ( '<STR_LIT:action>' , '<STR_LIT:store>' , _StoreAction ) \n self . register ( '<STR_LIT:action>' , '<STR_LIT>' , _StoreConstAction ) \n self . register ( '<STR_LIT:action>' , '<STR_LIT:store_true>' , _StoreTrueAction ) \n self . register ( '<STR_LIT:action>' , '<STR_LIT>' , _StoreFalseAction ) \n self . register ( '<STR_LIT:action>' , '<STR_LIT>' , _AppendAction ) \n self . register ( '<STR_LIT:action>' , '<STR_LIT>' , _AppendConstAction ) \n self . register ( '<STR_LIT:action>' , '<STR_LIT:count>' , _CountAction ) \n self . register ( '<STR_LIT:action>' , '<STR_LIT>' , _HelpAction ) \n self . register ( '<STR_LIT:action>' , '<STR_LIT:version>' , _VersionAction ) \n self . register ( '<STR_LIT:action>' , '<STR_LIT>' , _SubParsersAction ) \n self . _get_handler ( ) \n self . _actions = [ ] \n self . _option_string_actions = { } \n self . _action_groups = [ ] \n self . _mutually_exclusive_groups = [ ] \n self . _defaults = { } \n self . _negative_number_matcher = _re . compile ( r'<STR_LIT>' ) \n self . _has_negative_number_optionals = [ ] \n def register ( self , registry_name , value , object ) : \n registry = self . _registries . setdefault ( registry_name , { } ) \n registry [ value ] = object \n def _registry_get ( self , registry_name , value , default = None ) : \n return self . _registries [ registry_name ] . get ( value , default ) \n def set_defaults ( self , ** kwargs ) : \n self . _defaults . update ( kwargs ) \n for action in self . _actions : \n if action . dest in kwargs : \n action . default = kwargs [ action . dest ] \n def get_default ( self , dest ) : \n for action in self . _actions : \n if action . dest == dest and action . default is not None : \n return action . default \n return self . _defaults . get ( dest , None ) \n def add_argument ( self , * args , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n chars = self . prefix_chars \n if not args or len ( args ) == <NUM_LIT:1> and args [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] not in chars : \n if args and '<STR_LIT>' in kwargs : \n raise ValueError ( '<STR_LIT>' ) \n kwargs = self . _get_positional_kwargs ( * args , ** kwargs ) \n else : \n kwargs = self . _get_optional_kwargs ( * args , ** kwargs ) \n if '<STR_LIT:default>' not in kwargs : \n dest = kwargs [ '<STR_LIT>' ] \n if dest in self . _defaults : \n kwargs [ '<STR_LIT:default>' ] = self . _defaults [ dest ] \n elif self . argument_default is not None : \n kwargs [ '<STR_LIT:default>' ] = self . argument_default \n action_class = self . _pop_action_class ( kwargs ) \n if not _callable ( action_class ) : \n raise ValueError ( '<STR_LIT>' % action_class ) \n action = action_class ( ** kwargs ) \n type_func = self . _registry_get ( '<STR_LIT:type>' , action . type , action . type ) \n if not _callable ( type_func ) : \n raise ValueError ( '<STR_LIT>' % type_func ) \n return self . _add_action ( action ) \n def add_argument_group ( self , * args , ** kwargs ) : \n group = _ArgumentGroup ( self , * args , ** kwargs ) \n self . _action_groups . append ( group ) \n return group \n def add_mutually_exclusive_group ( self , ** kwargs ) : \n group = _MutuallyExclusiveGroup ( self , ** kwargs ) \n self . _mutually_exclusive_groups . append ( group ) \n return group \n def _add_action ( self , action ) : \n self . _check_conflict ( action ) \n self . _actions . append ( action ) \n action . container = self \n for option_string in action . option_strings : \n self . _option_string_actions [ option_string ] = action \n for option_string in action . option_strings : \n if self . _negative_number_matcher . match ( option_string ) : \n if not self . _has_negative_number_optionals : \n self . _has_negative_number_optionals . append ( True ) \n return action \n def _remove_action ( self , action ) : \n self . _actions . remove ( action ) \n def _add_container_actions ( self , container ) : \n title_group_map = { } \n for group in self . _action_groups : \n if group . title in title_group_map : \n msg = _ ( '<STR_LIT>' ) \n raise ValueError ( msg % ( group . title ) ) \n title_group_map [ group . title ] = group \n group_map = { } \n for group in container . _action_groups : \n if group . title not in title_group_map : \n title_group_map [ group . title ] = self . add_argument_group ( \n title = group . title , \n description = group . description , \n conflict_handler = group . conflict_handler ) \n for action in group . _group_actions : \n group_map [ action ] = title_group_map [ group . title ] \n for group in container . _mutually_exclusive_groups : \n mutex_group = self . add_mutually_exclusive_group ( \n required = group . required ) \n for action in group . _group_actions : \n group_map [ action ] = mutex_group \n for action in container . _actions : \n group_map . get ( action , self ) . _add_action ( action ) \n def _get_positional_kwargs ( self , dest , ** kwargs ) : \n if '<STR_LIT>' in kwargs : \n msg = _ ( \"<STR_LIT>\" ) \n raise TypeError ( msg ) \n if kwargs . get ( '<STR_LIT>' ) not in [ OPTIONAL , ZERO_OR_MORE ] : \n kwargs [ '<STR_LIT>' ] = True \n if kwargs . get ( '<STR_LIT>' ) == ZERO_OR_MORE and '<STR_LIT:default>' not in kwargs : \n kwargs [ '<STR_LIT>' ] = True \n return dict ( kwargs , dest = dest , option_strings = [ ] ) \n def _get_optional_kwargs ( self , * args , ** kwargs ) : \n option_strings = [ ] \n long_option_strings = [ ] \n for option_string in args : \n if not option_string [ <NUM_LIT:0> ] in self . prefix_chars : \n msg = _ ( '<STR_LIT>' \n '<STR_LIT>' ) \n tup = option_string , self . prefix_chars \n raise ValueError ( msg % tup ) \n option_strings . append ( option_string ) \n if option_string [ <NUM_LIT:0> ] in self . prefix_chars : \n if len ( option_string ) > <NUM_LIT:1> : \n if option_string [ <NUM_LIT:1> ] in self . prefix_chars : \n long_option_strings . append ( option_string ) \n dest = kwargs . pop ( '<STR_LIT>' , None ) \n if dest is None : \n if long_option_strings : \n dest_option_string = long_option_strings [ <NUM_LIT:0> ] \n else : \n dest_option_string = option_strings [ <NUM_LIT:0> ] \n dest = dest_option_string . lstrip ( self . prefix_chars ) \n if not dest : \n msg = _ ( '<STR_LIT>' ) \n raise ValueError ( msg % option_string ) \n dest = dest . replace ( '<STR_LIT:->' , '<STR_LIT:_>' ) \n return dict ( kwargs , dest = dest , option_strings = option_strings ) \n def _pop_action_class ( self , kwargs , default = None ) : \n action = kwargs . pop ( '<STR_LIT:action>' , default ) \n return self . _registry_get ( '<STR_LIT:action>' , action , action ) \n def _get_handler ( self ) : \n handler_func_name = '<STR_LIT>' % self . conflict_handler \n try : \n return getattr ( self , handler_func_name ) \n except AttributeError : \n msg = _ ( '<STR_LIT>' ) \n raise ValueError ( msg % self . conflict_handler ) \n def _check_conflict ( self , action ) : \n confl_optionals = [ ] \n for option_string in action . option_strings : \n if option_string in self . _option_string_actions : \n confl_optional = self . _option_string_actions [ option_string ] \n confl_optionals . append ( ( option_string , confl_optional ) ) \n if confl_optionals : \n conflict_handler = self . _get_handler ( ) \n conflict_handler ( action , confl_optionals ) \n def _handle_conflict_error ( self , action , conflicting_actions ) : \n message = _ ( '<STR_LIT>' ) \n conflict_string = '<STR_LIT:U+002CU+0020>' . join ( [ option_string \n for option_string , action \n in conflicting_actions ] ) \n raise ArgumentError ( action , message % conflict_string ) \n def _handle_conflict_resolve ( self , action , conflicting_actions ) : \n for option_string , action in conflicting_actions : \n action . option_strings . remove ( option_string ) \n self . _option_string_actions . pop ( option_string , None ) \n if not action . option_strings : \n action . container . _remove_action ( action ) \n class _ArgumentGroup ( _ActionsContainer ) : \n def __init__ ( self , container , title = None , description = None , ** kwargs ) : \n update = kwargs . setdefault \n update ( '<STR_LIT>' , container . conflict_handler ) \n update ( '<STR_LIT>' , container . prefix_chars ) \n update ( '<STR_LIT>' , container . argument_default ) \n super_init = super ( _ArgumentGroup , self ) . __init__ \n super_init ( description = description , ** kwargs ) \n self . title = title \n self . _group_actions = [ ] \n self . _registries = container . _registries \n self . _actions = container . _actions \n self . _option_string_actions = container . _option_string_actions \n self . _defaults = container . _defaults \n self . _has_negative_number_optionals = container . _has_negative_number_optionals \n def _add_action ( self , action ) : \n action = super ( _ArgumentGroup , self ) . _add_action ( action ) \n self . _group_actions . append ( action ) \n return action \n def _remove_action ( self , action ) : \n super ( _ArgumentGroup , self ) . _remove_action ( action ) \n self . _group_actions . remove ( action ) \n class _MutuallyExclusiveGroup ( _ArgumentGroup ) : \n def __init__ ( self , container , required = False ) : \n super ( _MutuallyExclusiveGroup , self ) . __init__ ( container ) \n self . required = required \n self . _container = container \n def _add_action ( self , action ) : \n if action . required : \n msg = _ ( '<STR_LIT>' ) \n raise ValueError ( msg ) \n action = self . _container . _add_action ( action ) \n self . _group_actions . append ( action ) \n return action \n def _remove_action ( self , action ) : \n self . _container . _remove_action ( action ) \n self . _group_actions . remove ( action ) \n class ArgumentParser ( _AttributeHolder , _ActionsContainer ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , \n prog = None , \n usage = None , \n description = None , \n epilog = None , \n version = None , \n parents = [ ] , \n formatter_class = HelpFormatter , \n prefix_chars = '<STR_LIT:->' , \n fromfile_prefix_chars = None , \n argument_default = None , \n conflict_handler = '<STR_LIT:error>' , \n add_help = True ) : \n if version is not None : \n import warnings \n warnings . warn ( \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" \n \"\"\"<STR_LIT>\"\"\" , DeprecationWarning ) \n superinit = super ( ArgumentParser , self ) . __init__ \n superinit ( description = description , \n prefix_chars = prefix_chars , \n argument_default = argument_default , \n conflict_handler = conflict_handler ) \n if prog is None : \n prog = _os . path . basename ( _sys . argv [ <NUM_LIT:0> ] ) \n self . prog = prog \n self . usage = usage \n self . epilog = epilog \n self . version = version \n self . formatter_class = formatter_class \n self . fromfile_prefix_chars = fromfile_prefix_chars \n self . add_help = add_help \n add_group = self . add_argument_group \n self . _positionals = add_group ( _ ( '<STR_LIT>' ) ) \n self . _optionals = add_group ( _ ( '<STR_LIT>' ) ) \n self . _subparsers = None \n def identity ( string ) : \n return string \n self . register ( '<STR_LIT:type>' , None , identity ) \n if '<STR_LIT:->' in prefix_chars : \n default_prefix = '<STR_LIT:->' \n else : \n default_prefix = prefix_chars [ <NUM_LIT:0> ] \n if self . add_help : \n self . add_argument ( \n default_prefix + '<STR_LIT:h>' , default_prefix * <NUM_LIT:2> + '<STR_LIT>' , \n action = '<STR_LIT>' , default = SUPPRESS , \n help = _ ( '<STR_LIT>' ) ) \n if self . version : \n self . add_argument ( \n default_prefix + '<STR_LIT:v>' , default_prefix * <NUM_LIT:2> + '<STR_LIT:version>' , \n action = '<STR_LIT:version>' , default = SUPPRESS , \n version = self . version , \n help = _ ( \"<STR_LIT>\" ) ) \n for parent in parents : \n self . _add_container_actions ( parent ) \n try : \n defaults = parent . _defaults \n except AttributeError : \n pass \n else : \n self . _defaults . update ( defaults ) \n def _get_kwargs ( self ) : \n names = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT:description>' , \n '<STR_LIT:version>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n return [ ( name , getattr ( self , name ) ) for name in names ] \n def add_subparsers ( self , ** kwargs ) : \n if self . _subparsers is not None : \n self . error ( _ ( '<STR_LIT>' ) ) \n kwargs . setdefault ( '<STR_LIT>' , type ( self ) ) \n if '<STR_LIT:title>' in kwargs or '<STR_LIT:description>' in kwargs : \n title = _ ( kwargs . pop ( '<STR_LIT:title>' , '<STR_LIT>' ) ) \n description = _ ( kwargs . pop ( '<STR_LIT:description>' , None ) ) \n self . _subparsers = self . add_argument_group ( title , description ) \n else : \n self . _subparsers = self . _positionals \n if kwargs . get ( '<STR_LIT>' ) is None : \n formatter = self . _get_formatter ( ) \n positionals = self . _get_positional_actions ( ) \n groups = self . _mutually_exclusive_groups \n formatter . add_usage ( self . usage , positionals , groups , '<STR_LIT>' ) \n kwargs [ '<STR_LIT>' ] = formatter . format_help ( ) . strip ( ) \n parsers_class = self . _pop_action_class ( kwargs , '<STR_LIT>' ) \n action = parsers_class ( option_strings = [ ] , ** kwargs ) \n self . _subparsers . _add_action ( action ) \n return action \n def _add_action ( self , action ) : \n if action . option_strings : \n self . _optionals . _add_action ( action ) \n else : \n self . _positionals . _add_action ( action ) \n return action \n def _get_optional_actions ( self ) : \n return [ action \n for action in self . _actions \n if action . option_strings ] \n def _get_positional_actions ( self ) : \n return [ action \n for action in self . _actions \n if not action . option_strings ] \n def parse_args ( self , args = None , namespace = None ) : \n args , argv = self . parse_known_args ( args , namespace ) \n if argv : \n msg = _ ( '<STR_LIT>' ) \n self . error ( msg % '<STR_LIT:U+0020>' . join ( argv ) ) \n return args \n def parse_known_args ( self , args = None , namespace = None ) : \n if args is None : \n args = _sys . argv [ <NUM_LIT:1> : ] \n if namespace is None : \n namespace = Namespace ( ) \n for action in self . _actions : \n if action . dest is not SUPPRESS : \n if not hasattr ( namespace , action . dest ) : \n if action . default is not SUPPRESS : \n default = action . default \n if isinstance ( action . default , basestring ) : \n default = self . _get_value ( action , default ) \n setattr ( namespace , action . dest , default ) \n for dest in self . _defaults : \n if not hasattr ( namespace , dest ) : \n setattr ( namespace , dest , self . _defaults [ dest ] ) \n try : \n namespace , args = self . _parse_known_args ( args , namespace ) \n if hasattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) : \n args . extend ( getattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) ) \n delattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) \n return namespace , args \n except ArgumentError : \n err = _sys . exc_info ( ) [ <NUM_LIT:1> ] \n self . error ( str ( err ) ) \n def _parse_known_args ( self , arg_strings , namespace ) : \n if self . fromfile_prefix_chars is not None : \n arg_strings = self . _read_args_from_files ( arg_strings ) \n action_conflicts = { } \n for mutex_group in self . _mutually_exclusive_groups : \n group_actions = mutex_group . _group_actions \n for i , mutex_action in enumerate ( mutex_group . _group_actions ) : \n conflicts = action_conflicts . setdefault ( mutex_action , [ ] ) \n conflicts . extend ( group_actions [ : i ] ) \n conflicts . extend ( group_actions [ i + <NUM_LIT:1> : ] ) \n option_string_indices = { } \n arg_string_pattern_parts = [ ] \n arg_strings_iter = iter ( arg_strings ) \n for i , arg_string in enumerate ( arg_strings_iter ) : \n if arg_string == '<STR_LIT>' : \n arg_string_pattern_parts . append ( '<STR_LIT:->' ) \n for arg_string in arg_strings_iter : \n arg_string_pattern_parts . append ( '<STR_LIT:A>' ) \n else : \n option_tuple = self . _parse_optional ( arg_string ) \n if option_tuple is None : \n pattern = '<STR_LIT:A>' \n else : \n option_string_indices [ i ] = option_tuple \n pattern = '<STR_LIT:O>' \n arg_string_pattern_parts . append ( pattern ) \n arg_strings_pattern = '<STR_LIT>' . join ( arg_string_pattern_parts ) \n seen_actions = set ( ) \n seen_non_default_actions = set ( ) \n def take_action ( action , argument_strings , option_string = None ) : \n seen_actions . add ( action ) \n argument_values = self . _get_values ( action , argument_strings ) \n if argument_values is not action . default : \n seen_non_default_actions . add ( action ) \n for conflict_action in action_conflicts . get ( action , [ ] ) : \n if conflict_action in seen_non_default_actions : \n msg = _ ( '<STR_LIT>' ) \n action_name = _get_action_name ( conflict_action ) \n raise ArgumentError ( action , msg % action_name ) \n if argument_values is not SUPPRESS : \n action ( self , namespace , argument_values , option_string ) \n def consume_optional ( start_index ) : \n option_tuple = option_string_indices [ start_index ] \n action , option_string , explicit_arg = option_tuple \n match_argument = self . _match_argument \n action_tuples = [ ] \n while True : \n if action is None : \n extras . append ( arg_strings [ start_index ] ) \n return start_index + <NUM_LIT:1> \n if explicit_arg is not None : \n arg_count = match_argument ( action , '<STR_LIT:A>' ) \n chars = self . prefix_chars \n if arg_count == <NUM_LIT:0> and option_string [ <NUM_LIT:1> ] not in chars : \n action_tuples . append ( ( action , [ ] , option_string ) ) \n char = option_string [ <NUM_LIT:0> ] \n option_string = char + explicit_arg [ <NUM_LIT:0> ] \n new_explicit_arg = explicit_arg [ <NUM_LIT:1> : ] or None \n optionals_map = self . _option_string_actions \n if option_string in optionals_map : \n action = optionals_map [ option_string ] \n explicit_arg = new_explicit_arg \n else : \n msg = _ ( '<STR_LIT>' ) \n raise ArgumentError ( action , msg % explicit_arg ) \n elif arg_count == <NUM_LIT:1> : \n stop = start_index + <NUM_LIT:1> \n args = [ explicit_arg ] \n action_tuples . append ( ( action , args , option_string ) ) \n break \n else : \n msg = _ ( '<STR_LIT>' ) \n raise ArgumentError ( action , msg % explicit_arg ) \n else : \n start = start_index + <NUM_LIT:1> \n selected_patterns = arg_strings_pattern [ start : ] \n arg_count = match_argument ( action , selected_patterns ) \n stop = start + arg_count \n args = arg_strings [ start : stop ] \n action_tuples . append ( ( action , args , option_string ) ) \n break \n assert action_tuples \n for action , args , option_string in action_tuples : \n take_action ( action , args , option_string ) \n return stop \n positionals = self . _get_positional_actions ( ) \n def consume_positionals ( start_index ) : \n match_partial = self . _match_arguments_partial \n selected_pattern = arg_strings_pattern [ start_index : ] \n arg_counts = match_partial ( positionals , selected_pattern ) \n for action , arg_count in zip ( positionals , arg_counts ) : \n args = arg_strings [ start_index : start_index + arg_count ] \n start_index += arg_count \n take_action ( action , args ) \n positionals [ : ] = positionals [ len ( arg_counts ) : ] \n return start_index \n extras = [ ] \n start_index = <NUM_LIT:0> \n if option_string_indices : \n max_option_string_index = max ( option_string_indices ) \n else : \n max_option_string_index = - <NUM_LIT:1> \n while start_index <= max_option_string_index : \n next_option_string_index = min ( [ \n index \n for index in option_string_indices \n if index >= start_index ] ) \n if start_index != next_option_string_index : \n positionals_end_index = consume_positionals ( start_index ) \n if positionals_end_index > start_index : \n start_index = positionals_end_index \n continue \n else : \n start_index = positionals_end_index \n if start_index not in option_string_indices : \n strings = arg_strings [ start_index : next_option_string_index ] \n extras . extend ( strings ) \n start_index = next_option_string_index \n start_index = consume_optional ( start_index ) \n stop_index = consume_positionals ( start_index ) \n extras . extend ( arg_strings [ stop_index : ] ) \n if positionals : \n self . error ( _ ( '<STR_LIT>' ) ) \n for action in self . _actions : \n if action . required : \n if action not in seen_actions : \n name = _get_action_name ( action ) \n self . error ( _ ( '<STR_LIT>' ) % name ) \n for group in self . _mutually_exclusive_groups : \n if group . required : \n for action in group . _group_actions : \n if action in seen_non_default_actions : \n break \n else : \n names = [ _get_action_name ( action ) \n for action in group . _group_actions \n if action . help is not SUPPRESS ] \n msg = _ ( '<STR_LIT>' ) \n self . error ( msg % '<STR_LIT:U+0020>' . join ( names ) ) \n return namespace , extras \n def _read_args_from_files ( self , arg_strings ) : \n new_arg_strings = [ ] \n for arg_string in arg_strings : \n if arg_string [ <NUM_LIT:0> ] not in self . fromfile_prefix_chars : \n new_arg_strings . append ( arg_string ) \n else : \n try : \n args_file = open ( arg_string [ <NUM_LIT:1> : ] ) \n try : \n arg_strings = [ ] \n for arg_line in args_file . read ( ) . splitlines ( ) : \n for arg in self . convert_arg_line_to_args ( arg_line ) : \n arg_strings . append ( arg ) \n arg_strings = self . _read_args_from_files ( arg_strings ) \n new_arg_strings . extend ( arg_strings ) \n finally : \n args_file . close ( ) \n except IOError : \n err = _sys . exc_info ( ) [ <NUM_LIT:1> ] \n self . error ( str ( err ) ) \n return new_arg_strings \n def convert_arg_line_to_args ( self , arg_line ) : \n return [ arg_line ] \n def _match_argument ( self , action , arg_strings_pattern ) : \n nargs_pattern = self . _get_nargs_pattern ( action ) \n match = _re . match ( nargs_pattern , arg_strings_pattern ) \n if match is None : \n nargs_errors = { \n None : _ ( '<STR_LIT>' ) , \n OPTIONAL : _ ( '<STR_LIT>' ) , \n ONE_OR_MORE : _ ( '<STR_LIT>' ) , \n } \n default = _ ( '<STR_LIT>' ) % action . nargs \n msg = nargs_errors . get ( action . nargs , default ) \n raise ArgumentError ( action , msg ) \n return len ( match . group ( <NUM_LIT:1> ) ) \n def _match_arguments_partial ( self , actions , arg_strings_pattern ) : \n result = [ ] \n for i in range ( len ( actions ) , <NUM_LIT:0> , - <NUM_LIT:1> ) : \n actions_slice = actions [ : i ] \n pattern = '<STR_LIT>' . join ( [ self . _get_nargs_pattern ( action ) \n for action in actions_slice ] ) \n match = _re . match ( pattern , arg_strings_pattern ) \n if match is not None : \n result . extend ( [ len ( string ) for string in match . groups ( ) ] ) \n break \n return result \n def _parse_optional ( self , arg_string ) : \n if not arg_string : \n return None \n if not arg_string [ <NUM_LIT:0> ] in self . prefix_chars : \n return None \n if arg_string in self . _option_string_actions : \n action = self . _option_string_actions [ arg_string ] \n return action , arg_string , None \n if len ( arg_string ) == <NUM_LIT:1> : \n return None \n if '<STR_LIT:=>' in arg_string : \n option_string , explicit_arg = arg_string . split ( '<STR_LIT:=>' , <NUM_LIT:1> ) \n if option_string in self . _option_string_actions : \n action = self . _option_string_actions [ option_string ] \n return action , option_string , explicit_arg \n option_tuples = self . _get_option_tuples ( arg_string ) \n if len ( option_tuples ) > <NUM_LIT:1> : \n options = '<STR_LIT:U+002CU+0020>' . join ( [ option_string \n for action , option_string , explicit_arg in option_tuples ] ) \n tup = arg_string , options \n self . error ( _ ( '<STR_LIT>' ) % tup ) \n elif len ( option_tuples ) == <NUM_LIT:1> : \n option_tuple , = option_tuples \n return option_tuple \n if self . _negative_number_matcher . match ( arg_string ) : \n if not self . _has_negative_number_optionals : \n return None \n if '<STR_LIT:U+0020>' in arg_string : \n return None \n return None , arg_string , None \n def _get_option_tuples ( self , option_string ) : \n result = [ ] \n chars = self . prefix_chars \n if option_string [ <NUM_LIT:0> ] in chars and option_string [ <NUM_LIT:1> ] in chars : \n if '<STR_LIT:=>' in option_string : \n option_prefix , explicit_arg = option_string . split ( '<STR_LIT:=>' , <NUM_LIT:1> ) \n else : \n option_prefix = option_string \n explicit_arg = None \n for option_string in self . _option_string_actions : \n if option_string . startswith ( option_prefix ) : \n action = self . _option_string_actions [ option_string ] \n tup = action , option_string , explicit_arg \n result . append ( tup ) \n elif option_string [ <NUM_LIT:0> ] in chars and option_string [ <NUM_LIT:1> ] not in chars : \n option_prefix = option_string \n explicit_arg = None \n short_option_prefix = option_string [ : <NUM_LIT:2> ] \n short_explicit_arg = option_string [ <NUM_LIT:2> : ] \n for option_string in self . _option_string_actions : \n if option_string == short_option_prefix : \n action = self . _option_string_actions [ option_string ] \n tup = action , option_string , short_explicit_arg \n result . append ( tup ) \n elif option_string . startswith ( option_prefix ) : \n action = self . _option_string_actions [ option_string ] \n tup = action , option_string , explicit_arg \n result . append ( tup ) \n else : \n self . error ( _ ( '<STR_LIT>' ) % option_string ) \n return result \n def _get_nargs_pattern ( self , action ) : \n nargs = action . nargs \n if nargs is None : \n nargs_pattern = '<STR_LIT>' \n elif nargs == OPTIONAL : \n nargs_pattern = '<STR_LIT>' \n elif nargs == ZERO_OR_MORE : \n nargs_pattern = '<STR_LIT>' \n elif nargs == ONE_OR_MORE : \n nargs_pattern = '<STR_LIT>' \n elif nargs == REMAINDER : \n nargs_pattern = '<STR_LIT>' \n elif nargs == PARSER : \n nargs_pattern = '<STR_LIT>' \n else : \n nargs_pattern = '<STR_LIT>' % '<STR_LIT>' . join ( '<STR_LIT:A>' * nargs ) \n if action . option_strings : \n nargs_pattern = nargs_pattern . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n nargs_pattern = nargs_pattern . replace ( '<STR_LIT:->' , '<STR_LIT>' ) \n return nargs_pattern \n def _get_values ( self , action , arg_strings ) : \n if action . nargs not in [ PARSER , REMAINDER ] : \n arg_strings = [ s for s in arg_strings if s != '<STR_LIT>' ] \n if not arg_strings and action . nargs == OPTIONAL : \n if action . option_strings : \n value = action . const \n else : \n value = action . default \n if isinstance ( value , basestring ) : \n value = self . _get_value ( action , value ) \n self . _check_value ( action , value ) \n elif ( not arg_strings and action . nargs == ZERO_OR_MORE and \n not action . option_strings ) : \n if action . default is not None : \n value = action . default \n else : \n value = arg_strings \n self . _check_value ( action , value ) \n elif len ( arg_strings ) == <NUM_LIT:1> and action . nargs in [ None , OPTIONAL ] : \n arg_string , = arg_strings \n value = self . _get_value ( action , arg_string ) \n self . _check_value ( action , value ) \n elif action . nargs == REMAINDER : \n value = [ self . _get_value ( action , v ) for v in arg_strings ] \n elif action . nargs == PARSER : \n value = [ self . _get_value ( action , v ) for v in arg_strings ] \n self . _check_value ( action , value [ <NUM_LIT:0> ] ) \n else : \n value = [ self . _get_value ( action , v ) for v in arg_strings ] \n for v in value : \n self . _check_value ( action , v ) \n return value \n def _get_value ( self , action , arg_string ) : \n type_func = self . _registry_get ( '<STR_LIT:type>' , action . type , action . type ) \n if not _callable ( type_func ) : \n msg = _ ( '<STR_LIT>' ) \n raise ArgumentError ( action , msg % type_func ) \n try : \n result = type_func ( arg_string ) \n except ArgumentTypeError : \n name = getattr ( action . type , '<STR_LIT>' , repr ( action . type ) ) \n msg = str ( _sys . exc_info ( ) [ <NUM_LIT:1> ] ) \n raise ArgumentError ( action , msg ) \n except ( TypeError , ValueError ) : \n name = getattr ( action . type , '<STR_LIT>' , repr ( action . type ) ) \n msg = _ ( '<STR_LIT>' ) \n raise ArgumentError ( action , msg % ( name , arg_string ) ) \n return result \n def _check_value ( self , action , value ) : \n if action . choices is not None and value not in action . choices : \n tup = value , '<STR_LIT:U+002CU+0020>' . join ( map ( repr , action . choices ) ) \n msg = _ ( '<STR_LIT>' ) % tup \n raise ArgumentError ( action , msg ) \n def format_usage ( self ) : \n formatter = self . _get_formatter ( ) \n formatter . add_usage ( self . usage , self . _actions , \n self . _mutually_exclusive_groups ) \n return formatter . format_help ( ) \n def format_help ( self ) : \n formatter = self . _get_formatter ( ) \n formatter . add_usage ( self . usage , self . _actions , \n self . _mutually_exclusive_groups ) \n formatter . add_text ( self . description ) \n for action_group in self . _action_groups : \n formatter . start_section ( action_group . title ) \n formatter . add_text ( action_group . description ) \n formatter . add_arguments ( action_group . _group_actions ) \n formatter . end_section ( ) \n formatter . add_text ( self . epilog ) \n return formatter . format_help ( ) \n def format_version ( self ) : \n import warnings \n warnings . warn ( \n '<STR_LIT>' \n '<STR_LIT>' , \n DeprecationWarning ) \n formatter = self . _get_formatter ( ) \n formatter . add_text ( self . version ) \n return formatter . format_help ( ) \n def _get_formatter ( self ) : \n return self . formatter_class ( prog = self . prog ) \n def print_usage ( self , file = None ) : \n if file is None : \n file = _sys . stdout \n self . _print_message ( self . format_usage ( ) , file ) \n def print_help ( self , file = None ) : \n if file is None : \n file = _sys . stdout \n self . _print_message ( self . format_help ( ) , file ) \n def print_version ( self , file = None ) : \n import warnings \n warnings . warn ( \n '<STR_LIT>' \n '<STR_LIT>' , \n DeprecationWarning ) \n self . _print_message ( self . format_version ( ) , file ) \n def _print_message ( self , message , file = None ) : \n if message : \n if file is None : \n file = _sys . stderr \n file . write ( message ) \n def exit ( self , status = <NUM_LIT:0> , message = None ) : \n if message : \n self . _print_message ( message , _sys . stderr ) \n _sys . exit ( status ) \n def error ( self , message ) : \n \"\"\"<STR_LIT>\"\"\" \n self . print_usage ( _sys . stderr ) \n self . exit ( <NUM_LIT:2> , _ ( '<STR_LIT>' ) % ( self . prog , <mask0> ) ) \n", "gt": "message"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n import os \n import traceback \n libpath = os . path . dirname ( os . path . abspath ( __file__ ) ) \n sys . path [ : <NUM_LIT:0> ] = [ os . path . join ( libpath , '<STR_LIT>' ) ] \n import common \n import environment \n import xmltodict \n from collections import OrderedDict \n logger = common . logging . getLogger ( ) . getChild ( '<STR_LIT>' ) \n try : \n import splunk . Intersplunk \n import splunk . entity as entity \n except ImportError as e : \n logger . error ( \"<STR_LIT>\" \n \"<STR_LIT>\" % __file__ ) \n sys . exit ( <NUM_LIT:3> ) \n libpath = os . path . dirname ( os . path . abspath ( __file__ ) ) \n sys . path [ : <NUM_LIT:0> ] = [ os . path . join ( libpath , '<STR_LIT>' ) ] \n sys . path [ : <NUM_LIT:0> ] = [ os . path . join ( libpath , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ] \n sys . path [ : <NUM_LIT:0> ] = [ os . path . join ( libpath , '<STR_LIT>' , '<STR_LIT>' ) ] \n try : \n import pandevice . base \n import pan . xapi \n except ImportError : \n print \"<STR_LIT>\" \n exit ( <NUM_LIT:3> ) \n from common import log \n def usage ( ) : \n common . exit_with_error ( \"<STR_LIT>\" ) \n def parse_apps ( apps_xml ) : \n obj = xmltodict . parse ( apps_xml ) \n try : \n apps = obj [ '<STR_LIT>' ] [ '<STR_LIT:result>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n except KeyError as e : \n logger . error ( \"<STR_LIT>\" ) \n raise e \n csv_apps = [ ] \n for app in apps : \n a = OrderedDict ( ) \n try : \n a [ '<STR_LIT>' ] = app [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = app . get ( '<STR_LIT>' , \"<STR_LIT>\" ) \n a [ '<STR_LIT>' ] = app . get ( '<STR_LIT>' , \"<STR_LIT>\" ) \n a [ '<STR_LIT>' ] = app . get ( '<STR_LIT>' , \"<STR_LIT>\" ) \n a [ '<STR_LIT>' ] = app [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = app [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = app [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = app [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = app [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = app [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = app [ '<STR_LIT>' ] \n if a [ '<STR_LIT>' ] != u\"<STR_LIT:yes>\" and a [ '<STR_LIT>' ] != u\"<STR_LIT>\" : \n a [ '<STR_LIT>' ] = a [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = app [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = app [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = app . get ( '<STR_LIT>' , \"<STR_LIT>\" ) \n a [ '<STR_LIT>' ] = \"<STR_LIT>\" \n try : \n default = app [ '<STR_LIT:default>' ] \n if isinstance ( default , list ) : \n for d in default : \n a [ '<STR_LIT>' ] = d [ '<STR_LIT:port>' ] [ '<STR_LIT>' ] \n break \n else : \n a [ '<STR_LIT>' ] = default [ '<STR_LIT:port>' ] [ '<STR_LIT>' ] \n except KeyError : \n pass \n else : \n if not isinstance ( a [ '<STR_LIT>' ] , basestring ) : \n a [ '<STR_LIT>' ] = \"<STR_LIT:|>\" . join ( a [ '<STR_LIT>' ] ) \n except Exception as e : \n logger . error ( \"<STR_LIT>\" % app [ '<STR_LIT>' ] ) \n logger . error ( traceback . format_exc ( ) ) \n common . exit_with_error ( str ( e ) ) \n for key in a : \n a [ key ] = str ( a [ key ] ) \n csv_apps . append ( a ) \n logger . info ( \"<STR_LIT>\" % len ( csv_apps ) ) \n return csv_apps \n def parse_threats ( threats_xml ) : \n obj = xmltodict . parse ( threats_xml ) \n try : \n phone_home = obj [ '<STR_LIT>' ] [ '<STR_LIT:result>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n vulnerability = obj [ '<STR_LIT>' ] [ '<STR_LIT:result>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n threats = phone_home + vulnerability \n except KeyError as e : \n logger . error ( \"<STR_LIT>\" ) \n raise e \n csv_threats = [ ] \n for threat in threats : \n a = OrderedDict ( ) \n try : \n a [ '<STR_LIT>' ] = threat [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = threat [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = threat [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = threat [ '<STR_LIT>' ] \n a [ '<STR_LIT>' ] = threat . get ( '<STR_LIT>' , None ) \n if a [ '<STR_LIT>' ] is not None : \n a [ '<STR_LIT>' ] = threat [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n if not isinstance ( a [ '<STR_LIT>' ] , basestring ) : \n a [ '<STR_LIT>' ] = \"<STR_LIT:U+002CU+0020>\" . join ( a [ '<STR_LIT>' ] ) \n else : \n a [ '<STR_LIT>' ] = \"<STR_LIT>\" \n except KeyError as e : \n logger . error ( \"<STR_LIT>\" % threat [ '<STR_LIT>' ] ) \n raise e \n for key in a : \n a [ key ] = str ( a [ key ] ) \n csv_threats . append ( a ) \n logger . info ( \"<STR_LIT>\" % len ( csv_threats ) ) \n return csv_threats \n def main ( ) : \n args , kwargs = splunk . Intersplunk . getKeywordsAndOptions ( ) \n debug = common . check_debug ( kwargs ) \n if len ( args ) < <NUM_LIT:2> : \n logger . error ( \"<STR_LIT>\" % len ( args ) ) \n usage ( ) \n if args [ <NUM_LIT:1> ] == \"<STR_LIT>\" : \n logger . info ( \"<STR_LIT>\" % args [ <NUM_LIT:0> ] ) \n elif args [ <NUM_LIT:1> ] == \"<STR_LIT>\" : \n logger . info ( \"<STR_LIT>\" % args [ <NUM_LIT:0> ] ) \n else : \n usage ( ) \n results , unused1 , settings = splunk . Intersplunk . getOrganizedResults ( ) \n sessionKey = settings [ '<STR_LIT>' ] \n log ( debug , \"<STR_LIT>\" ) \n apikey = common . apikey ( sessionKey , args [ <NUM_LIT:0> ] , debug ) \n device = pandevice . base . PanDevice ( args [ <NUM_LIT:0> ] , api_key = apikey ) \n try : \n if args [ <NUM_LIT:1> ] == \"<STR_LIT>\" : \n device . xapi . get ( \"<STR_LIT>\" ) \n app_xml = device . xapi . xml_document \n csv = parse_apps ( app_xml ) \n else : \n device . xapi . get ( \"<STR_LIT>\" ) \n threat_xml = device . xapi . xml_document \n csv = parse_threats ( threat_xml ) \n except pan . xapi . PanXapiError as e : \n common . exit_with_error ( str ( e ) ) \n splunk . Intersplunk . outputResults ( csv ) \n if __name__ == \"<STR_LIT:__main__>\" : \n <mask0> ( ) \n", "gt": "main"}
{"input": "\n from __future__ import division \n \"\"\"<STR_LIT>\"\"\" \n __license__ = \"\"\"<STR_LIT>\"\"\" \n __all__ = [ \"<STR_LIT>\" ] \n import itertools \n import logging \n import socket \n import sys \n import time \n import traceback \n from uuid import uuid4 \n import weakref \n from kazoo . client import KazooClient \n from kazoo . handlers . gevent import SequentialGeventHandler \n from kazoo . exceptions import NoNodeException , NodeExistsError \n from kazoo . recipe . watchers import ChildrenWatch \n from . common import OffsetType \n from . exceptions import KafkaException , PartitionOwnedError , ConsumerStoppedException \n from . handlers import GEventHandler \n from . simpleconsumer import SimpleConsumer \n from . utils . compat import range , get_bytes , itervalues , iteritems , get_string \n try : \n from . import rdkafka \n except ImportError : \n rdkafka = False \n log = logging . getLogger ( __name__ ) \n def _catch_thread_exception ( fn ) : \n \"\"\"<STR_LIT>\"\"\" \n def wrapped ( self , * args , ** kwargs ) : \n try : \n ret = fn ( self , * args , ** kwargs ) \n except Exception : \n self . _worker_exception = sys . exc_info ( ) \n else : \n return ret \n return wrapped \n class BalancedConsumer ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , \n topic , \n cluster , \n consumer_group , \n fetch_message_max_bytes = <NUM_LIT> * <NUM_LIT> , \n num_consumer_fetchers = <NUM_LIT:1> , \n auto_commit_enable = False , \n auto_commit_interval_ms = <NUM_LIT> * <NUM_LIT:1000> , \n queued_max_messages = <NUM_LIT> , \n fetch_min_bytes = <NUM_LIT:1> , \n fetch_wait_max_ms = <NUM_LIT:100> , \n offsets_channel_backoff_ms = <NUM_LIT:1000> , \n offsets_commit_max_retries = <NUM_LIT:5> , \n auto_offset_reset = OffsetType . EARLIEST , \n consumer_timeout_ms = - <NUM_LIT:1> , \n rebalance_max_retries = <NUM_LIT:5> , \n rebalance_backoff_ms = <NUM_LIT:2> * <NUM_LIT:1000> , \n zookeeper_connection_timeout_ms = <NUM_LIT:6> * <NUM_LIT:1000> , \n zookeeper_connect = '<STR_LIT>' , \n zookeeper = None , \n auto_start = True , \n reset_offset_on_start = False , \n post_rebalance_callback = None , \n use_rdkafka = False , \n compacted_topic = False ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _cluster = cluster \n if not isinstance ( consumer_group , bytes ) : \n raise TypeError ( \"<STR_LIT>\" ) \n self . _consumer_group = consumer_group \n self . _topic = topic \n self . _auto_commit_enable = auto_commit_enable \n self . _auto_commit_interval_ms = auto_commit_interval_ms \n self . _fetch_message_max_bytes = fetch_message_max_bytes \n self . _fetch_min_bytes = fetch_min_bytes \n self . _rebalance_max_retries = rebalance_max_retries \n self . _num_consumer_fetchers = num_consumer_fetchers \n self . _queued_max_messages = queued_max_messages \n self . _fetch_wait_max_ms = fetch_wait_max_ms \n self . _rebalance_backoff_ms = rebalance_backoff_ms \n self . _consumer_timeout_ms = consumer_timeout_ms \n self . _offsets_channel_backoff_ms = offsets_channel_backoff_ms \n self . _offsets_commit_max_retries = offsets_commit_max_retries \n self . _auto_offset_reset = auto_offset_reset \n self . _zookeeper_connect = zookeeper_connect \n self . _zookeeper_connection_timeout_ms = zookeeper_connection_timeout_ms \n self . _reset_offset_on_start = reset_offset_on_start \n self . _post_rebalance_callback = post_rebalance_callback \n self . _generation_id = - <NUM_LIT:1> \n self . _running = False \n self . _worker_exception = None \n self . _worker_trace_logged = False \n self . _is_compacted_topic = compacted_topic \n if not rdkafka and use_rdkafka : \n raise ImportError ( \"<STR_LIT>\" ) \n if isinstance ( self . _cluster . handler , GEventHandler ) and use_rdkafka : \n raise ImportError ( \"<STR_LIT>\" ) \n self . _use_rdkafka = rdkafka and use_rdkafka \n self . _rebalancing_lock = cluster . handler . Lock ( ) \n self . _consumer = None \n self . _consumer_id = get_bytes ( \"<STR_LIT>\" . format ( \n hostname = socket . gethostname ( ) , \n uuid = uuid4 ( ) \n ) ) \n self . _setting_watches = True \n self . _topic_path = '<STR_LIT>' . format ( \n group = self . _consumer_group , \n topic = self . _topic . name ) \n self . _consumer_id_path = '<STR_LIT>' . format ( \n group = self . _consumer_group ) \n self . _zookeeper = None \n self . _owns_zookeeper = zookeeper is None \n if zookeeper is not None : \n self . _zookeeper = zookeeper \n if auto_start is True : \n self . start ( ) \n def __del__ ( self ) : \n log . debug ( \"<STR_LIT>\" . format ( self ) ) \n if self . _running : \n self . stop ( ) \n def __repr__ ( self ) : \n return \"<STR_LIT>\" . format ( \n module = self . __class__ . __module__ , \n name = self . __class__ . __name__ , \n id_ = hex ( id ( self ) ) , \n group = self . _consumer_group \n ) \n def _raise_worker_exceptions ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _worker_exception is not None : \n _ , ex , tb = self . _worker_exception \n if not self . _worker_trace_logged : \n self . _worker_trace_logged = True \n log . error ( \"<STR_LIT>\" , \n \"<STR_LIT>\" . join ( traceback . format_tb ( tb ) ) ) \n raise ex \n @ property \n def topic ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _topic \n @ property \n def partitions ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _consumer . partitions if self . _consumer else dict ( ) \n @ property \n def _partitions ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return set ( \n [ ] if self . partitions is None else itervalues ( self . partitions ) ) \n @ property \n def held_offsets ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if not self . _consumer : \n return None \n return self . _consumer . held_offsets \n def start ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n if self . _zookeeper is None : \n self . _setup_zookeeper ( self . _zookeeper_connect , \n self . _zookeeper_connection_timeout_ms ) \n self . _zookeeper . ensure_path ( self . _topic_path ) \n self . _add_self ( ) \n self . _running = True \n self . _set_watches ( ) \n self . _rebalance ( ) \n except Exception : \n log . exception ( \"<STR_LIT>\" ) \n self . stop ( ) \n def stop ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n log . debug ( \"<STR_LIT>\" . format ( self ) ) \n with self . _rebalancing_lock : \n self . _running = False \n if self . _consumer is not None : \n self . _consumer . stop ( ) \n if self . _owns_zookeeper : \n self . _zookeeper . stop ( ) \n else : \n self . _remove_partitions ( self . _get_held_partitions ( ) ) \n try : \n self . _zookeeper . delete ( self . _path_self ) \n except NoNodeException : \n pass \n def _setup_zookeeper ( self , zookeeper_connect , timeout ) : \n \"\"\"<STR_LIT>\"\"\" \n kazoo_kwargs = { '<STR_LIT>' : timeout / <NUM_LIT:1000> } \n if isinstance ( self . _cluster . handler , GEventHandler ) : \n kazoo_kwargs [ '<STR_LIT>' ] = SequentialGeventHandler ( ) \n self . _zookeeper = KazooClient ( zookeeper_connect , ** kazoo_kwargs ) \n self . _zookeeper . start ( ) \n def _setup_internal_consumer ( self , partitions = None , start = True ) : \n \"\"\"<STR_LIT>\"\"\" \n if partitions is None : \n partitions = [ ] \n if partitions != self . _partitions : \n cns = self . _get_internal_consumer ( partitions = list ( partitions ) , start = start ) \n if self . _post_rebalance_callback is not None : \n old_offsets = ( self . _consumer . held_offsets \n if self . _consumer else dict ( ) ) \n new_offsets = cns . held_offsets \n try : \n reset_offsets = self . _post_rebalance_callback ( \n self , old_offsets , new_offsets ) \n except Exception : \n log . exception ( \"<STR_LIT>\" ) \n self . _worker_exception = sys . exc_info ( ) \n return False \n if reset_offsets : \n cns . reset_offsets ( partition_offsets = [ \n ( cns . partitions [ id_ ] , offset ) for \n ( id_ , offset ) in iteritems ( reset_offsets ) ] ) \n self . _consumer = cns \n return True \n def _get_internal_consumer ( self , partitions = None , start = True ) : \n \"\"\"<STR_LIT>\"\"\" \n if partitions is None : \n partitions = [ ] \n reset_offset_on_start = self . _reset_offset_on_start \n if self . _consumer is not None : \n self . _consumer . stop ( ) \n reset_offset_on_start = False \n Cls = ( rdkafka . RdKafkaSimpleConsumer \n if self . _use_rdkafka else SimpleConsumer ) \n return Cls ( \n self . _topic , \n self . _cluster , \n consumer_group = self . _consumer_group , \n partitions = partitions , \n auto_commit_enable = self . _auto_commit_enable , \n auto_commit_interval_ms = self . _auto_commit_interval_ms , \n fetch_message_max_bytes = self . _fetch_message_max_bytes , \n fetch_min_bytes = self . _fetch_min_bytes , \n num_consumer_fetchers = self . _num_consumer_fetchers , \n queued_max_messages = self . _queued_max_messages , \n fetch_wait_max_ms = self . _fetch_wait_max_ms , \n consumer_timeout_ms = self . _consumer_timeout_ms , \n offsets_channel_backoff_ms = self . _offsets_channel_backoff_ms , \n offsets_commit_max_retries = self . _offsets_commit_max_retries , \n auto_offset_reset = self . _auto_offset_reset , \n reset_offset_on_start = reset_offset_on_start , \n auto_start = start , \n compacted_topic = self . _is_compacted_topic , \n generation_id = self . _generation_id , \n consumer_id = self . _consumer_id \n ) \n def _decide_partitions ( self , participants , consumer_id = None ) : \n \"\"\"<STR_LIT>\"\"\" \n p_to_str = lambda p : '<STR_LIT:->' . join ( [ str ( p . topic . name ) , str ( p . leader . id ) , str ( p . id ) ] ) \n all_parts = self . _topic . partitions . values ( ) \n all_parts = sorted ( all_parts , key = p_to_str ) \n participants = sorted ( participants ) \n idx = participants . index ( consumer_id or self . _consumer_id ) \n parts_per_consumer = len ( all_parts ) // len ( participants ) \n remainder_ppc = len ( all_parts ) % len ( participants ) \n start = parts_per_consumer * idx + min ( idx , remainder_ppc ) \n num_parts = parts_per_consumer + ( <NUM_LIT:0> if ( idx + <NUM_LIT:1> > remainder_ppc ) else <NUM_LIT:1> ) \n new_partitions = itertools . islice ( all_parts , start , start + num_parts ) \n new_partitions = set ( new_partitions ) \n log . info ( '<STR_LIT>' , \n self . _consumer_id , len ( participants ) , len ( all_parts ) , \n len ( new_partitions ) ) \n log . debug ( '<STR_LIT>' , [ p_to_str ( p ) for p in new_partitions ] ) \n return new_partitions \n def _get_participants ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n consumer_ids = self . _zookeeper . get_children ( self . _consumer_id_path ) \n except NoNodeException : \n log . debug ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n return [ ] \n participants = [ ] \n for id_ in consumer_ids : \n try : \n topic , stat = self . _zookeeper . get ( \"<STR_LIT>\" % ( self . _consumer_id_path , id_ ) ) \n if topic == self . _topic . name : \n participants . append ( get_bytes ( id_ ) ) \n except NoNodeException : \n pass \n participants = sorted ( participants ) \n return participants \n def _build_watch_callback ( self , fn , proxy ) : \n \"\"\"<STR_LIT>\"\"\" \n def _callback ( children ) : \n try : \n proxy . __repr__ ( ) \n except ReferenceError : \n return False \n return fn ( proxy , children ) \n return _callback \n def _set_watches ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n proxy = weakref . proxy ( self ) \n _brokers_changed = self . _build_watch_callback ( BalancedConsumer . _brokers_changed , proxy ) \n _topics_changed = self . _build_watch_callback ( BalancedConsumer . _topics_changed , proxy ) \n _consumers_changed = self . _build_watch_callback ( BalancedConsumer . _consumers_changed , proxy ) \n self . _setting_watches = True \n broker_path = '<STR_LIT>' \n try : \n self . _broker_watcher = ChildrenWatch ( \n self . _zookeeper , broker_path , \n _brokers_changed \n ) \n except NoNodeException : \n raise Exception ( \n '<STR_LIT>' \n '<STR_LIT>' \n % broker_path ) \n self . _topics_watcher = ChildrenWatch ( \n self . _zookeeper , \n '<STR_LIT>' , \n _topics_changed \n ) \n self . _consumer_watcher = ChildrenWatch ( \n self . _zookeeper , self . _consumer_id_path , \n _consumers_changed \n ) \n self . _setting_watches = False \n def _add_self ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n participants = self . _get_participants ( ) \n if len ( self . _topic . partitions ) <= len ( participants ) : \n raise KafkaException ( \"<STR_LIT>\" ) \n self . _zookeeper . create ( \n self . _path_self , self . _topic . name , ephemeral = True , makepath = True ) \n @ property \n def _path_self ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return '<STR_LIT>' . format ( \n path = self . _consumer_id_path , \n id_ = get_string ( self . _consumer_id ) \n ) \n def _update_member_assignment ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for i in range ( self . _rebalance_max_retries ) : \n try : \n participants = self . _get_participants ( ) \n if self . _consumer_id not in participants : \n self . _add_self ( ) \n participants . append ( self . _consumer_id ) \n new_partitions = self . _decide_partitions ( participants ) \n if not new_partitions : \n log . warning ( \"<STR_LIT>\" , \n self . _consumer_id ) \n current_zk_parts = self . _get_held_partitions ( ) \n self . _remove_partitions ( current_zk_parts - new_partitions ) \n self . _add_partitions ( new_partitions - current_zk_parts ) \n if self . _setup_internal_consumer ( new_partitions ) : \n log . info ( '<STR_LIT>' ) \n break \n except PartitionOwnedError as ex : \n if i == self . _rebalance_max_retries - <NUM_LIT:1> : \n log . warning ( '<STR_LIT>' , \n ex . partition , i ) \n raise \n log . info ( '<STR_LIT>' , ex . partition ) \n self . _cluster . handler . sleep ( i * ( self . _rebalance_backoff_ms / <NUM_LIT:1000> ) ) \n def _rebalance ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _consumer is not None : \n self . commit_offsets ( ) \n with self . _rebalancing_lock : \n if not self . _running : \n raise ConsumerStoppedException \n log . info ( '<STR_LIT>' % ( \n self . _consumer_id , self . _topic . name ) ) \n self . _update_member_assignment ( ) \n def _path_from_partition ( self , p ) : \n \"\"\"<STR_LIT>\"\"\" \n return \"<STR_LIT>\" % ( self . _topic_path , p . leader . id , p . id ) \n def _remove_partitions ( self , partitions ) : \n \"\"\"<STR_LIT>\"\"\" \n for p in partitions : \n self . _zookeeper . delete ( self . _path_from_partition ( p ) ) \n def _add_partitions ( self , partitions ) : \n \"\"\"<STR_LIT>\"\"\" \n for p in partitions : \n try : \n self . _zookeeper . create ( \n self . _path_from_partition ( p ) , \n value = get_bytes ( self . _consumer_id ) , \n ephemeral = True \n ) \n except NodeExistsError : \n raise PartitionOwnedError ( p ) \n def _get_held_partitions ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n zk_partition_ids = set ( ) \n all_partitions = self . _zookeeper . get_children ( self . _topic_path ) \n for partition_slug in all_partitions : \n try : \n owner_id , stat = self . _zookeeper . get ( \n '<STR_LIT>' . format ( \n path = self . _topic_path , slug = partition_slug ) ) \n if owner_id == get_bytes ( self . _consumer_id ) : \n zk_partition_ids . add ( int ( partition_slug . split ( '<STR_LIT:->' ) [ <NUM_LIT:1> ] ) ) \n except NoNodeException : \n pass \n return set ( self . _topic . partitions [ _id ] for _id in zk_partition_ids ) \n @ _catch_thread_exception \n def _brokers_changed ( self , brokers ) : \n if not self . _running : \n return False \n if self . _setting_watches : \n return \n log . debug ( \"<STR_LIT>\" . format ( \n self . _consumer_id ) ) \n self . _rebalance ( ) \n @ _catch_thread_exception \n def _consumers_changed ( self , consumers ) : \n if not self . _running : \n return False \n if self . _setting_watches : \n return \n log . debug ( \"<STR_LIT>\" . format ( \n self . _consumer_id ) ) \n self . _rebalance ( ) \n @ _catch_thread_exception \n def _topics_changed ( self , topics ) : \n if not self . _running : \n return False \n if self . _setting_watches : \n return \n log . debug ( \"<STR_LIT>\" . format ( \n self . _consumer_id ) ) \n self . _rebalance ( ) \n def reset_offsets ( self , partition_offsets = None ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _raise_worker_exceptions ( ) \n if not self . _consumer : \n raise ConsumerStoppedException ( \"<STR_LIT>\" ) \n self . _consumer . reset_offsets ( partition_offsets = partition_offsets ) \n def consume ( self , block = True ) : \n \"\"\"<STR_LIT>\"\"\" \n def consumer_timed_out ( ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . _consumer_timeout_ms == - <NUM_LIT:1> : \n return False \n disp = ( time . time ( ) - self . _last_message_time ) * <NUM_LIT> \n return disp > self . _consumer_timeout_ms \n message = None \n self . _last_message_time = time . time ( ) \n while message is None and not consumer_timed_out ( ) : \n self . _raise_worker_exceptions ( ) \n try : \n message = self . _consumer . consume ( block = block ) \n except ( ConsumerStoppedException , AttributeError ) : \n if not self . _running : \n raise ConsumerStoppedException \n continue \n if message : \n self . _last_message_time = time . time ( ) \n if not block : \n return message \n return message \n def __iter__ ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n while True : \n message = self . consume ( block = True ) \n if not message : \n raise StopIteration \n yield message \n def commit_offsets ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _raise_worker_exceptions ( ) \n if not self . _consumer : \n raise KafkaException ( \"<STR_LIT>\" ) \n return self . _consumer . <mask0> ( ) \n", "gt": "commit_offsets"}
{"input": "\n <mask0> = \"\"\"<STR_LIT>\"\"\" \n", "gt": "__license__"}
{"input": "\n from itertools import cycle \n from streamparse import Spout \n class WordSpout ( Spout ) : \n outputs = [ '<STR_LIT>' ] \n def initialize ( self , stormconf , context ) : \n self . words = cycle ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n def next_tuple ( self ) : \n word = next ( self . words ) \n self . emit ( [ <mask0> ] ) \n", "gt": "word"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from . stream import Grouping , Stream \n from . topology import <mask0> \n", "gt": "Topology"}
{"input": "\n from . constants import eStart , eError , eItsMe \n HZ_cls = ( \n <NUM_LIT:1> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:4> , <NUM_LIT:0> , <NUM_LIT:5> , <NUM_LIT:2> , <NUM_LIT:0> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> , \n ) \n HZ_st = ( \n eStart , eError , <NUM_LIT:3> , eStart , eStart , eStart , eError , eError , \n eError , eError , eError , eError , eItsMe , eItsMe , eItsMe , eItsMe , \n eItsMe , eItsMe , eError , eError , eStart , eStart , <NUM_LIT:4> , eError , \n <NUM_LIT:5> , eError , <NUM_LIT:6> , eError , <NUM_LIT:5> , <NUM_LIT:5> , <NUM_LIT:4> , eError , \n <NUM_LIT:4> , eError , <NUM_LIT:4> , <NUM_LIT:4> , <NUM_LIT:4> , eError , <NUM_LIT:4> , eError , \n <NUM_LIT:4> , eItsMe , eStart , eStart , eStart , eStart , eStart , eStart , \n ) \n HZCharLenTable = ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> ) \n HZSMModel = { '<STR_LIT>' : HZ_cls , \n '<STR_LIT>' : <NUM_LIT:6> , \n '<STR_LIT>' : HZ_st , \n '<STR_LIT>' : HZCharLenTable , \n '<STR_LIT:name>' : \"<STR_LIT>\" } \n ISO2022CN_cls = ( \n <NUM_LIT:2> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:3> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:4> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n ) \n ISO2022CN_st = ( \n eStart , <NUM_LIT:3> , eError , eStart , eStart , eStart , eStart , eStart , \n eStart , eError , eError , eError , eError , eError , eError , eError , \n eError , eError , eItsMe , eItsMe , eItsMe , eItsMe , eItsMe , eItsMe , \n eItsMe , eItsMe , eItsMe , eError , eError , eError , <NUM_LIT:4> , eError , \n eError , eError , eError , eItsMe , eError , eError , eError , eError , \n <NUM_LIT:5> , <NUM_LIT:6> , eError , eError , eError , eError , eError , eError , \n eError , eError , eError , eItsMe , eError , eError , eError , eError , \n eError , eError , eError , eError , eError , eItsMe , eError , eStart , \n ) \n ISO2022CNCharLenTable = ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> ) \n ISO2022CNSMModel = { '<STR_LIT>' : ISO2022CN_cls , \n '<STR_LIT>' : <NUM_LIT:9> , \n '<STR_LIT>' : ISO2022CN_st , \n '<STR_LIT>' : ISO2022CNCharLenTable , \n '<STR_LIT:name>' : \"<STR_LIT>\" } \n ISO2022JP_cls = ( \n <NUM_LIT:2> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:7> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:3> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:6> , <NUM_LIT:0> , <NUM_LIT:4> , <NUM_LIT:0> , <NUM_LIT:8> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:9> , <NUM_LIT:5> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n ) \n ISO2022JP_st = ( \n eStart , <NUM_LIT:3> , eError , eStart , eStart , eStart , eStart , eStart , \n eStart , eStart , eError , eError , eError , eError , eError , eError , \n eError , eError , eError , eError , eItsMe , eItsMe , eItsMe , eItsMe , \n eItsMe , eItsMe , eItsMe , eItsMe , eItsMe , eItsMe , eError , eError , \n eError , <NUM_LIT:5> , eError , eError , eError , <NUM_LIT:4> , eError , eError , \n eError , eError , eError , <NUM_LIT:6> , eItsMe , eError , eItsMe , eError , \n eError , eError , eError , eError , eError , eError , eItsMe , eItsMe , \n eError , eError , eError , eItsMe , eError , eError , eError , eError , \n eError , eError , eError , eError , eItsMe , eError , eStart , eStart , \n ) \n ISO2022JPCharLenTable = ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> ) \n ISO2022JPSMModel = { '<STR_LIT>' : ISO2022JP_cls , \n '<STR_LIT>' : <NUM_LIT:10> , \n '<STR_LIT>' : ISO2022JP_st , \n '<STR_LIT>' : ISO2022JPCharLenTable , \n '<STR_LIT:name>' : \"<STR_LIT>\" } \n ISO2022KR_cls = ( \n <NUM_LIT:2> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:1> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:3> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:4> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:5> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , <NUM_LIT:2> , \n ) \n ISO2022KR_st = ( \n eStart , <NUM_LIT:3> , eError , eStart , eStart , eStart , eError , eError , \n eError , eError , eError , eError , eItsMe , eItsMe , eItsMe , eItsMe , \n eItsMe , eItsMe , eError , eError , eError , <NUM_LIT:4> , eError , eError , \n eError , eError , eError , eError , <NUM_LIT:5> , eError , eError , eError , \n eError , eError , eError , eItsMe , eStart , eStart , eStart , eStart , \n ) \n ISO2022KRCharLenTable = ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> ) \n ISO2022KRSMModel = { '<STR_LIT>' : ISO2022KR_cls , \n '<STR_LIT>' : <NUM_LIT:6> , \n '<STR_LIT>' : ISO2022KR_st , \n '<STR_LIT>' : ISO2022KRCharLenTable , \n <mask0> : \"<STR_LIT>\" } \n", "gt": "'<STR_LIT:name>'"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import argparse \n import sys \n import time \n import inspect \n import logging \n import signal \n import sys \n from multiprocessing import Process \n import tasa \n from tasa . worker import BaseWorker \n logger = logging . getLogger ( __name__ ) \n logging . basicConfig ( level = logging . INFO ) \n def signal_handler ( signal , frame ) : \n sys . exit ( <NUM_LIT:0> ) \n def _get_argparser ( ) : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \n '<STR_LIT>' , '<STR_LIT>' , action = '<STR_LIT:version>' , \n version = '<STR_LIT>' % ( \n tasa . __version__ , sys . version ) ) \n return parser \n def run ( ) : \n sys . path . insert ( <NUM_LIT:0> , '<STR_LIT>' ) \n parser = _get_argparser ( ) \n parser . description = '<STR_LIT>' \n parser . add_argument ( '<STR_LIT>' , \n type = lambda w : w . partition ( '<STR_LIT::>' ) [ : : <NUM_LIT:2> ] , \n help = '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n args = parser . parse_args ( ) \n worker_class_name = args . worker [ <NUM_LIT:1> ] or '<STR_LIT>' \n worker_module = __import__ ( args . worker [ <NUM_LIT:0> ] , globals ( ) , locals ( ) , \n [ worker_class_name ] ) \n try : \n WorkerClass = getattr ( worker_module , worker_class_name ) \n except AttributeError : \n print \"<STR_LIT>\" \n potential_workers = inspect . getmembers ( \n worker_module , \n lambda x : type ( x ) == type and issubclass ( x , BaseWorker ) ) \n if potential_workers : \n print \"<STR_LIT>\" \n for name , value in potential_workers : \n print '<STR_LIT::>' . join ( [ args . worker [ <NUM_LIT:0> ] , name ] ) \n exit ( <NUM_LIT:1> ) \n worker = WorkerClass ( ) \n print '<STR_LIT>' % ( args . worker [ <NUM_LIT:0> ] , \n worker . __class__ . __name__ ) \n try : \n for job in worker : \n if job : \n logger . info ( \"<STR_LIT>\" , \n worker . __class__ . __name__ , \n str ( job ) [ : <NUM_LIT:50> ] ) \n else : \n time . sleep ( <NUM_LIT> ) \n except KeyboardInterrupt : \n print '<STR_LIT>' \n def runm ( ) : \n \"\"\"<STR_LIT>\"\"\" \n signal . signal ( signal . SIGINT , signal_handler ) \n count = int ( sys . argv . pop ( <NUM_LIT:1> ) ) \n processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n try : \n for p in processes : \n p . start ( ) \n except KeyError : \n pass \n finally : \n for p in processes : \n p . join ( ) \n def log ( ) : \n parser = _get_argparser ( ) \n parser . description = '<STR_LIT>' \n args = parser . parse_args ( ) \n raise NotImplemented ( ) \n if __name__ == '<STR_LIT:__main__>' : \n cmd = '<STR_LIT>' if len ( sys . argv ) < <NUM_LIT:2> else sys . argv . pop ( <NUM_LIT:1> ) \n if cmd == '<STR_LIT>' : \n run ( ) \n elif cmd == '<STR_LIT>' : \n log ( ) \n else : \n <mask0> \"<STR_LIT>\" \n", "gt": "print"}
{"input": "\n from datetime import date , timedelta \n import uuid \n import phonenumbers \n from django . conf import settings \n from django . db import models \n from django . core . validators import RegexValidator \n from django . utils import timezone \n from django . utils . translation import ugettext as _ \n from . behaviors import TimeStampedModel \n from . import helpers , managers \n class Tag ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n color_regex = RegexValidator ( regex = r'<STR_LIT>' , message = \"<STR_LIT>\" ) \n name = models . CharField ( max_length = <NUM_LIT:64> , unique = True , help_text = '<STR_LIT>' ) \n color = models . CharField ( max_length = <NUM_LIT:6> , validators = [ color_regex ] , help_text = '<STR_LIT>' ) \n description = models . CharField ( max_length = <NUM_LIT:64> , blank = True , help_text = '<STR_LIT>' ) \n class Meta : \n ordering = [ '<STR_LIT:name>' ] \n def __str__ ( self ) : \n return self . name \n def save ( self , * args , ** kwargs ) : \n self . color = self . color . lower ( ) \n super ( Tag , self ) . save ( * args , ** kwargs ) \n class Person ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n phone_regex = RegexValidator ( regex = r'<STR_LIT>' , message = \"<STR_LIT>\" ) \n DEVELOPER_ROLE = '<STR_LIT>' \n QUALITY_ASSURANCE_ROLE = '<STR_LIT>' \n OPERATIONS_ROLE = '<STR_LIT>' \n MANAGER_ROLE = '<STR_LIT>' \n SECURITY_OFFICER_ROLE = '<STR_LIT>' \n SECURITY_CHAMPION_ROLE = '<STR_LIT>' \n ROLE_CHOICES = ( \n ( DEVELOPER_ROLE , '<STR_LIT>' ) , \n ( QUALITY_ASSURANCE_ROLE , '<STR_LIT>' ) , \n ( OPERATIONS_ROLE , '<STR_LIT>' ) , \n ( MANAGER_ROLE , '<STR_LIT>' ) , \n ( SECURITY_OFFICER_ROLE , '<STR_LIT>' ) , \n ( SECURITY_CHAMPION_ROLE , '<STR_LIT>' ) , \n ) \n first_name = models . CharField ( max_length = <NUM_LIT:64> ) \n last_name = models . CharField ( max_length = <NUM_LIT:64> ) \n email = models . EmailField ( max_length = <NUM_LIT> , unique = True ) \n role = models . CharField ( max_length = <NUM_LIT> , choices = ROLE_CHOICES ) \n phone_work = models . CharField ( max_length = <NUM_LIT:15> , validators = [ phone_regex ] , blank = True ) \n phone_mobile = models . CharField ( max_length = <NUM_LIT:15> , validators = [ phone_regex ] , blank = True ) \n job_title = models . CharField ( max_length = <NUM_LIT> , blank = True ) \n class Meta : \n ordering = [ '<STR_LIT>' ] \n verbose_name_plural = '<STR_LIT>' \n def __str__ ( self ) : \n return self . first_name + '<STR_LIT:U+0020>' + self . last_name \n def save ( self , * args , ** kwargs ) : \n if self . phone_work : \n self . phone_work = phonenumbers . format_number ( phonenumbers . parse ( self . phone_work , '<STR_LIT>' ) , phonenumbers . PhoneNumberFormat . E164 ) \n if self . phone_mobile : \n self . phone_mobile = phonenumbers . format_number ( phonenumbers . parse ( self . phone_mobile , '<STR_LIT>' ) , phonenumbers . PhoneNumberFormat . E164 ) \n self . email = self . email . lower ( ) \n super ( Person , self ) . save ( * args , ** kwargs ) \n class Organization ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n name = models . CharField ( max_length = <NUM_LIT:32> , unique = True , help_text = '<STR_LIT>' ) \n description = models . TextField ( blank = True , help_text = '<STR_LIT>' ) \n people = models . ManyToManyField ( Person , blank = True ) \n class Meta : \n ordering = [ '<STR_LIT:name>' ] \n def __str__ ( self ) : \n return self . name \n class DataElement ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n GLOBAL_CATEGORY = '<STR_LIT>' \n PERSONAL_CATEGORY = '<STR_LIT>' \n COMPANY_CATEGORY = '<STR_LIT>' \n STUDENT_CATEGORY = '<STR_LIT>' \n GOVERNMENT_CATEGORY = '<STR_LIT>' \n PCI_CATEGORY = '<STR_LIT>' \n MEDICAL_CATEGORY = '<STR_LIT>' \n CATEGORY_CHOICES = ( \n ( GLOBAL_CATEGORY , '<STR_LIT>' ) , \n ( PERSONAL_CATEGORY , '<STR_LIT>' ) , \n ( COMPANY_CATEGORY , '<STR_LIT>' ) , \n ( STUDENT_CATEGORY , '<STR_LIT>' ) , \n ( GOVERNMENT_CATEGORY , '<STR_LIT>' ) , \n ( PCI_CATEGORY , '<STR_LIT>' ) , \n ( MEDICAL_CATEGORY , '<STR_LIT>' ) , \n ) \n name = models . CharField ( max_length = <NUM_LIT> , unique = True ) \n description = models . TextField ( blank = True ) \n category = models . CharField ( max_length = <NUM_LIT:10> , choices = CATEGORY_CHOICES ) \n weight = models . PositiveIntegerField ( ) \n class Meta : \n ordering = [ '<STR_LIT:id>' ] \n def __str__ ( self ) : \n return self . name \n class Technology ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n PROGRAMMING_LANGUAGE_CATEGORY = '<STR_LIT>' \n OPERATING_SYSTEM_CATEGORY = '<STR_LIT>' \n DATA_STORE_CATEGORY = '<STR_LIT>' \n FRAMEWORK_CATEGORY = '<STR_LIT>' \n THIRD_PARTY_COMPONENT = '<STR_LIT>' \n WEB_SERVER_CATEGORY = '<STR_LIT>' \n APPLICATION_SERVER_CATEGORY = '<STR_LIT>' \n HOSTING_PROVIDER_CATEGORY = '<STR_LIT>' \n DENIAL_OF_SERVICE_CATEGORY = '<STR_LIT>' \n FIREWALL_CATEGORY = '<STR_LIT>' \n CATEGORY_CHOICES = ( \n ( PROGRAMMING_LANGUAGE_CATEGORY , '<STR_LIT>' ) , \n ( OPERATING_SYSTEM_CATEGORY , '<STR_LIT>' ) , \n ( DATA_STORE_CATEGORY , '<STR_LIT>' ) , \n ( FRAMEWORK_CATEGORY , '<STR_LIT>' ) , \n ( THIRD_PARTY_COMPONENT , '<STR_LIT>' ) , \n ( APPLICATION_SERVER_CATEGORY , '<STR_LIT>' ) , \n ( WEB_SERVER_CATEGORY , '<STR_LIT>' ) , \n ( HOSTING_PROVIDER_CATEGORY , '<STR_LIT>' ) , \n ( DENIAL_OF_SERVICE_CATEGORY , '<STR_LIT>' ) , \n ( FIREWALL_CATEGORY , '<STR_LIT>' ) , \n ) \n name = models . CharField ( max_length = <NUM_LIT:64> , help_text = '<STR_LIT>' ) \n category = models . CharField ( max_length = <NUM_LIT> , choices = CATEGORY_CHOICES , help_text = '<STR_LIT>' ) \n description = models . CharField ( max_length = <NUM_LIT> , blank = True , help_text = '<STR_LIT>' ) \n reference = models . URLField ( blank = True , help_text = '<STR_LIT>' ) \n class Meta : \n ordering = [ '<STR_LIT>' , '<STR_LIT:name>' ] \n verbose_name_plural = '<STR_LIT>' \n def __str__ ( self ) : \n return self . get_category_display ( ) + '<STR_LIT>' + self . name \n class Regulation ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n PRIVACY_CATEGORY = '<STR_LIT>' \n FINANCE_CATEGORY = '<STR_LIT>' \n EDUCATION_CATEGORY = '<STR_LIT>' \n MEDICAL_CATEGORY = '<STR_LIT>' \n OTHER_CATEGORY = '<STR_LIT>' \n CATEGORY_CHOICES = ( \n ( PRIVACY_CATEGORY , '<STR_LIT>' ) , \n ( FINANCE_CATEGORY , '<STR_LIT>' ) , \n ( EDUCATION_CATEGORY , '<STR_LIT>' ) , \n ( MEDICAL_CATEGORY , '<STR_LIT>' ) , \n ( OTHER_CATEGORY , '<STR_LIT>' ) , \n ) \n name = models . CharField ( max_length = <NUM_LIT> , help_text = '<STR_LIT>' ) \n acronym = models . CharField ( max_length = <NUM_LIT:20> , unique = True , help_text = '<STR_LIT>' ) \n category = models . CharField ( max_length = <NUM_LIT:9> , choices = CATEGORY_CHOICES , help_text = '<STR_LIT>' ) \n jurisdiction = models . CharField ( max_length = <NUM_LIT:64> , help_text = '<STR_LIT>' ) \n description = models . TextField ( blank = True , help_text = '<STR_LIT>' ) \n reference = models . URLField ( blank = True , help_text = '<STR_LIT>' ) \n class Meta : \n ordering = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT:name>' ] \n def __str__ ( self ) : \n return self . acronym + '<STR_LIT>' + self . jurisdiction + '<STR_LIT:)>' \n class ServiceLevelAgreement ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n name = models . CharField ( max_length = <NUM_LIT:64> , help_text = '<STR_LIT>' ) \n description = models . CharField ( max_length = <NUM_LIT> , blank = True , help_text = '<STR_LIT>' ) \n class Meta : \n ordering = [ '<STR_LIT:name>' ] \n def __str__ ( self ) : \n return self . name \n class ThreadFix ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n name = models . CharField ( max_length = <NUM_LIT:32> , unique = True , help_text = '<STR_LIT>' ) \n host = models . URLField ( help_text = '<STR_LIT>' ) \n api_key = models . CharField ( max_length = <NUM_LIT:50> , help_text = '<STR_LIT>' ) \n verify_ssl = models . BooleanField ( default = True , help_text = '<STR_LIT>' ) \n class Meta : \n verbose_name = '<STR_LIT>' \n verbose_name_plural = '<STR_LIT>' \n def __str__ ( self ) : \n return self . name + '<STR_LIT>' + self . host \n class Application ( TimeStampedModel , models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n WEB_PLATFORM = '<STR_LIT>' \n DESKTOP_PLATFORM = '<STR_LIT>' \n MOBILE_PLATFORM = '<STR_LIT>' \n WEB_SERVICE_PLATFORM = '<STR_LIT>' \n PLATFORM_CHOICES = ( \n ( WEB_PLATFORM , '<STR_LIT>' ) , \n ( DESKTOP_PLATFORM , '<STR_LIT>' ) , \n ( MOBILE_PLATFORM , '<STR_LIT>' ) , \n ( WEB_SERVICE_PLATFORM , '<STR_LIT>' ) , \n ) \n IDEA_LIFECYCLE = '<STR_LIT>' \n EXPLORE_LIFECYCLE = '<STR_LIT>' \n VALIDATE_LIFECYCLE = '<STR_LIT>' \n GROW_LIFECYCLE = '<STR_LIT>' \n SUSTAIN_LIFECYCLE = '<STR_LIT>' \n RETIRE_LIFECYCLE = '<STR_LIT>' \n LIFECYCLE_CHOICES = ( \n ( IDEA_LIFECYCLE , '<STR_LIT>' ) , \n ( EXPLORE_LIFECYCLE , '<STR_LIT>' ) , \n ( VALIDATE_LIFECYCLE , '<STR_LIT>' ) , \n ( GROW_LIFECYCLE , '<STR_LIT>' ) , \n ( SUSTAIN_LIFECYCLE , '<STR_LIT>' ) , \n ( RETIRE_LIFECYCLE , '<STR_LIT>' ) , \n ) \n THIRD_PARTY_LIBRARY_ORIGIN = '<STR_LIT>' \n PURCHASED_ORIGIN = '<STR_LIT>' \n CONTRACTOR_ORIGIN = '<STR_LIT>' \n INTERNALLY_DEVELOPED_ORIGIN = '<STR_LIT>' \n OPEN_SOURCE_ORIGIN = '<STR_LIT>' \n OUTSOURCED_ORIGIN = '<STR_LIT>' \n ORIGIN_CHOICES = ( \n ( THIRD_PARTY_LIBRARY_ORIGIN , '<STR_LIT>' ) , \n ( PURCHASED_ORIGIN , '<STR_LIT>' ) , \n ( CONTRACTOR_ORIGIN , '<STR_LIT>' ) , \n ( INTERNALLY_DEVELOPED_ORIGIN , '<STR_LIT>' ) , \n ( OPEN_SOURCE_ORIGIN , '<STR_LIT>' ) , \n ( OUTSOURCED_ORIGIN , '<STR_LIT>' ) , \n ) \n VERY_HIGH_CRITICALITY = '<STR_LIT>' \n HIGH_CRITICALITY = '<STR_LIT>' \n MEDIUM_CRITICALITY = '<STR_LIT>' \n LOW_CRITICALITY = '<STR_LIT>' \n VERY_LOW_CRITICALITY = '<STR_LIT>' \n NONE_CRITICALITY = '<STR_LIT:none>' \n BUSINESS_CRITICALITY_CHOICES = ( \n ( VERY_HIGH_CRITICALITY , '<STR_LIT>' ) , \n ( HIGH_CRITICALITY , '<STR_LIT>' ) , \n ( MEDIUM_CRITICALITY , '<STR_LIT>' ) , \n ( LOW_CRITICALITY , '<STR_LIT>' ) , \n ( VERY_LOW_CRITICALITY , '<STR_LIT>' ) , \n ( NONE_CRITICALITY , '<STR_LIT:None>' ) , \n ) \n DCL_1 = <NUM_LIT:1> \n DCL_2 = <NUM_LIT:2> \n DCL_3 = <NUM_LIT:3> \n DCL_4 = <NUM_LIT:4> \n DATA_CLASSIFICATION_CHOICES = ( \n ( None , '<STR_LIT>' ) , \n ( DCL_1 , '<STR_LIT>' ) , \n ( DCL_2 , '<STR_LIT>' ) , \n ( DCL_3 , '<STR_LIT>' ) , \n ( DCL_4 , '<STR_LIT>' ) , \n ) \n ASVS_0 = <NUM_LIT:0> \n ASVS_1 = <NUM_LIT:1> \n ASVS_2 = <NUM_LIT:2> \n ASVS_3 = <NUM_LIT:3> \n ASVS_CHOICES = ( \n ( None , '<STR_LIT>' ) , \n ( ASVS_0 , '<STR_LIT:0>' ) , \n ( ASVS_1 , '<STR_LIT:1>' ) , \n ( ASVS_2 , '<STR_LIT:2>' ) , \n ( ASVS_3 , '<STR_LIT:3>' ) , \n ) \n name = models . CharField ( max_length = <NUM_LIT> , unique = True , help_text = '<STR_LIT>' ) \n description = models . TextField ( blank = True , help_text = '<STR_LIT>' ) \n business_criticality = models . CharField ( max_length = <NUM_LIT:9> , choices = BUSINESS_CRITICALITY_CHOICES , blank = True , null = True ) \n platform = models . CharField ( max_length = <NUM_LIT:11> , choices = PLATFORM_CHOICES , blank = True , null = True ) \n lifecycle = models . CharField ( max_length = <NUM_LIT:8> , choices = LIFECYCLE_CHOICES , blank = True , null = True ) \n origin = models . CharField ( max_length = <NUM_LIT> , choices = ORIGIN_CHOICES , blank = True , null = True ) \n user_records = models . PositiveIntegerField ( blank = True , null = True , help_text = '<STR_LIT>' ) \n revenue = models . DecimalField ( max_digits = <NUM_LIT:15> , decimal_places = <NUM_LIT:2> , blank = True , null = True , help_text = '<STR_LIT>' ) \n external_audience = models . BooleanField ( default = False , help_text = '<STR_LIT>' ) \n internet_accessible = models . BooleanField ( default = False , help_text = '<STR_LIT>' ) \n requestable = models . NullBooleanField ( default = True , help_text = _ ( '<STR_LIT>' ) ) \n technologies = models . ManyToManyField ( Technology , blank = True ) \n regulations = models . ManyToManyField ( Regulation , blank = True ) \n service_level_agreements = models . ManyToManyField ( ServiceLevelAgreement , blank = True ) \n data_elements = models . ManyToManyField ( DataElement , blank = True ) \n override_dcl = models . IntegerField ( choices = DATA_CLASSIFICATION_CHOICES , blank = True , null = True , help_text = '<STR_LIT>' ) \n override_reason = models . TextField ( blank = True , help_text = '<STR_LIT>' ) \n threadfix = models . ForeignKey ( ThreadFix , blank = True , null = True , help_text = '<STR_LIT>' ) \n threadfix_team_id = models . PositiveIntegerField ( blank = True , null = True , help_text = '<STR_LIT>' ) \n threadfix_application_id = models . PositiveIntegerField ( blank = True , null = True , help_text = '<STR_LIT>' ) \n asvs_level = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = '<STR_LIT>' ) \n asvs_level_percent_achieved = models . PositiveIntegerField ( blank = True , null = True , help_text = '<STR_LIT>' ) \n asvs_doc_url = models . URLField ( blank = True , help_text = '<STR_LIT>' ) \n asvs_level_target = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = '<STR_LIT>' ) \n \"\"\"<STR_LIT>\"\"\" \n organization = models . ForeignKey ( Organization , help_text = '<STR_LIT>' ) \n people = models . ManyToManyField ( Person , through = '<STR_LIT>' , blank = True ) \n tags = models . ManyToManyField ( Tag , blank = True ) \n objects = managers . ApplicationManager . from_queryset ( managers . ApplicationQuerySet ) ( ) \n class Meta : \n get_latest_by = '<STR_LIT>' \n ordering = [ '<STR_LIT:name>' ] \n def __str__ ( self ) : \n return self . name \n def data_classification_level ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return helpers . data_classification_level ( self . data_sensitivity_value ( ) ) \n def data_sensitivity_value ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return helpers . data_sensitivity_value ( self . data_elements . all ( ) ) \n def is_new ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n delta = self . created_date - timezone . now ( ) \n return delta >= timedelta ( days = - <NUM_LIT:7> ) \n class ThreadFixMetrics ( TimeStampedModel , models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n critical_count = models . PositiveIntegerField ( default = <NUM_LIT:0> ) \n high_count = models . PositiveIntegerField ( default = <NUM_LIT:0> ) \n medium_count = models . PositiveIntegerField ( default = <NUM_LIT:0> ) \n low_count = models . PositiveIntegerField ( default = <NUM_LIT:0> ) \n informational_count = models . PositiveIntegerField ( default = <NUM_LIT:0> ) \n application = models . ForeignKey ( Application ) \n class Meta : \n get_latest_by = '<STR_LIT>' \n verbose_name = '<STR_LIT>' \n verbose_name_plural = '<STR_LIT>' \n def total ( self ) : \n return self . critical_count + self . high_count + self . medium_count + self . low_count + self . informational_count \n class Relation ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n owner = models . BooleanField ( default = False , help_text = '<STR_LIT>' ) \n emergency = models . BooleanField ( default = False , help_text = '<STR_LIT>' ) \n notes = models . TextField ( blank = True , help_text = '<STR_LIT>' ) \n person = models . ForeignKey ( Person , help_text = '<STR_LIT>' ) \n application = models . ForeignKey ( Application ) \n class Meta : \n unique_together = ( '<STR_LIT>' , '<STR_LIT>' ) \n def __str__ ( self ) : \n return self . person . first_name + '<STR_LIT:U+0020>' + self . person . last_name + '<STR_LIT>' + self . application . name \n class Environment ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n DEVELOPMENT_ENVIRONMENT = '<STR_LIT>' \n INTEGRATION_ENVIRONMENT = '<STR_LIT:int>' \n QUALITY_ASSURANCE_ENVIRONMENT = '<STR_LIT>' \n PRE_PRODUCTION_ENVIRONMENT = '<STR_LIT>' \n CUSTOMER_ACCEPTANCE_ENVIRONMENT = '<STR_LIT>' \n PRODUCTION_ENVIRONMENT = '<STR_LIT>' \n ENVIRONMENT_CHOICES = ( \n ( DEVELOPMENT_ENVIRONMENT , '<STR_LIT>' ) , \n ( INTEGRATION_ENVIRONMENT , '<STR_LIT>' ) , \n ( QUALITY_ASSURANCE_ENVIRONMENT , '<STR_LIT>' ) , \n ( PRE_PRODUCTION_ENVIRONMENT , '<STR_LIT>' ) , \n ( CUSTOMER_ACCEPTANCE_ENVIRONMENT , '<STR_LIT>' ) , \n ( PRODUCTION_ENVIRONMENT , '<STR_LIT>' ) , \n ) \n environment_type = models . CharField ( max_length = <NUM_LIT:4> , choices = ENVIRONMENT_CHOICES , help_text = '<STR_LIT>' ) \n description = models . TextField ( blank = True , help_text = '<STR_LIT>' ) \n testing_approved = models . BooleanField ( default = False , help_text = '<STR_LIT>' ) \n application = models . ForeignKey ( Application ) \n class Meta : \n ordering = [ '<STR_LIT>' , '<STR_LIT>' ] \n def __str__ ( self ) : \n return self . application . name + '<STR_LIT>' + dict ( Environment . ENVIRONMENT_CHOICES ) [ self . environment_type ] + '<STR_LIT:)>' \n class EnvironmentLocation ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n location = models . URLField ( help_text = '<STR_LIT>' ) \n notes = models . TextField ( blank = True , help_text = '<STR_LIT>' ) \n environment = models . ForeignKey ( Environment ) \n def __str__ ( self ) : \n return self . location \n class EnvironmentCredentials ( TimeStampedModel , models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n username = models . CharField ( max_length = <NUM_LIT> , blank = True ) \n password = models . CharField ( max_length = <NUM_LIT> , blank = True ) \n role_description = models . CharField ( max_length = <NUM_LIT> , blank = True , help_text = '<STR_LIT>' ) \n notes = models . TextField ( blank = True , help_text = '<STR_LIT>' ) \n environment = models . ForeignKey ( Environment ) \n class Meta : \n verbose_name_plural = '<STR_LIT>' \n ordering = [ '<STR_LIT:username>' , '<STR_LIT:password>' ] \n class Engagement ( TimeStampedModel , models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n PENDING_STATUS = '<STR_LIT>' \n OPEN_STATUS = '<STR_LIT>' \n CLOSED_STATUS = '<STR_LIT>' \n STATUS_CHOICES = ( \n ( PENDING_STATUS , '<STR_LIT>' ) , \n ( OPEN_STATUS , '<STR_LIT>' ) , \n ( CLOSED_STATUS , '<STR_LIT>' ) \n ) \n status = models . CharField ( max_length = <NUM_LIT:7> , choices = STATUS_CHOICES , default = PENDING_STATUS ) \n start_date = models . DateField ( help_text = '<STR_LIT>' ) \n end_date = models . DateField ( help_text = '<STR_LIT>' ) \n description = models . TextField ( blank = True ) \n open_date = models . DateTimeField ( blank = True , null = True , help_text = '<STR_LIT>' ) \n close_date = models . DateTimeField ( blank = True , null = True , help_text = '<STR_LIT>' ) \n duration = models . DurationField ( blank = True , null = True ) \n requestor = models . ForeignKey ( Person , blank = True , null = True , help_text = '<STR_LIT>' ) \n application = models . ForeignKey ( Application ) \n objects = managers . EngagementManager . from_queryset ( managers . EngagementQuerySet ) ( ) \n metrics = managers . EngagementMetrics . from_queryset ( managers . EngagementQuerySet ) ( ) \n class Meta : \n get_latest_by = '<STR_LIT>' \n ordering = [ '<STR_LIT>' ] \n def save ( self , * args , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . pk is not None : \n engagement = Engagement . objects . get ( pk = self . pk ) \n now = timezone . now ( ) \n if engagement . status != self . status : \n if self . status == Engagement . PENDING_STATUS : \n self . open_date = None \n self . close_date = None \n elif self . status == Engagement . OPEN_STATUS : \n self . open_date = now \n self . close_date = None \n elif self . status == Engagement . CLOSED_STATUS : \n if self . open_date is None : \n self . open_date = now \n self . close_date = now \n if self . open_date is not None and self . close_date is not None : \n self . duration = self . close_date - self . open_date \n super ( Engagement , self ) . save ( * args , ** kwargs ) \n def is_pending ( self ) : \n return self . status == Engagement . PENDING_STATUS \n def is_open ( self ) : \n return self . status == Engagement . OPEN_STATUS \n def is_closed ( self ) : \n return self . status == Engagement . CLOSED_STATUS \n def is_ready_for_work ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . status == Engagement . PENDING_STATUS : \n if date . today ( ) >= self . start_date : \n return True \n return False \n def is_past_due ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . status == Engagement . PENDING_STATUS or self . status == Engagement . OPEN_STATUS : \n if date . today ( ) > self . end_date : \n return True \n return False \n class ActivityType ( TimeStampedModel , models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n name = models . CharField ( max_length = <NUM_LIT> , unique = True , help_text = _ ( '<STR_LIT>' ) ) \n documentation = models . TextField ( blank = True , help_text = _ ( '<STR_LIT>' ) ) \n objects = managers . ActivityTypeManager . from_queryset ( managers . ActivityTypeQuerySet ) ( ) \n metrics = managers . ActivityTypeMetrics . from_queryset ( managers . ActivityTypeQuerySet ) ( ) \n class Meta : \n ordering = [ '<STR_LIT:name>' ] \n def __str__ ( self ) : \n return self . name \n class Activity ( models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n PENDING_STATUS = '<STR_LIT>' \n OPEN_STATUS = '<STR_LIT>' \n CLOSED_STATUS = '<STR_LIT>' \n STATUS_CHOICES = ( \n ( PENDING_STATUS , '<STR_LIT>' ) , \n ( OPEN_STATUS , '<STR_LIT>' ) , \n ( CLOSED_STATUS , '<STR_LIT>' ) \n ) \n status = models . CharField ( max_length = <NUM_LIT:7> , choices = STATUS_CHOICES , default = PENDING_STATUS ) \n description = models . TextField ( blank = True ) \n open_date = models . DateTimeField ( blank = True , null = True , help_text = '<STR_LIT>' ) \n close_date = models . DateTimeField ( blank = True , null = True , help_text = '<STR_LIT>' ) \n duration = models . DurationField ( blank = True , null = True ) \n activity_type = models . ForeignKey ( ActivityType ) \n engagement = models . ForeignKey ( Engagement ) \n users = models . ManyToManyField ( settings . AUTH_USER_MODEL , blank = True ) \n objects = managers . ActivityManager . from_queryset ( managers . ActivityQuerySet ) ( ) \n class Meta : \n ordering = [ '<STR_LIT>' ] \n verbose_name_plural = '<STR_LIT>' \n def __str__ ( self ) : \n return self . activity_type . name \n def save ( self , * args , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . pk is not None : \n activity = Activity . objects . get ( pk = self . pk ) \n if activity . status != self . status : \n now = timezone . now ( ) \n if self . status == Activity . PENDING_STATUS : \n self . open_date = None \n self . close_date = None \n elif self . status == Activity . OPEN_STATUS : \n self . open_date = now \n self . close_date = None \n if self . engagement . status is not Engagement . OPEN_STATUS : \n self . engagement . status = Engagement . OPEN_STATUS \n self . engagement . save ( ) \n elif self . status == Activity . CLOSED_STATUS : \n if self . open_date is None : \n self . open_date = now \n self . close_date = now \n close = True \n for current_activity in self . engagement . activity_set . exclude ( id = self . id ) : \n if current_activity . status != Activity . CLOSED_STATUS : \n close = False \n break \n if close : \n self . engagement . status = Engagement . CLOSED_STATUS \n self . engagement . save ( ) \n if self . open_date is not None and self . close_date is not None : \n self . duration = self . close_date - self . open_date \n super ( Activity , self ) . save ( * args , ** kwargs ) \n def is_pending ( self ) : \n return self . status == Activity . PENDING_STATUS \n def is_open ( self ) : \n return self . status == Activity . OPEN_STATUS \n def is_closed ( self ) : \n return self . status == Activity . CLOSED_STATUS \n def is_ready_for_work ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . status == Activity . PENDING_STATUS : \n if date . today ( ) >= self . engagement . start_date : \n return True \n return False \n def is_past_due ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if self . status == Activity . PENDING_STATUS or self . status == Activity . OPEN_STATUS : \n if date . today ( ) > self . engagement . end_date : \n return True \n return False \n class Comment ( TimeStampedModel , models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n message = models . TextField ( ) \n user = models . ForeignKey ( settings . AUTH_USER_MODEL ) \n def __str__ ( self ) : \n return self . message \n class Meta : \n abstract = True \n class EngagementComment ( Comment ) : \n \"\"\"<STR_LIT>\"\"\" \n engagement = models . ForeignKey ( Engagement ) \n class ActivityComment ( Comment ) : \n \"\"\"<STR_LIT>\"\"\" \n activity = models . ForeignKey ( Activity ) \n class ExternalRequest ( TimeStampedModel , models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n token = models . UUIDField ( default = uuid . uuid4 , editable = False ) \n requestor = models . ForeignKey ( Person ) \n application = models . ForeignKey ( Application , blank = True ) \n activities = models . ManyToManyField ( ActivityType , limit_choices_to = { '<STR_LIT>' : True } ) \n class FileUpload ( TimeStampedModel , models . Model ) : \n \"\"\"<STR_LIT>\"\"\" \n file = models . FileField ( ) \n user = models . ForeignKey ( settings . AUTH_USER_MODEL ) \n class Meta : \n abstract = True \n class ApplicationFileUpload ( FileUpload ) : \n \"\"\"<STR_LIT>\"\"\" \n REPORT_FILE_TYPE = '<STR_LIT>' \n DOCUMENTATION_FILE_TYPE = '<STR_LIT>' \n FILE_TYPE_CHOICES = ( \n ( REPORT_FILE_TYPE , '<STR_LIT>' ) , \n ( DOCUMENTATION_FILE_TYPE , '<STR_LIT>' ) , \n ) \n file_type = models . CharField ( max_length = <NUM_LIT> , choices = FILE_TYPE_CHOICES ) \n application = models . ForeignKey ( <mask0> ) \n", "gt": "Application"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from time import sleep \n from flask import Flask \n from flask_tut . models import ( \n db , \n User , \n Address , \n ) \n app = Flask ( __name__ ) \n with app . app_context ( ) : \n db . create_all ( ) \n i = <NUM_LIT:0> \n while i < <NUM_LIT:30> : \n address = Address ( description = '<STR_LIT>' + str ( i ) . rjust ( <NUM_LIT:2> , \"<STR_LIT:0>\" ) ) \n db . session . add ( address ) \n user = User ( name = '<STR_LIT>' + str ( i ) . rjust ( <NUM_LIT:2> , \"<STR_LIT:0>\" ) ) \n user . address = address \n db . session . add ( user ) \n sleep ( <NUM_LIT:1> ) \n i += <NUM_LIT:1> \n db . session . <mask0> ( ) \n", "gt": "commit"}
{"input": "\n import urllib2 \n import google \n import time \n import pyprind \n import os \n import random \n from urlparse import urlparse \n \"\"\"<STR_LIT>\"\"\" \n class Crawler ( object ) : \n version = \"<STR_LIT>\" \n outputDir = \"<STR_LIT>\" \n languageDir = \"<STR_LIT>\" \n basicString = \"<STR_LIT>\" \n searchString = \"<STR_LIT>\" \n def __init__ ( self , language = \"<STR_LIT>\" ) : \n \"\"\"<STR_LIT>\"\"\" \n self . language = language . lower ( ) \n self . parsedUrls = [ ] \n self . foundedAccounts = <NUM_LIT:0> \n def change_language ( self , language = \"<STR_LIT>\" ) : \n \"\"\"<STR_LIT>\"\"\" \n if os . path . isfile ( self . languageDir + \"<STR_LIT:/>\" + language + \"<STR_LIT>\" ) : \n self . language = language \n return True \n else : \n return False \n def search_links ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n for url in google . search ( self . searchString , num = <NUM_LIT:30> , stop = <NUM_LIT:1> ) : \n parsed = urlparse ( url ) \n self . parsedUrls . append ( parsed . scheme + \"<STR_LIT>\" + parsed . netloc ) \n def search_accounts ( self , url = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if not self . parsedUrls : \n return \"<STR_LIT>\" \n try : \n if not url : \n url = random . choice ( self . parsedUrls ) \n fileName = self . languageDir + \"<STR_LIT:/>\" + self . language + \"<STR_LIT>\" \n fileLength = self . file_length ( fileName ) \n progressBar = pyprind . ProgBar ( fileLength , title = \"<STR_LIT>\" + url + \"<STR_LIT>\" , stream = <NUM_LIT:1> , monitor = True ) \n foundedAccounts = <NUM_LIT:0> \n with open ( fileName ) as f : \n rows = f . readlines ( ) \n for row in rows : \n opener = urllib2 . build_opener ( ) \n opener . addheaders = [ ( '<STR_LIT>' , '<STR_LIT>' ) ] \n response = opener . open ( url + self . basicString % ( row . rstrip ( ) . lstrip ( ) , row . rstrip ( ) . lstrip ( ) ) ) \n fetched = response . read ( ) \n fileLength = fileLength - <NUM_LIT:1> \n progressBar . update ( ) \n if len ( fetched ) > <NUM_LIT:0> : \n newPath = self . outputDir + \"<STR_LIT:/>\" + url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n self . create_file ( row , newPath , fetched ) \n self . parsedUrls . remove ( url ) \n if self . foundedAccounts != <NUM_LIT:0> : \n return \"<STR_LIT>\" + url + \"<STR_LIT>\" + str ( self . foundedAccounts ) \n else : \n return \"<STR_LIT>\" + url \n except IOError : \n return \"<STR_LIT>\" \n except urllib2 . HTTPError , e : \n return \"<STR_LIT>\" + str ( e . code ) \n except urllib2 . URLError , e : \n return \"<STR_LIT>\" + str ( e . reason ) \n except Exception : \n return \"<STR_LIT>\" \n def create_file ( self , row , newPath , fetched ) : \n \"\"\"<STR_LIT>\"\"\" \n if os . path . exists ( newPath ) is False : \n os . makedirs ( newPath ) \n outputFile = open ( str ( newPath ) + \"<STR_LIT>\" % row . rstrip ( ) . lstrip ( ) , \"<STR_LIT:w>\" ) \n outputFile . write ( fetched ) \n self . foundedAccounts = self . foundedAccounts + <NUM_LIT:1> \n outputFile . close ( ) \n def file_length ( self , fileName ) : \n \"\"\"<STR_LIT>\"\"\" \n with open ( fileName ) as f : \n for i , l in enumerate ( f ) : \n pass \n return <mask0> + <NUM_LIT:1> \n", "gt": "i"}
{"input": "\n from cStringIO import StringIO \n from binascii import b2a_hex \n from urllib import quote \n import Connecter \n try : \n True \n except : \n True = <NUM_LIT:1> \n False = <NUM_LIT:0> \n DEBUG = False \n protocol_name = '<STR_LIT>' \n option_pattern = chr ( <NUM_LIT:0> ) * <NUM_LIT:8> \n def toint ( s ) : \n return long ( b2a_hex ( s ) , <NUM_LIT:16> ) \n def tohex ( s ) : \n return b2a_hex ( s ) . upper ( ) \n def make_readable ( s ) : \n if not s : \n return '<STR_LIT>' \n if quote ( s ) . find ( '<STR_LIT:%>' ) >= <NUM_LIT:0> : \n return tohex ( s ) \n return '<STR_LIT:\">' + s + '<STR_LIT:\">' \n streamno = <NUM_LIT:0> \n class StreamCheck : \n def __init__ ( self ) : \n global streamno \n self . no = streamno \n streamno += <NUM_LIT:1> \n self . buffer = StringIO ( ) \n self . next_len , self . next_func = <NUM_LIT:1> , self . read_header_len \n def read_header_len ( self , s ) : \n if ord ( s ) != len ( protocol_name ) : \n print self . no , '<STR_LIT>' \n return len ( protocol_name ) , self . read_header \n def read_header ( self , s ) : \n if s != protocol_name : \n print self . no , '<STR_LIT>' \n return <NUM_LIT:8> , self . read_reserved \n def read_reserved ( self , s ) : \n return <NUM_LIT:20> , self . read_download_id \n def read_download_id ( self , s ) : \n if DEBUG : \n print self . no , '<STR_LIT>' + tohex ( s ) \n return <NUM_LIT:20> , self . read_peer_id \n def read_peer_id ( self , s ) : \n if DEBUG : \n print self . no , '<STR_LIT>' + make_readable ( s ) \n return <NUM_LIT:4> , self . read_len \n def read_len ( self , s ) : \n l = toint ( s ) \n if l > <NUM_LIT:2> ** <NUM_LIT> : \n print self . no , '<STR_LIT>' + str ( l ) + '<STR_LIT>' + s + '<STR_LIT:)>' \n return l , self . read_message \n def read_message ( self , s ) : \n if not s : \n return <NUM_LIT:4> , self . read_len \n m = s [ <NUM_LIT:0> ] \n if ord ( m ) > <NUM_LIT:8> : \n print self . no , '<STR_LIT>' + str ( ord ( m ) ) \n if m == Connecter . REQUEST : \n if len ( s ) != <NUM_LIT> : \n print self . no , '<STR_LIT>' + str ( len ( s ) ) \n return <NUM_LIT:4> , self . read_len \n index = toint ( s [ <NUM_LIT:1> : <NUM_LIT:5> ] ) \n begin = toint ( s [ <NUM_LIT:5> : <NUM_LIT:9> ] ) \n length = toint ( s [ <NUM_LIT:9> : ] ) \n print self . no , '<STR_LIT>' + str ( index ) + '<STR_LIT>' + str ( begin ) + '<STR_LIT:->' + str ( begin ) + '<STR_LIT:+>' + str ( length ) \n elif m == Connecter . CANCEL : \n if len ( s ) != <NUM_LIT> : \n print self . no , '<STR_LIT>' + str ( len ( s ) ) \n return <NUM_LIT:4> , self . read_len \n index = toint ( s [ <NUM_LIT:1> : <NUM_LIT:5> ] ) \n begin = toint ( s [ <NUM_LIT:5> : <NUM_LIT:9> ] ) \n length = toint ( s [ <NUM_LIT:9> : ] ) \n print self . no , '<STR_LIT>' + str ( index ) + '<STR_LIT>' + str ( begin ) + '<STR_LIT:->' + str ( begin ) + '<STR_LIT:+>' + str ( length ) \n elif m == Connecter . PIECE : \n index = toint ( s [ <NUM_LIT:1> : <NUM_LIT:5> ] ) \n begin = toint ( s [ <NUM_LIT:5> : <NUM_LIT:9> ] ) \n length = len ( s ) - <NUM_LIT:9> \n print self . no , '<STR_LIT>' + str ( index ) + '<STR_LIT>' + str ( begin ) + '<STR_LIT:->' + str ( begin ) + '<STR_LIT:+>' + str ( length ) \n else : \n print self . no , '<STR_LIT>' + str ( ord ( m ) ) + '<STR_LIT>' + str ( len ( s ) ) + '<STR_LIT:)>' \n return <NUM_LIT:4> , self . read_len \n def write ( self , s ) : \n while <NUM_LIT:1> : \n i = self . next_len - self . buffer . tell ( ) \n if i > len ( s ) : \n self . buffer . write ( s ) \n return \n self . buffer . write ( s [ : i ] ) \n s = s [ i : ] \n m = self . buffer . getvalue ( ) \n self . buffer . reset ( ) \n self . buffer . truncate ( ) \n x = self . next_func ( m ) \n self . next_len , self . next_func = <mask0> \n", "gt": "x"}
{"input": "\n from types import * \n from cStringIO import StringIO \n def splitLine ( line , COLS = <NUM_LIT> , indent = <NUM_LIT:10> ) : \n indent = \"<STR_LIT:U+0020>\" * indent \n width = COLS - ( len ( indent ) + <NUM_LIT:1> ) \n if indent and width < <NUM_LIT:15> : \n width = COLS - <NUM_LIT:2> \n indent = \"<STR_LIT:U+0020>\" \n s = StringIO ( ) \n i = <NUM_LIT:0> \n for word in line . split ( ) : \n if i == <NUM_LIT:0> : \n s . write ( indent + word ) \n i = len ( word ) \n continue \n if i + len ( word ) >= width : \n s . write ( '<STR_LIT:\\n>' + indent + word ) \n i = len ( word ) \n continue \n s . write ( '<STR_LIT:U+0020>' + word ) \n i += len ( word ) + <NUM_LIT:1> \n return s . getvalue ( ) \n def formatDefinitions ( options , COLS , presets = { } ) : \n s = StringIO ( ) \n for ( longname , default , doc ) in options : \n s . write ( '<STR_LIT>' + longname + '<STR_LIT>' ) \n default = presets . get ( longname , default ) \n if type ( default ) in ( IntType , LongType ) : \n try : \n default = int ( default ) \n except : \n pass \n if default is not None : \n doc += '<STR_LIT>' + repr ( default ) + '<STR_LIT:)>' \n s . write ( splitLine ( doc , COLS , <NUM_LIT:10> ) ) \n s . write ( '<STR_LIT>' ) \n return s . getvalue ( ) \n def usage ( string ) : \n raise ValueError ( string ) \n def defaultargs ( options ) : \n l = { } \n for ( longname , default , doc ) in options : \n if default is not None : \n l [ longname ] = default \n return l \n def parseargs ( argv , options , minargs = None , maxargs = None , presets = { } ) : \n config = { } \n longkeyed = { } \n for option in options : \n longname , default , doc = option \n longkeyed [ longname ] = option \n config [ longname ] = default \n for longname in presets . keys ( ) : \n config [ longname ] = presets [ longname ] \n options = [ ] \n args = [ ] \n pos = <NUM_LIT:0> \n while pos < len ( argv ) : \n if argv [ pos ] [ : <NUM_LIT:2> ] != '<STR_LIT>' : \n args . append ( argv [ pos ] ) \n pos += <NUM_LIT:1> \n else : \n if pos == len ( argv ) - <NUM_LIT:1> : \n usage ( '<STR_LIT>' ) \n key , value = argv [ pos ] [ <NUM_LIT:2> : ] , argv [ pos + <NUM_LIT:1> ] \n pos += <NUM_LIT:2> \n if not longkeyed . has_key ( key ) : \n usage ( '<STR_LIT>' + key ) \n longname , default , doc = longkeyed [ key ] \n try : \n t = type ( config [ longname ] ) \n if t is NoneType or t is StringType : \n config [ longname ] = value \n elif t in ( IntType , LongType ) : \n config [ longname ] = long ( value ) \n elif t is FloatType : \n config [ longname ] = float ( value ) \n else : \n assert <NUM_LIT:0> \n except ValueError , e : \n usage ( '<STR_LIT>' % ( key , str ( e ) ) ) \n for key , value in config . items ( ) : \n if value is None : \n usage ( \"<STR_LIT>\" % key ) \n if minargs is not None and len ( args ) < minargs : \n usage ( \"<STR_LIT>\" % minargs ) \n if maxargs is not None and len ( args ) > maxargs : \n usage ( \"<STR_LIT>\" % maxargs ) \n return ( config , args ) \n def test_parseargs ( ) : \n assert parseargs ( ( '<STR_LIT:d>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT:e>' , '<STR_LIT>' , '<STR_LIT:3>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT:f>' ) , ( ( '<STR_LIT:a>' , '<STR_LIT:x>' , '<STR_LIT>' ) , ( '<STR_LIT:b>' , <NUM_LIT:1> , '<STR_LIT>' ) , ( '<STR_LIT:c>' , <NUM_LIT> , '<STR_LIT>' ) ) ) == ( { '<STR_LIT:a>' : '<STR_LIT>' , '<STR_LIT:b>' : <NUM_LIT:3> , '<STR_LIT:c>' : <NUM_LIT> } , [ '<STR_LIT:d>' , '<STR_LIT:e>' , '<STR_LIT:f>' ] ) \n assert parseargs ( [ ] , [ ( '<STR_LIT:a>' , '<STR_LIT:x>' , '<STR_LIT>' ) ] ) == ( { '<STR_LIT:a>' : '<STR_LIT:x>' } , [ ] ) \n assert parseargs ( [ '<STR_LIT>' , '<STR_LIT:x>' , '<STR_LIT>' , '<STR_LIT:y>' ] , [ ( '<STR_LIT:a>' , '<STR_LIT>' , '<STR_LIT>' ) ] ) == ( { '<STR_LIT:a>' : '<STR_LIT:y>' } , [ ] ) \n try : \n parseargs ( [ ] , [ ( '<STR_LIT:a>' , '<STR_LIT:x>' , '<STR_LIT>' ) ] ) \n except ValueError : \n pass \n try : \n parseargs ( [ '<STR_LIT>' , '<STR_LIT:x>' ] , [ ] ) \n except ValueError : \n pass \n try : \n parseargs ( [ '<STR_LIT>' ] , [ ( '<STR_LIT:a>' , '<STR_LIT:x>' , '<STR_LIT>' ) ] ) \n except ValueError : \n pass \n try : \n parseargs ( [ ] , [ ] , <NUM_LIT:1> , <NUM_LIT:2> ) \n except ValueError : \n pass \n assert parseargs ( [ '<STR_LIT:x>' ] , [ ] , <NUM_LIT:1> , <NUM_LIT:2> ) == ( { } , [ '<STR_LIT:x>' ] ) \n assert parseargs ( [ '<STR_LIT:x>' , '<STR_LIT:y>' ] , [ ] , <NUM_LIT:1> , <NUM_LIT:2> ) == ( { } , [ '<STR_LIT:x>' , '<STR_LIT:y>' ] ) \n try : \n parseargs ( [ '<STR_LIT:x>' , '<STR_LIT:y>' , '<STR_LIT:z>' ] , [ ] , <NUM_LIT:1> , <NUM_LIT:2> ) \n except ValueError : \n pass \n try : \n parseargs ( [ '<STR_LIT>' , '<STR_LIT>' ] , [ ( '<STR_LIT:a>' , <NUM_LIT:3> , '<STR_LIT>' ) ] ) \n except ValueError : \n pass \n try : \n parseargs ( [ '<STR_LIT>' , '<STR_LIT:z>' ] , [ ( '<STR_LIT:a>' , <NUM_LIT> , '<STR_LIT>' ) ] ) \n except ValueError : \n <mask0> \n", "gt": "pass"}
{"input": "\n import datetime \n from south . db import db \n from south . v2 import SchemaMigration \n from django . db import models \n class Migration ( SchemaMigration ) : \n def forwards ( self , orm ) : \n db . add_column ( '<STR_LIT>' , '<STR_LIT>' , \n self . gf ( '<STR_LIT>' ) ( to = orm [ '<STR_LIT>' ] , null = True , blank = True ) , \n keep_default = False ) \n def backwards ( self , orm ) : \n db . delete_column ( '<STR_LIT>' , '<STR_LIT>' ) \n models = { \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT:object_name>' : '<STR_LIT>' } , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:name>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:max_length>' : '<STR_LIT>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" , '<STR_LIT>' : '<STR_LIT:False>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT>' : \"<STR_LIT>\" , '<STR_LIT>' : \"<STR_LIT>\" , '<STR_LIT:object_name>' : '<STR_LIT>' } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:name>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT:object_name>' : '<STR_LIT>' } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT>' } ) , \n '<STR_LIT:email>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" , '<STR_LIT>' : '<STR_LIT:False>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:password>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" , '<STR_LIT>' : '<STR_LIT:False>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:username>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:max_length>' : '<STR_LIT>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT>' : \"<STR_LIT>\" , '<STR_LIT>' : \"<STR_LIT>\" , '<STR_LIT:object_name>' : '<STR_LIT>' , '<STR_LIT>' : \"<STR_LIT>\" } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' } ) , \n '<STR_LIT:name>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT:object_name>' : '<STR_LIT>' } , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:name>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:max_length>' : '<STR_LIT:100>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT:object_name>' : '<STR_LIT>' } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:related_name>' : \"<STR_LIT>\" , '<STR_LIT:to>' : \"<STR_LIT>\" } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:related_name>' : \"<STR_LIT>\" , '<STR_LIT:to>' : \"<STR_LIT>\" } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT:object_name>' : '<STR_LIT>' } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:description>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:name>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT:None>' , '<STR_LIT>' : '<STR_LIT>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT:object_name>' : '<STR_LIT>' } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" , '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:description>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT:None>' , '<STR_LIT>' : '<STR_LIT>' } ) , \n '<STR_LIT:title>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" , '<STR_LIT>' : '<STR_LIT:False>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT:object_name>' : '<STR_LIT>' } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:name>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT:2>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT:object_name>' : '<STR_LIT>' } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:description>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:1>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) \n } , \n '<STR_LIT>' : { \n '<STR_LIT:Meta>' : { '<STR_LIT:object_name>' : '<STR_LIT>' } , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" , '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:date>' : ( '<STR_LIT>' , [ ] , { } ) , \n '<STR_LIT:description>' : ( '<STR_LIT>' , [ ] , { } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT:2>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' } ) , \n '<STR_LIT:id>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:primary_key>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:1>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT:100>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT>' : '<STR_LIT:True>' , '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT:None>' , '<STR_LIT>' : '<STR_LIT>' } ) , \n '<STR_LIT:title>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:default>' : '<STR_LIT:False>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT:user>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:to>' : \"<STR_LIT>\" , '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:null>' : '<STR_LIT:True>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) , \n '<STR_LIT>' : ( '<STR_LIT>' , [ ] , { '<STR_LIT:max_length>' : '<STR_LIT>' , '<STR_LIT:blank>' : '<STR_LIT:True>' } ) \n } \n } \n <mask0> = [ '<STR_LIT>' ] \n", "gt": "complete_apps"}
{"input": "\n import os \n from collections import namedtuple \n from shutil import rmtree \n from stat import S_IFDIR , S_IFREG , S_IFLNK \n from pygit2 import ( clone_repository , Signature , GIT_SORT_TOPOLOGICAL , \n GIT_FILEMODE_TREE , GIT_STATUS_CURRENT , \n GIT_FILEMODE_LINK , GIT_FILEMODE_BLOB , GIT_BRANCH_REMOTE , \n GIT_BRANCH_LOCAL , GIT_FILEMODE_BLOB_EXECUTABLE ) \n from six import iteritems \n from gitfs . cache import CommitCache \n from gitfs . log import log \n from gitfs . utils . path import split_path_into_components \n from gitfs . utils . commits import CommitsList \n DivergeCommits = namedtuple ( \"<STR_LIT>\" , [ \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n class Repository ( object ) : \n def __init__ ( self , repository , commits = None ) : \n self . _repo = repository \n self . commits = commits or CommitCache ( self ) \n self . behind = False \n def __getitem__ ( self , item ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _repo [ item ] \n def __getattr__ ( self , attr ) : \n \"\"\"<STR_LIT>\"\"\" \n if attr not in self . __dict__ : \n return getattr ( self . _repo , attr ) \n else : \n return self . __dict__ [ attr ] \n def ahead ( self , upstream , branch ) : \n ahead , _ = self . diverge ( upstream , branch ) \n return ahead \n def diverge ( self , upstream , branch ) : \n reference = \"<STR_LIT>\" . format ( upstream , branch ) \n remote_branch = self . lookup_branch ( reference , GIT_BRANCH_REMOTE ) \n local_branch = self . lookup_branch ( branch , GIT_BRANCH_LOCAL ) \n if remote_branch . target == local_branch . target : \n return False , False \n diverge_commits = self . find_diverge_commits ( local_branch , \n remote_branch ) \n behind = len ( diverge_commits . second_commits ) > <NUM_LIT:0> \n ahead = len ( diverge_commits . first_commits ) > <NUM_LIT:0> \n return ahead , behind \n def checkout ( self , ref , * args , ** kwargs ) : \n result = self . _repo . checkout ( ref , * args , ** kwargs ) \n self . ignore . update ( ) \n status = self . _repo . status ( ) \n for path , status in iteritems ( status ) : \n if status == GIT_STATUS_CURRENT : \n continue \n full_path = self . _full_path ( path ) \n if path not in self . _repo . index : \n if path not in self . ignore : \n try : \n os . unlink ( full_path ) \n except OSError : \n rmtree ( \n full_path , \n onerror = lambda function , fpath , excinfo : log . info ( \n \"<STR_LIT>\" , fpath \n ) \n ) \n continue \n stats = self . get_git_object_default_stats ( ref , path ) \n current_stat = os . lstat ( full_path ) \n if stats [ '<STR_LIT>' ] != current_stat . st_mode : \n os . chmod ( full_path , current_stat . st_mode ) \n self . _repo . index . add ( self . _sanitize ( path ) ) \n return result \n def _sanitize ( self , path ) : \n if path is not None and path . startswith ( \"<STR_LIT:/>\" ) : \n path = path [ <NUM_LIT:1> : ] \n return path \n def push ( self , upstream , branch , credentials ) : \n \"\"\"<STR_LIT>\"\"\" \n remote = self . get_remote ( upstream ) \n remote . push ( [ \"<STR_LIT>\" % ( branch ) ] , callbacks = credentials ) \n def fetch ( self , upstream , branch_name , credentials ) : \n \"\"\"<STR_LIT>\"\"\" \n remote = self . get_remote ( upstream ) \n remote . fetch ( callbacks = credentials ) \n _ , behind = self . diverge ( upstream , branch_name ) \n self . behind = behind \n return behind \n def commit ( self , message , author , commiter , parents = None , ref = \"<STR_LIT>\" ) : \n \"\"\"<STR_LIT>\"\"\" \n status = self . _repo . status ( ) \n if status == { } : \n return None \n author = Signature ( author [ <NUM_LIT:0> ] , author [ <NUM_LIT:1> ] ) \n commiter = Signature ( commiter [ <NUM_LIT:0> ] , commiter [ <NUM_LIT:1> ] ) \n tree = self . _repo . index . write_tree ( ) \n self . _repo . index . write ( ) \n if parents is None : \n parents = [ self . _repo . revparse_single ( ref ) . id ] \n return self . _repo . create_commit ( ref , author , commiter , message , \n tree , parents ) \n @ classmethod \n def clone ( cls , remote_url , path , branch = None , credentials = None ) : \n \"\"\"<STR_LIT>\"\"\" \n repo = clone_repository ( remote_url , path , checkout_branch = branch , \n callbacks = credentials ) \n repo . checkout_head ( ) \n return cls ( repo ) \n def _is_searched_entry ( self , entry_name , searched_entry , path_components ) : \n \"\"\"<STR_LIT>\"\"\" \n return ( entry_name == searched_entry and \n len ( path_components ) == <NUM_LIT:1> and \n entry_name == path_components [ <NUM_LIT:0> ] ) \n def _get_git_object ( self , tree , obj_name , path_components , modifier ) : \n \"\"\"<STR_LIT>\"\"\" \n git_obj = None \n for entry in tree : \n if self . _is_searched_entry ( entry . name , obj_name , path_components ) : \n return modifier ( entry ) \n elif entry . filemode == GIT_FILEMODE_TREE : \n git_obj = self . _get_git_object ( self . _repo [ entry . id ] , obj_name , \n path_components [ <NUM_LIT:1> : ] , modifier ) \n if git_obj : \n return git_obj \n return git_obj \n def get_git_object_type ( self , tree , path ) : \n \"\"\"<STR_LIT>\"\"\" \n path_components = split_path_into_components ( path ) \n try : \n return self . _get_git_object ( tree , path_components [ - <NUM_LIT:1> ] , \n path_components , \n lambda entry : entry . filemode ) \n except : \n return GIT_FILEMODE_TREE \n def get_git_object ( self , tree , path ) : \n \"\"\"<STR_LIT>\"\"\" \n path_components = split_path_into_components ( path ) \n return self . _get_git_object ( tree , path_components [ - <NUM_LIT:1> ] , path_components , \n lambda entry : self . _repo [ entry . id ] ) \n def get_git_object_default_stats ( self , ref , path ) : \n types = { \n GIT_FILEMODE_LINK : { \n '<STR_LIT>' : S_IFLNK | <NUM_LIT> , \n } , GIT_FILEMODE_TREE : { \n '<STR_LIT>' : S_IFDIR | <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT:2> \n } , GIT_FILEMODE_BLOB : { \n '<STR_LIT>' : S_IFREG | <NUM_LIT> , \n } , GIT_FILEMODE_BLOB_EXECUTABLE : { \n '<STR_LIT>' : S_IFREG | <NUM_LIT> , \n } , \n } \n if path == \"<STR_LIT:/>\" : \n return types [ GIT_FILEMODE_TREE ] \n obj_type = self . get_git_object_type ( ref , path ) \n if obj_type is None : \n return obj_type \n stats = types [ obj_type ] \n if obj_type in [ GIT_FILEMODE_BLOB , GIT_FILEMODE_BLOB_EXECUTABLE ] : \n stats [ '<STR_LIT>' ] = self . get_blob_size ( ref , path ) \n return stats \n def get_blob_size ( self , tree , path ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . get_git_object ( tree , path ) . size \n def get_blob_data ( self , tree , path ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . get_git_object ( tree , path ) . data \n def get_commit_dates ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return list ( self . commits . keys ( ) ) \n def get_commits_by_date ( self , date ) : \n \"\"\"<STR_LIT>\"\"\" \n return list ( map ( str , self . commits [ date ] ) ) \n def walk_branches ( self , sort , * branches ) : \n \"\"\"<STR_LIT>\"\"\" \n iterators = [ self . _repo . walk ( branch . target , sort ) \n for branch in branches ] \n stop_iteration = [ False for branch in branches ] \n commits = [ ] \n for iterator in iterators : \n try : \n commit = next ( iterator ) \n except StopIteration : \n commit = None \n commits . append ( commit ) \n yield ( commit for commit in commits ) \n while not all ( stop_iteration ) : \n for index , iterator in enumerate ( iterators ) : \n try : \n commit = next ( iterator ) \n commits [ index ] = commit \n except StopIteration : \n stop_iteration [ index ] = True \n if not all ( stop_iteration ) : \n yield ( commit for commit in commits ) \n def remote_head ( self , upstream , branch ) : \n ref = \"<STR_LIT>\" % ( upstream , branch ) \n remote = self . _repo . lookup_branch ( ref , GIT_BRANCH_REMOTE ) \n return remote . get_object ( ) \n def get_remote ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n remote = [ remote for remote in self . _repo . remotes \n if remote . name == name ] \n if not remote : \n raise ValueError ( \"<STR_LIT>\" ) \n return remote [ <NUM_LIT:0> ] \n def _full_path ( self , partial ) : \n if partial . startswith ( \"<STR_LIT:/>\" ) : \n partial = partial [ <NUM_LIT:1> : ] \n return os . path . join ( self . _repo . workdir , partial ) \n def find_diverge_commits ( self , first_branch , second_branch ) : \n \"\"\"<STR_LIT>\"\"\" \n common_parent = None \n first_commits = CommitsList ( ) \n second_commits = CommitsList ( ) \n walker = self . walk_branches ( GIT_SORT_TOPOLOGICAL , \n first_branch , second_branch ) \n for first_commit , second_commit in walker : \n if ( first_commit in second_commits or \n second_commit in first_commits ) : \n break \n if first_commit not in first_commits : \n first_commits . append ( first_commit ) \n if second_commit not in second_commits : \n second_commits . append ( second_commit ) \n if second_commit . hex == first_commit . hex : \n break \n try : \n index = second_commits . index ( first_commit ) \n except ValueError : \n pass \n else : \n second_commits = second_commits [ : index ] \n common_parent = first_commit \n try : \n index = first_commits . index ( second_commit ) \n except ValueError : \n pass \n else : \n first_commits = first_commits [ : index ] \n common_parent = second_commit \n return DivergeCommits ( common_parent , first_commits , <mask0> ) \n", "gt": "second_commits"}
{"input": "\n from datetime import datetime \n from mock import MagicMock , call \n from pygit2 import GIT_SORT_TIME \n from gitfs . cache . commits import Commit , CommitCache \n class TestCommit ( object ) : \n def test_commit ( self ) : \n commit = Commit ( <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT:1> ) \n new_commit = Commit ( <NUM_LIT:2> , <NUM_LIT:2> , \"<STR_LIT>\" ) \n assert new_commit > commit \n assert repr ( new_commit ) == \"<STR_LIT>\" \n class TestCommitCache ( object ) : \n def test_cache ( self ) : \n mocked_repo = MagicMock ( ) \n mocked_commit = MagicMock ( ) \n mocked_repo . lookup_reference ( ) . resolve ( ) . target = \"<STR_LIT>\" \n mocked_repo . walk . return_value = [ mocked_commit ] \n mocked_commit . commit_time = <NUM_LIT> \n mocked_commit . hex = '<STR_LIT>' \n cache = CommitCache ( mocked_repo ) \n cache . update ( ) \n cache [ '<STR_LIT>' ] = Commit ( <NUM_LIT:1> , <NUM_LIT:1> , \"<STR_LIT>\" ) \n assert sorted ( cache . keys ( ) ) == [ '<STR_LIT>' , '<STR_LIT>' ] \n asserted_time = datetime . fromtimestamp ( mocked_commit . commit_time ) \n asserted_time = \"<STR_LIT>\" . format ( asserted_time . hour , asserted_time . minute , \n asserted_time . second ) \n assert repr ( cache [ '<STR_LIT>' ] ) == '<STR_LIT>' % asserted_time \n del cache [ '<STR_LIT>' ] \n for commit_date in cache : \n assert commit_date == '<STR_LIT>' \n mocked_repo . lookup_reference . has_calls ( [ call ( \"<STR_LIT>\" ) ] ) \n mocked_repo . walk . assert_called_once_with ( \"<STR_LIT>\" , GIT_SORT_TIME ) \n assert mocked_repo . lookup_reference ( ) . resolve . <mask0> == <NUM_LIT:2> \n", "gt": "call_count"}
{"input": "\n import pytest \n import datetime as dt \n from mock import MagicMock \n from gitfs . utils . strptime import TimeParser \n from gitfs . utils import strptime \n class TestDateTimeUtils ( object ) : \n def test_strptime ( self ) : \n date = dt . date ( <NUM_LIT> , <NUM_LIT:8> , <NUM_LIT> ) \n datetime = dt . datetime ( <NUM_LIT> , <NUM_LIT:8> , <NUM_LIT> , <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) \n assert strptime ( \"<STR_LIT>\" , \"<STR_LIT>\" ) == date \n assert strptime ( \"<STR_LIT>\" , \"<STR_LIT>\" , \n to_datetime = True ) == datetime \n date = dt . date ( <NUM_LIT> , <NUM_LIT:8> , <NUM_LIT:30> ) \n datetime = dt . datetime ( <NUM_LIT> , <NUM_LIT:8> , <NUM_LIT:30> , <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:3> ) \n assert strptime ( \"<STR_LIT>\" , \"<STR_LIT>\" ) == date \n assert strptime ( \"<STR_LIT>\" , \"<STR_LIT>\" , \n to_datetime = True ) == datetime \n date = dt . date ( <NUM_LIT> , <NUM_LIT:1> , <NUM_LIT:1> ) \n datetime = dt . datetime ( <NUM_LIT> , <NUM_LIT:1> , <NUM_LIT:1> , <NUM_LIT> , <NUM_LIT:30> ) \n assert strptime ( \"<STR_LIT>\" , \"<STR_LIT>\" ) == date \n assert strptime ( \"<STR_LIT>\" , \"<STR_LIT>\" , \n to_datetime = True ) == datetime \n with pytest . raises ( ValueError ) : \n strptime ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def test_time_parser_match_with_value_error ( self ) : \n mocked_pattern = MagicMock ( ) \n mocked_pattern . match . return_value = False \n parser = TimeParser ( \"<STR_LIT>\" ) \n parser . pattern = mocked_pattern \n with pytest . raises ( ValueError ) : \n parser . match ( \"<STR_LIT>\" ) \n mocked_pattern . match . <mask0> ( \"<STR_LIT>\" ) \n", "gt": "assert_called_once_with"}
{"input": "\n from __future__ import unicode_literals \n from django . db import migrations \n import jsonfield . fields \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ] \n operations = [ \n migrations . AddField ( \n model_name = '<STR_LIT>' , \n name = '<STR_LIT>' , \n field = jsonfield . fields . JSONField ( null = True , blank = True ) , \n ) , \n migrations . AddField ( \n model_name = '<STR_LIT>' , \n name = '<STR_LIT>' , \n field = jsonfield . fields . JSONField ( null = True , blank = True ) , \n ) , \n migrations . AddField ( \n model_name = '<STR_LIT>' , \n name = '<STR_LIT>' , \n field = jsonfield . fields . JSONField ( null = True , blank = <mask0> ) , \n ) , \n ] \n", "gt": "True"}
{"input": "\n from zipa import api_github_com as github \n repos = github . orgs . django . repos \n for repo in repos [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] : \n print repo . <mask0> \n", "gt": "name"}
{"input": "\n import supybot . conf as conf \n import supybot . registry as registry \n from supybot . i18n import PluginInternationalization , internationalizeDocstring \n _ = PluginInternationalization ( '<STR_LIT>' ) \n def configure ( advanced ) : \n from supybot . questions import expect , anything , something , yn \n conf . registerPlugin ( '<STR_LIT>' , True ) \n Alias = conf . registerPlugin ( '<STR_LIT>' ) \n conf . registerGroup ( Alias , '<STR_LIT>' ) \n conf . registerGroup ( Alias , '<STR_LIT>' ) \n conf . registerGlobalValue ( Alias , '<STR_LIT>' , \n registry . String ( <mask0> , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n", "gt": "r'<STR_LIT>'"}
{"input": "\n import supybot . conf as conf \n import supybot . registry as registry \n try : \n from supybot . i18n import PluginInternationalization \n from supybot . i18n import internationalizeDocstring \n _ = PluginInternationalization ( '<STR_LIT>' ) \n except : \n _ = lambda x : x \n internationalizeDocstring = lambda x : x \n def configure ( advanced ) : \n from supybot . questions import expect , anything , something , yn \n conf . registerPlugin ( '<STR_LIT>' , True ) \n Conditional = conf . <mask0> ( '<STR_LIT>' ) \n", "gt": "registerPlugin"}
{"input": "\n import supybot . conf as conf \n import supybot . registry as registry \n from supybot . i18n import PluginInternationalization , internationalizeDocstring \n _ = PluginInternationalization ( '<STR_LIT>' ) \n Filter = conf . registerPlugin ( '<STR_LIT>' ) \n conf . registerGroup ( Filter , '<STR_LIT>' ) \n conf . registerGlobalValue ( Filter . spellit , \n '<STR_LIT>' , registry . Boolean ( True , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerGlobalValue ( Filter . spellit , \n '<STR_LIT>' , registry . Boolean ( True , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerGlobalValue ( Filter . spellit , \n '<STR_LIT>' , registry . Boolean ( True , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerGroup ( Filter , '<STR_LIT>' ) \n conf . registerChannelValue ( Filter . shrink , '<STR_LIT>' , \n registry . PositiveInteger ( <NUM_LIT:4> , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n def configure ( advanced ) : \n from supybot . questions import expect , anything , something , yn \n conf . registerPlugin ( '<STR_LIT>' , <mask0> ) \n", "gt": "True"}
{"input": "\n import supybot . conf as conf \n import supybot . registry as registry \n from supybot . i18n import PluginInternationalization , internationalizeDocstring \n _ = PluginInternationalization ( '<STR_LIT>' ) \n def configure ( advanced ) : \n from supybot . questions import expect , anything , something , yn \n conf . registerPlugin ( '<STR_LIT>' , True ) \n Karma = conf . registerPlugin ( '<STR_LIT>' ) \n conf . registerChannelValue ( Karma , '<STR_LIT>' , \n registry . Boolean ( False , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerChannelValue ( Karma , '<STR_LIT>' , \n registry . SpaceSeparatedListOfStrings ( [ '<STR_LIT>' ] , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerChannelValue ( Karma , '<STR_LIT>' , \n registry . SpaceSeparatedListOfStrings ( [ '<STR_LIT>' ] , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerChannelValue ( Karma , '<STR_LIT>' , \n registry . Boolean ( False , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerChannelValue ( Karma , '<STR_LIT>' , \n registry . Integer ( <NUM_LIT:3> , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerChannelValue ( Karma , '<STR_LIT>' , \n registry . Integer ( <NUM_LIT> , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerChannelValue ( Karma , '<STR_LIT>' , \n registry . Boolean ( False , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerChannelValue ( Karma , '<STR_LIT>' , \n registry . Boolean ( True , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n conf . registerChannelValue ( Karma , '<STR_LIT>' , \n registry . Boolean ( <mask0> , _ ( \"\"\"<STR_LIT>\"\"\" ) ) ) \n", "gt": "False"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import supybot \n import supybot . world as world \n __version__ = \"<STR_LIT>\" \n __author__ = supybot . authors . strike \n __contributors__ = { } \n from . import config \n from . import plugin \n from imp import reload \n reload ( plugin ) \n if world . testing : \n from . import test \n Class = plugin . Class \n configure = config . <mask0> \n", "gt": "configure"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import supybot \n import supybot . world as world \n __version__ = \"<STR_LIT>\" \n __author__ = supybot . authors . jemfinch \n __contributors__ = { } \n from . import config \n from . import plugin \n from imp import reload \n reload ( plugin ) \n if world . testing : \n from . import test \n Class = plugin . Class \n configure = config . <mask0> \n", "gt": "configure"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import supybot \n import supybot . world as world \n __version__ = \"<STR_LIT>\" \n __author__ = supybot . authors . jemfinch \n __contributors__ = { } \n from . import config \n from . import plugin \n from imp import reload \n reload ( plugin ) \n if world . testing : \n from . import test \n Class = plugin . Class \n configure = config . <mask0> \n", "gt": "configure"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import supybot \n import supybot . world as world \n __version__ = \"<STR_LIT>\" \n __author__ = supybot . authors . jemfinch \n __contributors__ = { } \n from . import config \n from . import plugin \n from imp import reload \n reload ( plugin ) \n if world . testing : \n from . import test \n Class = plugin . Class \n configure = config . <mask0> \n", "gt": "configure"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import supybot \n import supybot . world as world \n __version__ = \"<STR_LIT>\" \n __author__ = supybot . authors . jemfinch \n __contributors__ = { } \n __url__ = '<STR_LIT>' \n from . import config \n from . import plugin \n from imp import reload \n reload ( plugin ) \n if world . testing : \n from . import test \n Class = plugin . Class \n configure = config . <mask0> \n", "gt": "configure"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import division \n import os \n import time \n import errno \n import select \n import socket \n from . . import ( conf , drivers , log , utils , world ) \n from . . utils import minisix \n from . . utils . str import decode_raw_line \n try : \n import ssl \n SSLError = ssl . SSLError \n except : \n drivers . log . debug ( '<STR_LIT>' \n '<STR_LIT>' ) \n class SSLError ( Exception ) : \n pass \n class SocketDriver ( drivers . IrcDriver , drivers . ServersMixin ) : \n _instances = [ ] \n _selecting = [ False ] \n def __init__ ( self , irc ) : \n self . _instances . append ( self ) \n assert irc is not None \n self . irc = irc \n drivers . IrcDriver . __init__ ( self , irc ) \n drivers . ServersMixin . __init__ ( self , irc ) \n self . conn = None \n self . _attempt = - <NUM_LIT:1> \n self . servers = ( ) \n self . eagains = <NUM_LIT:0> \n self . inbuffer = b'<STR_LIT>' \n self . outbuffer = '<STR_LIT>' \n self . zombie = False \n self . connected = False \n self . writeCheckTime = None \n self . nextReconnectTime = None \n self . resetDelay ( ) \n if self . networkGroup . get ( '<STR_LIT>' ) . value and '<STR_LIT>' not in globals ( ) : \n drivers . log . error ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n self . ssl = False \n else : \n self . ssl = self . networkGroup . get ( '<STR_LIT>' ) . value \n self . connect ( ) \n def getDelay ( self ) : \n ret = self . currentDelay \n self . currentDelay = min ( self . currentDelay * <NUM_LIT:2> , \n conf . supybot . drivers . maxReconnectWait ( ) ) \n return ret \n def resetDelay ( self ) : \n self . currentDelay = <NUM_LIT> \n def _getNextServer ( self ) : \n oldServer = getattr ( self , '<STR_LIT>' , None ) \n server = drivers . ServersMixin . _getNextServer ( self ) \n if self . currentServer != oldServer : \n self . resetDelay ( ) \n return server \n def _handleSocketError ( self , e ) : \n if e . args [ <NUM_LIT:0> ] != <NUM_LIT:11> or self . eagains > <NUM_LIT> : \n drivers . log . disconnect ( self . currentServer , e ) \n if self in self . _instances : \n self . _instances . remove ( self ) \n try : \n self . conn . close ( ) \n except : \n pass \n self . connected = False \n self . scheduleReconnect ( ) \n else : \n log . debug ( '<STR_LIT>' , self . eagains ) \n self . eagains += <NUM_LIT:1> \n def _sendIfMsgs ( self ) : \n if not self . connected : \n return \n if not self . zombie : \n msgs = [ self . irc . takeMsg ( ) ] \n while msgs [ - <NUM_LIT:1> ] is not None : \n msgs . append ( self . irc . takeMsg ( ) ) \n del msgs [ - <NUM_LIT:1> ] \n self . outbuffer += '<STR_LIT>' . join ( map ( str , msgs ) ) \n if self . outbuffer : \n try : \n if minisix . PY2 : \n sent = self . conn . send ( self . outbuffer ) \n else : \n sent = self . conn . send ( self . outbuffer . encode ( ) ) \n self . outbuffer = self . outbuffer [ sent : ] \n self . eagains = <NUM_LIT:0> \n except socket . error as e : \n self . _handleSocketError ( e ) \n if self . zombie and not self . outbuffer : \n self . _reallyDie ( ) \n @ classmethod \n def _select ( cls ) : \n if cls . _selecting [ <NUM_LIT:0> ] : \n return \n try : \n cls . _selecting [ <NUM_LIT:0> ] = True \n for inst in cls . _instances : \n if not inst . connected or ( minisix . PY3 and inst . conn . _closed ) or ( minisix . PY2 and \n inst . conn . _sock . __class__ is socket . _closedsocket ) : \n cls . _instances . remove ( inst ) \n elif inst . conn . fileno ( ) == - <NUM_LIT:1> : \n inst . reconnect ( ) \n if not cls . _instances : \n return \n rlist , wlist , xlist = select . select ( [ x . conn for x in cls . _instances ] , \n [ ] , [ ] , conf . supybot . drivers . poll ( ) ) \n for instance in cls . _instances : \n if instance . conn in rlist : \n instance . _read ( ) \n except select . error as e : \n if e . args [ <NUM_LIT:0> ] != errno . EINTR : \n raise \n finally : \n cls . _selecting [ <NUM_LIT:0> ] = False \n for instance in cls . _instances : \n if instance . irc and not instance . irc . zombie : \n instance . _sendIfMsgs ( ) \n def run ( self ) : \n now = time . time ( ) \n if self . nextReconnectTime is not None and now > self . nextReconnectTime : \n self . reconnect ( ) \n elif self . writeCheckTime is not None and now > self . writeCheckTime : \n self . _checkAndWriteOrReconnect ( ) \n if not self . connected : \n time . sleep ( conf . supybot . drivers . poll ( ) ) \n return \n self . _sendIfMsgs ( ) \n self . _select ( ) \n def _read ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n self . inbuffer += self . conn . recv ( <NUM_LIT> ) \n self . eagains = <NUM_LIT:0> \n lines = self . inbuffer . split ( b'<STR_LIT:\\n>' ) \n self . inbuffer = lines . pop ( ) \n for line in lines : \n line = decode_raw_line ( line ) \n msg = drivers . parseMsg ( line ) \n if msg is not None and self . irc is not None : \n self . irc . feedMsg ( msg ) \n except socket . timeout : \n pass \n except SSLError as e : \n if e . args [ <NUM_LIT:0> ] == '<STR_LIT>' : \n pass \n else : \n self . _handleSocketError ( e ) \n return \n except socket . error as e : \n self . _handleSocketError ( e ) \n return \n if self . irc and not self . irc . zombie : \n self . _sendIfMsgs ( ) \n def connect ( self , ** kwargs ) : \n self . reconnect ( reset = False , ** kwargs ) \n def reconnect ( self , wait = False , reset = True ) : \n self . _attempt += <NUM_LIT:1> \n self . nextReconnectTime = None \n if self . connected : \n drivers . log . reconnect ( self . irc . network ) \n if self in self . _instances : \n self . _instances . remove ( self ) \n try : \n self . conn . shutdown ( socket . SHUT_RDWR ) \n except : \n pass \n self . conn . close ( ) \n self . connected = False \n if reset : \n drivers . log . debug ( '<STR_LIT>' , self . irc ) \n self . irc . reset ( ) \n else : \n drivers . log . debug ( '<STR_LIT>' , self . irc ) \n if wait : \n self . scheduleReconnect ( ) \n return \n self . server = self . _getNextServer ( ) \n network_config = getattr ( conf . supybot . networks , self . irc . network ) \n socks_proxy = network_config . socksproxy ( ) \n try : \n if socks_proxy : \n import socks \n except ImportError : \n log . error ( '<STR_LIT>' \n '<STR_LIT>' ) \n socks_proxy = '<STR_LIT>' \n if socks_proxy : \n address = self . server [ <NUM_LIT:0> ] \n else : \n try : \n address = utils . net . getAddressFromHostname ( self . server [ <NUM_LIT:0> ] , \n attempt = self . _attempt ) \n except ( socket . gaierror , socket . error ) as e : \n drivers . log . connectError ( self . currentServer , e ) \n self . scheduleReconnect ( ) \n return \n port = self . server [ <NUM_LIT:1> ] \n drivers . log . connect ( self . currentServer ) \n try : \n self . conn = utils . net . getSocket ( address , port = port , \n socks_proxy = socks_proxy , \n vhost = conf . supybot . protocols . irc . vhost ( ) , \n vhostv6 = conf . supybot . protocols . irc . vhostv6 ( ) , \n ) \n except socket . error as e : \n drivers . log . connectError ( self . currentServer , e ) \n self . scheduleReconnect ( ) \n return \n self . conn . settimeout ( max ( <NUM_LIT:10> , conf . supybot . drivers . poll ( ) * <NUM_LIT:10> ) ) \n try : \n self . conn . connect ( ( address , port ) ) \n if network_config . ssl ( ) : \n self . starttls ( ) \n elif not network_config . requireStarttls ( ) : \n drivers . log . warning ( ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n % self . irc . network ) \n def setTimeout ( ) : \n self . conn . settimeout ( conf . supybot . drivers . poll ( ) ) \n conf . supybot . drivers . poll . addCallback ( setTimeout ) \n setTimeout ( ) \n self . connected = True \n self . resetDelay ( ) \n except socket . error as e : \n if e . args [ <NUM_LIT:0> ] == <NUM_LIT> : \n now = time . time ( ) \n when = now + <NUM_LIT> \n whenS = log . timestamp ( when ) \n drivers . log . debug ( '<STR_LIT>' \n '<STR_LIT>' , whenS ) \n self . writeCheckTime = when \n else : \n drivers . log . connectError ( self . currentServer , e ) \n self . scheduleReconnect ( ) \n return \n self . _instances . append ( self ) \n def _checkAndWriteOrReconnect ( self ) : \n self . writeCheckTime = None \n drivers . log . debug ( '<STR_LIT>' ) \n ( _ , w , _ ) = select . select ( [ ] , [ self . conn ] , [ ] , <NUM_LIT:0> ) \n if w : \n drivers . log . debug ( '<STR_LIT>' ) \n self . connected = True \n self . resetDelay ( ) \n else : \n drivers . log . connectError ( self . currentServer , '<STR_LIT>' ) \n self . reconnect ( ) \n def scheduleReconnect ( self ) : \n when = time . time ( ) + self . getDelay ( ) \n if not world . dying : \n drivers . log . reconnect ( self . irc . network , when ) \n if self . nextReconnectTime : \n drivers . log . error ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n self . nextReconnectTime = when \n def die ( self ) : \n if self in self . _instances : \n self . _instances . remove ( self ) \n self . zombie = True \n if self . nextReconnectTime is not None : \n self . nextReconnectTime = None \n if self . writeCheckTime is not None : \n self . writeCheckTime = None \n drivers . log . die ( self . irc ) \n def _reallyDie ( self ) : \n if self . conn is not None : \n self . conn . close ( ) \n drivers . IrcDriver . die ( self ) \n def name ( self ) : \n return '<STR_LIT>' % ( self . __class__ . __name__ , self . irc ) \n def starttls ( self ) : \n assert '<STR_LIT>' in globals ( ) \n network_config = getattr ( conf . supybot . networks , self . irc . network ) \n certfile = network_config . certfile ( ) \n if not certfile : \n certfile = conf . supybot . protocols . irc . certfile ( ) \n if not certfile : \n certfile = None \n elif not os . path . isfile ( certfile ) : \n drivers . log . warning ( '<STR_LIT>' % \n certfile ) \n certfile = None \n verifyCertificates = conf . supybot . protocols . ssl . verifyCertificates ( ) \n if not verifyCertificates : \n drivers . log . warning ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n try : \n self . conn = utils . net . ssl_wrap_socket ( self . conn , \n logger = drivers . log , hostname = self . server [ <NUM_LIT:0> ] , \n certfile = certfile , \n verify = verifyCertificates , \n trusted_fingerprints = network_config . ssl . serverFingerprints ( ) , \n ca_file = network_config . ssl . authorityCertificate ( ) , \n ) \n except getattr ( ssl , '<STR_LIT>' , None ) as e : \n drivers . log . error ( ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n % ( self . irc . network , e . args [ <NUM_LIT:0> ] ) ) \n raise ssl . SSLError ( '<STR_LIT>' \n '<STR_LIT>' ) \n except ssl . SSLError as e : \n drivers . log . error ( ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n % ( self . irc . network , e . args [ <NUM_LIT:1> ] ) ) \n raise ssl . SSLError ( '<STR_LIT>' \n '<STR_LIT>' ) \n Driver = <mask0> \n", "gt": "SocketDriver"}
{"input": "\n import sys \n import types \n import fnmatch \n import threading \n def universalImport ( * names ) : \n \"\"\"<STR_LIT>\"\"\" \n f = sys . _getframe ( <NUM_LIT:1> ) \n for name in names : \n try : \n ret = __import__ ( name , f . f_globals ) \n except ImportError : \n continue \n else : \n if '<STR_LIT:.>' in name : \n parts = name . split ( '<STR_LIT:.>' ) [ <NUM_LIT:1> : ] \n while parts : \n ret = getattr ( ret , parts [ <NUM_LIT:0> ] ) \n del parts [ <NUM_LIT:0> ] \n return ret \n raise ImportError ( '<STR_LIT:U+002C>' . join ( names ) ) \n def changeFunctionName ( f , name , doc = None ) : \n if doc is None : \n doc = f . __doc__ \n if hasattr ( f , '<STR_LIT>' ) : \n closure = f . __closure__ \n else : \n closure = f . func_closure \n newf = types . FunctionType ( f . __code__ , f . __globals__ , name , \n f . __defaults__ , closure ) \n newf . __doc__ = doc \n return newf \n class Object ( object ) : \n def __ne__ ( self , other ) : \n return not self == other \n class MetaSynchronized ( type ) : \n METHODS = '<STR_LIT>' \n LOCK = '<STR_LIT>' \n def __new__ ( cls , name , bases , dict ) : \n sync = set ( ) \n for base in bases : \n if hasattr ( base , MetaSynchronized . METHODS ) : \n sync . update ( getattr ( base , MetaSynchronized . METHODS ) ) \n if MetaSynchronized . METHODS in dict : \n sync . update ( dict [ MetaSynchronized . METHODS ] ) \n if sync : \n def synchronized ( f ) : \n def g ( self , * args , ** kwargs ) : \n lock = getattr ( self , MetaSynchronized . LOCK ) \n lock . acquire ( ) \n try : \n f ( self , * args , ** kwargs ) \n finally : \n lock . release ( ) \n return changeFunctionName ( g , f . __name__ , f . __doc__ ) \n for attr in sync : \n if attr in dict : \n dict [ attr ] = synchronized ( dict [ attr ] ) \n original__init__ = dict . get ( '<STR_LIT>' ) \n def __init__ ( self , * args , ** kwargs ) : \n if not hasattr ( self , MetaSynchronized . LOCK ) : \n setattr ( self , MetaSynchronized . LOCK , threading . RLock ( ) ) \n if original__init__ : \n original__init__ ( self , * args , ** kwargs ) \n else : \n super ( newclass , self ) . __init__ ( * args , ** kwargs ) \n dict [ '<STR_LIT>' ] = __init__ \n newclass = super ( MetaSynchronized , cls ) . __new__ ( cls , name , bases , dict ) \n return newclass \n Synchronized = MetaSynchronized ( '<STR_LIT>' , ( ) , { } ) \n def glob2re ( g ) : \n return fnmatch . translate ( g ) [ : - <NUM_LIT:7> ] \n _debug_software_name = '<STR_LIT>' \n _debug_software_version = None \n def collect_extra_debug_data ( ) : \n \"\"\"<STR_LIT>\"\"\" \n data = '<STR_LIT>' \n try : \n tb = sys . exc_info ( ) [ <NUM_LIT:2> ] \n stack = [ ] \n while tb : \n stack . append ( tb . tb_frame ) \n tb = tb . tb_next \n finally : \n del tb \n if _debug_software_version : \n data += '<STR_LIT>' % ( _debug_software_name , _debug_software_version ) \n else : \n data += '<STR_LIT>' % _debug_software_name \n data += '<STR_LIT>' \n for frame in stack : \n data += '<STR_LIT>' \n data += ( '<STR_LIT>' % ( frame . f_code . co_name , \n frame . f_code . co_filename , \n frame . f_lineno ) ) \n frame_locals = frame . f_locals \n for inspected in ( '<STR_LIT>' , '<STR_LIT>' ) : \n if inspected in frame_locals : \n if hasattr ( frame_locals [ inspected ] , '<STR_LIT>' ) and frame_locals [ inspected ] . __dict__ : \n for ( key , value ) in frame_locals [ inspected ] . __dict__ . items ( ) : \n frame_locals [ '<STR_LIT>' % ( inspected , key ) ] = value \n for key , value in frame_locals . items ( ) : \n if key == '<STR_LIT>' : \n continue \n data += ( '<STR_LIT>' % key ) \n try : \n data += repr ( value ) + '<STR_LIT:\\n>' \n except : \n data += '<STR_LIT>' \n data += '<STR_LIT:\\n>' \n data += '<STR_LIT>' \n data += '<STR_LIT>' \n data += '<STR_LIT>' \n data += '<STR_LIT:\\n>' \n return <mask0> \n", "gt": "data"}
{"input": "\n import sys , os \n extensions = [ ] \n templates_path = [ '<STR_LIT>' ] \n source_suffix = '<STR_LIT>' \n master_doc = '<STR_LIT:index>' \n project = u'<STR_LIT>' \n copyright = u'<STR_LIT>' \n version = '<STR_LIT>' \n release = '<STR_LIT>' \n exclude_patterns = [ '<STR_LIT>' ] \n pygments_style = '<STR_LIT>' \n html_theme = '<STR_LIT>' \n html_static_path = [ '<STR_LIT>' ] \n htmlhelp_basename = '<STR_LIT>' \n latex_elements = { \n } \n latex_documents = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n u'<STR_LIT>' , '<STR_LIT>' ) , \n ] \n man_pages = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n [ u'<STR_LIT>' ] , <NUM_LIT:1> ) \n ] \n texinfo_documents = [ \n ( '<STR_LIT:index>' , '<STR_LIT>' , u'<STR_LIT>' , \n <mask0> , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' ) , \n ] \n", "gt": "u'<STR_LIT>'"}
{"input": "\n try : \n from django . conf . urls import patterns , url \n except ImportError : \n from django . conf . urls . defaults import patterns , url \n urlpatterns = patterns ( '<STR_LIT>' , \n url ( r'<STR_LIT>' , '<STR_LIT>' , name = '<STR_LIT>' ) , \n url ( r'<STR_LIT>' , '<STR_LIT>' , <mask0> = '<STR_LIT>' ) , \n ) \n", "gt": "name"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from django . http import * \n from django . core import serializers \n from django . core . exceptions import ValidationError , SuspiciousOperation , ObjectDoesNotExist \n from django . db import IntegrityError , connection , transaction \n from django . shortcuts import render_to_response \n from django . core . urlresolvers import reverse \n from django . core . context_processors import csrf \n from django . contrib . comments . models import Comment \n from django . contrib . comments . forms import CommentForm \n from django . contrib . contenttypes . models import ContentType \n from django . contrib . auth . decorators import login_required , user_passes_test \n from django . contrib . sessions . models import Session \n from django . contrib . sessions . backends . db import SessionStore \n from django . contrib . gis . geos . collections import MultiPolygon \n from django . contrib . gis . geos import GEOSGeometry \n from django . contrib . gis . gdal import * \n from django . contrib . gis . gdal . libgdal import lgdal \n from django . contrib . sites . models import Site \n from django . contrib import humanize \n from django . template import loader , Context as DjangoContext , RequestContext \n from django . utils import simplejson as json , translation \n from django . utils . translation import ugettext as _ , ungettext as _n \n from django . template . defaultfilters import slugify , force_escape \n from django . conf import settings \n from tagging . utils import parse_tag_input \n from tagging . models import Tag , TaggedItem \n from datetime import datetime , time , timedelta \n from decimal import * \n from functools import wraps \n from redistricting . calculators import * \n from redistricting . models import * \n from redistricting . tasks import * \n import random , string , math , types , copy , time , threading , traceback , os \n import commands , sys , tempfile , csv , hashlib , inflect , logging \n import ModestMaps \n from PIL import Image , ImageChops , ImageMath \n import urllib , urllib2 \n from xhtml2pdf . pisa import CreatePDF \n import StringIO \n logger = logging . getLogger ( __name__ ) \n UNASSIGNED_DISTRICT_ID = <NUM_LIT:0> \n def using_unique_session ( u ) : \n \"\"\"<STR_LIT>\"\"\" \n if u . is_anonymous ( ) or u . is_superuser : \n return True \n sessions = Session . objects . all ( ) \n count = <NUM_LIT:0> \n for session in sessions : \n try : \n decoded = session . get_decoded ( ) \n if '<STR_LIT>' in decoded and decoded [ '<STR_LIT>' ] == u . id : \n if '<STR_LIT>' in decoded and decoded [ '<STR_LIT>' ] < datetime . now ( ) : \n Session . objects . filter ( session_key = session . session_key ) . delete ( ) \n else : \n count += <NUM_LIT:1> \n except SuspiciousOperation : \n logger . debug ( \"<STR_LIT>\" , session . session_key ) \n for session in sessions : \n try : \n decoded = session . get_decoded ( ) \n if '<STR_LIT>' in decoded and decoded [ '<STR_LIT>' ] == u . id : \n websession = SessionStore ( session_key = session . session_key ) \n websession [ '<STR_LIT:count>' ] = count \n websession . save ( ) \n except SuspiciousOperation : \n logger . debug ( \"<STR_LIT>\" , session . session_key ) \n return ( count <= <NUM_LIT:1> ) \n def unique_session_or_json_redirect ( function ) : \n \"\"\"<STR_LIT>\"\"\" \n def decorator ( request , * args , ** kwargs ) : \n def return_nonunique_session_result ( ) : \n status = { '<STR_LIT:success>' : False } \n status [ '<STR_LIT:message>' ] = _ ( \n \"<STR_LIT>\" ) \n status [ '<STR_LIT>' ] = '<STR_LIT>' \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if not using_unique_session ( request . user ) : \n return return_nonunique_session_result ( ) \n else : \n return function ( request , * args , ** kwargs ) \n return wraps ( function ) ( decorator ) \n def is_session_available ( req ) : \n \"\"\"<STR_LIT>\"\"\" \n if req . user . is_superuser or req . user . is_staff : \n return True \n sessions = Session . objects . filter ( expire_date__gt = datetime . now ( ) ) \n count = <NUM_LIT:0> \n for session in sessions : \n try : \n decoded = session . get_decoded ( ) \n if ( not req . user . is_anonymous ( ) ) and '<STR_LIT>' in decoded and decoded [ '<STR_LIT>' ] > datetime . now ( ) : \n count += <NUM_LIT:1> \n except SuspiciousOperation : \n logger . debug ( \"<STR_LIT>\" , session . session_key ) \n avail = count < settings . CONCURRENT_SESSIONS \n req . session [ '<STR_LIT>' ] = avail \n return avail \n def note_session_activity ( req ) : \n \"\"\"<STR_LIT>\"\"\" \n window = timedelta ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , settings . SESSION_TIMEOUT ) \n req . session [ '<STR_LIT>' ] = datetime . now ( ) + window \n @ login_required \n def unloadplan ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n ps = Plan . objects . filter ( pk = planid ) \n if len ( ps ) > <NUM_LIT:0> : \n p = ps [ <NUM_LIT:0> ] \n if not can_copy ( request . user , p ) : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) % { '<STR_LIT:user>' : request . user . username } \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if settings . MAX_UNDOS_AFTER_EDIT > <NUM_LIT:0> : \n p . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n status [ '<STR_LIT:success>' ] = True \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ login_required \n @ unique_session_or_json_redirect \n def copyplan ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n if not is_plan_ready ( planid ) : \n return HttpResponseRedirect ( '<STR_LIT:/>' ) \n status = { '<STR_LIT:success>' : False } \n p = Plan . objects . get ( pk = planid ) \n if not can_copy ( request . user , p ) : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" \"<STR_LIT>\" % { '<STR_LIT:username>' : request . user . username } ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n newname = p . name + \"<STR_LIT:U+0020>\" + str ( random . random ( ) ) \n if ( request . method == \"<STR_LIT:POST>\" ) : \n newname = request . POST [ \"<STR_LIT:name>\" ] [ <NUM_LIT:0> : <NUM_LIT:200> ] \n shared = request . POST . get ( \"<STR_LIT>\" , False ) \n plan_copy = Plan . objects . filter ( name = newname , owner = request . user , legislative_body = p . legislative_body ) \n if len ( plan_copy ) > <NUM_LIT:0> : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n plan_copy = Plan ( name = newname , owner = request . user , is_shared = shared , legislative_body = p . legislative_body , processing_state = ProcessingState . READY ) \n plan_copy . create_unassigned = False \n plan_copy . save ( ) \n districts = p . get_districts_at_version ( p . version , include_geom = True ) \n for district in districts : \n district_copy = copy . copy ( district ) \n district_copy . id = None \n district_copy . version = <NUM_LIT:0> \n district_copy . is_locked = False \n district_copy . plan = plan_copy \n try : \n district_copy . save ( ) \n except Exception as inst : \n status [ \"<STR_LIT:message>\" ] = _ ( \"<STR_LIT>\" ) \n status [ \"<STR_LIT>\" ] = inst . message \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n district_copy . clone_relations_from ( district ) \n data = serializers . serialize ( \"<STR_LIT>\" , [ plan_copy ] ) \n return HttpResponse ( data , mimetype = '<STR_LIT:application/json>' ) \n @ login_required \n @ unique_session_or_json_redirect \n def scoreplan ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n plan = Plan . objects . get ( pk = planid ) \n criterion = ValidationCriteria . objects . filter ( legislative_body = plan . legislative_body ) \n status [ '<STR_LIT:success>' ] = True \n for criteria in criterion : \n try : \n score = ComputedPlanScore . compute ( criteria . function , plan ) \n except : \n logger . debug ( traceback . format_exc ( ) ) \n if not score or not score [ '<STR_LIT:value>' ] : \n status [ '<STR_LIT:success>' ] = False \n status [ '<STR_LIT:message>' ] = '<STR_LIT>' % ( criteria . get_short_label ( ) , criteria . get_long_description ( ) or criteria . function . get_long_description ( ) ) \n break \n if status [ '<STR_LIT:success>' ] : \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n plan . is_valid = True \n plan . save ( ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n def get_user_info ( user ) : \n \"\"\"<STR_LIT>\"\"\" \n if user . is_anonymous ( ) : \n return None \n profile = user . get_profile ( ) \n return { \n '<STR_LIT:username>' : user . username , \n '<STR_LIT:email>' : user . email , \n '<STR_LIT>' : profile . pass_hint , \n '<STR_LIT>' : user . first_name , \n '<STR_LIT>' : user . last_name , \n '<STR_LIT>' : profile . organization , \n '<STR_LIT:id>' : user . id \n } \n def commonplan ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n plan = Plan . objects . filter ( id = planid ) \n if plan . count ( ) == <NUM_LIT:1> : \n plan = plan [ <NUM_LIT:0> ] \n plan . edited = getutc ( plan . edited ) \n levels = plan . legislative_body . get_geolevels ( ) \n districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n editable = can_edit ( request . user , plan ) \n default_demo = plan . legislative_body . get_default_subject ( ) \n max_dists = plan . legislative_body . max_districts \n body_member_short_label = plan . legislative_body . get_short_label ( ) \n body_member_long_label = plan . legislative_body . get_label ( ) \n body_members = plan . legislative_body . get_members_label ( ) \n reporting_template = '<STR_LIT>' % plan . legislative_body . name if not plan . is_community ( ) else None \n index = body_member_short_label . find ( '<STR_LIT>' ) \n if index >= <NUM_LIT:0> : \n body_member_short_label = body_member_short_label [ <NUM_LIT:0> : index ] \n index = body_member_long_label . find ( '<STR_LIT>' ) \n if index >= <NUM_LIT:0> : \n body_member_long_label = body_member_long_label [ <NUM_LIT:0> : index ] \n if not editable and not can_view ( request . user , plan ) : \n plan = { } \n tags = [ ] \n calculator_reports = [ ] \n else : \n tags = Tag . objects . filter ( name__startswith = '<STR_LIT>' ) . order_by ( '<STR_LIT:id>' ) . values_list ( '<STR_LIT:name>' , flat = True ) \n tags = map ( lambda x : x [ <NUM_LIT:5> : ] , tags ) \n calculator_reports = [ ] \n if settings . REPORTS_ENABLED == '<STR_LIT>' : \n report_displays = ScoreDisplay . objects . filter ( name = \"<STR_LIT>\" % plan . legislative_body . name ) \n if len ( report_displays ) > <NUM_LIT:0> : \n calculator_reports = map ( lambda p : { \n '<STR_LIT:title>' : p . __unicode__ ( ) , \n '<STR_LIT>' : map ( lambda f : { \n '<STR_LIT:label>' : f . get_label ( ) , \n '<STR_LIT:id>' : f . id \n } , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body ) ) \n } , report_displays [ <NUM_LIT:0> ] . scorepanel_set . all ( ) . order_by ( '<STR_LIT>' ) ) \n else : \n plan = { } \n levels = list ( ) \n districts = { } \n editable = False \n default_demo = None \n max_dists = <NUM_LIT:0> \n body_member_short_label = '<STR_LIT>' \n body_member_long_label = _ ( '<STR_LIT>' ) + '<STR_LIT:U+0020>' \n body_members = _n ( '<STR_LIT>' , '<STR_LIT>' , <NUM_LIT:2> ) \n reporting_template = None \n tags = [ ] \n calculator_reports = [ ] \n demos = Subject . objects . all ( ) . order_by ( '<STR_LIT>' ) [ <NUM_LIT:0> : <NUM_LIT:3> ] \n layers = [ ] \n snaplayers = [ ] \n if len ( levels ) > <NUM_LIT:0> : \n study_area_extent = list ( levels [ <NUM_LIT:0> ] . geounit_set . extent ( field_name = '<STR_LIT>' ) ) \n else : \n for lb in LegislativeBody . objects . all ( ) : \n biglevel = lb . get_geolevels ( ) [ <NUM_LIT:0> ] \n if biglevel . geounit_set . count ( ) > <NUM_LIT:0> : \n study_area_extent = biglevel . geounit_set . extent ( field_name = '<STR_LIT>' ) \n break \n for level in levels : \n snaplayers . append ( { \n '<STR_LIT>' : level . id , \n '<STR_LIT>' : level . name , \n '<STR_LIT>' : '<STR_LIT>' + level . name , \n '<STR_LIT>' : level . get_long_description ( ) , \n '<STR_LIT>' : level . min_zoom \n } ) \n default_selected = False \n for demo in demos : \n isdefault = str ( ( not default_demo is None ) and ( demo . id == default_demo . id ) ) . lower ( ) \n if isdefault == '<STR_LIT:true>' : \n default_selected = True \n layers . append ( { \n '<STR_LIT:id>' : demo . id , \n '<STR_LIT:text>' : demo . get_short_label ( ) , \n '<STR_LIT:value>' : demo . name , \n '<STR_LIT>' : isdefault , \n '<STR_LIT>' : str ( demo . is_displayed ) . lower ( ) \n } ) \n if default_demo and not default_selected : \n layers . insert ( <NUM_LIT:0> , { \n '<STR_LIT:id>' : default_demo . id , \n '<STR_LIT:text>' : default_demo . get_short_label ( ) , \n '<STR_LIT:value>' : default_demo . name , \n '<STR_LIT>' : str ( True ) . lower ( ) , \n '<STR_LIT>' : str ( default_demo . is_displayed ) . lower ( ) \n } ) \n if '<STR_LIT>' in settings . __members__ : \n mapserver_protocol = settings . MAP_SERVER_PROTOCOL \n else : \n mapserver_protocol = '<STR_LIT>' \n short_label = body_member_short_label . strip ( ) . lower ( ) \n long_label = body_member_long_label . strip ( ) . lower ( ) \n has_regions = Region . objects . all ( ) . count ( ) > <NUM_LIT:1> \n bodies = LegislativeBody . objects . all ( ) . order_by ( '<STR_LIT>' , '<STR_LIT>' ) \n l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter ( is_page = True ) ] ] \n try : \n loader . get_template ( reporting_template ) \n except : \n reporting_template = None \n return RequestContext ( request , { \n '<STR_LIT>' : bodies , \n '<STR_LIT>' : has_regions , \n '<STR_LIT>' : l_bodies , \n '<STR_LIT>' : plan , \n '<STR_LIT>' : districts , \n '<STR_LIT>' : settings . MAP_SERVER , \n '<STR_LIT>' : mapserver_protocol , \n '<STR_LIT>' : settings . BASE_MAPS , \n '<STR_LIT>' : settings . MAP_SERVER_NS , \n '<STR_LIT>' : settings . MAP_SERVER_NSHREF , \n '<STR_LIT>' : settings . FEATURE_LIMIT , \n '<STR_LIT>' : settings . ADJACENCY , \n '<STR_LIT>' : settings . CONVEX_CHOROPLETH , \n '<STR_LIT>' : layers , \n '<STR_LIT>' : snaplayers , \n '<STR_LIT>' : UNASSIGNED_DISTRICT_ID , \n '<STR_LIT>' : request . user . username != '<STR_LIT>' and request . user . username != '<STR_LIT>' , \n '<STR_LIT>' : settings . DEBUG and request . user . is_staff , \n '<STR_LIT>' : get_user_info ( request . user ) , \n '<STR_LIT>' : editable , \n '<STR_LIT>' : max_dists + <NUM_LIT:1> , \n '<STR_LIT>' : settings . GA_ACCOUNT , \n '<STR_LIT>' : settings . GA_DOMAIN , \n '<STR_LIT>' : short_label , \n '<STR_LIT>' : long_label , \n '<STR_LIT>' : body_members , \n '<STR_LIT>' : reporting_template , \n '<STR_LIT>' : study_area_extent , \n '<STR_LIT>' : len ( ScoreDisplay . objects . filter ( is_page = True ) ) > <NUM_LIT:0> , \n '<STR_LIT>' : json . dumps ( calculator_reports ) , \n '<STR_LIT>' : ( '<STR_LIT>' in settings . __members__ ) , \n '<STR_LIT>' : tags , \n '<STR_LIT>' : Site . objects . get_current ( ) , \n '<STR_LIT>' : _ ( \"<STR_LIT>\" ) if ( plan and plan . is_community ( ) ) else _ ( \"<STR_LIT>\" ) , \n '<STR_LIT>' : translation . get_language ( ) , \n '<STR_LIT>' : settings . LANGUAGES \n } ) \n def is_plan_ready ( planid ) : \n \"\"\"<STR_LIT>\"\"\" \n planid = int ( planid ) \n return planid == <NUM_LIT:0> or len ( Plan . objects . filter ( id = planid , processing_state = ProcessingState . READY ) ) > <NUM_LIT:0> \n @ user_passes_test ( using_unique_session ) \n def viewplan ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n if not is_session_available ( request ) or not is_plan_ready ( planid ) : \n return HttpResponseRedirect ( '<STR_LIT:/>' ) \n if not request . user . is_anonymous ( ) and ( int ( planid ) == <NUM_LIT:0> ) and ( settings . MAX_UNDOS_AFTER_EDIT > <NUM_LIT:0> ) : \n for p in Plan . objects . filter ( owner = request . user ) : \n p . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n return render_to_response ( '<STR_LIT>' , commonplan ( request , planid ) ) \n @ user_passes_test ( using_unique_session ) \n def editplan ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n if request . user . is_anonymous ( ) or not is_session_available ( request ) or not is_plan_ready ( planid ) : \n return HttpResponseRedirect ( '<STR_LIT:/>' ) \n cfg = commonplan ( request , planid ) \n if cfg [ '<STR_LIT>' ] == False : \n return HttpResponseRedirect ( '<STR_LIT>' % planid ) \n plan = Plan . objects . get ( id = planid , owner = request . user ) \n cfg [ '<STR_LIT>' ] = len ( cfg [ '<STR_LIT>' ] ) > plan . legislative_body . max_districts \n cfg [ '<STR_LIT>' ] = plan . get_available_districts ( ) \n if settings . MAX_UNDOS_AFTER_EDIT > <NUM_LIT:0> : \n plan . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n return render_to_response ( '<STR_LIT>' , cfg ) \n @ user_passes_test ( using_unique_session ) \n def printplan ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n if not is_session_available ( request ) : \n return HttpResponseRedirect ( '<STR_LIT:/>' ) \n cfg = commonplan ( request , planid ) \n sha = hashlib . sha1 ( ) \n sha . update ( str ( planid ) + str ( datetime . now ( ) ) ) \n cfg [ '<STR_LIT>' ] = '<STR_LIT>' % sha . hexdigest ( ) \n cfg [ '<STR_LIT>' ] = '<STR_LIT>' % request . META [ '<STR_LIT>' ] \n if request . method == '<STR_LIT:POST>' : \n if not '<STR_LIT>' in request . REQUEST or not '<STR_LIT>' in request . REQUEST or not '<STR_LIT>' in request . REQUEST or not '<STR_LIT>' in request . REQUEST or not '<STR_LIT>' in request . REQUEST : \n logger . warning ( '<STR_LIT>' ) \n return HttpResponseRedirect ( '<STR_LIT>' ) \n height = <NUM_LIT> * <NUM_LIT:2> \n if '<STR_LIT>' in request . REQUEST : \n height = int ( request . REQUEST [ '<STR_LIT>' ] ) * <NUM_LIT:2> \n width = <NUM_LIT> * <NUM_LIT:2> \n if '<STR_LIT:width>' in request . REQUEST : \n width = int ( request . REQUEST [ '<STR_LIT:width>' ] ) * <NUM_LIT:2> \n opacity = <NUM_LIT> \n if '<STR_LIT>' in request . REQUEST : \n opacity = float ( request . REQUEST [ '<STR_LIT>' ] ) \n full_legend = json . loads ( request . REQUEST [ '<STR_LIT>' ] ) \n cfg [ '<STR_LIT>' ] = request . REQUEST [ '<STR_LIT>' ] \n cfg [ '<STR_LIT>' ] = request . REQUEST [ '<STR_LIT>' ] \n cfg [ '<STR_LIT>' ] = request . REQUEST [ '<STR_LIT>' ] \n cfg [ '<STR_LIT>' ] = request . REQUEST [ '<STR_LIT>' ] \n cfg [ '<STR_LIT>' ] = full_legend [ '<STR_LIT>' ] \n cfg [ '<STR_LIT>' ] = full_legend [ '<STR_LIT>' ] \n cfg [ '<STR_LIT>' ] = full_legend [ '<STR_LIT>' ] \n cfg [ '<STR_LIT>' ] = full_legend [ '<STR_LIT>' ] \n cfg [ '<STR_LIT>' ] = Plan . objects . get ( id = int ( request . REQUEST [ '<STR_LIT>' ] ) ) \n cfg [ '<STR_LIT>' ] = datetime . now ( ) \n bbox = request . REQUEST [ '<STR_LIT>' ] . split ( '<STR_LIT:U+002C>' ) \n pt1 = Point ( float ( bbox [ <NUM_LIT:0> ] ) , float ( bbox [ <NUM_LIT:1> ] ) , srid = <NUM_LIT> ) \n pt1 . transform ( SpatialReference ( '<STR_LIT>' ) ) \n ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n pt2 = Point ( float ( bbox [ <NUM_LIT:2> ] ) , float ( bbox [ <NUM_LIT:3> ] ) , srid = <NUM_LIT> ) \n pt2 . transform ( SpatialReference ( '<STR_LIT>' ) ) \n ur = ModestMaps . Geo . Location ( pt2 . y , pt2 . x ) \n dims = ModestMaps . Core . Point ( width , height ) \n provider = ModestMaps . OpenStreetMap . Provider ( ) \n basemap = ModestMaps . mapByExtent ( provider , ll , ur , dims ) \n fullImg = basemap . draw ( ) \n provider = ModestMaps . WMS . Provider ( cfg [ '<STR_LIT>' ] , { \n '<STR_LIT>' : cfg [ '<STR_LIT>' ] , \n '<STR_LIT>' : '<STR_LIT:true>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> \n } ) \n overlayImg = ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) \n maskImg = ImageChops . invert ( overlayImg ) \n provider = ModestMaps . WMS . Provider ( cfg [ '<STR_LIT>' ] , { \n '<STR_LIT>' : cfg [ '<STR_LIT>' ] , \n '<STR_LIT>' : '<STR_LIT:false>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : request . REQUEST [ '<STR_LIT>' ] , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> \n } ) \n overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , <NUM_LIT:0.5> ) \n fullImg = Image . composite ( fullImg , Image . blend ( fullImg , overlayImg , opacity ) , maskImg ) \n provider = ModestMaps . WMS . Provider ( cfg [ '<STR_LIT>' ] , { \n '<STR_LIT>' : cfg [ '<STR_LIT>' ] , \n '<STR_LIT>' : '<STR_LIT:true>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : request . REQUEST [ '<STR_LIT>' ] , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> \n } ) \n overlayImg = ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) \n maskImg = ImageChops . invert ( overlayImg ) \n fullImg = Image . composite ( fullImg , Image . blend ( fullImg , overlayImg , opacity ) , maskImg ) \n fullImg . save ( settings . WEB_TEMP + ( '<STR_LIT>' % sha . hexdigest ( ) ) , '<STR_LIT>' , quality = <NUM_LIT:100> ) \n t = loader . get_template ( '<STR_LIT>' ) \n page = t . render ( DjangoContext ( cfg ) ) \n result = StringIO . StringIO ( ) \n CreatePDF ( page , result , show_error_as_pdf = True ) \n response = HttpResponse ( result . getvalue ( ) , mimetype = '<STR_LIT>' ) \n response [ '<STR_LIT>' ] = '<STR_LIT>' \n return response \n else : \n return HttpResponseRedirect ( '<STR_LIT>' ) \n @ login_required \n @ unique_session_or_json_redirect \n def createplan ( request ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n if request . method == \"<STR_LIT:POST>\" : \n name = request . POST [ '<STR_LIT:name>' ] [ <NUM_LIT:0> : <NUM_LIT:200> ] \n body = LegislativeBody . objects . get ( id = int ( request . POST [ '<STR_LIT>' ] ) ) \n plan = Plan ( name = name , owner = request . user , legislative_body = body , processing_state = ProcessingState . READY ) \n try : \n plan . save ( ) \n status = serializers . serialize ( \"<STR_LIT>\" , [ plan ] ) \n except : \n status = { '<STR_LIT:success>' : False , '<STR_LIT:message>' : _ ( \"<STR_LIT>\" ) } \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ unique_session_or_json_redirect \n def uploadfile ( request ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n if request . user . is_anonymous ( ) : \n return HttpResponseRedirect ( '<STR_LIT:/>' ) \n status = commonplan ( request , <NUM_LIT:0> ) \n status [ '<STR_LIT>' ] = True \n status [ '<STR_LIT>' ] = True \n index_file = request . FILES . get ( '<STR_LIT>' , False ) \n if not index_file : \n status [ '<STR_LIT>' ] = False \n return render_to_response ( '<STR_LIT>' , status ) \n else : \n filename = index_file . name \n if index_file . size > settings . MAX_UPLOAD_SIZE : \n logger . error ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = False \n return render_to_response ( '<STR_LIT>' , status ) \n if not filename . endswith ( ( '<STR_LIT>' , '<STR_LIT>' ) ) : \n logger . error ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = False \n elif request . POST [ '<STR_LIT>' ] == '<STR_LIT>' : \n logger . error ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = False \n else : \n try : \n dest = tempfile . NamedTemporaryFile ( mode = '<STR_LIT>' , delete = False ) \n for chunk in request . FILES [ '<STR_LIT>' ] . chunks ( ) : \n dest . write ( chunk ) \n dest . close ( ) \n if request . FILES [ '<STR_LIT>' ] . name . endswith ( '<STR_LIT>' ) : \n os . rename ( dest . name , '<STR_LIT>' % ( dest . name , '<STR_LIT>' ) ) \n filename = '<STR_LIT>' % ( dest . name , '<STR_LIT>' ) \n else : \n filename = dest . name \n except Exception as ex : \n logger . error ( '<STR_LIT>' ) \n logger . error ( '<STR_LIT>' , ex ) \n status [ '<STR_LIT>' ] = False \n return render_to_response ( '<STR_LIT>' , status ) \n DistrictIndexFile . index2plan . delay ( request . POST [ '<STR_LIT>' ] , request . POST [ '<STR_LIT>' ] , filename , owner = request . user , template = False , purge = True , email = request . POST [ '<STR_LIT>' ] , language = translation . get_language ( ) ) \n return render_to_response ( '<STR_LIT>' , status ) \n def generate_report_hash ( qdict ) : \n \"\"\"<STR_LIT>\"\"\" \n params = qdict . get ( '<STR_LIT>' , '<STR_LIT:U+0020>' ) + qdict . get ( '<STR_LIT>' , '<STR_LIT:U+0020>' ) + qdict . get ( '<STR_LIT>' , '<STR_LIT:U+0020>' ) + qdict . get ( '<STR_LIT>' , '<STR_LIT:U+0020>' ) + qdict . get ( '<STR_LIT>' , '<STR_LIT>' ) + qdict . get ( '<STR_LIT>' , '<STR_LIT:U+0020>' ) + qdict . get ( '<STR_LIT>' , '<STR_LIT:U+0020>' ) + qdict . get ( '<STR_LIT>' , '<STR_LIT:U+0020>' ) + qdict . get ( '<STR_LIT>' , '<STR_LIT:U+0020>' ) \n sha = hashlib . sha1 ( ) \n sha . update ( params ) \n return sha . hexdigest ( ) \n @ unique_session_or_json_redirect \n def getreport ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n try : \n plan = Plan . objects . get ( pk = planid ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if not can_view ( request . user , plan ) : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if not settings . REPORTS_ENABLED is None : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if request . method != '<STR_LIT:POST>' : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n stamp = request . POST . get ( '<STR_LIT>' , generate_report_hash ( request . POST ) ) \n rptstatus = PlanReport . checkreport ( planid , stamp ) \n if rptstatus == '<STR_LIT>' : \n status = { \n '<STR_LIT:success>' : True , \n '<STR_LIT:url>' : PlanReport . getreport ( planid , stamp ) , \n '<STR_LIT>' : <NUM_LIT:0> , \n '<STR_LIT:message>' : _ ( '<STR_LIT>' ) , \n '<STR_LIT>' : stamp \n } \n elif rptstatus == '<STR_LIT>' : \n status = { \n '<STR_LIT:success>' : True , \n '<STR_LIT:url>' : reverse ( getreport , args = [ planid ] ) , \n '<STR_LIT>' : <NUM_LIT:10> , \n '<STR_LIT:message>' : _ ( '<STR_LIT>' ) , \n '<STR_LIT>' : stamp \n } \n elif rptstatus == '<STR_LIT>' : \n status = { \n '<STR_LIT:success>' : True , \n '<STR_LIT:url>' : reverse ( getreport , args = [ planid ] ) , \n '<STR_LIT>' : <NUM_LIT:10> , \n '<STR_LIT:message>' : _ ( '<STR_LIT>' ) , \n '<STR_LIT>' : stamp \n } \n req = { \n '<STR_LIT>' : request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : request . POST . getlist ( '<STR_LIT>' ) , \n '<STR_LIT>' : request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) \n } \n PlanReport . markpending ( planid , stamp ) \n PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n else : \n status [ '<STR_LIT:message>' ] = _ ( \n '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ unique_session_or_json_redirect \n def getcalculatorreport ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n try : \n plan = Plan . objects . get ( pk = planid ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if not can_view ( request . user , plan ) : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if request . method != '<STR_LIT:POST>' : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n function_ids = request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) \n sha = hashlib . sha1 ( ) \n sha . update ( function_ids ) \n stamp = request . POST . get ( '<STR_LIT>' , sha . hexdigest ( ) ) \n rptstatus = CalculatorReport . checkreport ( planid , stamp ) \n if rptstatus == '<STR_LIT>' : \n status = { \n '<STR_LIT:success>' : True , \n '<STR_LIT:url>' : CalculatorReport . getreport ( planid , stamp ) , \n '<STR_LIT>' : <NUM_LIT:0> , \n '<STR_LIT:message>' : _ ( '<STR_LIT>' ) , \n '<STR_LIT>' : stamp \n } \n elif rptstatus == '<STR_LIT>' : \n status = { \n '<STR_LIT:success>' : True , \n '<STR_LIT:url>' : reverse ( getcalculatorreport , args = [ planid ] ) , \n '<STR_LIT>' : <NUM_LIT:5> , \n '<STR_LIT:message>' : _ ( '<STR_LIT>' ) , \n '<STR_LIT>' : stamp \n } \n elif rptstatus == '<STR_LIT>' : \n status = { \n '<STR_LIT:success>' : True , \n '<STR_LIT:url>' : reverse ( getcalculatorreport , args = [ planid ] ) , \n '<STR_LIT>' : <NUM_LIT:5> , \n '<STR_LIT:message>' : _ ( '<STR_LIT>' ) , \n '<STR_LIT>' : stamp \n } \n req = { '<STR_LIT>' : function_ids } \n CalculatorReport . markpending ( planid , stamp ) \n CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n else : \n status [ '<STR_LIT:message>' ] = _ ( \n '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ login_required \n @ unique_session_or_json_redirect \n def newdistrict ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n if len ( request . REQUEST . items ( ) ) >= <NUM_LIT:3> : \n plan = Plan . objects . get ( pk = planid , owner = request . user ) \n if '<STR_LIT>' in request . REQUEST : \n geolevel = request . REQUEST [ '<STR_LIT>' ] \n else : \n geolevel = None \n if '<STR_LIT>' in request . REQUEST : \n geounit_ids = string . split ( request . REQUEST [ '<STR_LIT>' ] , '<STR_LIT:|>' ) \n else : \n geounit_ids = None \n if '<STR_LIT>' in request . REQUEST : \n district_id = int ( request . REQUEST [ '<STR_LIT>' ] ) \n else : \n district_id = None \n if '<STR_LIT>' in request . REQUEST : \n district_short = request . REQUEST [ '<STR_LIT>' ] [ <NUM_LIT:0> : <NUM_LIT:10> ] \n elif not district_id is None : \n district_short = plan . legislative_body . get_short_label ( ) % { '<STR_LIT>' : district_id } \n else : \n district_short = None \n if '<STR_LIT>' in request . REQUEST : \n district_long = request . REQUEST [ '<STR_LIT>' ] [ <NUM_LIT:0> : <NUM_LIT> ] \n elif not district_id is None : \n district_long = plan . legislative_body . get_label ( ) % { '<STR_LIT>' : district_id } \n else : \n district_long = None \n if '<STR_LIT:version>' in request . REQUEST : \n version = request . REQUEST [ '<STR_LIT:version>' ] \n else : \n version = plan . version \n if geolevel and geounit_ids and district_id : \n try : \n fixed = plan . add_geounits ( ( district_id , district_short , district_long , ) , geounit_ids , geolevel , version ) \n district = plan . district_set . filter ( district_id = district_id , short_label = district_short , long_label = district_long ) [ <NUM_LIT:0> ] \n if plan . legislative_body . multi_members_allowed : \n district . num_members = plan . legislative_body . min_multi_district_members \n district . save ( ) \n ct = ContentType . objects . get ( app_label = '<STR_LIT>' , model = '<STR_LIT>' ) \n if '<STR_LIT>' in request . POST and request . POST [ '<STR_LIT>' ] != '<STR_LIT>' : \n comment = Comment ( \n object_pk = district . id , \n content_type = ct , \n site_id = Site . objects . get_current ( ) . id , \n user_name = request . user . username , \n user_email = request . user . email , \n comment = request . POST [ '<STR_LIT>' ] ) \n comment . save ( ) \n if len ( request . REQUEST . getlist ( '<STR_LIT>' ) ) > <NUM_LIT:0> : \n strtags = request . REQUEST . getlist ( '<STR_LIT>' ) \n for strtag in strtags : \n if strtag == '<STR_LIT>' : \n continue \n if strtag . count ( '<STR_LIT:U+0020>' ) > <NUM_LIT:0> : \n strtag = '<STR_LIT>' % strtag \n else : \n strtag = '<STR_LIT>' % strtag \n Tag . objects . add_tag ( district , strtag ) \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n plan = Plan . objects . get ( pk = planid , owner = request . user ) \n status [ '<STR_LIT>' ] = getutc ( plan . edited ) . isoformat ( ) \n status [ '<STR_LIT>' ] = district_id \n status [ '<STR_LIT:version>' ] = plan . version \n except ValidationError : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n except Exception , ex : \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n else : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ login_required \n @ unique_session_or_json_redirect \n @ transaction . commit_manually \n def add_districts_to_plan ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n status = { '<STR_LIT:success>' : False } \n try : \n plan = Plan . objects . get ( pk = planid ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if not can_edit ( request . user , plan ) : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n district_list = request . POST . getlist ( '<STR_LIT>' ) \n if len ( district_list ) == <NUM_LIT:0> : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n else : \n districts = District . objects . filter ( id__in = district_list ) \n version = int ( request . POST . get ( '<STR_LIT:version>' , None ) ) \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' '<STR_LIT>' ) % { '<STR_LIT>' : len ( districts ) } \n allowed_districts = plan . get_available_districts ( version = version ) \n if len ( districts ) > allowed_districts : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' '<STR_LIT>' ) % { '<STR_LIT>' : allowed_districts } \n try : \n results = plan . paste_districts ( districts , version = version ) \n transaction . commit ( ) \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) % { '<STR_LIT>' : len ( results ) } \n status [ '<STR_LIT:version>' ] = plan . version \n except Exception as ex : \n transaction . rollback ( ) \n status [ '<STR_LIT:message>' ] = str ( ex ) \n status [ '<STR_LIT>' ] = traceback . format_exc ( ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ login_required \n @ unique_session_or_json_redirect \n @ transaction . commit_manually \n def assign_district_members ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n status = { '<STR_LIT:success>' : False } \n try : \n plan = Plan . objects . get ( pk = planid ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if not can_edit ( request . user , plan ) : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n leg_bod = plan . legislative_body \n if ( not leg_bod . multi_members_allowed ) : \n status [ '<STR_LIT:message>' ] = _ ( \n '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n districts = request . POST . getlist ( '<STR_LIT>' ) \n counts = request . POST . getlist ( '<STR_LIT>' ) \n version = int ( request . POST . get ( '<STR_LIT:version>' , None ) ) \n try : \n changed = <NUM_LIT:0> \n for i in range ( <NUM_LIT:0> , len ( districts ) ) : \n id = int ( districts [ i ] ) \n count = int ( counts [ i ] ) \n district = District . objects . filter ( plan = plan , district_id = id , version__lte = version ) . order_by ( '<STR_LIT:version>' ) . reverse ( ) [ <NUM_LIT:0> ] \n if district . num_members != count : \n if ( changed == <NUM_LIT:0> ) : \n if version != plan . version : \n plan . purge ( after = version ) \n plan . version = plan . version + <NUM_LIT:1> \n plan . save ( ) \n plan . update_num_members ( district , count ) \n changed += <NUM_LIT:1> \n transaction . commit ( ) \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:version>' ] = plan . version \n status [ '<STR_LIT>' ] = changed \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' \n '<STR_LIT>' ) % { '<STR_LIT>' : changed } \n except Exception , ex : \n transaction . rollback ( ) \n status [ '<STR_LIT:message>' ] = str ( ex ) \n status [ '<STR_LIT>' ] = traceback . format_exc ( ) \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ login_required \n @ unique_session_or_json_redirect \n def combine_districts ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n status = { '<STR_LIT:success>' : False } \n try : \n plan = Plan . objects . get ( pk = planid ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if not can_edit ( request . user , plan ) : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n version = int ( request . POST . get ( '<STR_LIT:version>' , plan . version ) ) \n from_id = int ( request . POST . get ( '<STR_LIT>' , - <NUM_LIT:1> ) ) \n to_id = int ( request . POST . get ( '<STR_LIT>' , None ) ) \n try : \n all_districts = plan . get_districts_at_version ( version , include_geom = True ) \n from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts ) \n to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ <NUM_LIT:0> ] \n locked = to_district . is_locked \n for district in from_districts : \n if district . is_locked : \n locked = True \n if locked : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n result = plan . combine_districts ( to_district , from_districts , version = version ) \n if result [ <NUM_LIT:0> ] == True : \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n status [ '<STR_LIT:version>' ] = result [ <NUM_LIT:1> ] \n except Exception , ex : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = traceback . format_exc ( ) \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ login_required \n @ unique_session_or_json_redirect \n def fix_unassigned ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n status = { '<STR_LIT:success>' : False } \n try : \n plan = Plan . objects . get ( pk = planid ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if not can_edit ( request . user , plan ) : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n try : \n version = int ( request . POST . get ( '<STR_LIT:version>' , plan . version ) ) \n result = plan . fix_unassigned ( version ) \n status [ '<STR_LIT:success>' ] = result [ <NUM_LIT:0> ] \n status [ '<STR_LIT:message>' ] = result [ <NUM_LIT:1> ] \n status [ '<STR_LIT:version>' ] = plan . version \n except Exception , ex : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = traceback . format_exc ( ) \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ unique_session_or_json_redirect \n def get_splits ( request , planid , otherid , othertype ) : \n \"\"\"<STR_LIT>\"\"\" \n otherid = int ( otherid ) \n status = { '<STR_LIT:success>' : False } \n try : \n plan = Plan . objects . get ( pk = planid ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if not can_view ( request . user , plan ) : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n version = int ( request . REQUEST [ '<STR_LIT:version>' ] if '<STR_LIT:version>' in request . REQUEST else plan . version ) \n try : \n if othertype == '<STR_LIT>' : \n try : \n otherplan = Plan . objects . get ( pk = otherid ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if not can_view ( request . user , otherplan ) : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n otherversion = int ( request . REQUEST [ '<STR_LIT>' ] if '<STR_LIT>' in request . REQUEST else otherplan . version ) \n splits = plan . find_plan_splits ( otherplan , version , otherversion ) \n elif othertype == '<STR_LIT>' : \n splits = plan . find_geolevel_splits ( otherid , version ) \n else : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) % { '<STR_LIT>' : othertype } \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n split_word = _ ( '<STR_LIT>' ) if len ( splits ) == <NUM_LIT:1> else inflect . engine ( ) . plural ( _ ( '<STR_LIT>' ) ) \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) % { '<STR_LIT>' : len ( splits ) , '<STR_LIT>' : split_word } \n status [ '<STR_LIT>' ] = splits \n status [ '<STR_LIT>' ] = list ( set ( [ i [ <NUM_LIT:0> ] for i in splits ] ) ) \n status [ '<STR_LIT>' ] = list ( set ( [ i [ <NUM_LIT:1> ] for i in splits ] ) ) \n except Exception , ex : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = traceback . format_exc ( ) \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n def get_processing_status ( request ) : \n \"\"\"<STR_LIT>\"\"\" \n status = { '<STR_LIT:success>' : False } \n plan_ids = request . REQUEST . getlist ( '<STR_LIT>' ) \n if len ( plan_ids ) == <NUM_LIT:0> : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n else : \n statuses = { } \n for p in Plan . objects . filter ( id__in = plan_ids ) : \n statuses [ str ( p . id ) ] = p . get_processing_state_display ( ) \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:message>' ] = statuses \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n def get_splits_report ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n try : \n plan = Plan . objects . get ( pk = planid ) \n except : \n return HttpResponse ( _ ( '<STR_LIT>' ) , mimetype = '<STR_LIT>' ) \n if not using_unique_session ( request . user ) or not can_view ( request . user , plan ) : \n return HttpResponseForbidden ( ) \n version = int ( request . REQUEST [ '<STR_LIT:version>' ] if '<STR_LIT:version>' in request . REQUEST else plan . version ) \n inverse = request . REQUEST [ '<STR_LIT>' ] == '<STR_LIT:true>' if '<STR_LIT>' in request . REQUEST else False \n extended = request . REQUEST [ '<STR_LIT>' ] == '<STR_LIT:true>' if '<STR_LIT>' in request . REQUEST else False \n layers = request . REQUEST . getlist ( '<STR_LIT>' ) \n if len ( layers ) == <NUM_LIT:0> : \n return HttpResponse ( _ ( '<STR_LIT>' ) , mimetype = '<STR_LIT>' ) \n try : \n report = loader . get_template ( '<STR_LIT>' ) \n html = '<STR_LIT>' \n for layer in layers : \n my_context = { '<STR_LIT>' : extended } \n my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended = extended ) ) \n last_item = layer is layers [ - <NUM_LIT:1> ] \n community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse , include_counts = last_item ) \n if community_info is not None : \n my_context . update ( community_info ) \n calc_context = DjangoContext ( my_context ) \n html += report . render ( calc_context ) \n if not last_item : \n html += '<STR_LIT>' \n return HttpResponse ( html , mimetype = '<STR_LIT>' ) \n except Exception , ex : \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n return HttpResponse ( str ( ex ) , mimetype = '<STR_LIT>' ) \n @ login_required \n @ unique_session_or_json_redirect \n def addtodistrict ( request , planid , districtid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n if len ( request . REQUEST . items ( ) ) >= <NUM_LIT:2> : \n try : \n geolevel = request . REQUEST [ \"<STR_LIT>\" ] \n geounit_ids = string . split ( request . REQUEST [ \"<STR_LIT>\" ] , \"<STR_LIT:|>\" ) \n plan = Plan . objects . get ( pk = planid , owner = request . user ) \n except : \n status [ '<STR_LIT>' ] = traceback . format_exc ( ) \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n if '<STR_LIT:version>' in request . REQUEST : \n version = request . REQUEST [ '<STR_LIT:version>' ] \n else : \n version = plan . version \n try : \n fixed = plan . add_geounits ( districtid , geounit_ids , geolevel , version ) \n status [ '<STR_LIT:success>' ] = True ; \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) % { '<STR_LIT>' : fixed } \n status [ '<STR_LIT>' ] = fixed \n plan = Plan . objects . get ( pk = planid , owner = request . user ) \n status [ '<STR_LIT>' ] = getutc ( plan . edited ) . isoformat ( ) \n status [ '<STR_LIT:version>' ] = plan . version \n except Exception , ex : \n status [ '<STR_LIT>' ] = traceback . format_exc ( ) \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n else : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ unique_session_or_json_redirect \n @ login_required \n def setdistrictlock ( request , planid , district_id ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n if request . method != '<STR_LIT:POST>' : \n return HttpResponseForbidden ( ) \n lock = request . POST . get ( '<STR_LIT>' ) . lower ( ) == '<STR_LIT:true>' \n version = request . POST . get ( '<STR_LIT:version>' ) \n if lock == None : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n elif version == None : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n try : \n plan = Plan . objects . get ( pk = planid ) \n district = plan . district_set . filter ( district_id = district_id , version__lte = version ) . order_by ( '<STR_LIT:version>' ) . reverse ( ) [ <NUM_LIT:0> ] \n except ObjectDoesNotExist : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n if plan . owner != request . user : \n return HttpResponseForbidden ( ) \n district . is_locked = lock \n district . save ( ) \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) % { '<STR_LIT>' : _ ( '<STR_LIT>' ) if lock else _ ( '<STR_LIT>' ) } \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ unique_session_or_json_redirect \n def getdistricts ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n plan = Plan . objects . filter ( id = planid ) \n if plan . count ( ) == <NUM_LIT:1> : \n plan = plan [ <NUM_LIT:0> ] \n if '<STR_LIT:version>' in request . REQUEST : \n version = int ( request . REQUEST [ '<STR_LIT:version>' ] ) \n else : \n version = plan . version \n districts = plan . get_districts_at_version ( version , include_geom = False ) \n status [ '<STR_LIT>' ] = [ ] \n status [ '<STR_LIT>' ] = plan . legislative_body . max_districts - len ( districts ) + <NUM_LIT:1> \n max_version = max ( [ d . version for d in districts ] ) \n can_undo = max_version > plan . min_version \n for district in districts : \n status [ '<STR_LIT>' ] . append ( { \n '<STR_LIT:id>' : district . district_id , \n '<STR_LIT>' : '<STR_LIT:U+0020>' . join ( map ( _ , district . short_label . split ( '<STR_LIT:U+0020>' ) ) ) , \n '<STR_LIT>' : '<STR_LIT:U+0020>' . join ( map ( _ , district . long_label . split ( '<STR_LIT:U+0020>' ) ) ) , \n '<STR_LIT:version>' : district . version \n } ) \n status [ '<STR_LIT>' ] = can_undo \n status [ '<STR_LIT:success>' ] = True \n else : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n def simple_district_versioned ( request , planid , district_ids = None ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:type>' : '<STR_LIT>' } \n plan = Plan . objects . filter ( id = planid ) \n if plan . count ( ) == <NUM_LIT:1> : \n plan = plan [ <NUM_LIT:0> ] \n if '<STR_LIT>' in request . REQUEST : \n version = request . REQUEST [ '<STR_LIT>' ] \n else : \n version = plan . version \n subject_id = None \n if '<STR_LIT>' in request . REQUEST : \n subject_id = request . REQUEST [ '<STR_LIT>' ] \n elif plan . legislative_body . get_default_subject ( ) : \n subject_id = plan . legislative_body . get_default_subject ( ) . id \n geolevel = plan . legislative_body . get_geolevels ( ) [ <NUM_LIT:0> ] . id \n if '<STR_LIT>' in request . REQUEST : \n geolevel = int ( request . REQUEST [ '<STR_LIT>' ] ) \n if '<STR_LIT>' in request . REQUEST : \n district_ids = request . REQUEST [ '<STR_LIT>' ] \n if len ( district_ids ) > <NUM_LIT:0> : \n district_ids = district_ids . split ( '<STR_LIT:U+002C>' ) \n else : \n district_ids = [ ] \n if subject_id : \n bbox = None \n if '<STR_LIT>' in request . REQUEST : \n bbox = request . REQUEST [ '<STR_LIT>' ] \n bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( '<STR_LIT:U+002C>' ) ) ) \n else : \n bbox = plan . district_set . all ( ) . extent ( field_name = '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = plan . get_wfs_districts ( version , subject_id , bbox , geolevel , district_ids ) \n else : \n status [ '<STR_LIT>' ] = [ ] \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n else : \n status [ '<STR_LIT>' ] = [ ] \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n def get_unlocked_simple_geometries ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:type>' : '<STR_LIT>' } \n plan = Plan . objects . filter ( id = planid ) \n if plan . count ( ) == <NUM_LIT:1> : \n plan = plan [ <NUM_LIT:0> ] \n version = request . POST . get ( '<STR_LIT>' , plan . version ) \n geolevel = request . POST . get ( '<STR_LIT>' , plan . legislative_body . get_geolevels ( ) [ <NUM_LIT:0> ] . id ) \n geom = request . POST . get ( '<STR_LIT>' , None ) \n if geom is not None : \n try : \n wkt = request . POST . get ( '<STR_LIT>' , None ) \n geom = GEOSGeometry ( wkt ) \n except GEOSException : \n wkt = request . REQUEST [ '<STR_LIT>' ] . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n wkt = wkt . replace ( '<STR_LIT>' , '<STR_LIT:(>' ) . replace ( '<STR_LIT>' , '<STR_LIT:)>' ) \n try : \n geom = GEOSGeometry ( wkt ) \n except GEOSException : \n geom = None \n selection = Q ( geom__intersects = geom ) \n districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if d . is_locked ] \n locked = District . objects . filter ( id__in = districts ) . collect ( ) \n locked_buffered = locked . simplify ( <NUM_LIT:100> , True ) . buffer ( <NUM_LIT:100> ) if locked else None \n filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n features = [ ] \n for feature in filtered : \n geom = feature . simple \n if locked and geom . intersects ( locked_buffered ) : \n if feature . geom . within ( locked ) : \n continue \n if feature . geom . overlaps ( locked ) : \n geom = geom . difference ( locked_buffered ) \n features . append ( { \n '<STR_LIT:id>' : '<STR_LIT>' % feature . id , \n '<STR_LIT>' : json . loads ( geom . json ) , \n '<STR_LIT>' : { \n '<STR_LIT:name>' : feature . name , \n '<STR_LIT>' : geolevel , \n '<STR_LIT:id>' : feature . id \n } \n } ) \n status [ '<STR_LIT>' ] = features \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n else : \n status [ '<STR_LIT>' ] = [ ] \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n else : \n status [ '<STR_LIT>' ] = [ ] \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ unique_session_or_json_redirect \n def get_statistics ( request , planid ) : \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n try : \n plan = Plan . objects . get ( pk = planid ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( \n \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' , status = <NUM_LIT> ) \n if '<STR_LIT:version>' in request . REQUEST : \n try : \n version = int ( request . REQUEST [ '<STR_LIT:version>' ] ) \n except : \n version = plan . version \n else : \n version = plan . version \n try : \n display = ScoreDisplay . objects . get ( legislative_body = plan . legislative_body , name = \"<STR_LIT>\" % plan . legislative_body . name ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = traceback . format_exc ( ) \n if '<STR_LIT>' in request . REQUEST : \n try : \n display = ScoreDisplay . objects . get ( pk = request . POST [ '<STR_LIT>' ] ) \n except : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = traceback . format_exc ( ) \n else : \n logger . warn ( '<STR_LIT>' ) \n logger . warn ( str ( request . POST ) ) \n try : \n html = display . render ( plan , request , version = version ) \n return HttpResponse ( html , mimetype = '<STR_LIT>' ) \n except Exception , ex : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n status [ '<STR_LIT>' ] = traceback . format_exc ( ) \n logger . warn ( \"<STR_LIT>\" ) \n logger . debug ( '<STR_LIT>' , ex ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' , status = <NUM_LIT> ) \n def getutc ( t ) : \n \"\"\"<STR_LIT>\"\"\" \n t_tuple = t . timetuple ( ) \n t_seconds = time . mktime ( t_tuple ) \n return t . utcfromtimestamp ( t_seconds ) \n @ unique_session_or_json_redirect \n def getdistrictfilestatus ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n plan = Plan . objects . get ( pk = planid ) \n if not can_copy ( request . user , plan ) : \n return HttpResponseForbidden ( ) \n try : \n is_shape = '<STR_LIT:type>' in request . REQUEST and request . REQUEST [ '<STR_LIT:type>' ] == '<STR_LIT>' \n file_status = DistrictFile . get_file_status ( plan , shape = is_shape ) \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:status>' ] = file_status \n except Exception as ex : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = ex \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ unique_session_or_json_redirect \n def getdistrictfile ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n plan = Plan . objects . get ( pk = planid ) \n if not can_copy ( request . user , plan ) : \n return HttpResponseForbidden ( ) \n is_shape = '<STR_LIT:type>' in request . REQUEST and request . REQUEST [ '<STR_LIT:type>' ] == '<STR_LIT>' \n file_status = DistrictFile . get_file_status ( plan , shape = is_shape ) \n if file_status == '<STR_LIT>' : \n if is_shape : \n archive = DistrictShapeFile . plan2shape ( plan ) \n else : \n archive = DistrictIndexFile . plan2index ( plan ) \n response = HttpResponse ( open ( archive . name ) . read ( ) , content_type = '<STR_LIT>' ) \n response [ '<STR_LIT>' ] = '<STR_LIT>' % plan . get_friendly_name ( ) \n else : \n if is_shape : \n DistrictShapeFile . plan2shape . delay ( plan ) \n else : \n DistrictIndexFile . plan2index . delay ( plan ) \n response = HttpResponse ( _ ( '<STR_LIT>' \n '<STR_LIT>' ) ) \n return response \n @ unique_session_or_json_redirect \n def emaildistrictindexfile ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n if request . method != '<STR_LIT:POST>' : \n return HttpResponseForbidden ( ) \n plan = Plan . objects . get ( pk = planid ) \n if not can_copy ( request . user , plan ) : \n return HttpResponseForbidden ( ) \n DistrictIndexFile . emailfile . delay ( plan , request . user , request . POST , translation . get_language ( ) ) \n return HttpResponse ( json . dumps ( { \n '<STR_LIT:success>' : True , \n '<STR_LIT:message>' : _ ( '<STR_LIT>' ) } ) , \n mimetype = '<STR_LIT:application/json>' ) \n def getvalidplans ( leg_body , owner = None ) : \n \"\"\"<STR_LIT>\"\"\" \n pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n if owner is not None : \n pfilter = pfilter & Q ( owner = owner ) \n return list ( Plan . objects . filter ( pfilter ) ) \n def getleaderboarddisplay ( leg_body , owner_filter ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n return ScoreDisplay . objects . get ( name = \"<STR_LIT>\" % ( leg_body . name , owner_filter ) ) \n except : \n return None \n def getleaderboard ( request ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n if not using_unique_session ( request . user ) : \n return HttpResponseForbidden ( ) \n owner_filter = request . REQUEST [ '<STR_LIT>' ] \n body_pk = int ( request . REQUEST [ '<STR_LIT>' ] ) ; \n leg_body = LegislativeBody . objects . get ( pk = body_pk ) \n display = getleaderboarddisplay ( leg_body , owner_filter ) \n if display is None : \n return HttpResponse ( _ ( '<STR_LIT>' ) , mimetype = '<STR_LIT>' ) \n plans = getvalidplans ( leg_body , request . user if owner_filter == '<STR_LIT>' else None ) \n try : \n html = display . render ( plans , request ) \n return HttpResponse ( html , mimetype = '<STR_LIT>' ) \n except Exception , ex : \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n return HttpResponse ( str ( ex ) , mimetype = '<STR_LIT>' ) \n def getleaderboardcsv ( request ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n if not using_unique_session ( request . user ) : \n return HttpResponseForbidden ( ) \n owner_filter = request . REQUEST [ '<STR_LIT>' ] \n body_pk = int ( request . REQUEST [ '<STR_LIT>' ] ) ; \n leg_body = LegislativeBody . objects . get ( pk = body_pk ) \n plans = getvalidplans ( leg_body , request . user if owner_filter == '<STR_LIT>' else None ) \n display = getleaderboarddisplay ( leg_body , owner_filter ) \n plans = getvalidplans ( leg_body , request . user if owner_filter == '<STR_LIT>' else None ) \n panels = display . scorepanel_set . all ( ) . order_by ( '<STR_LIT>' ) \n try : \n response = HttpResponse ( mimetype = '<STR_LIT>' ) \n response [ '<STR_LIT>' ] = '<STR_LIT>' \n writer = csv . writer ( response ) \n writer . writerow ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] + [ p . __unicode__ ( ) for p in panels ] ) \n for plan in plans : \n row = [ plan . id , plan . name , plan . owner . username ] \n for panel in panels : \n function = panel . score_functions . all ( ) [ <NUM_LIT:0> ] \n score = ComputedPlanScore . compute ( function , plan ) \n row . append ( score [ '<STR_LIT:value>' ] ) \n writer . writerow ( row ) \n return response \n except Exception , ex : \n logger . warn ( \"<STR_LIT>\" ) \n logger . debug ( '<STR_LIT>' , ex ) \n return HttpResponse ( str ( ex ) , mimetype = '<STR_LIT>' ) \n def getplans ( request ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n if not using_unique_session ( request . user ) : \n return HttpResponseForbidden ( ) \n if request . method == '<STR_LIT:POST>' : \n page = int ( request . POST . get ( '<STR_LIT>' , <NUM_LIT:1> ) ) \n rows = int ( request . POST . get ( '<STR_LIT>' , <NUM_LIT:10> ) ) \n sidx = request . POST . get ( '<STR_LIT>' , '<STR_LIT:id>' ) \n sord = request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) \n owner_filter = request . POST . get ( '<STR_LIT>' ) ; \n body_pk = request . POST . get ( '<STR_LIT>' ) ; \n body_pk = int ( body_pk ) if body_pk else body_pk ; \n search = request . POST . get ( '<STR_LIT>' , False ) ; \n search_string = request . POST . get ( '<STR_LIT>' , '<STR_LIT>' ) ; \n is_community = request . POST . get ( '<STR_LIT>' , False ) == '<STR_LIT:true>' ; \n else : \n return HttpResponseForbidden ( ) \n end = page * rows \n start = end - rows \n if owner_filter == '<STR_LIT>' : \n available = Q ( is_template = True ) \n elif owner_filter == '<STR_LIT>' : \n available = Q ( is_shared = True ) \n elif owner_filter == '<STR_LIT>' : \n if request . user . is_anonymous ( ) : \n return HttpResponseForbidden ( ) \n else : \n available = Q ( owner__exact = request . user ) \n elif owner_filter == '<STR_LIT>' : \n available = Q ( is_template = True ) | Q ( is_shared = True ) \n if not request . user . is_anonymous ( ) : \n available = available | Q ( owner__exact = request . user ) \n else : \n return HttpResponseBadRequest ( _ ( \"<STR_LIT>\" ) ) \n not_creating = ~ Q ( processing_state = ProcessingState . CREATING ) & ~ Q ( processing_state = ProcessingState . UNKNOWN ) \n if sidx . startswith ( '<STR_LIT>' ) : \n sidx = sidx [ len ( '<STR_LIT>' ) : ] \n if sidx == '<STR_LIT>' : \n sidx = '<STR_LIT>' \n if sidx == '<STR_LIT>' : \n sidx = '<STR_LIT>' \n if sord == '<STR_LIT>' : \n sidx = '<STR_LIT:->' + sidx \n if search : \n search_filter = Q ( name__icontains = search_string ) | Q ( description__icontains = search_string ) | Q ( owner__username__icontains = search_string ) \n else : \n search_filter = None \n if body_pk : \n body_filter = Q ( legislative_body = body_pk ) \n all_plans = Plan . objects . filter ( available , not_creating , body_filter , search_filter ) . order_by ( sidx ) \n else : \n community_filter = Q ( legislative_body__is_community = is_community ) \n all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by ( sidx ) \n if all_plans . count ( ) > <NUM_LIT:0> : \n total_pages = math . ceil ( all_plans . count ( ) / float ( rows ) ) \n else : \n total_pages = <NUM_LIT:1> \n plans = all_plans [ start : end ] \n plans_list = list ( ) \n for plan in plans : \n plans_list . append ( { \n '<STR_LIT>' : plan . id , \n '<STR_LIT>' : { \n '<STR_LIT:name>' : plan . name , \n '<STR_LIT:description>' : plan . description , \n '<STR_LIT>' : time . mktime ( plan . edited . timetuple ( ) ) , \n '<STR_LIT>' : plan . is_template , \n '<STR_LIT>' : plan . is_shared , \n '<STR_LIT>' : plan . owner . username , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : can_edit ( request . user , plan ) , \n '<STR_LIT>' : plan . legislative_body . get_long_description ( ) , \n '<STR_LIT>' : plan . get_processing_state_display ( ) \n } \n } ) \n json_response = \"<STR_LIT>\" % ( total_pages , page , len ( all_plans ) , json . dumps ( plans_list ) ) \n return HttpResponse ( json_response , mimetype = '<STR_LIT:application/json>' ) \n def get_shared_districts ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n if not using_unique_session ( request . user ) : \n return HttpResponseForbidden ( ) \n if request . method == '<STR_LIT:POST>' : \n page = int ( request . POST . get ( '<STR_LIT>' , <NUM_LIT:1> ) ) \n rows = int ( request . POST . get ( '<STR_LIT>' , <NUM_LIT:10> ) ) \n else : \n return HttpResponseForbidden ( ) \n end = page * rows \n start = end - rows \n try : \n plan = Plan . objects . get ( pk = planid ) \n if not can_copy ( request . user , plan ) : \n return HttpResponseForbidden ( ) \n all_districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n except : \n plan = None \n all_districts = ( ) \n if len ( all_districts ) > <NUM_LIT:0> : \n total_pages = math . ceil ( len ( all_districts ) / float ( rows ) ) \n else : \n total_pages = <NUM_LIT:1> \n districts = all_districts [ start : end ] \n districts_list = list ( ) \n for district in districts : \n if not district . is_unassigned : \n districts_list . append ( { \n '<STR_LIT>' : district . id , \n '<STR_LIT>' : { \n '<STR_LIT>' : district . short_label , \n '<STR_LIT>' : district . long_label , \n '<STR_LIT>' : district . district_id , \n } \n } ) \n json_response = \"<STR_LIT>\" % ( total_pages , page , len ( all_districts ) , json . dumps ( districts_list ) ) \n return HttpResponse ( json_response , mimetype = '<STR_LIT:application/json>' ) \n @ login_required \n @ unique_session_or_json_redirect \n def editplanattributes ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n if request . method != '<STR_LIT:POST>' : \n return HttpResponseNotAllowed ( [ '<STR_LIT:POST>' ] ) \n new_name = request . POST . get ( '<STR_LIT:name>' , None ) \n new_description = request . POST . get ( '<STR_LIT:description>' , '<STR_LIT>' ) \n if not planid or not ( new_name or new_description ) : \n return HttpResponseBadRequest ( \n _ ( '<STR_LIT>' ) ) \n plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n if plan . count ( ) == <NUM_LIT:1> : \n plan = plan [ <NUM_LIT:0> ] \n if not new_name is None : \n plan . name = new_name \n plan . description = new_description \n try : \n plan . save ( ) \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n except Exception , ex : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = ex \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n else : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ login_required \n @ unique_session_or_json_redirect \n def deleteplan ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n if request . method != '<STR_LIT:POST>' : \n return HttpResponseNotAllowed ( [ '<STR_LIT:POST>' ] ) \n if not planid : \n return HttpResponseBadRequest ( _ ( '<STR_LIT>' ) ) \n plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n if plan . count ( ) == <NUM_LIT:1> : \n plan = plan [ <NUM_LIT:0> ] \n try : \n plan . delete ( ) \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n except Exception , ex : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = ex \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n else : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n @ login_required \n @ unique_session_or_json_redirect \n def reaggregateplan ( request , planid ) : \n \"\"\"<STR_LIT>\"\"\" \n note_session_activity ( request ) \n status = { '<STR_LIT:success>' : False } \n if request . method != '<STR_LIT:POST>' : \n return HttpResponseNotAllowed ( [ '<STR_LIT:POST>' ] ) \n if not planid : \n return HttpResponseBadRequest ( _ ( '<STR_LIT>' ) ) \n plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n if plan . count ( ) == <NUM_LIT:1> : \n plan = plan [ <NUM_LIT:0> ] \n try : \n reaggregate_plan . delay ( plan . id ) \n plan . processing_state = ProcessingState . REAGGREGATING \n plan . save ( ) \n status [ '<STR_LIT:success>' ] = True \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n except Exception , ex : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n status [ '<STR_LIT>' ] = ex \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n else : \n status [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n def get_health ( request ) : \n def num_users ( minutes ) : \n users = <NUM_LIT:0> \n for session in Session . objects . all ( ) : \n try : \n decoded = session . get_decoded ( ) \n except : \n session . delete ( ) \n continue \n if '<STR_LIT>' in decoded : \n activity_delta = decoded [ '<STR_LIT>' ] - timedelta ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , settings . SESSION_TIMEOUT ) \n if activity_delta > ( datetime . now ( ) - timedelta ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , minutes ) ) : \n users += <NUM_LIT:1> \n return users \n try : \n result = _ ( '<STR_LIT>' ) % { '<STR_LIT:time>' : datetime . now ( ) } \n result += _ ( '<STR_LIT>' ) % { '<STR_LIT>' : Plan . objects . all ( ) . count ( ) } \n result += _ ( '<STR_LIT>' ) % { '<STR_LIT>' : Session . objects . all ( ) . count ( ) , \n '<STR_LIT>' : settings . CONCURRENT_SESSIONS } \n result += _ ( '<STR_LIT>' ) % { '<STR_LIT>' : num_users ( <NUM_LIT:10> ) } \n space = os . statvfs ( '<STR_LIT>' ) \n result += _ ( '<STR_LIT>' ) % { '<STR_LIT>' : ( ( space . f_bsize * space . f_bavail ) / ( <NUM_LIT> * <NUM_LIT> ) ) } \n result += _ ( '<STR_LIT>' ) % { '<STR_LIT>' : commands . getoutput ( '<STR_LIT>' ) } \n return HttpResponse ( result , mimetype = '<STR_LIT>' ) \n except : \n return HttpResponse ( _ ( \"<STR_LIT>\" ) % traceback . format_exc ( ) ) \n def statistics_sets ( request , planid ) : \n result = { '<STR_LIT:success>' : False } \n plan = Plan . objects . filter ( id = planid ) \n if plan . count ( ) == <NUM_LIT:0> : \n result [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n return HttpResponse ( json . dumps ( result ) , mimetype = '<STR_LIT:application/json>' ) \n else : \n plan = plan [ <NUM_LIT:0> ] \n if request . method == '<STR_LIT:GET>' : \n sets = [ ] \n scorefunctions = [ ] \n user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by ( '<STR_LIT:name>' ) \n for f in user_functions : \n if '<STR_LIT>' not in f . name . lower ( ) and '<STR_LIT>' not in f . name . lower ( ) : \n scorefunctions . append ( { '<STR_LIT:id>' : f . id , '<STR_LIT:name>' : force_escape ( f . get_label ( ) ) } ) \n result [ '<STR_LIT>' ] = scorefunctions \n admin_display_names = [ \n \"<STR_LIT>\" % plan . legislative_body . name , \n ] \n if plan . legislative_body . is_community : \n admin_display_names . append ( \"<STR_LIT>\" % \n plan . legislative_body . name ) \n else : \n admin_display_names . append ( \"<STR_LIT>\" % \n plan . legislative_body . name ) \n admin_displays = ScoreDisplay . objects . filter ( \n owner__is_superuser = True , \n legislative_body = plan . legislative_body , \n name__in = admin_display_names \n ) \n for admin_display in admin_displays : \n sets . append ( { \n '<STR_LIT:id>' : admin_display . id , \n '<STR_LIT:name>' : force_escape ( admin_display . get_label ( ) ) , \n '<STR_LIT>' : [ ] , \n '<STR_LIT>' : False \n } ) \n try : \n user_displays = ScoreDisplay . objects . filter ( \n owner = request . user , \n legislative_body = plan . legislative_body , \n is_page = False ) . order_by ( '<STR_LIT:title>' ) \n result [ '<STR_LIT>' ] = len ( user_displays ) \n for display in user_displays : \n functions = [ ] \n for panel in display . scorepanel_set . all ( ) : \n if panel . type == '<STR_LIT>' : \n functions = map ( lambda x : x . id , panel . score_functions . all ( ) ) \n if len ( functions ) == <NUM_LIT:0> : \n result [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) % { '<STR_LIT>' : panel } \n sets . append ( { '<STR_LIT:id>' : display . id , '<STR_LIT:name>' : force_escape ( display . __unicode__ ( ) ) , '<STR_LIT>' : functions , '<STR_LIT>' : display . owner == request . user } ) \n except Exception , ex : \n result [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) % { '<STR_LIT:user>' : request . user } \n logger . warn ( '<STR_LIT>' ) \n logger . debug ( '<STR_LIT>' , ex ) \n result [ '<STR_LIT>' ] = sets \n result [ '<STR_LIT:success>' ] = True \n elif request . method == '<STR_LIT:POST>' and '<STR_LIT>' in request . POST : \n try : \n display = ScoreDisplay . objects . get ( pk = request . REQUEST . get ( '<STR_LIT:id>' , - <NUM_LIT:1> ) ) \n result [ '<STR_LIT>' ] = { '<STR_LIT:name>' : force_escape ( display . __unicode__ ( ) ) , '<STR_LIT:id>' : display . id } \n qset = display . scorepanel_set . all ( ) \n for panel in qset : \n if panel . displays . count ( ) == <NUM_LIT:1> : \n panel . delete ( ) \n display . delete ( ) \n result [ '<STR_LIT:success>' ] = True \n except Exception , ex : \n result [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n result [ '<STR_LIT>' ] = traceback . format_exc ( ) \n logger . warn ( \"<STR_LIT>\" ) \n logger . debug ( '<STR_LIT>' , ex ) \n elif request . method == '<STR_LIT:POST>' : \n def validate_num ( user , limit = <NUM_LIT:3> ) : \n return ScoreDisplay . objects . filter ( owner = user , legislative_body = plan . legislative_body , is_page = False ) . count ( ) < limit \n if '<STR_LIT>' in request . POST : \n functions = request . POST . getlist ( '<STR_LIT>' ) \n functions = map ( lambda x : int ( x ) , functions ) \n try : \n display = ScoreDisplay . objects . get ( title = request . POST . get ( '<STR_LIT:name>' ) , owner = request . user ) \n display = display . copy_from ( display = display , functions = functions ) \n except : \n limit = <NUM_LIT:3> \n if validate_num ( request . user , limit ) : \n demo = ScoreDisplay . objects . filter ( \n owner__is_superuser = True , \n legislative_body = plan . legislative_body , \n is_page = False , \n title = \"<STR_LIT>\" \n ) \n for disp in demo : \n has_comments = False \n for pnl in disp . scorepanel_set . all ( ) : \n for fn in pnl . score_functions . all ( ) : \n has_comments = has_comments or fn . calculator . endswith ( '<STR_LIT>' ) \n if not has_comments : \n demo = disp \n break \n display = ScoreDisplay ( ) \n display = display . copy_from ( display = demo , title = request . POST . get ( '<STR_LIT:name>' ) , owner = request . user , functions = functions ) \n result [ '<STR_LIT>' ] = True \n else : \n result [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) % { '<STR_LIT>' : limit } \n result [ '<STR_LIT:error>' ] = '<STR_LIT>' \n return HttpResponse ( json . dumps ( result ) , mimetype = '<STR_LIT:application/json>' ) \n result [ '<STR_LIT>' ] = { '<STR_LIT:name>' : force_escape ( display . __unicode__ ( ) ) , '<STR_LIT:id>' : display . id , '<STR_LIT>' : functions , '<STR_LIT>' : display . owner == request . user } \n result [ '<STR_LIT:success>' ] = True \n else : \n result [ '<STR_LIT:message>' ] = _ ( \"<STR_LIT>\" ) \n return HttpResponse ( json . dumps ( result ) , mimetype = '<STR_LIT:application/json>' ) \n def purge_plan_clear_cache ( district , version ) : \n \"\"\"<STR_LIT>\"\"\" \n district . plan . purge ( after = version ) \n district . plan . version = version \n district . plan . save ( ) \n cache = district . computeddistrictscore_set . filter ( function__calculator__endswith = '<STR_LIT>' ) \n cache . delete ( ) \n @ unique_session_or_json_redirect \n def district_info ( request , planid , district_id ) : \n \"\"\"<STR_LIT>\"\"\" \n status = { '<STR_LIT:success>' : False } \n plan = Plan . objects . filter ( id = planid ) \n if plan . count ( ) == <NUM_LIT:0> : \n status [ '<STR_LIT:message>' ] = _ ( '<STR_LIT>' ) \n else : \n plan = plan [ <NUM_LIT:0> ] \n version = plan . version \n if '<STR_LIT:version>' in request . REQUEST : \n try : \n version = int ( request . REQUEST [ '<STR_LIT:version>' ] ) \n version = min ( plan . version , int ( version ) ) \n except : \n pass \n district_id = int ( district_id ) \n district = plan . get_districts_at_version ( version , include_geom = False ) \n district = filter ( lambda d : d . district_id == district_id , district ) \n if request . method == '<STR_LIT:POST>' : \n district = plan . district_set . get ( id = request . POST [ '<STR_LIT>' ] ) \n district . short_label = request . POST [ '<STR_LIT>' ] [ <NUM_LIT:0> : <NUM_LIT:10> ] \n district . long_label = request . POST [ '<STR_LIT>' ] [ <NUM_LIT:0> : <NUM_LIT> ] \n if district . version < version : \n district_copy = copy . copy ( district ) \n district_copy . id = None \n district_copy . version = version \n district_copy . save ( ) \n district_copy . clone_relations_from ( district ) \n district = district_copy \n else : \n district . save ( ) \n has_comment = '<STR_LIT>' in request . POST and request . POST [ '<STR_LIT>' ] != '<STR_LIT>' \n if has_comment : \n ct = ContentType . objects . get ( app_label = '<STR_LIT>' , model = '<STR_LIT>' ) \n Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n comment = Comment ( \n object_pk = district . id , \n content_type = ct , \n site_id = Site . objects . get_current ( ) . id , \n user_name = request . user . username , \n user_email = request . user . email , \n comment = request . POST [ '<STR_LIT>' ] ) \n comment . save ( ) \n else : \n district . save ( ) \n tset = Tag . objects . get_for_object ( district ) . filter ( name__startswith = '<STR_LIT:type>' ) \n TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n purge_plan_clear_cache ( district , version ) \n if len ( request . REQUEST . getlist ( '<STR_LIT>' ) ) > <NUM_LIT:0> : \n strtags = request . REQUEST . getlist ( '<STR_LIT>' ) \n for strtag in strtags : \n if strtag == '<STR_LIT>' : \n continue \n if strtag . count ( '<STR_LIT:U+0020>' ) > <NUM_LIT:0> : \n strtag = '<STR_LIT>' % strtag \n else : \n strtag = '<STR_LIT>' % strtag \n Tag . objects . add_tag ( district , strtag ) \n status [ '<STR_LIT:version>' ] = version \n status [ '<STR_LIT:success>' ] = True \n return HttpResponse ( json . dumps ( status ) , mimetype = '<STR_LIT:application/json>' ) \n def plan_feed ( request ) : \n feed = loader . get_template ( '<STR_LIT>' ) \n plans = Plan . objects . all ( ) . order_by ( '<STR_LIT>' ) [ <NUM_LIT:0> : <NUM_LIT:10> ] \n geolevel = plans [ <NUM_LIT:0> ] . legislative_body . get_geolevels ( ) [ <NUM_LIT:0> ] \n extent = geolevel . geounit_set . collect ( ) . extent \n if extent [ <NUM_LIT:2> ] - extent [ <NUM_LIT:0> ] > extent [ <NUM_LIT:3> ] - extent [ <NUM_LIT:1> ] : \n width = <NUM_LIT> \n height = int ( <NUM_LIT> * ( extent [ <NUM_LIT:3> ] - extent [ <NUM_LIT:1> ] ) / ( extent [ <NUM_LIT:2> ] - extent [ <NUM_LIT:0> ] ) ) \n else : \n width = int ( <NUM_LIT> * ( extent [ <NUM_LIT:2> ] - extent [ <NUM_LIT:0> ] ) / ( extent [ <NUM_LIT:3> ] - extent [ <NUM_LIT:1> ] ) ) \n height = <NUM_LIT> \n mapserver = settings . MAP_SERVER if settings . MAP_SERVER != '<STR_LIT>' else request . META [ '<STR_LIT>' ] \n context = { \n '<STR_LIT>' : plans , \n '<STR_LIT>' : mapserver , \n '<STR_LIT>' : settings . MAP_SERVER_NS , \n '<STR_LIT>' : extent , \n '<STR_LIT:width>' : width , \n '<STR_LIT>' : height \n } \n xml = feed . render ( DjangoContext ( context ) ) \n return HttpResponse ( xml , mimetype = '<STR_LIT>' ) \n def share_feed ( request ) : \n feed = loader . get_template ( '<STR_LIT>' ) \n plans = Plan . objects . filter ( is_shared = True ) . order_by ( '<STR_LIT>' ) [ <NUM_LIT:0> : <NUM_LIT:10> ] \n if plans . count ( ) < <NUM_LIT:0> : \n geolevel = plans [ <NUM_LIT:0> ] . legislative_body . get_geolevels ( ) [ <NUM_LIT:0> ] \n extent = geolevel . geounit_set . collect ( ) . extent \n if extent [ <NUM_LIT:2> ] - extent [ <NUM_LIT:0> ] > extent [ <NUM_LIT:3> ] - extent [ <NUM_LIT:1> ] : \n width = <NUM_LIT> \n height = int ( <NUM_LIT> * ( extent [ <NUM_LIT:3> ] - extent [ <NUM_LIT:1> ] ) / ( extent [ <NUM_LIT:2> ] - extent [ <NUM_LIT:0> ] ) ) \n else : \n width = int ( <NUM_LIT> * ( extent [ <NUM_LIT:2> ] - extent [ <NUM_LIT:0> ] ) / ( extent [ <NUM_LIT:3> ] - extent [ <NUM_LIT:1> ] ) ) \n height = <NUM_LIT> \n else : \n extent = ( <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , <NUM_LIT:0> , ) \n width = <NUM_LIT:1> \n height = <NUM_LIT:1> \n mapserver = settings . MAP_SERVER if settings . MAP_SERVER != '<STR_LIT>' else request . META [ '<STR_LIT>' ] \n context = { \n '<STR_LIT>' : plans , \n '<STR_LIT>' : mapserver , \n '<STR_LIT>' : settings . MAP_SERVER_NS , \n '<STR_LIT>' : extent , \n '<STR_LIT:width>' : width , \n '<STR_LIT>' : height \n } \n xml = feed . render ( DjangoContext ( context ) ) \n return HttpResponse ( xml , <mask0> = '<STR_LIT>' ) \n", "gt": "mimetype"}
{"input": "\n url = '<STR_LIT>' \n from functools import ( partial , reduce , wraps , \n cmp_to_key ) \n from functools import ( partial , reduce , wraps , \n cmp_to_key ) \n a = <NUM_LIT:1> \n if a == None : \n <mask0> \n", "gt": "pass"}
{"input": "\n DSIZE = <NUM_LIT:4> \n SIZE = <NUM_LIT> \n a_offset = <NUM_LIT:1> * <NUM_LIT> * <NUM_LIT> \n b_offset = <NUM_LIT:2> * <NUM_LIT> * <NUM_LIT> \n iochannel = CoramIoChannel ( idx = <NUM_LIT:0> , datawidth = <NUM_LIT:32> ) \n mem0 = CoramMemory ( idx = <NUM_LIT:0> , datawidth = <NUM_LIT:8> * DSIZE , size = SIZE ) \n mem1 = CoramMemory ( idx = <NUM_LIT:1> , datawidth = <NUM_LIT:8> * DSIZE , size = SIZE ) \n mem2 = CoramMemory ( idx = <NUM_LIT:2> , datawidth = <NUM_LIT:8> * DSIZE , size = SIZE ) \n mem3 = CoramMemory ( idx = <NUM_LIT:3> , datawidth = <NUM_LIT:8> * DSIZE , size = SIZE ) \n mem_d0 = CoramMemory ( idx = <NUM_LIT:4> , datawidth = <NUM_LIT:8> * DSIZE , size = SIZE ) \n mem_d1 = CoramMemory ( idx = <NUM_LIT:5> , datawidth = <NUM_LIT:8> * DSIZE , size = SIZE ) \n channel = CoramChannel ( idx = <NUM_LIT:0> , datawidth = <NUM_LIT:8> * DSIZE ) \n def st_set_mesh_size ( mesh_size ) : \n channel . write ( mesh_size ) \n def st_step ( mesh_size , read_start , write_start ) : \n read_page = <NUM_LIT:3> \n write_page = <NUM_LIT:0> \n read_addr = read_start \n mem0 . write ( <NUM_LIT:0> , read_addr , mesh_size ) \n read_addr += mesh_size * DSIZE \n mem1 . write ( <NUM_LIT:0> , read_addr , mesh_size ) \n read_addr += mesh_size * DSIZE \n mem2 . write ( <NUM_LIT:0> , read_addr , mesh_size ) \n read_addr += mesh_size * DSIZE \n write_addr = write_start + mesh_size * DSIZE + DSIZE \n for i in range ( mesh_size - <NUM_LIT:2> ) : \n hot_spot = <NUM_LIT:1> if i == <NUM_LIT:0> else <NUM_LIT:0> \n pos = ( ( hot_spot << <NUM_LIT:6> ) | \n ( ( <NUM_LIT> << write_page ) << <NUM_LIT:4> ) | \n ( <NUM_LIT> << read_page ) ) \n mem0 . wait ( ) \n mem1 . wait ( ) \n mem2 . wait ( ) \n mem3 . wait ( ) \n channel . write ( pos ) \n if read_page == <NUM_LIT:0> : \n mem0 . write_nonblocking ( <NUM_LIT:0> , read_addr , mesh_size ) \n elif read_page == <NUM_LIT:1> : \n mem1 . write_nonblocking ( <NUM_LIT:0> , read_addr , mesh_size ) \n elif read_page == <NUM_LIT:2> : \n mem2 . write_nonblocking ( <NUM_LIT:0> , read_addr , mesh_size ) \n elif read_page == <NUM_LIT:3> : \n mem3 . write_nonblocking ( <NUM_LIT:0> , read_addr , mesh_size ) \n read_page = <NUM_LIT:0> if read_page == <NUM_LIT:3> else read_page + <NUM_LIT:1> \n read_addr += mesh_size * DSIZE \n channel . read ( ) \n mem_d0 . wait ( ) \n mem_d1 . wait ( ) \n if write_page == <NUM_LIT:0> : \n mem_d0 . read_nonblocking ( <NUM_LIT:1> , write_addr , mesh_size - <NUM_LIT:2> ) \n elif write_page == <NUM_LIT:1> : \n mem_d1 . read_nonblocking ( <NUM_LIT:1> , write_addr , mesh_size - <NUM_LIT:2> ) \n write_addr += mesh_size * DSIZE \n write_page = <NUM_LIT:0> if write_page == <NUM_LIT:1> else write_page + <NUM_LIT:1> \n mem_d0 . wait ( ) \n mem_d1 . wait ( ) \n def st_computation ( num_iter , mesh_size ) : \n for i in range ( num_iter / <NUM_LIT:2> ) : \n st_step ( mesh_size , a_offset , b_offset ) \n st_step ( mesh_size , b_offset , a_offset ) \n def st_sum ( mesh_size ) : \n check_sum = <NUM_LIT:0> \n read_addr = a_offset \n for i in range ( mesh_size ) : \n mem0 . write ( <NUM_LIT:0> , read_addr , mesh_size ) \n init_sum = <NUM_LIT:1> if i == <NUM_LIT:0> else <NUM_LIT:0> \n calc_sum = <NUM_LIT:1> \n pos = ( init_sum << <NUM_LIT:8> ) | ( calc_sum << <NUM_LIT:7> ) \n channel . write ( pos ) \n read_addr += mesh_size * DSIZE \n check_sum = channel . read ( ) \n channel . write ( <NUM_LIT:0> ) \n return check_sum \n def st_main ( ) : \n global a_offset \n global b_offset \n mesh_size = iochannel . read ( ) \n print ( \"<STR_LIT>\" % mesh_size ) \n num_iter = iochannel . read ( ) \n print ( \"<STR_LIT>\" % num_iter ) \n a_offset = iochannel . read ( ) \n print ( \"<STR_LIT>\" % a_offset ) \n b_offset = iochannel . read ( ) \n print ( \"<STR_LIT>\" % b_offset ) \n print ( \"<STR_LIT>\" ) \n st_set_mesh_size ( mesh_size ) \n print ( \"<STR_LIT>\" ) \n st_computation ( num_iter , mesh_size ) \n print ( \"<STR_LIT>\" ) \n check_sum = st_sum ( mesh_size ) \n iochannel . write ( check_sum ) \n while True : \n <mask0> ( ) \n", "gt": "st_main"}
{"input": "\n DSIZE = <NUM_LIT:4> \n SIZE = <NUM_LIT> \n a_offset = <NUM_LIT:1> * <NUM_LIT> * <NUM_LIT> \n b_offset = <NUM_LIT:2> * <NUM_LIT> * <NUM_LIT> \n iochannel = CoramIoChannel ( idx = <NUM_LIT:0> , datawidth = <NUM_LIT:32> ) \n mem0 = CoramMemory ( idx = <NUM_LIT:0> , datawidth = <NUM_LIT:8> * DSIZE , size = SIZE ) \n mem1 = CoramMemory ( idx = <NUM_LIT:1> , datawidth = <NUM_LIT:8> * DSIZE , size = SIZE ) \n mem2 = CoramMemory ( idx = <NUM_LIT:2> , datawidth = <NUM_LIT:8> * DSIZE , size = SIZE ) \n mem_d0 = CoramMemory ( idx = <NUM_LIT:4> , datawidth = <NUM_LIT:8> * DSIZE , size = SIZE ) \n channel = CoramChannel ( idx = <NUM_LIT:0> , datawidth = <NUM_LIT:8> * DSIZE ) \n def st_set_mesh_size ( mesh_size ) : \n channel . write ( mesh_size ) \n def st_step ( mesh_size , read_start , write_start ) : \n read_page = <NUM_LIT:0> \n read_addr = read_start \n mem0 . write ( <NUM_LIT:0> , read_addr , mesh_size ) \n read_addr += mesh_size * DSIZE \n mem1 . write ( <NUM_LIT:0> , read_addr , mesh_size ) \n read_addr += mesh_size * DSIZE \n mem2 . write ( <NUM_LIT:0> , read_addr , mesh_size ) \n read_addr += mesh_size * DSIZE \n write_addr = write_start + mesh_size * DSIZE + DSIZE \n for i in range ( mesh_size - <NUM_LIT:2> ) : \n hot_spot = <NUM_LIT:1> if i == <NUM_LIT:0> else <NUM_LIT:0> \n pos = hot_spot \n mem0 . wait ( ) \n mem1 . wait ( ) \n mem2 . wait ( ) \n channel . write ( pos ) \n channel . read ( ) \n if read_page == <NUM_LIT:0> : \n mem0 . write_nonblocking ( <NUM_LIT:0> , read_addr , mesh_size ) \n elif read_page == <NUM_LIT:1> : \n mem1 . write_nonblocking ( <NUM_LIT:0> , read_addr , mesh_size ) \n elif read_page == <NUM_LIT:2> : \n mem2 . write_nonblocking ( <NUM_LIT:0> , read_addr , mesh_size ) \n read_page = <NUM_LIT:0> if read_page == <NUM_LIT:2> else read_page + <NUM_LIT:1> \n read_addr += mesh_size * DSIZE \n mem_d0 . read_nonblocking ( <NUM_LIT:1> , write_addr , mesh_size - <NUM_LIT:2> ) \n write_addr += mesh_size * DSIZE \n mem_d0 . wait ( ) \n def st_computation ( num_iter , mesh_size ) : \n for i in range ( num_iter / <NUM_LIT:2> ) : \n st_step ( mesh_size , a_offset , b_offset ) \n st_step ( mesh_size , b_offset , a_offset ) \n def st_sum ( mesh_size ) : \n check_sum = <NUM_LIT:0> \n read_addr = a_offset \n for i in range ( mesh_size ) : \n mem0 . write ( <NUM_LIT:0> , read_addr , mesh_size ) \n init_sum = <NUM_LIT:1> if i == <NUM_LIT:0> else <NUM_LIT:0> \n calc_sum = <NUM_LIT:1> \n pos = ( init_sum << <NUM_LIT:2> ) | ( calc_sum << <NUM_LIT:1> ) \n channel . write ( pos ) \n read_addr += mesh_size * DSIZE \n check_sum = channel . read ( ) \n channel . write ( <NUM_LIT> ) \n return check_sum \n def st_main ( ) : \n global a_offset \n global b_offset \n mesh_size = iochannel . read ( ) \n print ( \"<STR_LIT>\" % mesh_size ) \n num_iter = iochannel . read ( ) \n print ( \"<STR_LIT>\" % num_iter ) \n a_offset = iochannel . read ( ) \n print ( \"<STR_LIT>\" % a_offset ) \n b_offset = iochannel . read ( ) \n print ( \"<STR_LIT>\" % b_offset ) \n print ( \"<STR_LIT>\" ) \n st_set_mesh_size ( mesh_size ) \n print ( \"<STR_LIT>\" ) \n st_computation ( num_iter , mesh_size ) \n print ( \"<STR_LIT>\" ) \n check_sum = st_sum ( mesh_size ) \n iochannel . write ( check_sum ) \n while True : \n <mask0> ( ) \n", "gt": "st_main"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import re \n import sys \n import os \n def getRamId ( oid , sid ) : \n if <NUM_LIT:0> <= sid and sid <= <NUM_LIT> : \n return <NUM_LIT:0> \n if <NUM_LIT:32> <= sid and sid <= <NUM_LIT> : \n return <NUM_LIT:1> \n if <NUM_LIT:64> <= sid and sid <= <NUM_LIT> : \n return <NUM_LIT:2> \n if <NUM_LIT> <= sid and sid <= <NUM_LIT> : \n return <NUM_LIT:3> \n def getRamSubId ( oid , sid ) : \n if <NUM_LIT:0> <= sid and sid <= <NUM_LIT> : \n return sid \n if <NUM_LIT:32> <= sid and sid <= <NUM_LIT> : \n return sid - <NUM_LIT:32> \n if <NUM_LIT:64> <= sid and sid <= <NUM_LIT> : \n return sid - <NUM_LIT:64> \n if <NUM_LIT> <= sid and sid <= <NUM_LIT> : \n return sid - <NUM_LIT> \n def getChannelId ( oid , sid ) : \n return oid \n def getChannelSubId ( oid , sid ) : \n return sid \n def getRegisterId ( oid , sid ) : \n return oid \n def getRegisterSubId ( oid , sid ) : \n return sid \n def main ( ) : \n f = open ( sys . argv [ <NUM_LIT:1> ] , '<STR_LIT:r>' ) \n lines = f . readlines ( ) \n output = [ ] \n p_thread = re . compile ( '<STR_LIT>' ) \n p_thread_id = re . compile ( '<STR_LIT>' ) \n p_object_id = re . compile ( '<STR_LIT>' ) \n p_width = re . compile ( '<STR_LIT>' ) \n p_depth = re . compile ( '<STR_LIT>' ) \n p_indexwidth = re . compile ( '<STR_LIT>' ) \n p_logdepth = re . compile ( '<STR_LIT>' ) \n p_sub_id = re . compile ( '<STR_LIT>' ) \n module_name = None \n thread_name = None \n thread_id = None \n object_id = None \n sub_id = None \n width = None \n indexwidth = None \n depth = None \n mode = False \n sub_id_num = None \n sub_id_base = None \n buffer = [ ] \n print ( \"<STR_LIT>\" ) \n for line in lines : \n if not mode : \n m = p_thread . match ( line ) \n if m : \n thread_name = re . match ( '<STR_LIT>' , m . group ( <NUM_LIT:2> ) ) . group ( <NUM_LIT:1> ) \n module_name = re . search ( '<STR_LIT>' , line ) . group ( <NUM_LIT:1> ) \n mode = True \n buffer = [ ] \n buffer . append ( line ) \n continue \n else : \n m = p_thread_id . match ( line ) \n if m : \n tid_str = m . group ( <NUM_LIT:2> ) [ <NUM_LIT:1> : - <NUM_LIT:1> ] \n thread_id = re . match ( '<STR_LIT>' , tid_str ) . group ( <NUM_LIT:2> ) \n buffer . append ( line ) \n continue \n m = p_object_id . match ( line ) \n if m : \n oid_str = m . group ( <NUM_LIT:2> ) [ <NUM_LIT:1> : - <NUM_LIT:1> ] \n object_id = re . match ( '<STR_LIT>' , oid_str ) . group ( <NUM_LIT:2> ) \n buffer . append ( line ) \n continue \n m = p_width . match ( line ) \n if m : \n width_str = m . group ( <NUM_LIT:2> ) \n width = re . match ( '<STR_LIT>' , width_str ) . group ( <NUM_LIT:1> ) \n buffer . append ( line ) \n continue \n m = p_depth . match ( line ) \n if m : \n depth_str = m . group ( <NUM_LIT:2> ) \n depth = re . match ( '<STR_LIT>' , depth_str ) . group ( <NUM_LIT:1> ) \n buffer . append ( line ) \n continue \n m = p_indexwidth . match ( line ) \n if m : \n indexwidth_str = m . group ( <NUM_LIT:2> ) \n indexwidth = re . match ( '<STR_LIT>' , indexwidth_str ) . group ( <NUM_LIT:1> ) \n buffer . append ( line ) \n continue \n m = p_logdepth . match ( line ) \n if m : \n logdepth_str = m . group ( <NUM_LIT:2> ) \n logdepth = re . match ( '<STR_LIT>' , logdepth_str ) . group ( <NUM_LIT:1> ) \n buffer . append ( line ) \n continue \n m = p_sub_id . match ( line ) \n if m : \n sid_str = m . group ( <NUM_LIT:2> ) \n sub_id_m = re . search ( '<STR_LIT>' , sid_str ) \n sub_id = sub_id_m . group ( <NUM_LIT:0> ) \n sub_id_num = sub_id_m . group ( <NUM_LIT:2> ) \n sub_id_base = ( <NUM_LIT:10> if sub_id_m . group ( <NUM_LIT:1> ) . count ( \"<STR_LIT>\" ) > <NUM_LIT:0> else \n <NUM_LIT:16> if sub_id_m . group ( <NUM_LIT:1> ) . count ( \"<STR_LIT>\" ) > <NUM_LIT:0> else \n <NUM_LIT:2> if sub_id_m . group ( <NUM_LIT:1> ) . count ( \"<STR_LIT>\" ) > <NUM_LIT:0> else \n <NUM_LIT:10> ) \n buffer . append ( line ) \n continue \n if mode : \n print ( \"<STR_LIT>\" % module_name ) \n print ( \"<STR_LIT>\" % '<STR_LIT>' . join ( ( thread_name [ : - <NUM_LIT:1> ] , '<STR_LIT:_>' , thread_id , '<STR_LIT:\">' ) ) ) \n print ( \"<STR_LIT>\" % thread_id ) \n if module_name . count ( '<STR_LIT>' ) > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % getRamId ( int ( object_id ) , int ( sub_id_num , sub_id_base ) ) ) \n if module_name . count ( '<STR_LIT>' ) > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % getChannelId ( int ( object_id ) , int ( sub_id_num , sub_id_base ) ) ) \n if module_name . count ( '<STR_LIT>' ) > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % getRegisterId ( int ( object_id ) , int ( sub_id_num , sub_id_base ) ) ) \n if module_name . count ( '<STR_LIT>' ) > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % getRamSubId ( int ( object_id ) , int ( sub_id_num , sub_id_base ) ) ) \n if module_name . count ( '<STR_LIT>' ) > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % '<STR_LIT:0>' ) \n if module_name . count ( '<STR_LIT>' ) > <NUM_LIT:0> : \n print ( \"<STR_LIT>\" % '<STR_LIT:0>' ) \n print ( \"<STR_LIT>\" % indexwidth ) \n print ( \"<STR_LIT>\" % width ) \n print ( \"<STR_LIT>\" % thread_name ) \n print ( '<STR_LIT>' . join ( buffer [ <NUM_LIT:1> : ] ) ) \n mode = False \n print ( line , end = '<STR_LIT>' ) \n <mask0> ( ) \n", "gt": "main"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import sys \n import os \n from optparse import OptionParser \n sys . path . insert ( <NUM_LIT:0> , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n import pyverilog . utils . version \n from pyverilog . dataflow . dataflow_analyzer import VerilogDataflowAnalyzer \n def main ( ) : \n INFO = \"<STR_LIT>\" \n VERSION = pyverilog . utils . version . VERSION \n USAGE = \"<STR_LIT>\" \n def showVersion ( ) : \n print ( INFO ) \n print ( VERSION ) \n print ( USAGE ) \n sys . exit ( ) \n optparser = OptionParser ( ) \n optparser . add_option ( \"<STR_LIT>\" , \"<STR_LIT>\" , action = \"<STR_LIT:store_true>\" , dest = \"<STR_LIT>\" , \n default = False , help = \"<STR_LIT>\" ) \n optparser . add_option ( \"<STR_LIT>\" , \"<STR_LIT>\" , dest = \"<STR_LIT>\" , action = \"<STR_LIT>\" , \n default = [ ] , help = \"<STR_LIT>\" ) \n optparser . add_option ( \"<STR_LIT>\" , dest = \"<STR_LIT>\" , action = \"<STR_LIT>\" , \n default = [ ] , help = \"<STR_LIT>\" ) \n optparser . add_option ( \"<STR_LIT>\" , \"<STR_LIT>\" , dest = \"<STR_LIT>\" , \n default = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n optparser . add_option ( \"<STR_LIT>\" , action = \"<STR_LIT:store_true>\" , dest = \"<STR_LIT>\" , \n default = False , help = \"<STR_LIT>\" ) \n optparser . add_option ( \"<STR_LIT>\" , action = \"<STR_LIT:store_true>\" , dest = \"<STR_LIT>\" , \n default = False , help = \"<STR_LIT>\" ) \n ( options , args ) = optparser . parse_args ( ) \n filelist = args \n if options . showversion : \n showVersion ( ) \n for f in filelist : \n if not os . path . exists ( f ) : raise IOError ( \"<STR_LIT>\" + f ) \n if len ( filelist ) == <NUM_LIT:0> : \n showVersion ( ) \n analyzer = VerilogDataflowAnalyzer ( filelist , options . topmodule , \n noreorder = options . noreorder , \n nobind = options . nobind , \n preprocess_include = options . include , \n preprocess_define = options . define ) \n analyzer . generate ( ) \n directives = analyzer . get_directives ( ) \n print ( '<STR_LIT>' ) \n for dr in sorted ( directives , key = lambda x : str ( x ) ) : \n print ( dr ) \n instances = analyzer . getInstances ( ) \n print ( '<STR_LIT>' ) \n for module , instname in sorted ( instances , key = lambda x : str ( x [ <NUM_LIT:1> ] ) ) : \n print ( ( module , instname ) ) \n if options . nobind : \n print ( '<STR_LIT>' ) \n signals = analyzer . getSignals ( ) \n for sig in signals : \n print ( sig ) \n print ( '<STR_LIT>' ) \n consts = analyzer . getConsts ( ) \n for con in consts : \n print ( con ) \n else : \n terms = analyzer . getTerms ( ) \n print ( '<STR_LIT>' ) \n for tk , tv in sorted ( terms . items ( ) , key = lambda x : str ( x [ <NUM_LIT:0> ] ) ) : \n print ( tv . tostr ( ) ) \n binddict = analyzer . getBinddict ( ) \n print ( '<STR_LIT>' ) \n for bk , bv in sorted ( binddict . items ( ) , key = lambda x : str ( x [ <NUM_LIT:0> ] ) ) : \n for bvi in bv : \n print ( bvi . tostr ( ) ) \n if __name__ == '<STR_LIT:__main__>' : \n <mask0> ( ) \n", "gt": "main"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import sys \n import os \n from pyverilog . dataflow . dataflow import * \n def replaceUndefined ( tree , termname ) : \n if tree is None : return DFTerminal ( termname ) \n if isinstance ( tree , DFUndefined ) : return DFTerminal ( termname ) \n if isinstance ( tree , DFConstant ) : return tree \n if isinstance ( tree , DFEvalValue ) : return tree \n if isinstance ( tree , DFTerminal ) : return tree \n if isinstance ( tree , DFBranch ) : \n condnode = replaceUndefined ( tree . condnode , termname ) \n truenode = replaceUndefined ( tree . truenode , termname ) \n falsenode = replaceUndefined ( tree . falsenode , termname ) \n return DFBranch ( condnode , truenode , falsenode ) \n if isinstance ( tree , DFOperator ) : \n nextnodes = [ ] \n for n in tree . nextnodes : \n nextnodes . append ( replaceUndefined ( n , termname ) ) \n return DFOperator ( tuple ( nextnodes ) , tree . operator ) \n if isinstance ( tree , DFPartselect ) : \n msb = replaceUndefined ( tree . msb , termname ) \n lsb = replaceUndefined ( tree . lsb , termname ) \n var = replaceUndefined ( tree . var , termname ) \n return DFPartselect ( var , msb , lsb ) \n if isinstance ( tree , DFPointer ) : \n ptr = replaceUndefined ( tree . ptr , termname ) \n var = replaceUndefined ( tree . var , termname ) \n return DFPointer ( var , ptr ) \n if isinstance ( tree , DFConcat ) : \n nextnodes = [ ] \n for n in tree . nextnodes : \n nextnodes . append ( replaceUndefined ( n , termname ) ) \n return DFConcat ( tuple ( nextnodes ) ) \n raise DefinitionError ( '<STR_LIT>' % ( str ( type ( tree ) ) , str ( <mask0> ) ) ) \n", "gt": "tree"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import os \n import sys \n from pyverilog . dataflow . dataflow_analyzer import VerilogDataflowAnalyzer \n from pyverilog . dataflow . optimizer import VerilogDataflowOptimizer \n from pyverilog . controlflow . controlflow_analyzer import VerilogControlflowAnalyzer \n codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + '<STR_LIT>' \n expected = \"\"\"<STR_LIT>\"\"\" \n def test ( ) : \n filelist = [ codedir + '<STR_LIT>' ] \n topmodule = '<STR_LIT>' \n noreorder = False \n nobind = False \n include = None \n define = None \n analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n noreorder = noreorder , \n nobind = nobind , \n preprocess_include = include , \n preprocess_define = define ) \n analyzer . generate ( ) \n directives = analyzer . get_directives ( ) \n instances = analyzer . getInstances ( ) \n terms = analyzer . getTerms ( ) \n binddict = analyzer . getBinddict ( ) \n optimizer = VerilogDataflowOptimizer ( terms , binddict ) \n optimizer . resolveConstant ( ) \n c_analyzer = VerilogControlflowAnalyzer ( topmodule , terms , \n binddict , \n resolved_terms = optimizer . getResolvedTerms ( ) , \n resolved_binddict = optimizer . getResolvedBinddict ( ) , \n constlist = optimizer . getConstlist ( ) \n ) \n output = [ ] \n for tk in sorted ( c_analyzer . resolved_terms . keys ( ) , key = lambda x : str ( x ) ) : \n tree = c_analyzer . makeTree ( tk ) \n output . append ( str ( tk ) + '<STR_LIT>' + tree . tocode ( ) ) \n rslt = '<STR_LIT:\\n>' . join ( output ) + '<STR_LIT:\\n>' \n print ( rslt ) \n assert ( expected == rslt ) \n if __name__ == '<STR_LIT:__main__>' : \n <mask0> ( ) \n", "gt": "test"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import dataflow_example \n expected_verilog = \"\"\"<STR_LIT>\"\"\" \n def test ( ) : \n test_module = dataflow_example . mkTest ( ) \n code = test_module . to_verilog ( ) \n from pyverilog . vparser . parser import VerilogParser \n from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n parser = VerilogParser ( ) \n expected_ast = parser . parse ( expected_verilog ) \n codegen = ASTCodeGenerator ( ) \n expected_code = codegen . visit ( expected_ast ) \n assert ( expected_code == <mask0> ) \n", "gt": "code"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import sys \n import os \n sys . path . insert ( <NUM_LIT:0> , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) \n from veriloggen import * \n def mkLed ( ) : \n m = Module ( '<STR_LIT>' ) \n interval = m . Parameter ( '<STR_LIT>' , <NUM_LIT:16> ) \n clk = m . Input ( '<STR_LIT>' ) \n rst = m . Input ( '<STR_LIT>' ) \n led = m . OutputReg ( '<STR_LIT>' , <NUM_LIT:8> , initval = <NUM_LIT:0> ) \n count = m . Reg ( '<STR_LIT:count>' , <NUM_LIT:32> , initval = <NUM_LIT:0> ) \n seq = Seq ( m , '<STR_LIT>' , clk , rst ) \n seq . add ( Systask ( '<STR_LIT>' , '<STR_LIT>' , led , count ) ) \n seq . add ( count ( count + <NUM_LIT:1> ) , cond = count < interval - <NUM_LIT:1> ) \n seq . add ( count ( <NUM_LIT:0> ) , cond = count == interval - <NUM_LIT:1> ) \n seq . add ( led ( led + <NUM_LIT:1> ) , cond = count == interval - <NUM_LIT:1> ) \n seq . make_always ( ) \n return m \n def mkTest ( ) : \n m = Module ( '<STR_LIT:test>' ) \n led = mkLed ( ) \n params = m . copy_params ( led ) \n ports = m . copy_sim_ports ( led ) \n clk = ports [ '<STR_LIT>' ] \n rst = ports [ '<STR_LIT>' ] \n uut = m . Instance ( led , '<STR_LIT>' , \n params = m . connect_params ( led ) , \n ports = m . connect_ports ( led ) ) \n simulation . setup_clock ( m , clk , hperiod = <NUM_LIT:5> ) \n init = simulation . setup_reset ( m , rst , m . make_reset ( ) , period = <NUM_LIT:100> ) \n init . add ( \n Delay ( <NUM_LIT:1000> ) , \n Systask ( '<STR_LIT>' ) , \n ) \n return m \n if __name__ == '<STR_LIT:__main__>' : \n test = mkTest ( ) \n verilog = test . to_verilog ( ) \n print ( <mask0> ) \n", "gt": "verilog"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import sys \n import os \n sys . path . insert ( <NUM_LIT:0> , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) ) \n from veriloggen import * \n def mkLed ( ) : \n m = Module ( '<STR_LIT>' ) \n width = m . Parameter ( '<STR_LIT>' , <NUM_LIT:8> ) \n clk = m . Input ( '<STR_LIT>' ) \n rst = m . Input ( '<STR_LIT>' ) \n led = m . OutputReg ( '<STR_LIT>' , width ) \n count = m . Reg ( '<STR_LIT:count>' , <NUM_LIT:32> ) \n m . Always ( Posedge ( clk ) ) ( \n If ( rst ) ( \n count ( <NUM_LIT:0> ) \n ) . Else ( \n count ( Cond ( count == <NUM_LIT> , <NUM_LIT:0> , count + <NUM_LIT:1> ) ) \n ) ) \n m . Always ( Posedge ( clk ) ) ( \n If ( rst ) ( \n led ( <NUM_LIT:0> ) \n ) . Else ( \n led ( Cond ( count == <NUM_LIT> - <NUM_LIT:1> , led + <NUM_LIT:1> , led ) ) \n ) ) \n return m \n if __name__ == '<STR_LIT:__main__>' : \n led = mkLed ( ) \n verilog = led . to_verilog ( '<STR_LIT>' ) \n print ( <mask0> ) \n", "gt": "verilog"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import sys \n import os \n sys . path . insert ( <NUM_LIT:0> , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) ) ) \n from veriloggen import * \n def mkSub ( ) : \n m = Module ( '<STR_LIT>' ) \n clk = m . Input ( '<STR_LIT>' ) \n rst = m . Input ( '<STR_LIT>' ) \n count = m . OutputReg ( '<STR_LIT:count>' , <NUM_LIT:32> ) \n m . Always ( Posedge ( clk ) ) ( \n If ( rst ) ( \n count ( <NUM_LIT:0> ) \n ) . Else ( \n If ( count == <NUM_LIT> ) ( \n count ( <NUM_LIT:0> ) \n ) . Else ( \n count ( count + <NUM_LIT:1> ) \n ) \n ) ) \n return m \n def mkLed ( ) : \n m = Module ( '<STR_LIT>' ) \n width = m . Parameter ( '<STR_LIT>' , <NUM_LIT:8> ) \n clk = m . Input ( '<STR_LIT>' ) \n rst = m . Input ( '<STR_LIT>' ) \n led = m . OutputReg ( '<STR_LIT>' , width ) \n count = m . Wire ( '<STR_LIT:count>' , <NUM_LIT:32> ) \n sub = mkSub ( ) \n m . Instance ( sub , '<STR_LIT>' , m . connect_params ( sub ) , m . connect_ports ( sub ) ) \n m . Always ( Posedge ( clk ) ) ( \n If ( rst ) ( \n led ( <NUM_LIT:0> ) \n ) . Else ( \n If ( count == <NUM_LIT> ) ( \n led ( led + <NUM_LIT:1> ) \n ) \n ) ) \n inst_sub = m . Reg ( '<STR_LIT>' , <NUM_LIT:32> ) \n return m \n if __name__ == '<STR_LIT:__main__>' : \n try : \n led = mkLed ( ) \n except ValueError as e : \n print ( e . args [ <NUM_LIT:0> ] ) \n print ( '<STR_LIT>' ) \n sys . exit ( ) \n raise <mask0> ( \"<STR_LIT>\" ) \n", "gt": "ValueError"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import sys \n import os \n sys . path . insert ( <NUM_LIT:0> , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) ) \n from veriloggen import * \n def mkLed ( ) : \n m = Module ( '<STR_LIT>' ) \n width = m . Parameter ( '<STR_LIT>' , <NUM_LIT:8> ) \n clk = m . Input ( '<STR_LIT>' ) \n rst = m . Input ( '<STR_LIT>' ) \n led = m . OutputReg ( '<STR_LIT>' , width ) \n count = m . Reg ( '<STR_LIT:count>' , <NUM_LIT:32> ) \n m . Always ( Posedge ( clk ) ) ( \n If ( rst ) ( \n count ( <NUM_LIT:0> ) \n ) . Else ( \n If ( count == <NUM_LIT> ) ( \n count ( <NUM_LIT:0> ) \n ) . Else ( \n count ( count + <NUM_LIT:1> ) \n ) \n ) ) \n m . Always ( Posedge ( clk ) ) ( \n If ( rst ) ( \n led ( <NUM_LIT:0> ) \n ) . Else ( \n If ( count == <NUM_LIT> - <NUM_LIT:1> ) ( \n led ( led + <NUM_LIT:1> ) , \n SingleStatement ( SystemTask ( '<STR_LIT>' , '<STR_LIT>' , led ) ) \n ) \n ) ) \n return m \n if __name__ == '<STR_LIT:__main__>' : \n led = mkLed ( ) \n verilog = led . to_verilog ( ) \n print ( <mask0> ) \n", "gt": "verilog"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import sys \n import os \n sys . path . insert ( <NUM_LIT:0> , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) ) ) \n from veriloggen import * \n import veriloggen . dataflow as dataflow \n def mkMain ( ) : \n x = dataflow . Variable ( '<STR_LIT>' , valid = '<STR_LIT>' , ready = '<STR_LIT>' , point = <NUM_LIT:8> ) \n y = dataflow . Variable ( '<STR_LIT>' , valid = '<STR_LIT>' , ready = '<STR_LIT>' , point = <NUM_LIT:4> ) \n z = x * y \n z . output ( '<STR_LIT>' , valid = '<STR_LIT>' , ready = '<STR_LIT>' ) \n df = dataflow . Dataflow ( z ) \n m = df . to_module ( '<STR_LIT>' ) \n return m \n def mkTest ( ) : \n m = Module ( '<STR_LIT:test>' ) \n main = mkMain ( ) \n params = m . copy_params ( main ) \n ports = m . copy_sim_ports ( main ) \n clk = ports [ '<STR_LIT>' ] \n rst = ports [ '<STR_LIT>' ] \n xdata = ports [ '<STR_LIT>' ] \n xvalid = ports [ '<STR_LIT>' ] \n xready = ports [ '<STR_LIT>' ] \n ydata = ports [ '<STR_LIT>' ] \n yvalid = ports [ '<STR_LIT>' ] \n yready = ports [ '<STR_LIT>' ] \n zdata = ports [ '<STR_LIT>' ] \n zvalid = ports [ '<STR_LIT>' ] \n zready = ports [ '<STR_LIT>' ] \n xdata_orig = m . RegLike ( ports [ '<STR_LIT>' ] , name = '<STR_LIT>' , initval = <NUM_LIT:0> ) \n ydata_orig = m . RegLike ( ports [ '<STR_LIT>' ] , name = '<STR_LIT>' , initval = <NUM_LIT:0> ) \n zdata_orig = m . WireLike ( ports [ '<STR_LIT>' ] , name = '<STR_LIT>' ) \n m . Always ( ) ( xdata ( fixed . to_fixed ( xdata_orig , <NUM_LIT:8> ) ) ) \n m . Always ( ) ( ydata ( fixed . to_fixed ( ydata_orig , <NUM_LIT:4> ) ) ) \n m . Assign ( zdata_orig ( fixed . fixed_to_int ( zdata , <NUM_LIT:8> ) ) ) \n uut = m . Instance ( main , '<STR_LIT>' , \n params = m . connect_params ( main ) , \n ports = m . connect_ports ( main ) ) \n reset_done = m . Reg ( '<STR_LIT>' , initval = <NUM_LIT:0> ) \n reset_stmt = [ ] \n reset_stmt . append ( reset_done ( <NUM_LIT:0> ) ) \n reset_stmt . append ( xdata ( <NUM_LIT:0> ) ) \n reset_stmt . append ( xvalid ( <NUM_LIT:0> ) ) \n reset_stmt . append ( ydata ( <NUM_LIT:0> ) ) \n reset_stmt . append ( yvalid ( <NUM_LIT:0> ) ) \n reset_stmt . append ( zready ( <NUM_LIT:0> ) ) \n reset_stmt . append ( xdata_orig ( <NUM_LIT:0> ) ) \n reset_stmt . append ( ydata_orig ( <NUM_LIT:0> ) ) \n simulation . setup_waveform ( m , uut , xdata_orig , ydata_orig , zdata_orig ) \n simulation . setup_clock ( m , clk , hperiod = <NUM_LIT:5> ) \n init = simulation . setup_reset ( m , rst , reset_stmt , period = <NUM_LIT:100> ) \n nclk = simulation . next_clock \n init . add ( \n Delay ( <NUM_LIT:1000> ) , \n reset_done ( <NUM_LIT:1> ) , \n nclk ( clk ) , \n Delay ( <NUM_LIT> ) , \n Systask ( '<STR_LIT>' ) , \n ) \n def send ( name , data , valid , ready , step = <NUM_LIT:1> , waitnum = <NUM_LIT:10> ) : \n fsm = FSM ( m , name + '<STR_LIT>' , clk , rst ) \n count = m . TmpReg ( <NUM_LIT:32> , initval = <NUM_LIT:0> ) \n fsm . add ( valid ( <NUM_LIT:0> ) ) \n fsm . goto_next ( cond = reset_done ) \n for _ in range ( waitnum ) : \n fsm . goto_next ( ) \n fsm . add ( valid ( <NUM_LIT:1> ) ) \n fsm . goto_next ( ) \n fsm . add ( data ( data + step ) , cond = ready ) \n fsm . add ( count . inc ( ) , cond = ready ) \n fsm . add ( valid ( <NUM_LIT:0> ) , cond = AndList ( count == <NUM_LIT:5> , ready ) ) \n fsm . goto_next ( cond = AndList ( count == <NUM_LIT:5> , ready ) ) \n for _ in range ( waitnum ) : \n fsm . goto_next ( ) \n fsm . add ( valid ( <NUM_LIT:1> ) ) \n fsm . add ( data ( data + step ) , cond = ready ) \n fsm . add ( count . inc ( ) , cond = ready ) \n fsm . add ( valid ( <NUM_LIT:0> ) , cond = AndList ( count == <NUM_LIT:10> , ready ) ) \n fsm . goto_next ( cond = AndList ( count == <NUM_LIT:10> , ready ) ) \n fsm . make_always ( ) \n def receive ( name , data , valid , ready , waitnum = <NUM_LIT:10> ) : \n fsm = FSM ( m , name + '<STR_LIT>' , clk , rst ) \n fsm . add ( ready ( <NUM_LIT:0> ) ) \n fsm . goto_next ( cond = reset_done ) \n fsm . goto_next ( ) \n yinit = fsm . current ( ) \n fsm . add ( ready ( <NUM_LIT:1> ) , cond = valid ) \n fsm . goto_next ( cond = valid ) \n for i in range ( waitnum ) : \n fsm . add ( ready ( <NUM_LIT:0> ) ) \n fsm . goto_next ( ) \n fsm . goto ( yinit ) \n fsm . make_always ( ) \n send ( '<STR_LIT:x>' , xdata_orig , xvalid , xready , step = <NUM_LIT:1> , waitnum = <NUM_LIT:10> ) \n send ( '<STR_LIT:y>' , ydata_orig , yvalid , yready , step = <NUM_LIT:1> , waitnum = <NUM_LIT:20> ) \n receive ( '<STR_LIT:z>' , zdata , zvalid , zready , waitnum = <NUM_LIT:50> ) \n m . Always ( Posedge ( clk ) ) ( \n If ( reset_done ) ( \n If ( AndList ( xvalid , xready ) ) ( \n Systask ( '<STR_LIT>' , '<STR_LIT>' , xdata_orig ) \n ) , \n If ( AndList ( yvalid , yready ) ) ( \n Systask ( '<STR_LIT>' , '<STR_LIT>' , ydata_orig ) \n ) , \n If ( AndList ( zvalid , zready ) ) ( \n Systask ( '<STR_LIT>' , '<STR_LIT>' , zdata_orig ) \n ) \n ) \n ) \n return m \n if __name__ == '<STR_LIT:__main__>' : \n test = mkTest ( ) \n verilog = test . to_verilog ( '<STR_LIT>' ) \n print ( verilog ) \n sim = simulation . Simulator ( test ) \n rslt = sim . run ( ) \n print ( <mask0> ) \n", "gt": "rslt"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import dataflow_mul \n expected_verilog = \"\"\"<STR_LIT>\"\"\" \n def test ( ) : \n test_module = dataflow_mul . mkTest ( ) \n code = test_module . to_verilog ( ) \n from pyverilog . vparser . parser import VerilogParser \n from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n parser = VerilogParser ( ) \n expected_ast = parser . parse ( expected_verilog ) \n codegen = ASTCodeGenerator ( ) \n expected_code = codegen . visit ( expected_ast ) \n assert ( expected_code == <mask0> ) \n", "gt": "code"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import sys \n import os \n sys . path . insert ( <NUM_LIT:0> , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) ) ) \n from veriloggen import * \n def mkLed ( ) : \n m = Module ( '<STR_LIT>' ) \n clk = m . Input ( '<STR_LIT>' ) \n rst = m . Input ( '<STR_LIT>' ) \n valid = m . OutputReg ( '<STR_LIT>' , initval = <NUM_LIT:0> ) \n count = m . Reg ( '<STR_LIT:count>' , width = <NUM_LIT:32> , initval = <NUM_LIT:0> ) \n up = m . Wire ( '<STR_LIT>' ) \n down = m . Wire ( '<STR_LIT>' ) \n m . Assign ( up ( <NUM_LIT:1> ) ) \n m . Assign ( down ( <NUM_LIT:0> ) ) \n fsm = FSM ( m , '<STR_LIT>' , clk , rst ) \n for i in range ( <NUM_LIT:4> ) : \n fsm . goto_next ( ) \n c = count >= <NUM_LIT:16> \n fsm . add ( valid ( up ) , cond = c , keep = <NUM_LIT:3> , eager_val = True , lazy_cond = True ) \n fsm . add ( valid ( down ) , cond = c , delay = <NUM_LIT:3> , eager_val = True , lazy_cond = True ) \n fsm . goto_next ( cond = c ) \n for i in range ( <NUM_LIT:8> ) : \n fsm . goto_next ( ) \n c = count >= <NUM_LIT:32> \n for i in range ( <NUM_LIT:8> ) : \n fsm . add ( valid ( up ) , cond = c , delay = <NUM_LIT:1> , keep = <NUM_LIT:3> , eager_val = True , lazy_cond = True ) \n fsm . add ( valid ( down ) , cond = c , delay = <NUM_LIT:4> , eager_val = True , lazy_cond = True ) \n fsm . goto_next ( cond = c ) \n fsm . make_always ( reset = [ count . reset ( ) ] , body = [ count ( count + <NUM_LIT:1> ) ] ) \n return m \n def mkTest ( ) : \n m = Module ( '<STR_LIT:test>' ) \n clk = m . Reg ( '<STR_LIT>' ) \n rst = m . Reg ( '<STR_LIT>' ) \n valid = m . Wire ( '<STR_LIT>' ) \n uut = m . Instance ( mkLed ( ) , '<STR_LIT>' , \n ports = ( ( '<STR_LIT>' , clk ) , ( '<STR_LIT>' , rst ) , ( '<STR_LIT>' , valid ) ) ) \n simulation . setup_waveform ( m , uut ) \n simulation . setup_clock ( m , clk , hperiod = <NUM_LIT:5> ) \n init = simulation . setup_reset ( m , rst , period = <NUM_LIT:100> ) \n init . add ( \n Delay ( <NUM_LIT:1000> ) , \n Systask ( '<STR_LIT>' ) , \n ) \n return m \n if __name__ == '<STR_LIT:__main__>' : \n test = mkTest ( ) \n verilog = test . to_verilog ( '<STR_LIT>' ) \n print ( <mask0> ) \n", "gt": "verilog"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import pipeline_draw_graph \n expected_verilog = \"\"\"<STR_LIT>\"\"\" \n def test ( ) : \n test_module = pipeline_draw_graph . mkTest ( ) \n code = test_module . to_verilog ( ) \n from pyverilog . vparser . parser import VerilogParser \n from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n parser = VerilogParser ( ) \n expected_ast = parser . parse ( expected_verilog ) \n codegen = ASTCodeGenerator ( ) \n expected_code = codegen . visit ( expected_ast ) \n assert ( expected_code == <mask0> ) \n", "gt": "code"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import sys \n import os \n sys . path . insert ( <NUM_LIT:0> , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) ) ) \n from veriloggen import * \n def mkLed ( ) : \n m = Module ( '<STR_LIT>' ) \n interval = m . Parameter ( '<STR_LIT>' , <NUM_LIT:16> ) \n clk = m . Input ( '<STR_LIT>' ) \n rst = m . Input ( '<STR_LIT>' ) \n led = m . OutputReg ( '<STR_LIT>' , <NUM_LIT:8> , initval = <NUM_LIT:0> ) \n count = m . Reg ( '<STR_LIT:count>' , <NUM_LIT:32> , initval = <NUM_LIT:0> ) \n seq = Seq ( m , '<STR_LIT>' , clk , rst ) \n seq . add ( Systask ( '<STR_LIT>' , '<STR_LIT>' , led , count ) ) \n seq . add ( count ( count + <NUM_LIT:1> ) , cond = count < interval - <NUM_LIT:1> ) \n seq . add ( count ( <NUM_LIT:0> ) , cond = count == interval - <NUM_LIT:1> ) \n seq . add ( led ( led + <NUM_LIT:1> ) , cond = count == interval - <NUM_LIT:1> ) \n seq . make_always ( ) \n return m \n def mkTest ( ) : \n m = Module ( '<STR_LIT:test>' ) \n led = mkLed ( ) \n params = m . copy_params ( led ) \n ports = m . copy_sim_ports ( led ) \n clk = ports [ '<STR_LIT>' ] \n rst = ports [ '<STR_LIT>' ] \n uut = m . Instance ( led , '<STR_LIT>' , \n params = m . connect_params ( led ) , \n ports = m . connect_ports ( led ) ) \n simulation . setup_clock ( m , clk , hperiod = <NUM_LIT:5> ) \n init = simulation . setup_reset ( m , rst , m . make_reset ( ) , period = <NUM_LIT:100> ) \n init . add ( \n Delay ( <NUM_LIT:1000> ) , \n Systask ( '<STR_LIT>' ) , \n ) \n return m \n if __name__ == '<STR_LIT:__main__>' : \n test = mkTest ( ) \n verilog = test . to_verilog ( ) \n print ( <mask0> ) \n", "gt": "verilog"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import read_verilog_module_str \n expected_verilog = \"\"\"<STR_LIT>\"\"\" \n def test ( ) : \n test_module = read_verilog_module_str . mkTop ( ) \n code = test_module . to_verilog ( ) \n from pyverilog . vparser . parser import VerilogParser \n from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n parser = VerilogParser ( ) \n expected_ast = parser . parse ( expected_verilog ) \n codegen = ASTCodeGenerator ( ) \n expected_code = codegen . visit ( expected_ast ) \n assert ( expected_code == <mask0> ) \n", "gt": "code"}
{"input": "\n from __future__ import absolute_import \n from __future__ import print_function \n import veriloggen . core . vtypes as vtypes \n import veriloggen . core . module as module \n def mkMultiplierCore ( index , lwidth = <NUM_LIT:32> , rwidth = <NUM_LIT:32> , lsigned = True , rsigned = True , depth = <NUM_LIT:6> ) : \n retwidth = lwidth + rwidth \n m = module . Module ( '<STR_LIT>' % index ) \n clk = m . Input ( '<STR_LIT>' ) \n update = m . Input ( '<STR_LIT>' ) \n a = m . Input ( '<STR_LIT:a>' , lwidth ) \n b = m . Input ( '<STR_LIT:b>' , rwidth ) \n c = m . Output ( '<STR_LIT:c>' , retwidth ) \n _a = m . Reg ( '<STR_LIT>' , lwidth , signed = lsigned ) \n _b = m . Reg ( '<STR_LIT>' , rwidth , signed = rsigned ) \n tmpval = [ m . Reg ( '<STR_LIT>' % i , retwidth , signed = True ) for i in range ( depth - <NUM_LIT:1> ) ] \n rslt = m . Wire ( '<STR_LIT>' , retwidth , signed = True ) \n __a = _a \n __b = _b \n if not lsigned : \n __a = vtypes . SystemTask ( '<STR_LIT>' , vtypes . Cat ( vtypes . Int ( <NUM_LIT:0> , width = <NUM_LIT:1> ) , _a ) ) \n if not rsigned : \n __b = vtypes . SystemTask ( '<STR_LIT>' , vtypes . Cat ( vtypes . Int ( <NUM_LIT:0> , width = <NUM_LIT:1> ) , _b ) ) \n m . Assign ( rslt ( __a * __b ) ) \n m . Assign ( c ( tmpval [ depth - <NUM_LIT:2> ] ) ) \n m . Always ( vtypes . Posedge ( clk ) ) ( \n vtypes . If ( update ) ( \n _a ( a ) , \n _b ( b ) , \n tmpval [ <NUM_LIT:0> ] ( rslt ) , \n [ tmpval [ i ] ( tmpval [ i - <NUM_LIT:1> ] ) for i in range ( <NUM_LIT:1> , depth - <NUM_LIT:1> ) ] \n ) ) \n return m \n def mkMultiplier ( index , lwidth = <NUM_LIT:32> , rwidth = <NUM_LIT:32> , lsigned = True , rsigned = True , depth = <NUM_LIT:6> ) : \n if lwidth < <NUM_LIT:0> : raise ValueError ( \"<STR_LIT>\" ) \n if rwidth < <NUM_LIT:0> : raise ValueError ( \"<STR_LIT>\" ) \n if depth < <NUM_LIT:2> : raise ValueError ( \"<STR_LIT>\" ) \n retwidth = lwidth + rwidth \n mult = mkMultiplierCore ( index , lwidth , rwidth , lsigned , rsigned , depth ) \n m = module . Module ( '<STR_LIT>' % index ) \n clk = m . Input ( '<STR_LIT>' ) \n rst = m . Input ( '<STR_LIT>' ) \n update = m . Input ( '<STR_LIT>' ) \n enable = m . Input ( '<STR_LIT>' ) \n valid = m . Output ( '<STR_LIT>' ) \n a = m . Input ( '<STR_LIT:a>' , lwidth ) \n b = m . Input ( '<STR_LIT:b>' , rwidth ) \n c = m . Output ( '<STR_LIT:c>' , retwidth ) \n valid_reg = [ m . Reg ( '<STR_LIT>' % i ) for i in range ( depth ) ] \n m . Assign ( valid ( valid_reg [ depth - <NUM_LIT:1> ] ) ) \n m . Always ( vtypes . Posedge ( clk ) ) ( \n vtypes . If ( rst ) ( \n [ valid_reg [ i ] ( <NUM_LIT:0> ) for i in range ( depth ) ] \n ) . Else ( \n vtypes . If ( update ) ( \n valid_reg [ <NUM_LIT:0> ] ( enable ) , \n [ valid_reg [ i ] ( valid_reg [ i - <NUM_LIT:1> ] ) for i in range ( <NUM_LIT:1> , depth ) ] \n ) \n ) ) \n ports = [ ( '<STR_LIT>' , clk ) , ( '<STR_LIT>' , update ) , ( '<STR_LIT:a>' , a ) , ( '<STR_LIT:b>' , b ) , ( '<STR_LIT:c>' , c ) ] \n m . Instance ( mult , '<STR_LIT>' , ports = ports ) \n return m \n index_count = <NUM_LIT:0> \n def get_mul ( lwidth = <NUM_LIT:32> , rwidth = <NUM_LIT:32> , lsigned = True , rsigned = True , depth = <NUM_LIT:6> ) : \n global index_count \n mul = mkMultiplier ( index_count , lwidth , rwidth , lsigned , rsigned , depth ) \n index_count += <NUM_LIT:1> \n return mul \n def reset ( ) : \n global index_count \n <mask0> = <NUM_LIT:0> \n", "gt": "index_count"}
{"input": "\n from pymysql import OperationalError , Warning \n from pymysql . tests import base \n import os \n import warnings \n __all__ = [ \"<STR_LIT>\" ] \n class TestLoadLocal ( base . PyMySQLTestCase ) : \n def test_no_file ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n conn = self . connections [ <NUM_LIT:0> ] \n c = conn . cursor ( ) \n c . execute ( \"<STR_LIT>\" ) \n try : \n self . assertRaises ( \n OperationalError , \n c . execute , \n ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n ) \n finally : \n c . execute ( \"<STR_LIT>\" ) \n c . close ( ) \n def test_load_file ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n conn = self . connections [ <NUM_LIT:0> ] \n c = conn . cursor ( ) \n c . execute ( \"<STR_LIT>\" ) \n filename = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , \n '<STR_LIT:data>' , \n '<STR_LIT>' ) \n try : \n c . execute ( \n ( \"<STR_LIT>\" + \n \"<STR_LIT>\" ) . format ( filename ) \n ) \n c . execute ( \"<STR_LIT>\" ) \n self . assertEqual ( <NUM_LIT> , c . fetchone ( ) [ <NUM_LIT:0> ] ) \n finally : \n c . execute ( \"<STR_LIT>\" ) \n def test_load_warnings ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n conn = self . connections [ <NUM_LIT:0> ] \n c = conn . cursor ( ) \n c . execute ( \"<STR_LIT>\" ) \n filename = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , \n '<STR_LIT:data>' , \n '<STR_LIT>' ) \n try : \n with warnings . catch_warnings ( record = True ) as w : \n warnings . simplefilter ( '<STR_LIT>' ) \n c . execute ( \n ( \"<STR_LIT>\" + \n \"<STR_LIT>\" ) . format ( filename ) \n ) \n self . assertEqual ( w [ <NUM_LIT:0> ] . category , Warning ) \n self . assertTrue ( \"<STR_LIT>\" in str ( w [ - <NUM_LIT:1> ] . message ) ) \n finally : \n c . execute ( \"<STR_LIT>\" ) \n c . close ( ) \n if __name__ == \"<STR_LIT:__main__>\" : \n import unittest \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n from __future__ import print_function \n from time import time \n import subprocess \n import random \n import numpy \n STEP = <NUM_LIT:1000> * <NUM_LIT:100> \n SCALE = <NUM_LIT:0.1> \n NI_NTIMES = <NUM_LIT:1> \n MROW = <NUM_LIT:1000> * <NUM_LIT> \n COLDCACHE = <NUM_LIT:5> \n WARMCACHE = <NUM_LIT:5> \n READ_TIMES = <NUM_LIT:10> \n rdm_cod = [ '<STR_LIT>' , '<STR_LIT>' ] \n prec = <NUM_LIT:6> \n def get_nrows ( nrows_str ) : \n if nrows_str . endswith ( \"<STR_LIT:k>\" ) : \n return int ( float ( nrows_str [ : - <NUM_LIT:1> ] ) * <NUM_LIT:1000> ) \n elif nrows_str . endswith ( \"<STR_LIT:m>\" ) : \n return int ( float ( nrows_str [ : - <NUM_LIT:1> ] ) * <NUM_LIT:1000> * <NUM_LIT:1000> ) \n elif nrows_str . endswith ( \"<STR_LIT:g>\" ) : \n return int ( float ( nrows_str [ : - <NUM_LIT:1> ] ) * <NUM_LIT:1000> * <NUM_LIT:1000> * <NUM_LIT:1000> ) \n else : \n raise ValueError ( \n \"<STR_LIT>\" ) \n class DB ( object ) : \n def __init__ ( self , nrows , rng , userandom ) : \n global step , scale \n self . step = STEP \n self . scale = SCALE \n self . rng = rng \n self . userandom = userandom \n self . filename = '<STR_LIT:->' . join ( [ rdm_cod [ userandom ] , nrows ] ) \n self . nrows = get_nrows ( nrows ) \n def get_db_size ( self ) : \n sout = subprocess . Popen ( \"<STR_LIT>\" % self . filename , shell = True , \n stdout = subprocess . PIPE ) . stdout \n line = [ l for l in sout ] [ <NUM_LIT:0> ] \n return int ( line . split ( ) [ <NUM_LIT:0> ] ) \n def print_mtime ( self , t1 , explain ) : \n mtime = time ( ) - t1 \n print ( \"<STR_LIT>\" % explain , round ( mtime , <NUM_LIT:6> ) ) \n print ( \"<STR_LIT>\" , round ( ( self . nrows / <NUM_LIT> ) / mtime , <NUM_LIT:6> ) ) \n def print_qtime ( self , colname , ltimes ) : \n qtime1 = ltimes [ <NUM_LIT:0> ] \n qtime2 = ltimes [ - <NUM_LIT:1> ] \n print ( \"<STR_LIT>\" % colname , round ( qtime1 , <NUM_LIT:6> ) ) \n print ( \"<STR_LIT>\" , round ( ( self . nrows / ( MROW ) ) / qtime1 , <NUM_LIT:6> ) ) \n print ( \"<STR_LIT>\" % colname , round ( qtime2 , <NUM_LIT:6> ) ) \n print ( \"<STR_LIT>\" , round ( ( self . nrows / ( MROW ) ) / qtime2 , <NUM_LIT:6> ) ) \n def norm_times ( self , ltimes ) : \n \"<STR_LIT>\" \n lmean = ltimes . mean ( ) \n lstd = ltimes . std ( ) \n ntimes = ltimes [ ltimes < lmean + lstd ] \n nmean = ntimes . mean ( ) \n nstd = ntimes . std ( ) \n return nmean , nstd \n def print_qtime_idx ( self , colname , ltimes , repeated , verbose ) : \n if repeated : \n r = \"<STR_LIT>\" \n else : \n r = \"<STR_LIT>\" \n ltimes = numpy . array ( ltimes ) \n ntimes = len ( ltimes ) \n qtime1 = ltimes [ <NUM_LIT:0> ] \n ctimes = ltimes [ <NUM_LIT:1> : COLDCACHE ] \n cmean , cstd = self . norm_times ( ctimes ) \n wtimes = ltimes [ WARMCACHE : ] \n wmean , wstd = self . norm_times ( wtimes ) \n if verbose : \n print ( \"<STR_LIT>\" , ctimes ) \n print ( \"<STR_LIT>\" % \n numpy . histogram ( wtimes ) ) \n print ( \"<STR_LIT>\" % ( r , colname ) , \n round ( qtime1 , prec ) ) \n print ( \"<STR_LIT>\" % ( r , colname ) , \n round ( cmean , prec ) , \"<STR_LIT>\" , round ( cstd , prec ) ) \n print ( \"<STR_LIT>\" % ( r , colname ) , \n round ( wmean , prec ) , \"<STR_LIT>\" , round ( wstd , prec ) ) \n def print_db_sizes ( self , init , filled , indexed ) : \n table_size = ( filled - init ) / <NUM_LIT> \n indexes_size = ( indexed - filled ) / <NUM_LIT> \n print ( \"<STR_LIT>\" , round ( table_size , <NUM_LIT:3> ) ) \n print ( \"<STR_LIT>\" , round ( indexes_size , <NUM_LIT:3> ) ) \n print ( \"<STR_LIT>\" , round ( table_size + indexes_size , <NUM_LIT:3> ) ) \n def fill_arrays ( self , start , stop ) : \n arr_f8 = numpy . arange ( start , stop , dtype = '<STR_LIT>' ) \n arr_i4 = numpy . arange ( start , stop , dtype = '<STR_LIT>' ) \n if self . userandom : \n arr_f8 += numpy . random . normal ( <NUM_LIT:0> , stop * self . scale , \n size = stop - start ) \n arr_i4 = numpy . array ( arr_f8 , dtype = '<STR_LIT>' ) \n return arr_i4 , arr_f8 \n def create_db ( self , dtype , kind , optlevel , verbose ) : \n self . con = self . open_db ( remove = <NUM_LIT:1> ) \n self . create_table ( self . con ) \n init_size = self . get_db_size ( ) \n t1 = time ( ) \n self . fill_table ( self . con ) \n table_size = self . get_db_size ( ) \n self . print_mtime ( t1 , '<STR_LIT>' ) \n self . index_db ( dtype , kind , optlevel , verbose ) \n indexes_size = self . get_db_size ( ) \n self . print_db_sizes ( init_size , table_size , indexes_size ) \n self . close_db ( self . con ) \n def index_db ( self , dtype , kind , optlevel , verbose ) : \n if dtype == \"<STR_LIT:int>\" : \n idx_cols = [ '<STR_LIT>' ] \n elif dtype == \"<STR_LIT:float>\" : \n idx_cols = [ '<STR_LIT>' ] \n else : \n idx_cols = [ '<STR_LIT>' , '<STR_LIT>' ] \n for colname in idx_cols : \n t1 = time ( ) \n self . index_col ( self . con , colname , kind , optlevel , verbose ) \n self . print_mtime ( t1 , '<STR_LIT>' % colname ) \n def query_db ( self , niter , dtype , onlyidxquery , onlynonidxquery , \n avoidfscache , verbose , inkernel ) : \n self . con = self . open_db ( ) \n if dtype == \"<STR_LIT:int>\" : \n reg_cols = [ '<STR_LIT>' ] \n idx_cols = [ '<STR_LIT>' ] \n elif dtype == \"<STR_LIT:float>\" : \n reg_cols = [ '<STR_LIT>' ] \n idx_cols = [ '<STR_LIT>' ] \n else : \n reg_cols = [ '<STR_LIT>' , '<STR_LIT>' ] \n idx_cols = [ '<STR_LIT>' , '<STR_LIT>' ] \n if avoidfscache : \n rseed = int ( numpy . random . randint ( self . nrows ) ) \n else : \n rseed = <NUM_LIT> \n numpy . random . seed ( rseed ) \n base = numpy . random . randint ( self . nrows ) \n if not onlyidxquery : \n for colname in reg_cols : \n ltimes = [ ] \n random . seed ( rseed ) \n for i in range ( NI_NTIMES ) : \n t1 = time ( ) \n results = self . do_query ( self . con , colname , base , inkernel ) \n ltimes . append ( time ( ) - t1 ) \n if verbose : \n print ( \"<STR_LIT>\" , results ) \n self . print_qtime ( colname , ltimes ) \n self . close_db ( self . con ) \n self . con = self . open_db ( ) \n if not onlynonidxquery : \n for colname in idx_cols : \n ltimes = [ ] \n numpy . random . seed ( rseed ) \n rndbase = numpy . random . randint ( self . nrows , size = niter ) \n for i in range ( niter ) : \n base = rndbase [ i ] \n t1 = time ( ) \n results = self . do_query ( self . con , colname , base , inkernel ) \n ltimes . append ( time ( ) - t1 ) \n if verbose : \n print ( \"<STR_LIT>\" , results ) \n self . print_qtime_idx ( colname , ltimes , False , verbose ) \n self . close_db ( self . con ) \n self . con = self . open_db ( ) \n ltimes = [ ] \n self . close_db ( self . con ) \n self . con = self . open_db ( ) \n self . close_db ( self . con ) \n def close_db ( self , con ) : \n con . close ( ) \n if __name__ == \"<STR_LIT:__main__>\" : \n import sys \n import getopt \n try : \n import psyco \n psyco_imported = <NUM_LIT:1> \n except : \n psyco_imported = <NUM_LIT:0> \n usage = \"\"\"<STR_LIT>\"\"\" % sys . argv [ <NUM_LIT:0> ] \n try : \n opts , pargs = getopt . getopt ( \n sys . argv [ <NUM_LIT:1> : ] , '<STR_LIT>' ) \n except : \n sys . stderr . write ( usage ) \n sys . exit ( <NUM_LIT:1> ) \n usepytables = <NUM_LIT:0> \n usepostgres = <NUM_LIT:0> \n verbose = <NUM_LIT:0> \n doprofile = <NUM_LIT:0> \n dokprofile = <NUM_LIT:0> \n usepsyco = <NUM_LIT:0> \n userandom = <NUM_LIT:0> \n docreate = <NUM_LIT:0> \n optlevel = <NUM_LIT:0> \n kind = \"<STR_LIT>\" \n docompress = <NUM_LIT:0> \n complib = \"<STR_LIT>\" \n doquery = False \n onlyidxquery = False \n onlynonidxquery = False \n inkernel = True \n avoidfscache = <NUM_LIT:0> \n rng = [ - <NUM_LIT:1000> , - <NUM_LIT:1000> ] \n repeatquery = <NUM_LIT:0> \n repeatvalue = <NUM_LIT:0> \n krows = '<STR_LIT>' \n niter = READ_TIMES \n dtype = \"<STR_LIT:all>\" \n datadir = \"<STR_LIT>\" \n for option in opts : \n if option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n usepytables = <NUM_LIT:1> \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n usepostgres = <NUM_LIT:1> \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n verbose = <NUM_LIT:1> \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n doprofile = <NUM_LIT:1> \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n dokprofile = <NUM_LIT:1> \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n usepsyco = <NUM_LIT:1> \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n userandom = <NUM_LIT:1> \n elif option [ <NUM_LIT:0> ] == '<STR_LIT:-c>' : \n docreate = <NUM_LIT:1> \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n doquery = True \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n doquery = True \n onlyidxquery = True \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n doquery = True \n onlynonidxquery = True \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n doquery = True \n onlynonidxquery = True \n inkernel = False \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n avoidfscache = <NUM_LIT:1> \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n docompress = int ( option [ <NUM_LIT:1> ] ) \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n complib = option [ <NUM_LIT:1> ] \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n rng = [ int ( i ) for i in option [ <NUM_LIT:1> ] . split ( \"<STR_LIT:U+002C>\" ) ] \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n niter = int ( option [ <NUM_LIT:1> ] ) \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n krows = option [ <NUM_LIT:1> ] \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n datadir = option [ <NUM_LIT:1> ] \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n optlevel = int ( option [ <NUM_LIT:1> ] ) \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n if option [ <NUM_LIT:1> ] in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : \n kind = option [ <NUM_LIT:1> ] \n else : \n print ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n sys . exit ( <NUM_LIT:1> ) \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n if option [ <NUM_LIT:1> ] in ( '<STR_LIT:int>' , '<STR_LIT:float>' ) : \n dtype = option [ <NUM_LIT:1> ] \n else : \n print ( \"<STR_LIT>\" ) \n sys . exit ( <NUM_LIT:1> ) \n elif option [ <NUM_LIT:0> ] == '<STR_LIT>' : \n repeatquery = <NUM_LIT:1> \n repeatvalue = int ( option [ <NUM_LIT:1> ] ) \n if not usepytables and not usepostgres : \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n sys . exit ( <NUM_LIT:1> ) \n if usepytables : \n from pytables_backend import PyTables_DB \n db = PyTables_DB ( krows , rng , userandom , datadir , \n docompress , complib , kind , optlevel ) \n elif usepostgres : \n from postgres_backend import Postgres_DB \n db = Postgres_DB ( krows , rng , userandom ) \n if not avoidfscache : \n numpy . random . seed ( <NUM_LIT:20> ) \n if verbose : \n if userandom : \n print ( \"<STR_LIT>\" ) \n if onlyidxquery : \n print ( \"<STR_LIT>\" ) \n if psyco_imported and usepsyco : \n psyco . bind ( db . create_db ) \n psyco . bind ( db . query_db ) \n if docreate : \n if verbose : \n print ( \"<STR_LIT>\" % krows ) \n db . create_db ( dtype , kind , optlevel , verbose ) \n if doquery : \n print ( \"<STR_LIT>\" % niter ) \n if doprofile : \n import pstats \n import cProfile as prof \n prof . run ( \n '<STR_LIT>' \n '<STR_LIT>' , \n '<STR_LIT>' ) \n stats = pstats . Stats ( '<STR_LIT>' ) \n stats . strip_dirs ( ) \n stats . sort_stats ( '<STR_LIT:time>' , '<STR_LIT>' ) \n if verbose : \n stats . print_stats ( ) \n else : \n stats . print_stats ( <NUM_LIT:20> ) \n elif dokprofile : \n from cProfile import Profile \n import lsprofcalltree \n prof = Profile ( ) \n prof . run ( \n '<STR_LIT>' \n '<STR_LIT>' ) \n kcg = lsprofcalltree . KCacheGrind ( prof ) \n ofile = open ( '<STR_LIT>' , '<STR_LIT:w>' ) \n kcg . output ( ofile ) \n ofile . close ( ) \n elif doprofile : \n import hotshot \n import hotshot . stats \n prof = hotshot . Profile ( \"<STR_LIT>\" ) \n benchtime , stones = prof . run ( \n '<STR_LIT>' \n '<STR_LIT>' ) \n prof . close ( ) \n stats = hotshot . stats . load ( \"<STR_LIT>\" ) \n stats . strip_dirs ( ) \n stats . sort_stats ( '<STR_LIT:time>' , '<STR_LIT>' ) \n stats . print_stats ( <NUM_LIT:20> ) \n else : \n db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n avoidfscache , verbose , inkernel ) \n if repeatquery : \n db . rng = [ <NUM_LIT:1> , <NUM_LIT:1> ] \n if verbose : \n print ( \"<STR_LIT>\" , db . rng ) \n db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n avoidfscache , verbose , inkernel ) \n for i in range ( repeatvalue ) : \n for j in ( <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:5> ) : \n rng = j * <NUM_LIT:10> ** i \n db . rng = [ - rng / <NUM_LIT:2> , rng / <NUM_LIT:2> ] \n if verbose : \n print ( \"<STR_LIT>\" , db . rng ) \n db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n avoidfscache , verbose , <mask0> ) \n", "gt": "inkernel"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import matplotlib as mpl \n from pylab import * \n KB_ = <NUM_LIT> \n MB_ = <NUM_LIT> * KB_ \n GB_ = <NUM_LIT> * MB_ \n NCHUNKS = <NUM_LIT> \n linewidth = <NUM_LIT:2> \n markers = [ '<STR_LIT:s>' , '<STR_LIT:o>' , '<STR_LIT:v>' , '<STR_LIT>' , '<STR_LIT:+>' , '<STR_LIT:x>' , '<STR_LIT:>>' , '<STR_LIT:<>' , '<STR_LIT:.>' , '<STR_LIT:U+002C>' ] \n markersize = <NUM_LIT:8> \n def get_values ( filename ) : \n f = open ( filename ) \n values = { \"<STR_LIT>\" : [ ] , \"<STR_LIT>\" : [ ] } \n for line in f : \n if line . startswith ( '<STR_LIT>' ) : \n tmp = line . split ( '<STR_LIT>' ) [ <NUM_LIT:1> ] \n nthreads , size , elsize , sbits , codec , shuffle = [ i for i in tmp . split ( '<STR_LIT:U+002CU+0020>' ) ] \n nthreads , size , elsize , sbits = map ( int , ( nthreads , size , elsize , sbits ) ) \n values [ \"<STR_LIT:size>\" ] = size * NCHUNKS / MB_ ; \n values [ \"<STR_LIT>\" ] = elsize ; \n values [ \"<STR_LIT>\" ] = sbits ; \n values [ \"<STR_LIT>\" ] = codec \n values [ \"<STR_LIT>\" ] = shuffle \n ( ratios , speedsw , speedsr ) = ( [ ] , [ ] , [ ] ) \n values [ nthreads ] = ( ratios , speedsw , speedsr ) \n elif line . startswith ( '<STR_LIT>' ) : \n tmp = line . split ( '<STR_LIT:U+002C>' ) [ <NUM_LIT:1> ] \n memcpyw = float ( tmp . split ( '<STR_LIT:U+0020>' ) [ <NUM_LIT:1> ] ) \n values [ \"<STR_LIT>\" ] . append ( memcpyw ) \n elif line . startswith ( '<STR_LIT>' ) : \n tmp = line . split ( '<STR_LIT:U+002C>' ) [ <NUM_LIT:1> ] \n memcpyr = float ( tmp . split ( '<STR_LIT:U+0020>' ) [ <NUM_LIT:1> ] ) \n values [ \"<STR_LIT>\" ] . append ( memcpyr ) \n elif line . startswith ( '<STR_LIT>' ) : \n tmp = line . split ( '<STR_LIT:U+002C>' ) [ <NUM_LIT:1> ] \n speedw = float ( tmp . split ( '<STR_LIT:U+0020>' ) [ <NUM_LIT:1> ] ) \n ratio = float ( line . split ( '<STR_LIT::>' ) [ - <NUM_LIT:1> ] ) \n speedsw . append ( speedw ) \n ratios . append ( ratio ) \n elif line . startswith ( '<STR_LIT>' ) : \n tmp = line . split ( '<STR_LIT:U+002C>' ) [ <NUM_LIT:1> ] \n speedr = float ( tmp . split ( '<STR_LIT:U+0020>' ) [ <NUM_LIT:1> ] ) \n speedsr . append ( speedr ) \n if \"<STR_LIT:OK>\" not in line : \n print \"<STR_LIT>\" \n f . close ( ) \n return nthreads , values \n def show_plot ( plots , yaxis , legends , gtitle , xmax = None ) : \n xlabel ( '<STR_LIT>' ) \n ylabel ( '<STR_LIT>' ) \n title ( gtitle ) \n xlim ( <NUM_LIT:0> , xmax ) \n ylim ( <NUM_LIT:0> , None ) \n grid ( True ) \n legend ( [ p [ <NUM_LIT:0> ] for p in plots \n if not isinstance ( p , mpl . lines . Line2D ) ] , \n legends , loc = \"<STR_LIT>\" ) \n if outfile : \n print \"<STR_LIT>\" , outfile \n savefig ( outfile , dpi = <NUM_LIT:64> ) \n else : \n show ( ) \n if __name__ == '<STR_LIT:__main__>' : \n from optparse import OptionParser \n usage = \"<STR_LIT>\" \n compress_title = '<STR_LIT>' \n decompress_title = '<STR_LIT>' \n yaxis = '<STR_LIT>' \n parser = OptionParser ( usage = usage ) \n parser . add_option ( '<STR_LIT>' , \n '<STR_LIT>' , \n dest = '<STR_LIT>' , \n help = ( '<STR_LIT>' \n '<STR_LIT>' ) ) \n parser . add_option ( '<STR_LIT>' , \n '<STR_LIT>' , \n dest = '<STR_LIT:title>' , \n help = '<STR_LIT>' , ) \n parser . add_option ( '<STR_LIT>' , \n '<STR_LIT>' , \n dest = '<STR_LIT>' , \n help = '<STR_LIT>' , ) \n parser . add_option ( '<STR_LIT>' , \n '<STR_LIT>' , \n dest = '<STR_LIT>' , \n help = '<STR_LIT>' , \n default = None ) \n parser . add_option ( '<STR_LIT>' , '<STR_LIT>' , action = '<STR_LIT:store_true>' , \n dest = '<STR_LIT>' , \n help = '<STR_LIT>' , \n default = False ) \n parser . add_option ( '<STR_LIT>' , '<STR_LIT>' , action = '<STR_LIT:store_true>' , \n dest = '<STR_LIT>' , \n help = '<STR_LIT>' , \n default = False ) \n parser . add_option ( '<STR_LIT:-c>' , '<STR_LIT>' , action = '<STR_LIT:store_true>' , \n dest = '<STR_LIT>' , \n help = '<STR_LIT>' , \n default = False ) \n ( options , args ) = parser . parse_args ( ) \n if len ( args ) == <NUM_LIT:0> : \n parser . error ( \"<STR_LIT>\" ) \n elif len ( args ) > <NUM_LIT:1> : \n parser . error ( \"<STR_LIT>\" ) \n else : \n pass \n if options . report and options . outfile : \n parser . error ( \"<STR_LIT>\" ) \n if options . dspeed and options . cspeed : \n parser . error ( \"<STR_LIT>\" ) \n elif options . cspeed : \n options . dspeed = False \n plot_title = compress_title \n else : \n options . dspeed = True \n plot_title = decompress_title \n filename = args [ <NUM_LIT:0> ] \n cspeed = options . cspeed \n dspeed = options . dspeed \n if options . outfile : \n outfile = options . outfile \n elif options . report : \n if cspeed : \n outfile = filename [ : filename . rindex ( '<STR_LIT:.>' ) ] + '<STR_LIT>' \n else : \n outfile = filename [ : filename . rindex ( '<STR_LIT:.>' ) ] + '<STR_LIT>' \n else : \n outfile = None \n plots = [ ] \n legends = [ ] \n nthreads , values = get_values ( filename ) \n if options . limit : \n thread_range = eval ( options . limit ) \n else : \n thread_range = range ( <NUM_LIT:1> , nthreads + <NUM_LIT:1> ) \n if options . title : \n plot_title = options . title \n else : \n plot_title += \"<STR_LIT>\" % values \n gtitle = plot_title \n for nt in thread_range : \n ( ratios , speedw , speedr ) = values [ nt ] \n if cspeed : \n speed = speedw \n else : \n speed = speedr \n plot_ = plot ( ratios , speed , linewidth = <NUM_LIT:2> ) \n plots . append ( plot_ ) \n nmarker = nt \n if nt >= len ( markers ) : \n nmarker = nt % len ( markers ) \n setp ( plot_ , marker = markers [ nmarker ] , markersize = markersize , \n linewidth = linewidth ) \n legends . append ( \"<STR_LIT>\" % nt ) \n if cspeed : \n mean = np . mean ( values [ \"<STR_LIT>\" ] ) \n message = \"<STR_LIT>\" \n else : \n mean = np . mean ( values [ \"<STR_LIT>\" ] ) \n message = \"<STR_LIT>\" \n plot_ = axhline ( mean , linewidth = <NUM_LIT:3> , linestyle = '<STR_LIT>' , color = '<STR_LIT>' ) \n text ( <NUM_LIT:1.0> , mean + <NUM_LIT:50> , message ) \n plots . append ( plot_ ) \n show_plot ( plots , yaxis , legends , gtitle , xmax = int ( options . xmax ) if \n options . xmax else <mask0> ) \n", "gt": "None"}
{"input": "\n import numpy as np \n import tables \n fileh = tables . open_file ( \"<STR_LIT>\" , mode = \"<STR_LIT:w>\" , \n title = \"<STR_LIT>\" ) \n root = fileh . root \n a = np . array ( [ <NUM_LIT:1> , <NUM_LIT:2> , <NUM_LIT:4> ] , np . int32 ) \n hdfarray = fileh . create_array ( root , '<STR_LIT>' , a , \"<STR_LIT>\" ) \n hdfarray . attrs . string = \"<STR_LIT>\" \n hdfarray . attrs . char = \"<STR_LIT:1>\" \n hdfarray . attrs . int = <NUM_LIT:12> \n hdfarray . attrs . float = <NUM_LIT> \n hdfarray . attrs . object = { \"<STR_LIT:a>\" : <NUM_LIT> , \"<STR_LIT:b>\" : <NUM_LIT:1> , \"<STR_LIT:c>\" : [ <NUM_LIT:1> , <NUM_LIT:2> ] } \n fileh . <mask0> ( ) \n", "gt": "close"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import tables \n def setUp ( filename ) : \n fileh = tables . open_file ( filename , mode = \"<STR_LIT:w>\" , title = \"<STR_LIT>\" ) \n fileh . create_group ( \"<STR_LIT:/>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n fileh . create_group ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n fileh . create_array ( \"<STR_LIT:/>\" , \"<STR_LIT>\" , [ <NUM_LIT:1> , <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n fileh . enable_undo ( ) \n return fileh \n def tearDown ( fileh ) : \n fileh . disable_undo ( ) \n fileh . close ( ) \n def demo_6times3marks ( ) : \n \"\"\"<STR_LIT>\"\"\" \n fileh = setUp ( \"<STR_LIT>\" ) \n fileh . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n fileh . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n fileh . mark ( ) \n fileh . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n fileh . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:6> , <NUM_LIT:7> ] , \"<STR_LIT>\" ) \n fileh . mark ( ) \n fileh . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:7> , <NUM_LIT:8> ] , \"<STR_LIT>\" ) \n fileh . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:8> , <NUM_LIT:9> ] , \"<STR_LIT>\" ) \n fileh . undo ( ) \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n fileh . undo ( ) \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n fileh . undo ( ) \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n fileh . redo ( ) \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n fileh . redo ( ) \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" not in fileh \n assert \"<STR_LIT>\" not in fileh \n fileh . redo ( ) \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n assert \"<STR_LIT>\" in fileh \n tearDown ( fileh ) \n def demo_manyops ( ) : \n \"\"\"<STR_LIT>\"\"\" \n fileh = setUp ( \"<STR_LIT>\" ) \n fileh . create_array ( fileh . root , '<STR_LIT>' , [ <NUM_LIT:3> ] , \"<STR_LIT>\" ) \n fileh . create_group ( fileh . root , '<STR_LIT>' , \"<STR_LIT>\" ) \n new_node = fileh . copy_node ( '<STR_LIT>' , '<STR_LIT>' ) \n new_node = fileh . copy_children ( '<STR_LIT>' , '<STR_LIT>' , recursive = <NUM_LIT:1> ) \n fileh . rename_node ( '<STR_LIT>' , '<STR_LIT>' ) \n new_node = fileh . copy_node ( '<STR_LIT>' , '<STR_LIT>' ) \n fileh . remove_node ( '<STR_LIT>' ) \n fileh . undo ( ) \n assert '<STR_LIT>' not in fileh \n assert '<STR_LIT>' not in fileh \n assert '<STR_LIT>' not in fileh \n assert '<STR_LIT>' not in fileh \n assert '<STR_LIT>' not in fileh \n assert '<STR_LIT>' in fileh \n fileh . redo ( ) \n assert '<STR_LIT>' in fileh \n assert '<STR_LIT>' in fileh \n assert '<STR_LIT>' in fileh \n assert '<STR_LIT>' not in fileh \n assert fileh . root . agroup . anarray3 is new_node \n assert '<STR_LIT>' not in fileh \n assert '<STR_LIT>' not in fileh \n tearDown ( fileh ) \n if __name__ == '<STR_LIT:__main__>' : \n demo_6times3marks ( ) \n <mask0> ( ) \n", "gt": "demo_manyops"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import absolute_import \n import warnings \n import functools \n from . registry import class_name_dict , class_id_dict \n from . exceptions import ( ClosedNodeError , NodeError , UndoRedoWarning , \n PerformanceWarning ) \n from . path import join_path , split_path , isvisiblepath \n from . utils import lazyattr \n from . undoredo import move_to_shadow \n from . attributeset import AttributeSet , NotLoggedAttributeSet \n import six \n __docformat__ = '<STR_LIT>' \n \"\"\"<STR_LIT>\"\"\" \n def _closedrepr ( oldmethod ) : \n \"\"\"<STR_LIT>\"\"\" \n @ functools . wraps ( oldmethod ) \n def newmethod ( self ) : \n if not self . _v_isopen : \n cmod = self . __class__ . __module__ \n cname = self . __class__ . __name__ \n addr = hex ( id ( self ) ) \n return '<STR_LIT>' % ( cmod , cname , addr ) \n return oldmethod ( self ) \n return newmethod \n class MetaNode ( type ) : \n \"\"\"<STR_LIT>\"\"\" \n def __new__ ( class_ , name , bases , dict_ ) : \n for mname in [ '<STR_LIT>' , '<STR_LIT>' ] : \n if mname in dict_ : \n dict_ [ mname ] = _closedrepr ( dict_ [ mname ] ) \n return type . __new__ ( class_ , name , bases , dict_ ) \n def __init__ ( class_ , name , bases , dict_ ) : \n super ( MetaNode , class_ ) . __init__ ( name , bases , dict_ ) \n class_name_dict [ class_ . __name__ ] = class_ \n cid = getattr ( class_ , '<STR_LIT>' , None ) \n if cid is not None : \n for base in bases : \n pcid = getattr ( base , '<STR_LIT>' , None ) \n if pcid == cid : \n break \n else : \n class_id_dict [ cid ] = class_ \n class Node ( six . with_metaclass ( MetaNode , object ) ) : \n \"\"\"<STR_LIT>\"\"\" \n _AttributeSet = AttributeSet \n def _g_getparent ( self ) : \n \"<STR_LIT>\" \n ( parentpath , nodename ) = split_path ( self . _v_pathname ) \n return self . _v_file . _get_node ( parentpath ) \n _v_parent = property ( _g_getparent ) \n @ lazyattr \n def _v_attrs ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _AttributeSet ( self ) \n def _g_gettitle ( self ) : \n \"<STR_LIT>\" \n if hasattr ( self . _v_attrs , '<STR_LIT>' ) : \n return self . _v_attrs . TITLE \n else : \n return '<STR_LIT>' \n def _g_settitle ( self , title ) : \n self . _v_attrs . TITLE = title \n _v_title = property ( _g_gettitle , _g_settitle ) \n _v_isopen = False \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , parentnode , name , _log = True ) : \n if isinstance ( parentnode , class_name_dict [ '<STR_LIT>' ] ) : \n parentnode = parentnode . dereference ( ) \n self . _v_file = None \n \"\"\"<STR_LIT>\"\"\" \n self . _v_isopen = False \n \"\"\"<STR_LIT>\"\"\" \n self . _v_pathname = None \n \"\"\"<STR_LIT>\"\"\" \n self . _v_name = None \n \"\"\"<STR_LIT>\"\"\" \n self . _v_depth = None \n \"\"\"<STR_LIT>\"\"\" \n self . _v_maxtreedepth = parentnode . _v_file . params [ '<STR_LIT>' ] \n \"\"\"<STR_LIT>\"\"\" \n self . _v__deleting = False \n \"\"\"<STR_LIT>\"\"\" \n self . _v_objectid = None \n \"\"\"<STR_LIT>\"\"\" \n validate = new = self . _v_new \n self . _g_check_group ( parentnode ) \n parentnode . _g_check_open ( ) \n file_ = parentnode . _v_file \n if new : \n file_ . _check_writable ( ) \n if new : \n parentnode . _g_refnode ( self , name , validate ) \n self . _g_set_location ( parentnode , name ) \n try : \n self . _g_new ( parentnode , name , init = True ) \n if new : \n self . _v_objectid = self . _g_create ( ) \n else : \n self . _v_objectid = self . _g_open ( ) \n if new and _log and file_ . is_undo_enabled ( ) : \n self . _g_log_create ( ) \n self . _g_post_init_hook ( ) \n except : \n self . _f_close ( ) \n raise \n def _g_log_create ( self ) : \n self . _v_file . _log ( '<STR_LIT>' , self . _v_pathname ) \n def __del__ ( self ) : \n if not self . _v_isopen : \n return \n self . _v__deleting = True \n try : \n node_manager = self . _v_file . _node_manager \n node_manager . drop_node ( self , check_unregistered = False ) \n finally : \n if self . _v_isopen : \n self . _v__deleting = True \n self . _f_close ( ) \n def _g_pre_kill_hook ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n def _g_create ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n raise NotImplementedError \n def _g_open ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n raise NotImplementedError \n def _g_check_open ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if not self . _v_isopen : \n raise ClosedNodeError ( \"<STR_LIT>\" ) \n assert self . _v_file . isopen , \"<STR_LIT>\" \n def _g_set_location ( self , parentnode , name ) : \n \"\"\"<STR_LIT>\"\"\" \n file_ = parentnode . _v_file \n parentdepth = parentnode . _v_depth \n self . _v_file = file_ \n self . _v_isopen = True \n root_uep = file_ . root_uep \n if name . startswith ( root_uep ) : \n assert parentdepth == <NUM_LIT:0> \n if root_uep == \"<STR_LIT:/>\" : \n self . _v_pathname = name \n else : \n self . _v_pathname = name [ len ( root_uep ) : ] \n _ , self . _v_name = split_path ( name ) \n self . _v_depth = name . count ( \"<STR_LIT:/>\" ) - root_uep . count ( \"<STR_LIT:/>\" ) + <NUM_LIT:1> \n else : \n self . _v_name = name \n self . _v_pathname = join_path ( parentnode . _v_pathname , name ) \n self . _v_depth = parentdepth + <NUM_LIT:1> \n if parentdepth >= self . _v_maxtreedepth : \n warnings . warn ( \"\"\"<STR_LIT>\"\"\" \n % ( self . _v_pathname , self . _v_maxtreedepth ) , \n PerformanceWarning ) \n if self . _v_pathname != '<STR_LIT:/>' : \n file_ . _node_manager . cache_node ( self , self . _v_pathname ) \n def _g_update_location ( self , newparentpath ) : \n \"\"\"<STR_LIT>\"\"\" \n oldpath = self . _v_pathname \n newpath = join_path ( newparentpath , self . _v_name ) \n newdepth = newpath . count ( '<STR_LIT:/>' ) \n self . _v_pathname = newpath \n self . _v_depth = newdepth \n if newdepth > self . _v_maxtreedepth : \n warnings . warn ( \"\"\"<STR_LIT>\"\"\" \n % ( self . _v_maxtreedepth , ) , PerformanceWarning ) \n node_manager = self . _v_file . _node_manager \n node_manager . rename_node ( oldpath , newpath ) \n self . _g_update_dependent ( ) \n def _g_del_location ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n node_manager = self . _v_file . _node_manager \n pathname = self . _v_pathname \n if not self . _v__deleting : \n node_manager . drop_from_cache ( pathname ) \n node_manager . registry . pop ( pathname , None ) \n self . _v_file = None \n self . _v_isopen = False \n self . _v_pathname = None \n self . _v_name = None \n self . _v_depth = None \n def _g_post_init_hook ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n pass \n def _g_update_dependent ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if '<STR_LIT>' in self . __dict__ : \n self . _v_attrs . _g_update_node_location ( self ) \n def _f_close ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if not self . _v_isopen : \n return \n myDict = self . __dict__ \n if '<STR_LIT>' in myDict : \n self . _v_attrs . _g_close ( ) \n self . _g_del_location ( ) \n myDict . clear ( ) \n self . _v_isopen = False \n def _g_remove ( self , recursive , force ) : \n \"\"\"<STR_LIT>\"\"\" \n parent = self . _v_parent \n parent . _g_unrefnode ( self . _v_name ) \n self . _f_close ( ) \n self . _g_delete ( parent ) \n def _f_remove ( self , recursive = False , force = False ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _g_check_open ( ) \n file_ = self . _v_file \n file_ . _check_writable ( ) \n if file_ . is_undo_enabled ( ) : \n self . _g_remove_and_log ( recursive , force ) \n else : \n self . _g_remove ( recursive , force ) \n def _g_remove_and_log ( self , recursive , force ) : \n file_ = self . _v_file \n oldpathname = self . _v_pathname \n file_ . _log ( '<STR_LIT>' , oldpathname ) \n move_to_shadow ( file_ , oldpathname ) \n def _g_move ( self , newparent , newname ) : \n \"\"\"<STR_LIT>\"\"\" \n oldparent = self . _v_parent \n oldname = self . _v_name \n oldpathname = self . _v_pathname \n newparent . _g_refnode ( self , newname ) \n oldparent . _g_unrefnode ( oldname ) \n self . _g_del_location ( ) \n self . _g_set_location ( newparent , newname ) \n self . _g_new ( newparent , self . _v_name , init = False ) \n self . _v_parent . _g_move_node ( oldparent . _v_objectid , oldname , \n newparent . _v_objectid , newname , \n oldpathname , self . _v_pathname ) \n self . _g_update_dependent ( ) \n def _f_rename ( self , newname , overwrite = False ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _f_move ( newname = newname , overwrite = overwrite ) \n def _f_move ( self , newparent = None , newname = None , \n overwrite = False , createparents = False ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _g_check_open ( ) \n file_ = self . _v_file \n oldparent = self . _v_parent \n oldname = self . _v_name \n if newparent is None and newname is None : \n raise NodeError ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n if newparent is None : \n newparent = oldparent \n if newname is None : \n newname = oldname \n if hasattr ( newparent , '<STR_LIT>' ) : \n newfile = newparent . _v_file \n newpath = newparent . _v_pathname \n elif hasattr ( newparent , '<STR_LIT>' ) : \n newfile = file_ \n newpath = newparent \n else : \n raise TypeError ( \"<STR_LIT>\" \n % ( newparent , ) ) \n if newfile is not file_ : \n raise NodeError ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n file_ . _check_writable ( ) \n oldpath = oldparent . _v_pathname \n if newpath == oldpath and newname == oldname : \n return \n self . _g_check_not_contains ( newpath ) \n newparent = file_ . _get_or_create_path ( newparent , createparents ) \n self . _g_check_group ( newparent ) \n self . _g_maybe_remove ( newparent , newname , overwrite ) \n oldpathname = self . _v_pathname \n self . _g_move ( newparent , newname ) \n if file_ . is_undo_enabled ( ) : \n self . _g_log_move ( oldpathname ) \n def _g_log_move ( self , oldpathname ) : \n self . _v_file . _log ( '<STR_LIT>' , oldpathname , self . _v_pathname ) \n def _g_copy ( self , newparent , newname , recursive , _log = True , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n raise NotImplementedError \n def _g_copy_as_child ( self , newparent , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n return self . _g_copy ( newparent , self . _v_name , \n recursive = False , _log = False , ** kwargs ) \n def _f_copy ( self , newparent = None , newname = None , \n overwrite = False , recursive = False , createparents = False , \n ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _g_check_open ( ) \n srcfile = self . _v_file \n srcparent = self . _v_parent \n srcname = self . _v_name \n dstparent = newparent \n dstname = newname \n if dstparent is None and dstname is None : \n raise NodeError ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n if dstparent is None : \n dstparent = srcparent \n if dstname is None : \n dstname = srcname \n if hasattr ( dstparent , '<STR_LIT>' ) : \n dstfile = dstparent . _v_file \n dstpath = dstparent . _v_pathname \n elif hasattr ( dstparent , '<STR_LIT>' ) : \n dstfile = srcfile \n dstpath = dstparent \n else : \n raise TypeError ( \"<STR_LIT>\" \n % ( dstparent , ) ) \n if dstfile is srcfile : \n srcpath = srcparent . _v_pathname \n if dstpath == srcpath and dstname == srcname : \n raise NodeError ( \n \"<STR_LIT>\" \n % self . _v_pathname ) \n if recursive : \n self . _g_check_not_contains ( dstpath ) \n dstparent = srcfile . _get_or_create_path ( dstparent , createparents ) \n self . _g_check_group ( dstparent ) \n if dstfile is not srcfile and srcfile . is_undo_enabled ( ) : \n warnings . warn ( \"<STR_LIT>\" \n \"<STR_LIT>\" , \n UndoRedoWarning ) \n self . _g_maybe_remove ( dstparent , dstname , overwrite ) \n return self . _g_copy ( dstparent , dstname , recursive , ** kwargs ) \n def _f_isvisible ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . _g_check_open ( ) \n return isvisiblepath ( self . _v_pathname ) \n def _g_check_group ( self , node ) : \n if not isinstance ( node , class_name_dict [ '<STR_LIT>' ] ) : \n raise TypeError ( \"<STR_LIT>\" \n % node . _v_pathname ) \n if not isinstance ( node , class_name_dict [ '<STR_LIT>' ] ) : \n raise TypeError ( \"<STR_LIT>\" \n % node . _v_pathname ) \n def _g_check_not_contains ( self , pathname ) : \n mypathname = self . _v_pathname \n if ( mypathname == '<STR_LIT:/>' \n or pathname == mypathname \n or pathname . startswith ( mypathname + '<STR_LIT:/>' ) ) : \n raise NodeError ( \"<STR_LIT>\" \n \"<STR_LIT>\" % mypathname ) \n def _g_maybe_remove ( self , parent , name , overwrite ) : \n if name in parent : \n if not overwrite : \n raise NodeError ( \"\"\"<STR_LIT>\"\"\" % ( parent . _v_pathname , name ) ) \n parent . _f_get_child ( name ) . _f_remove ( True ) \n def _g_check_name ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n if name . startswith ( '<STR_LIT>' ) : \n raise ValueError ( \n \"<STR_LIT>\" % name ) \n def _f_getattr ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n return getattr ( self . _v_attrs , name ) \n def _f_setattr ( self , name , value ) : \n \"\"\"<STR_LIT>\"\"\" \n setattr ( self . _v_attrs , name , value ) \n def _f_delattr ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n delattr ( self . _v_attrs , name ) \n class NotLoggedMixin : \n _AttributeSet = NotLoggedAttributeSet \n def _g_log_create ( self ) : \n pass \n def _g_log_move ( self , oldpathname ) : \n pass \n def _g_remove_and_log ( self , recursive , force ) : \n self . _g_remove ( recursive , <mask0> ) \n", "gt": "force"}
{"input": "\n from __future__ import print_function \n from __future__ import absolute_import \n import warnings \n import tables \n from tables import IsDescription , StringCol , BoolCol , IntCol , FloatCol \n from tables . node import NotLoggedMixin \n from tables . path import join_path \n from tables . tests import common \n from tables . tests . common import unittest \n from tables . tests . common import PyTablesTestCase as TestCase \n from six . moves import range \n class BasicTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n _reopen_flag = False \n \"\"\"<STR_LIT>\"\"\" \n def _do_reopen ( self ) : \n if self . _reopen_flag : \n self . _reopen ( '<STR_LIT>' ) \n def setUp ( self ) : \n super ( BasicTestCase , self ) . setUp ( ) \n h5file = self . h5file \n root = h5file . root \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , title = \"<STR_LIT>\" ) \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n group = h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_group ( group , '<STR_LIT>' , \"<STR_LIT>\" ) \n def test00_simple ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertEqual ( self . h5file . _curaction , <NUM_LIT:0> ) \n self . assertEqual ( self . h5file . _curmark , <NUM_LIT:0> ) \n self . _do_reopen ( ) \n self . h5file . redo ( ) \n if common . verbose : \n print ( \"<STR_LIT>\" , self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . _curaction , <NUM_LIT:1> ) \n self . assertEqual ( self . h5file . _curmark , <NUM_LIT:0> ) \n def test01_twice ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertEqual ( self . h5file . _curaction , <NUM_LIT:0> ) \n self . assertEqual ( self . h5file . _curmark , <NUM_LIT:0> ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:4> , <NUM_LIT:5> ] ) \n self . assertEqual ( self . h5file . root . otherarray . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray2 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . _curaction , <NUM_LIT:2> ) \n self . assertEqual ( self . h5file . _curmark , <NUM_LIT:0> ) \n def test02_twice2 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . _curaction , <NUM_LIT:3> ) \n self . assertEqual ( self . h5file . _curmark , <NUM_LIT:1> ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertEqual ( self . h5file . _curaction , <NUM_LIT:2> ) \n self . assertEqual ( self . h5file . _curmark , <NUM_LIT:1> ) \n self . h5file . undo ( ) \n self . assertEqual ( self . h5file . _curaction , <NUM_LIT:0> ) \n self . assertEqual ( self . h5file . _curmark , <NUM_LIT:0> ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . _do_reopen ( ) \n self . assertEqual ( self . h5file . _curaction , <NUM_LIT:2> ) \n self . assertEqual ( self . h5file . _curmark , <NUM_LIT:1> ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:4> , <NUM_LIT:5> ] ) \n self . assertEqual ( self . h5file . root . otherarray . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray2 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . _curaction , <NUM_LIT:3> ) \n self . assertEqual ( self . h5file . _curmark , <NUM_LIT:1> ) \n def test03_6times3marks ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:6> , <NUM_LIT:7> ] , \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:7> , <NUM_LIT:8> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:8> , <NUM_LIT:9> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . _do_reopen ( ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:4> , <NUM_LIT:5> ] ) \n self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ <NUM_LIT:5> , <NUM_LIT:6> ] ) \n self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ <NUM_LIT:6> , <NUM_LIT:7> ] ) \n self . assertEqual ( self . h5file . root . otherarray5 . read ( ) , [ <NUM_LIT:7> , <NUM_LIT:8> ] ) \n self . assertEqual ( self . h5file . root . otherarray6 . read ( ) , [ <NUM_LIT:8> , <NUM_LIT:9> ] ) \n self . assertEqual ( self . h5file . root . otherarray1 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray2 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray3 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray4 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray5 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray6 . title , \"<STR_LIT>\" ) \n def test04_6times3marksro ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( ) \n self . _do_reopen ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:6> , <NUM_LIT:7> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n if common . verbose : \n print ( \"<STR_LIT>\" , self . h5file . walk_nodes ( ) ) \n self . h5file . mark ( ) \n self . _do_reopen ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:7> , <NUM_LIT:8> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:8> , <NUM_LIT:9> ] , \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . _do_reopen ( ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:4> , <NUM_LIT:5> ] ) \n self . assertEqual ( self . h5file . root . otherarray5 . read ( ) , [ <NUM_LIT:7> , <NUM_LIT:8> ] ) \n self . assertEqual ( self . h5file . root . otherarray6 . read ( ) , [ <NUM_LIT:8> , <NUM_LIT:9> ] ) \n self . assertEqual ( self . h5file . root . otherarray1 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray2 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray5 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray6 . title , \"<STR_LIT>\" ) \n def test05_destructive ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( ) \n self . _do_reopen ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . _do_reopen ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray1 . title , \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ <NUM_LIT:5> , <NUM_LIT:6> ] ) \n self . assertEqual ( self . h5file . root . otherarray3 . title , \"<STR_LIT>\" ) \n def test05b_destructive ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:6> , <NUM_LIT:7> ] , \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray1 . title , \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ <NUM_LIT:5> , <NUM_LIT:6> ] ) \n self . assertEqual ( self . h5file . root . otherarray3 . title , \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n def test05c_destructive ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( ) \n self . _do_reopen ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( ) \n self . _do_reopen ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:6> , <NUM_LIT:7> ] , \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . h5file . undo ( ) \n self . _do_reopen ( ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n def test05d_destructive ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:6> , <NUM_LIT:7> ] , \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . _do_reopen ( ) \n self . h5file . undo ( <NUM_LIT:0> ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n def test05e_destructive ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . _do_reopen ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( <NUM_LIT:0> ) \n self . _do_reopen ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n def test05f_destructive ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n self . h5file . undo ( ) \n self . _do_reopen ( ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n newarr = self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . _do_reopen ( ) \n self . h5file . redo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n if not self . _reopen_flag : \n self . assertTrue ( self . h5file . root . newarray is newarr ) \n def test06_totalunwind ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . undo ( <NUM_LIT:0> ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n def test07_totalrewind ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( <NUM_LIT:0> ) \n self . _do_reopen ( ) \n self . h5file . redo ( - <NUM_LIT:1> ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:4> , <NUM_LIT:5> ] ) \n self . assertEqual ( self . h5file . root . otherarray . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray2 . title , \"<STR_LIT>\" ) \n def test08_marknames ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:6> , <NUM_LIT:7> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . undo ( \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . _do_reopen ( ) \n self . h5file . redo ( - <NUM_LIT:1> ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:4> , <NUM_LIT:5> ] ) \n self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ <NUM_LIT:5> , <NUM_LIT:6> ] ) \n self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ <NUM_LIT:6> , <NUM_LIT:7> ] ) \n def test08_initialmark ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n initmid = self . h5file . get_current_mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( ) \n self . _do_reopen ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( initmid ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( - <NUM_LIT:1> ) \n self . _do_reopen ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:4> , <NUM_LIT:5> ] ) \n self . assertEqual ( self . h5file . root . otherarray . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray2 . title , \"<STR_LIT>\" ) \n def test09_marknames ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:6> , <NUM_LIT:7> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( \"<STR_LIT>\" ) \n with self . assertRaises ( tables . UndoRedoError ) : \n self . h5file . undo ( \"<STR_LIT>\" ) \n self . h5file . redo ( \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n with self . assertRaises ( tables . UndoRedoError ) : \n self . h5file . redo ( \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n def test10_goto ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:6> , <NUM_LIT:7> ] , \"<STR_LIT>\" ) \n self . h5file . goto ( \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . goto ( \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . _do_reopen ( ) \n self . h5file . goto ( \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . goto ( - <NUM_LIT:1> ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:4> , <NUM_LIT:5> ] ) \n self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ <NUM_LIT:5> , <NUM_LIT:6> ] ) \n self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ <NUM_LIT:6> , <NUM_LIT:7> ] ) \n def test10_gotoint ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:4> , <NUM_LIT:5> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . _do_reopen ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:5> , <NUM_LIT:6> ] , \"<STR_LIT>\" ) \n self . h5file . mark ( \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:6> , <NUM_LIT:7> ] , \"<STR_LIT>\" ) \n self . h5file . goto ( <NUM_LIT:1> ) \n self . _do_reopen ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . goto ( <NUM_LIT:0> ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . _do_reopen ( ) \n self . h5file . goto ( <NUM_LIT:3> ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . goto ( <NUM_LIT:2> ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . _do_reopen ( ) \n self . h5file . goto ( - <NUM_LIT:1> ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:4> , <NUM_LIT:5> ] ) \n self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ <NUM_LIT:5> , <NUM_LIT:6> ] ) \n self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ <NUM_LIT:6> , <NUM_LIT:7> ] ) \n def test11_contiguous ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n m1 = self . h5file . mark ( ) \n m2 = self . h5file . mark ( ) \n self . assertNotEqual ( m1 , m2 ) \n self . _do_reopen ( ) \n self . h5file . undo ( m1 ) \n self . assertEqual ( self . h5file . get_current_mark ( ) , m1 ) \n self . h5file . redo ( m2 ) \n self . assertEqual ( self . h5file . get_current_mark ( ) , m2 ) \n self . h5file . goto ( m1 ) \n self . assertEqual ( self . h5file . get_current_mark ( ) , m1 ) \n self . h5file . goto ( m2 ) \n self . assertEqual ( self . h5file . get_current_mark ( ) , m2 ) \n self . h5file . goto ( - <NUM_LIT:1> ) \n self . _do_reopen ( ) \n self . assertEqual ( self . h5file . get_current_mark ( ) , m2 ) \n self . h5file . goto ( <NUM_LIT:0> ) \n self . assertEqual ( self . h5file . get_current_mark ( ) , <NUM_LIT:0> ) \n def test12_keepMark ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n mid = self . h5file . mark ( ) \n self . assertTrue ( mid is not None ) \n self . _do_reopen ( ) \n self . h5file . undo ( ) \n self . assertEqual ( self . h5file . get_current_mark ( ) , <NUM_LIT:0> ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n def test13_severalEnableDisable ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n self . h5file . undo ( ) \n self . _do_reopen ( ) \n self . assertEqual ( self . h5file . get_current_mark ( ) , <NUM_LIT:0> ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . h5file . disable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n mid = self . h5file . mark ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n self . h5file . undo ( ) \n self . assertEqual ( self . h5file . get_current_mark ( ) , mid ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . _do_reopen ( ) \n self . h5file . disable_undo ( ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . h5file . undo ( ) \n self . _do_reopen ( ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . h5file . disable_undo ( ) \n class PersistenceTestCase ( BasicTestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n _reopen_flag = True \n class CreateArrayTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n super ( CreateArrayTestCase , self ) . setUp ( ) \n h5file = self . h5file \n root = h5file . root \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , title = \"<STR_LIT>\" ) \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n group = h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_group ( group , '<STR_LIT>' , \"<STR_LIT>\" ) \n def test00 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:1> , <NUM_LIT:2> ] ) \n def test01 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:2> , <NUM_LIT:3> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray2 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:1> , <NUM_LIT:2> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:2> , <NUM_LIT:3> ] ) \n def test02 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:2> , <NUM_LIT:3> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray2 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray3 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:1> , <NUM_LIT:2> ] ) \n self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ <NUM_LIT:2> , <NUM_LIT:3> ] ) \n self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n def test03 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT>' , '<STR_LIT>' , \n [ <NUM_LIT:2> , <NUM_LIT:3> ] , \"<STR_LIT>\" ) \n self . h5file . create_array ( '<STR_LIT>' , '<STR_LIT>' , \n [ <NUM_LIT:3> , <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . otherarray1 . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . agroup . otherarray2 . title , \n \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . title , \n \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ <NUM_LIT:1> , <NUM_LIT:2> ] ) \n self . assertEqual ( self . h5file . root . agroup . otherarray2 . read ( ) , [ <NUM_LIT:2> , <NUM_LIT:3> ] ) \n self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . read ( ) , \n [ <NUM_LIT:3> , <NUM_LIT:4> ] ) \n class CreateGroupTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n super ( CreateGroupTestCase , self ) . setUp ( ) \n h5file = self . h5file \n root = h5file . root \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , title = \"<STR_LIT>\" ) \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n group = h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_group ( group , '<STR_LIT>' , \"<STR_LIT>\" ) \n def test00 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_group ( '<STR_LIT:/>' , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n \"<STR_LIT>\" ) \n def test01 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_group ( '<STR_LIT:/>' , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . create_group ( '<STR_LIT:/>' , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . othergroup2 . _v_title , \n \"<STR_LIT>\" ) \n def test02 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_group ( '<STR_LIT:/>' , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . create_group ( '<STR_LIT:/>' , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . create_group ( '<STR_LIT:/>' , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . othergroup2 . _v_title , \n \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . othergroup3 . _v_title , \n \"<STR_LIT>\" ) \n def test03 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_group ( '<STR_LIT:/>' , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . create_group ( \n '<STR_LIT>' , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . create_group ( \n '<STR_LIT>' , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \n \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . othergroup1 . othergroup2 . _v_title , \n \"<STR_LIT>\" ) \n self . assertEqual ( \n self . h5file . root . othergroup1 . othergroup2 . othergroup3 . _v_title , \n \"<STR_LIT>\" ) \n minRowIndex = <NUM_LIT:10> \n def populateTable ( where , name ) : \n \"\"\"<STR_LIT>\"\"\" \n class Indexed ( IsDescription ) : \n var1 = StringCol ( itemsize = <NUM_LIT:4> , dflt = b\"<STR_LIT>\" , pos = <NUM_LIT:1> ) \n var2 = BoolCol ( dflt = <NUM_LIT:0> , pos = <NUM_LIT:2> ) \n var3 = IntCol ( dflt = <NUM_LIT:0> , pos = <NUM_LIT:3> ) \n var4 = FloatCol ( dflt = <NUM_LIT:0> , pos = <NUM_LIT:4> ) \n nrows = minRowIndex \n table = where . _v_file . create_table ( where , name , Indexed , \"<STR_LIT>\" , \n None , nrows ) \n for i in range ( nrows ) : \n table . row [ '<STR_LIT>' ] = str ( i ) \n table . row [ '<STR_LIT>' ] = i % <NUM_LIT:2> \n table . row [ '<STR_LIT>' ] = i \n table . row [ '<STR_LIT>' ] = float ( nrows - i - <NUM_LIT:1> ) \n table . row . append ( ) \n table . flush ( ) \n indexrows = table . cols . var1 . create_index ( ) \n indexrows = table . cols . var2 . create_index ( ) \n indexrows = table . cols . var3 . create_index ( ) \n if common . verbose : \n print ( \"<STR_LIT>\" , nrows ) \n print ( \"<STR_LIT>\" , table . cols . var1 . index . nelements ) \n print ( \"<STR_LIT>\" , indexrows ) \n class RenameNodeTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n super ( RenameNodeTestCase , self ) . setUp ( ) \n h5file = self . h5file \n root = h5file . root \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , title = \"<STR_LIT>\" ) \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n group = h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_group ( group , '<STR_LIT>' , \"<STR_LIT>\" ) \n populateTable ( self . h5file . root , '<STR_LIT>' ) \n def test00 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . rename_node ( '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup2 . _v_title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup3 . _v_title , \"<STR_LIT>\" ) \n def test01 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . rename_node ( '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup . _v_title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup3 . _v_title , \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n def test01b ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . rename_node ( '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . rename_node ( '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup . _v_title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup4 . _v_title , \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n def test02 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . rename_node ( '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertEqual ( self . h5file . root . anarray . title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . anarray2 . title , \"<STR_LIT>\" ) \n def test03 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . rename_node ( '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n table = self . h5file . root . table \n self . assertTrue ( table . cols . var1 . index is not None ) \n self . assertTrue ( table . cols . var2 . index is not None ) \n self . assertTrue ( table . cols . var3 . index is not None ) \n self . assertTrue ( table . cols . var4 . index is None ) \n self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertEqual ( self . h5file . root . table . title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . table2 . title , \"<STR_LIT>\" ) \n table = self . h5file . root . table2 \n self . assertTrue ( table . cols . var1 . index is not None ) \n self . assertTrue ( table . cols . var2 . index is not None ) \n self . assertTrue ( table . cols . var3 . index is not None ) \n self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n self . assertTrue ( table . cols . var4 . index is None ) \n class MoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n super ( MoveNodeTestCase , self ) . setUp ( ) \n h5file = self . h5file \n root = h5file . root \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , title = \"<STR_LIT>\" ) \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n group = h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_group ( group , '<STR_LIT>' , \"<STR_LIT>\" ) \n populateTable ( self . h5file . root , '<STR_LIT>' ) \n def test00 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . move_node ( '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertEqual ( self . h5file . root . anarray . title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup . agroup3 . anarray . title , \n \"<STR_LIT>\" ) \n def test01 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . move_node ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup . _v_title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup2 . agroup3 . _v_title , \n \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n def test01b ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . move_node ( '<STR_LIT>' , '<STR_LIT:/>' , '<STR_LIT>' ) \n self . h5file . move_node ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup . _v_title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup2 . agroup4 . _v_title , \n \"<STR_LIT>\" ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n def test02 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . move_node ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertEqual ( self . h5file . root . anarray . title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( \n self . h5file . root . agroup2 . anarray2 . title , \"<STR_LIT>\" ) \n def test03 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . move_node ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n table = self . h5file . root . table \n self . assertTrue ( table . cols . var1 . index is not None ) \n self . assertTrue ( table . cols . var2 . index is not None ) \n self . assertTrue ( table . cols . var3 . index is not None ) \n self . assertTrue ( table . cols . var4 . index is None ) \n self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n self . assertEqual ( self . h5file . root . table . title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup2 . table2 . title , \"<STR_LIT>\" ) \n table = self . h5file . root . agroup2 . table2 \n self . assertTrue ( table . cols . var1 . index is not None ) \n self . assertTrue ( table . cols . var2 . index is not None ) \n self . assertTrue ( table . cols . var3 . index is not None ) \n self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n self . assertTrue ( table . cols . var4 . index is None ) \n class RemoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n super ( RemoveNodeTestCase , self ) . setUp ( ) \n h5file = self . h5file \n root = h5file . root \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , title = \"<STR_LIT>\" ) \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n group = h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_group ( group , '<STR_LIT>' , \"<STR_LIT>\" ) \n populateTable ( self . h5file . root , '<STR_LIT>' ) \n def test00 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . anarray . title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n def test00b ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . anarray . title , \"<STR_LIT>\" ) \n self . assertEqual ( \n self . h5file . root . agroup . anarray2 . title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n def test00c ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n table = self . h5file . root . table \n self . assertTrue ( table . cols . var1 . index is not None ) \n self . assertTrue ( table . cols . var2 . index is not None ) \n self . assertTrue ( table . cols . var3 . index is not None ) \n self . assertTrue ( table . cols . var4 . index is None ) \n self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n self . assertEqual ( self . h5file . root . table . title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n def test01 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . remove_node ( '<STR_LIT>' , recursive = <NUM_LIT:1> ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup . _v_title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n def test01b ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . remove_node ( '<STR_LIT>' , recursive = <NUM_LIT:1> ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup . _v_title , \"<STR_LIT>\" ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n class CopyNodeTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n super ( CopyNodeTestCase , self ) . setUp ( ) \n h5file = self . h5file \n root = h5file . root \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , title = \"<STR_LIT>\" ) \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n group = h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_group ( group , '<STR_LIT>' , \"<STR_LIT>\" ) \n populateTable ( self . h5file . root , '<STR_LIT>' ) \n def test00_copyLeaf ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n new_node = self . h5file . copy_node ( '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( self . h5file . root . agroup . agroup3 . anarray is new_node ) \n def test00b_copyTable ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n warnings . filterwarnings ( \"<STR_LIT:ignore>\" , category = UserWarning ) \n table = self . h5file . copy_node ( \n '<STR_LIT>' , '<STR_LIT>' , propindexes = True ) \n warnings . filterwarnings ( \"<STR_LIT:default>\" , category = UserWarning ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n table = self . h5file . root . agroup . agroup3 . table \n self . assertEqual ( table . title , \"<STR_LIT>\" ) \n self . assertTrue ( table . cols . var1 . index is not None ) \n self . assertTrue ( table . cols . var2 . index is not None ) \n self . assertTrue ( table . cols . var3 . index is not None ) \n self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n self . assertTrue ( table . cols . var4 . index is None ) \n self . h5file . undo ( ) \n table = self . h5file . root . table \n self . assertTrue ( table . cols . var1 . index is not None ) \n self . assertTrue ( table . cols . var2 . index is not None ) \n self . assertTrue ( table . cols . var3 . index is not None ) \n self . assertTrue ( table . cols . var4 . index is None ) \n self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n self . assertTrue ( \"<STR_LIT>\" not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n self . assertTrue ( \"<STR_LIT>\" in self . h5file ) \n table = self . h5file . root . agroup . agroup3 . table \n self . assertEqual ( table . title , \"<STR_LIT>\" ) \n self . assertTrue ( table . cols . var1 . index is not None ) \n self . assertTrue ( table . cols . var2 . index is not None ) \n self . assertTrue ( table . cols . var3 . index is not None ) \n self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n self . assertTrue ( table . cols . var4 . index is None ) \n def test01_copyGroup ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n new_node = self . h5file . copy_node ( \n '<STR_LIT>' , newname = '<STR_LIT>' , recursive = True ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( self . h5file . root . acopy is new_node ) \n def test02_copyLeafOverwrite ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n oldNode = self . h5file . root . agroup \n new_node = self . h5file . copy_node ( \n '<STR_LIT>' , newname = '<STR_LIT>' , overwrite = True ) \n self . h5file . undo ( ) \n self . assertTrue ( self . h5file . root . agroup is oldNode ) \n self . h5file . redo ( ) \n self . assertTrue ( self . h5file . root . agroup is new_node ) \n def test03_copyChildren ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . copy_children ( '<STR_LIT>' , '<STR_LIT>' , recursive = True ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n class ComplexTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n super ( ComplexTestCase , self ) . setUp ( ) \n h5file = self . h5file \n root = h5file . root \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] , title = \"<STR_LIT>\" ) \n h5file . create_array ( root , '<STR_LIT>' , [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n group = h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_array ( group , '<STR_LIT>' , [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n h5file . create_group ( root , '<STR_LIT>' , \"<STR_LIT>\" ) \n h5file . create_group ( group , '<STR_LIT>' , \"<STR_LIT>\" ) \n def test00 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_array ( self . h5file . root , '<STR_LIT>' , \n [ <NUM_LIT:1> ] , \"<STR_LIT>\" ) \n self . h5file . create_group ( self . h5file . root , '<STR_LIT>' , \"<STR_LIT>\" ) \n new_node = self . h5file . copy_node ( '<STR_LIT>' , '<STR_LIT>' ) \n new_node = self . h5file . copy_children ( \n '<STR_LIT>' , '<STR_LIT>' , recursive = <NUM_LIT:1> ) \n self . h5file . rename_node ( '<STR_LIT>' , '<STR_LIT>' ) \n new_node = self . h5file . copy_node ( '<STR_LIT>' , '<STR_LIT>' ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( self . h5file . root . agroup . anarray3 is new_node ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n def test01 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . create_array ( self . h5file . root , '<STR_LIT>' , \n [ <NUM_LIT:2> ] , \"<STR_LIT>\" ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . create_array ( self . h5file . root , '<STR_LIT>' , \n [ <NUM_LIT:3> ] , \"<STR_LIT>\" ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . create_array ( self . h5file . root , '<STR_LIT>' , \n [ <NUM_LIT:4> ] , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertEqual ( self . h5file . root . anarray . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . anarray [ : ] , [ <NUM_LIT:1> ] ) \n self . h5file . redo ( ) \n self . assertEqual ( self . h5file . root . anarray . title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . anarray [ : ] , [ <NUM_LIT:4> ] ) \n def test02 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . create_group ( self . h5file . root , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . create_group ( self . h5file . root , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . create_group ( self . h5file . root , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . create_group ( self . h5file . root . agroup2 , '<STR_LIT>' , \n \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertEqual ( self . h5file . root . agroup2 . _v_title , \"<STR_LIT>\" ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . h5file . redo ( ) \n self . assertEqual ( self . h5file . root . agroup2 . _v_title , \"<STR_LIT>\" ) \n self . assertEqual ( self . h5file . root . agroup2 . agroup5 . _v_title , \n \"<STR_LIT>\" ) \n def test03 ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . remove_node ( '<STR_LIT>' , recursive = <NUM_LIT:1> ) \n self . h5file . create_group ( self . h5file . root , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . create_group ( self . h5file . root , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . remove_node ( '<STR_LIT>' ) \n self . h5file . create_group ( self . h5file . root , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . create_group ( self . h5file . root . agroup , '<STR_LIT>' , \n \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup . _v_title , \"<STR_LIT>\" ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . h5file . redo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertEqual ( self . h5file . root . agroup . _v_title , \"<STR_LIT>\" ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertEqual ( \n self . h5file . root . agroup . agroup5 . _v_title , \"<STR_LIT>\" ) \n def test03b ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n self . h5file . enable_undo ( ) \n self . h5file . create_group ( self . h5file . root , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . create_group ( self . h5file . root . agroup3 , '<STR_LIT>' , \n \"<STR_LIT>\" ) \n self . h5file . remove_node ( '<STR_LIT>' , recursive = <NUM_LIT:1> ) \n self . h5file . create_group ( self . h5file . root , '<STR_LIT>' , \"<STR_LIT>\" ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . h5file . redo ( ) \n self . assertEqual ( self . h5file . root . agroup3 . _v_title , \"<STR_LIT>\" ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n class AttributesTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n super ( AttributesTestCase , self ) . setUp ( ) \n array = self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> , <NUM_LIT:2> ] ) \n attrs = array . attrs \n attrs . attr_1 = <NUM_LIT:10> \n attrs . attr_2 = <NUM_LIT:20> \n attrs . attr_3 = <NUM_LIT:30> \n def test00_setAttr ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n array = self . h5file . root . array \n attrs = array . attrs \n self . h5file . enable_undo ( ) \n setattr ( attrs , '<STR_LIT>' , <NUM_LIT:0> ) \n self . assertTrue ( '<STR_LIT>' in attrs ) \n self . assertEqual ( attrs . attr_0 , <NUM_LIT:0> ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' not in attrs ) \n self . h5file . redo ( ) \n self . assertTrue ( '<STR_LIT>' in attrs ) \n self . assertEqual ( attrs . attr_0 , <NUM_LIT:0> ) \n def test01_setAttrExisting ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n array = self . h5file . root . array \n attrs = array . attrs \n self . h5file . enable_undo ( ) \n setattr ( attrs , '<STR_LIT>' , <NUM_LIT:11> ) \n self . assertTrue ( '<STR_LIT>' in attrs ) \n self . assertEqual ( attrs . attr_1 , <NUM_LIT:11> ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' in attrs ) \n self . assertEqual ( attrs . attr_1 , <NUM_LIT:10> ) \n self . h5file . redo ( ) \n self . assertTrue ( '<STR_LIT>' in attrs ) \n self . assertEqual ( attrs . attr_1 , <NUM_LIT:11> ) \n def test02_delAttr ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n array = self . h5file . root . array \n attrs = array . attrs \n self . h5file . enable_undo ( ) \n delattr ( attrs , '<STR_LIT>' ) \n self . assertTrue ( '<STR_LIT>' not in attrs ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' in attrs ) \n self . assertEqual ( attrs . attr_1 , <NUM_LIT:10> ) \n self . h5file . redo ( ) \n self . assertTrue ( '<STR_LIT>' not in attrs ) \n def test03_copyNodeAttrs ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % \n self . __class__ . __name__ ) \n rattrs = self . h5file . root . _v_attrs \n rattrs . attr_0 = <NUM_LIT:0> \n rattrs . attr_1 = <NUM_LIT:100> \n array = self . h5file . root . array \n attrs = array . attrs \n self . h5file . enable_undo ( ) \n attrs . _f_copy ( self . h5file . root ) \n self . assertEqual ( rattrs . attr_0 , <NUM_LIT:0> ) \n self . assertEqual ( rattrs . attr_1 , <NUM_LIT:10> ) \n self . assertEqual ( rattrs . attr_2 , <NUM_LIT:20> ) \n self . assertEqual ( rattrs . attr_3 , <NUM_LIT:30> ) \n self . h5file . undo ( ) \n self . assertEqual ( rattrs . attr_0 , <NUM_LIT:0> ) \n self . assertEqual ( rattrs . attr_1 , <NUM_LIT:100> ) \n self . assertTrue ( '<STR_LIT>' not in rattrs ) \n self . assertTrue ( '<STR_LIT>' not in rattrs ) \n self . h5file . redo ( ) \n self . assertEqual ( rattrs . attr_0 , <NUM_LIT:0> ) \n self . assertEqual ( rattrs . attr_1 , <NUM_LIT:10> ) \n self . assertEqual ( rattrs . attr_2 , <NUM_LIT:20> ) \n self . assertEqual ( rattrs . attr_3 , <NUM_LIT:30> ) \n def test04_replaceNode ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n if common . verbose : \n print ( '<STR_LIT:\\n>' , '<STR_LIT>' * <NUM_LIT:30> ) \n print ( \"<STR_LIT>\" % self . __class__ . __name__ ) \n array = self . h5file . root . array \n attrs = array . attrs \n self . h5file . enable_undo ( ) \n attrs . attr_1 = <NUM_LIT:11> \n self . h5file . remove_node ( '<STR_LIT>' ) \n arr = self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n arr . attrs . attr_1 = <NUM_LIT:12> \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file . root . array . attrs ) \n self . assertEqual ( self . h5file . root . array . attrs . attr_1 , <NUM_LIT:10> ) \n self . h5file . redo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file . root . array . attrs ) \n self . assertEqual ( self . h5file . root . array . attrs . attr_1 , <NUM_LIT:12> ) \n class NotLoggedTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n class NotLoggedArray ( NotLoggedMixin , tables . Array ) : \n pass \n def test00_hierarchy ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n self . h5file . create_group ( '<STR_LIT:/>' , '<STR_LIT>' ) \n self . h5file . enable_undo ( ) \n arr = self . NotLoggedArray ( self . h5file . root , '<STR_LIT:test>' , \n [ <NUM_LIT:1> ] , self . _getMethodName ( ) ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n arr . move ( '<STR_LIT>' ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n arr . remove ( ) \n self . h5file . undo ( ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n def test01_attributes ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n arr = self . NotLoggedArray ( self . h5file . root , '<STR_LIT:test>' , \n [ <NUM_LIT:1> ] , self . _getMethodName ( ) ) \n self . h5file . enable_undo ( ) \n arr . _v_attrs . foo = '<STR_LIT:bar>' \n self . h5file . undo ( ) \n self . assertEqual ( arr . _v_attrs . foo , '<STR_LIT:bar>' ) \n arr . _v_attrs . foo = '<STR_LIT>' \n self . h5file . undo ( ) \n self . assertEqual ( arr . _v_attrs . foo , '<STR_LIT>' ) \n del arr . _v_attrs . foo \n self . h5file . undo ( ) \n self . assertRaises ( AttributeError , getattr , arr . _v_attrs , '<STR_LIT:foo>' ) \n class CreateParentsTestCase ( common . TempFileMixin , TestCase ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n super ( CreateParentsTestCase , self ) . setUp ( ) \n g1 = self . h5file . create_group ( '<STR_LIT:/>' , '<STR_LIT>' ) \n self . h5file . create_group ( g1 , '<STR_LIT>' ) \n def existing ( self , paths ) : \n \"\"\"<STR_LIT>\"\"\" \n return frozenset ( path for path in paths if path in self . h5file ) \n def basetest ( self , doit , pre , post ) : \n pre ( ) \n self . h5file . enable_undo ( ) \n paths = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n for newpath in paths : \n before = self . existing ( paths ) \n doit ( newpath ) \n after = self . existing ( paths ) \n self . assertTrue ( after . issuperset ( before ) ) \n self . h5file . undo ( ) \n post ( newpath ) \n after = self . existing ( paths ) \n self . assertEqual ( after , before ) \n def test00_create ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n def pre ( ) : \n pass \n def doit ( newpath ) : \n self . h5file . create_array ( newpath , '<STR_LIT>' , [ <NUM_LIT:1> ] , createparents = True ) \n self . assertTrue ( join_path ( newpath , '<STR_LIT>' ) in self . h5file ) \n def post ( newpath ) : \n self . assertTrue ( join_path ( newpath , '<STR_LIT>' ) not in self . h5file ) \n self . basetest ( doit , pre , post ) \n def test01_move ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n def pre ( ) : \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n def doit ( newpath ) : \n self . h5file . move_node ( '<STR_LIT>' , newpath , createparents = True ) \n self . assertTrue ( '<STR_LIT>' not in self . h5file ) \n self . assertTrue ( join_path ( newpath , '<STR_LIT>' ) in self . h5file ) \n def post ( newpath ) : \n self . assertTrue ( '<STR_LIT>' in self . h5file ) \n self . assertTrue ( join_path ( newpath , '<STR_LIT>' ) not in self . h5file ) \n self . basetest ( doit , pre , post ) \n def test02_copy ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n def pre ( ) : \n self . h5file . create_array ( '<STR_LIT:/>' , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n def doit ( newpath ) : \n self . h5file . copy_node ( '<STR_LIT>' , newpath , createparents = True ) \n self . assertTrue ( join_path ( newpath , '<STR_LIT>' ) in self . h5file ) \n def post ( newpath ) : \n self . assertTrue ( join_path ( newpath , '<STR_LIT>' ) not in self . h5file ) \n self . basetest ( doit , pre , post ) \n def test03_copyChildren ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n def pre ( ) : \n g = self . h5file . create_group ( '<STR_LIT:/>' , '<STR_LIT>' ) \n self . h5file . create_array ( g , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n self . h5file . create_array ( g , '<STR_LIT>' , [ <NUM_LIT:1> ] ) \n def doit ( newpath ) : \n self . h5file . copy_children ( '<STR_LIT>' , newpath , createparents = True ) \n self . assertTrue ( join_path ( newpath , '<STR_LIT>' ) in self . h5file ) \n self . assertTrue ( join_path ( newpath , '<STR_LIT>' ) in self . h5file ) \n def post ( newpath ) : \n self . assertTrue ( join_path ( newpath , '<STR_LIT>' ) not in self . h5file ) \n self . assertTrue ( join_path ( newpath , '<STR_LIT>' ) not in self . h5file ) \n self . basetest ( doit , pre , post ) \n def suite ( ) : \n theSuite = unittest . TestSuite ( ) \n niter = <NUM_LIT:1> \n for n in range ( niter ) : \n theSuite . addTest ( unittest . makeSuite ( BasicTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( PersistenceTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( CreateArrayTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( CreateGroupTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( RenameNodeTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( MoveNodeTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( RemoveNodeTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( CopyNodeTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( AttributesTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( ComplexTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( NotLoggedTestCase ) ) \n theSuite . addTest ( unittest . makeSuite ( CreateParentsTestCase ) ) \n if common . heavy : \n pass \n return theSuite \n if __name__ == '<STR_LIT:__main__>' : \n import sys \n common . parse_argv ( sys . argv ) \n common . print_versions ( ) \n unittest . main ( <mask0> = '<STR_LIT>' ) \n", "gt": "defaultTest"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import pylons \n from pylons . controllers import WSGIController \n def get_pylons ( decorator_args ) : \n \"\"\"<STR_LIT>\"\"\" \n if decorator_args : \n controller = decorator_args [ <NUM_LIT:0> ] \n if isinstance ( controller , WSGIController ) : \n return controller . _py_object \n return <mask0> \n", "gt": "pylons"}
{"input": "\n import warnings \n from paste . fixture import TestApp \n from paste . registry import RegistryManager \n from __init__ import TestWSGIController \n def make_cache_controller_app ( ) : \n from pylons . testutil import ControllerWrap , SetupCacheGlobal \n from pylons . decorators import jsonify \n from pylons . controllers import WSGIController \n class CacheController ( WSGIController ) : \n @ jsonify \n def test_bad_json ( self ) : \n return [ \"<STR_LIT>\" ] \n @ jsonify \n def test_bad_json2 ( self ) : \n return ( \"<STR_LIT>\" , ) \n @ jsonify \n def test_good_json ( self ) : \n return dict ( fred = <NUM_LIT> ) \n environ = { } \n app = ControllerWrap ( CacheController ) \n app = sap = SetupCacheGlobal ( app , environ ) \n app = RegistryManager ( app ) \n app = TestApp ( app ) \n return app , environ \n class TestJsonifyDecorator ( TestWSGIController ) : \n def setUp ( self ) : \n self . app , environ = make_cache_controller_app ( ) \n TestWSGIController . setUp ( self ) \n environ . update ( self . environ ) \n warnings . simplefilter ( '<STR_LIT:error>' , Warning ) \n def tearDown ( self ) : \n warnings . simplefilter ( '<STR_LIT>' , Warning ) \n def test_bad_json ( self ) : \n for action in '<STR_LIT>' , '<STR_LIT>' : \n try : \n response = self . get_response ( action = action ) \n except Warning , msg : \n assert '<STR_LIT>' in msg [ <NUM_LIT:0> ] \n def test_good_json ( self ) : \n response = self . get_response ( action = '<STR_LIT>' ) \n assert '<STR_LIT>' in response \n assert response . header ( <mask0> ) == '<STR_LIT>' \n", "gt": "'<STR_LIT:Content-Type>'"}
{"input": "\n import os \n import sys \n from setuptools import setup , find_packages \n here = os . path . abspath ( os . path . dirname ( __file__ ) ) \n if sys . version_info [ <NUM_LIT:0> ] > <NUM_LIT:2> : \n README = open ( os . path . join ( here , '<STR_LIT>' ) , encoding = \"<STR_LIT:utf-8>\" ) . read ( ) \n CHANGES = open ( os . path . join ( here , '<STR_LIT>' ) , encoding = \"<STR_LIT:utf-8>\" ) . read ( ) \n else : \n README = open ( os . path . join ( here , '<STR_LIT>' ) ) . read ( ) \n CHANGES = open ( os . path . join ( here , '<STR_LIT>' ) ) . read ( ) \n requires = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n if ( <NUM_LIT:3> , ) < sys . version_info < ( <NUM_LIT:3> , <NUM_LIT:3> ) : \n requires . extend ( [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] ) \n else : \n requires . extend ( [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] ) \n try : \n import wsgiref \n except ImportError : \n requires . append ( '<STR_LIT>' ) \n testing_extras = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n docs_extras = [ '<STR_LIT>' , '<STR_LIT>' ] \n setup ( name = '<STR_LIT>' , \n version = '<STR_LIT>' , \n description = '<STR_LIT>' , \n long_description = README + '<STR_LIT>' + CHANGES , \n classifiers = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n keywords = '<STR_LIT>' , \n author = \"<STR_LIT>\" , \n author_email = \"<STR_LIT>\" , \n maintainer = \"<STR_LIT>\" , \n maintainer_email = \"<STR_LIT>\" , \n url = \"<STR_LIT>\" , \n license = \"<STR_LIT>\" , \n packages = find_packages ( ) , \n include_package_data = True , \n zip_safe = False , \n install_requires = requires , \n extras_require = { \n '<STR_LIT>' : testing_extras , \n '<STR_LIT>' : docs_extras , \n } , \n tests_require = requires + [ '<STR_LIT>' ] , \n test_suite = \"<STR_LIT>\" , \n <mask0> = \"\"\"<STR_LIT>\"\"\" , \n ) \n", "gt": "entry_points"}
{"input": "\n import cryptacular . bcrypt \n from sqlalchemy import ( \n Table , \n Column , \n ForeignKey , \n ) \n from sqlalchemy . orm import ( \n scoped_session , \n sessionmaker , \n relation , \n backref , \n column_property , \n synonym , \n joinedload , \n ) \n from sqlalchemy . types import ( \n Integer , \n Unicode , \n UnicodeText , \n ) \n from sqlalchemy . sql import func \n from sqlalchemy . ext . declarative import declarative_base \n from zope . sqlalchemy import ZopeTransactionExtension \n from pyramid . security import ( \n Everyone , \n Authenticated , \n Allow , \n ) \n DBSession = scoped_session ( sessionmaker ( extension = ZopeTransactionExtension ( ) ) ) \n Base = declarative_base ( ) \n crypt = cryptacular . bcrypt . BCRYPTPasswordManager ( ) \n def hash_password ( password ) : \n return unicode ( crypt . encode ( password ) ) \n class User ( Base ) : \n \"\"\"<STR_LIT>\"\"\" \n __tablename__ = '<STR_LIT>' \n user_id = Column ( Integer , primary_key = True ) \n username = Column ( Unicode ( <NUM_LIT:20> ) , unique = True ) \n name = Column ( Unicode ( <NUM_LIT:50> ) ) \n email = Column ( Unicode ( <NUM_LIT:50> ) ) \n hits = Column ( Integer , default = <NUM_LIT:0> ) \n misses = Column ( Integer , default = <NUM_LIT:0> ) \n delivered_hits = Column ( Integer , default = <NUM_LIT:0> ) \n delivered_misses = Column ( Integer , default = <NUM_LIT:0> ) \n _password = Column ( '<STR_LIT:password>' , Unicode ( <NUM_LIT> ) ) \n def _get_password ( self ) : \n return self . _password \n def _set_password ( self , password ) : \n self . _password = hash_password ( password ) \n password = property ( _get_password , _set_password ) \n password = synonym ( '<STR_LIT>' , descriptor = password ) \n def __init__ ( self , username , password , name , email ) : \n self . username = username \n self . name = name \n self . email = email \n self . password = password \n @ classmethod \n def get_by_username ( cls , username ) : \n return DBSession . query ( cls ) . filter ( cls . username == username ) . first ( ) \n @ classmethod \n def check_password ( cls , username , password ) : \n user = cls . get_by_username ( username ) \n if not user : \n return False \n return crypt . check ( user . password , password ) \n ideas_tags = Table ( '<STR_LIT>' , Base . metadata , \n Column ( '<STR_LIT>' , Integer , ForeignKey ( '<STR_LIT>' ) ) , \n Column ( '<STR_LIT>' , Integer , ForeignKey ( '<STR_LIT>' ) ) \n ) \n class Tag ( Base ) : \n \"\"\"<STR_LIT>\"\"\" \n __tablename__ = '<STR_LIT>' \n tag_id = Column ( Integer , primary_key = True ) \n name = Column ( Unicode ( <NUM_LIT:50> ) , unique = True , index = True ) \n def __init__ ( self , name ) : \n self . name = name \n @ staticmethod \n def extract_tags ( tags_string ) : \n tags = tags_string . replace ( '<STR_LIT:;>' , '<STR_LIT:U+0020>' ) . replace ( '<STR_LIT:U+002C>' , '<STR_LIT:U+0020>' ) \n tags = [ tag . lower ( ) for tag in tags . split ( ) ] \n tags = set ( tags ) \n return tags \n @ classmethod \n def get_by_name ( cls , tag_name ) : \n tag = DBSession . query ( cls ) . filter ( cls . name == tag_name ) \n return tag . first ( ) \n @ classmethod \n def create_tags ( cls , tags_string ) : \n tags_list = cls . extract_tags ( tags_string ) \n tags = [ ] \n for tag_name in tags_list : \n tag = cls . get_by_name ( tag_name ) \n if not tag : \n tag = Tag ( name = tag_name ) \n DBSession . add ( tag ) \n tags . append ( tag ) \n return tags \n @ classmethod \n def tag_counts ( cls ) : \n query = DBSession . query ( Tag . name , func . count ( '<STR_LIT:*>' ) ) \n return query . join ( '<STR_LIT>' ) . group_by ( Tag . name ) \n voted_users = Table ( '<STR_LIT>' , Base . metadata , \n Column ( '<STR_LIT>' , Integer , ForeignKey ( '<STR_LIT>' ) ) , \n Column ( '<STR_LIT>' , Integer , ForeignKey ( '<STR_LIT>' ) ) \n ) \n class Idea ( Base ) : \n __tablename__ = '<STR_LIT>' \n idea_id = Column ( Integer , primary_key = True ) \n target_id = Column ( Integer , ForeignKey ( '<STR_LIT>' ) ) \n comments = relation ( '<STR_LIT>' , cascade = \"<STR_LIT>\" , \n backref = backref ( '<STR_LIT:target>' , remote_side = idea_id ) ) \n author_id = Column ( Integer , ForeignKey ( '<STR_LIT>' ) ) \n author = relation ( User , cascade = \"<STR_LIT>\" , backref = '<STR_LIT>' ) \n title = Column ( UnicodeText ) \n text = Column ( UnicodeText ) \n hits = Column ( Integer , default = <NUM_LIT:0> ) \n misses = Column ( Integer , default = <NUM_LIT:0> ) \n tags = relation ( Tag , secondary = ideas_tags , backref = '<STR_LIT>' ) \n voted_users = relation ( User , secondary = voted_users , lazy = '<STR_LIT>' , \n backref = '<STR_LIT>' ) \n hit_percentage = func . coalesce ( hits / ( hits + misses ) * <NUM_LIT:100> , <NUM_LIT:0> ) \n hit_percentage = column_property ( hit_percentage . label ( '<STR_LIT>' ) ) \n total_votes = column_property ( ( hits + misses ) . label ( '<STR_LIT>' ) ) \n vote_differential = column_property ( \n ( hits - misses ) . label ( '<STR_LIT>' ) \n ) \n @ classmethod \n def get_query ( cls , with_joinedload = True ) : \n query = DBSession . query ( cls ) \n if with_joinedload : \n query = query . options ( joinedload ( '<STR_LIT>' ) , joinedload ( '<STR_LIT>' ) ) \n return query \n @ classmethod \n def get_by_id ( cls , idea_id , with_joinedload = True ) : \n query = cls . get_query ( with_joinedload ) \n return query . filter ( cls . idea_id == idea_id ) . first ( ) \n @ classmethod \n def get_by_tagname ( cls , tag_name , with_joinedload = True ) : \n query = cls . get_query ( with_joinedload ) \n return query . filter ( Idea . tags . any ( name = tag_name ) ) \n @ classmethod \n def ideas_bunch ( cls , order_by , how_many = <NUM_LIT:10> , with_joinedload = True ) : \n query = cls . get_query ( with_joinedload ) . join ( '<STR_LIT>' ) \n query = query . filter ( cls . target == None ) . order_by ( order_by ) \n return query . limit ( how_many ) . all ( ) \n def user_voted ( self , username ) : \n return bool ( self . voted_users . filter_by ( username = username ) . first ( ) ) \n def vote ( self , user , positive ) : \n if positive : \n self . hits += <NUM_LIT:1> \n self . author . hits += <NUM_LIT:1> \n user . delivered_hits += <NUM_LIT:1> \n else : \n self . misses += <NUM_LIT:1> \n self . author . misses += <NUM_LIT:1> \n user . delivered_misses += <NUM_LIT:1> \n self . voted_users . append ( user ) \n class RootFactory ( object ) : \n __acl__ = [ \n ( Allow , Everyone , '<STR_LIT>' ) , \n ( Allow , Authenticated , '<STR_LIT>' ) \n ] \n def __init__ ( self , request ) : \n <mask0> \n", "gt": "pass"}
{"input": "\n import json \n import unittest \n from pyramid import testing \n import mock \n class Test_acl_modified ( unittest . TestCase ) : \n def setUp ( self ) : \n self . request = testing . DummyRequest ( ) \n self . config = testing . setUp ( request = self . request ) \n def tearDown ( self ) : \n testing . tearDown ( ) \n def _callFUT ( self , event ) : \n from . . subscribers import acl_modified \n return acl_modified ( event ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it ( self , mock_get_auditlog ) : \n from substanced . audit import AuditLog \n self . request . user = Dummy ( { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT>' : '<STR_LIT>' } ) \n event = Dummy ( ) \n context = testing . DummyResource ( ) \n auditlog = AuditLog ( ) \n mock_get_auditlog . side_effect = lambda c : auditlog \n context . __oid__ = <NUM_LIT:5> \n event . registry = _makeRegistry ( ) \n event . object = context \n event . old_acl = '<STR_LIT>' \n event . new_acl = '<STR_LIT>' \n self . _callFUT ( event ) \n self . assertEqual ( len ( auditlog ) , <NUM_LIT:1> ) \n entries = list ( auditlog . entries ) \n entry = entries [ <NUM_LIT:0> ] \n self . assertEqual ( entry [ <NUM_LIT:0> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . name , '<STR_LIT>' ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . oid , <NUM_LIT:5> ) \n self . assertEqual ( \n json . loads ( entry [ <NUM_LIT:2> ] . payload ) , \n { \n '<STR_LIT:time>' : entry [ <NUM_LIT:2> ] . timestamp , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT:name>' : '<STR_LIT>' } , \n '<STR_LIT>' : '<STR_LIT:/>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it_nolog ( self , mock_get_auditlog ) : \n mock_get_auditlog . side_effect = lambda c : None \n event = Dummy ( ) \n context = testing . DummyResource ( ) \n context . __oid__ = <NUM_LIT:5> \n event . object = context \n self . assertEqual ( self . _callFUT ( event ) , None ) \n _marker = object ( ) \n class Test_content_added_moved_or_duplicated ( unittest . TestCase ) : \n def setUp ( self ) : \n self . request = testing . DummyRequest ( ) \n self . config = testing . setUp ( request = self . request ) \n def tearDown ( self ) : \n testing . tearDown ( ) \n def _callFUT ( self , event ) : \n from . . subscribers import content_added_moved_or_duplicated \n return content_added_moved_or_duplicated ( event ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it_added ( self , mock_get_auditlog ) : \n auditlog = _makeAuditLog ( ) \n mock_get_auditlog . side_effect = lambda c : auditlog \n self . request . user = Dummy ( { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT>' : '<STR_LIT>' } ) \n event = _makeEvent ( ) \n self . _callFUT ( event ) \n self . assertEqual ( len ( auditlog ) , <NUM_LIT:1> ) \n entries = list ( auditlog . entries ) \n entry = entries [ <NUM_LIT:0> ] \n self . assertEqual ( entry [ <NUM_LIT:0> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . name , '<STR_LIT>' ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . oid , <NUM_LIT:10> ) \n self . assertEqual ( \n json . loads ( entry [ <NUM_LIT:2> ] . payload ) , \n { \n '<STR_LIT>' : '<STR_LIT:/>' , \n '<STR_LIT>' : <NUM_LIT:10> , \n '<STR_LIT:object_name>' : '<STR_LIT>' , \n '<STR_LIT>' : { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT:name>' : '<STR_LIT>' } , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:time>' : entry [ <NUM_LIT:2> ] . timestamp , \n '<STR_LIT>' : <NUM_LIT:5> \n } \n ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it_added_noscribe ( self , mock_get_auditlog ) : \n mock_get_auditlog . side_effect = lambda c : None \n event = _makeEvent ( ) \n self . _callFUT ( event ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it_moved ( self , mock_get_auditlog ) : \n auditlog = _makeAuditLog ( ) \n mock_get_auditlog . side_effect = lambda c : auditlog \n self . request . user = Dummy ( { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT>' : '<STR_LIT>' } ) \n event = _makeEvent ( ) \n event . moving = True \n event . duplicating = None \n self . _callFUT ( event ) \n self . assertEqual ( len ( auditlog ) , <NUM_LIT:1> ) \n entries = list ( auditlog . entries ) \n entry = entries [ <NUM_LIT:0> ] \n self . assertEqual ( entry [ <NUM_LIT:0> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . name , '<STR_LIT>' ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . oid , <NUM_LIT:10> ) \n self . assertEqual ( \n json . loads ( entry [ <NUM_LIT:2> ] . payload ) , \n { \n '<STR_LIT>' : '<STR_LIT:/>' , \n '<STR_LIT>' : <NUM_LIT:10> , \n '<STR_LIT:object_name>' : '<STR_LIT>' , \n '<STR_LIT>' : { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT:name>' : '<STR_LIT>' } , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:time>' : entry [ <NUM_LIT:2> ] . timestamp , \n '<STR_LIT>' : <NUM_LIT:5> \n } \n ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it_duplicated ( self , mock_get_auditlog ) : \n auditlog = _makeAuditLog ( ) \n mock_get_auditlog . side_effect = lambda c : auditlog \n self . request . user = Dummy ( { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT>' : '<STR_LIT>' } ) \n event = _makeEvent ( ) \n event . moving = None \n event . duplicating = True \n self . _callFUT ( event ) \n self . assertEqual ( len ( auditlog ) , <NUM_LIT:1> ) \n entries = list ( auditlog . entries ) \n entry = entries [ <NUM_LIT:0> ] \n self . assertEqual ( entry [ <NUM_LIT:0> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . name , '<STR_LIT>' ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . oid , <NUM_LIT:10> ) \n self . assertEqual ( \n json . loads ( entry [ <NUM_LIT:2> ] . payload ) , \n { \n '<STR_LIT>' : '<STR_LIT:/>' , \n '<STR_LIT>' : <NUM_LIT:10> , \n '<STR_LIT:object_name>' : '<STR_LIT>' , \n '<STR_LIT>' : { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT:name>' : '<STR_LIT>' } , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:time>' : entry [ <NUM_LIT:2> ] . timestamp , \n '<STR_LIT>' : <NUM_LIT:5> \n } \n ) \n class Test_content_removed ( unittest . TestCase ) : \n def setUp ( self ) : \n self . request = testing . DummyRequest ( ) \n self . config = testing . setUp ( request = self . request ) \n def tearDown ( self ) : \n testing . tearDown ( ) \n def _callFUT ( self , event ) : \n from . . subscribers import content_removed \n return content_removed ( event ) \n def test_it_moving ( self ) : \n event = Dummy ( ) \n event . moving = True \n self . assertEqual ( self . _callFUT ( event ) , None ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it ( self , mock_get_auditlog ) : \n auditlog = _makeAuditLog ( ) \n mock_get_auditlog . side_effect = lambda c : auditlog \n self . request . user = Dummy ( { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT>' : '<STR_LIT>' } ) \n event = _makeEvent ( ) \n event . moving = None \n event . duplicating = None \n self . _callFUT ( event ) \n self . assertEqual ( len ( auditlog ) , <NUM_LIT:1> ) \n entries = list ( auditlog . entries ) \n entry = entries [ <NUM_LIT:0> ] \n self . assertEqual ( entry [ <NUM_LIT:0> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . name , '<STR_LIT>' ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . oid , <NUM_LIT:10> ) \n self . assertEqual ( \n json . loads ( entry [ <NUM_LIT:2> ] . payload ) , \n { \n '<STR_LIT>' : '<STR_LIT:/>' , \n '<STR_LIT>' : <NUM_LIT:10> , \n '<STR_LIT:object_name>' : '<STR_LIT>' , \n '<STR_LIT>' : { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT:name>' : '<STR_LIT>' } , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:time>' : entry [ <NUM_LIT:2> ] . timestamp , \n '<STR_LIT>' : <NUM_LIT:5> \n } \n ) \n class Test_content_modified ( unittest . TestCase ) : \n def setUp ( self ) : \n self . request = testing . DummyRequest ( ) \n self . config = testing . setUp ( request = self . request ) \n def tearDown ( self ) : \n testing . tearDown ( ) \n def _callFUT ( self , event ) : \n from . . subscribers import content_modified \n return content_modified ( event ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it_noscribe ( self , mock_get_auditlog ) : \n mock_get_auditlog . side_effect = lambda c : None \n event = Dummy ( ) \n context = testing . DummyResource ( ) \n event . object = context \n self . assertEqual ( self . _callFUT ( event ) , None ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it ( self , mock_get_auditlog ) : \n auditlog = _makeAuditLog ( ) \n mock_get_auditlog . side_effect = lambda c : auditlog \n self . request . user = Dummy ( { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT>' : '<STR_LIT>' } ) \n event = Dummy ( ) \n context = testing . DummyResource ( ) \n context . __oid__ = <NUM_LIT:5> \n event . registry = _makeRegistry ( ) \n event . object = context \n self . _callFUT ( event ) \n self . assertEqual ( len ( auditlog ) , <NUM_LIT:1> ) \n entries = list ( auditlog . entries ) \n entry = entries [ <NUM_LIT:0> ] \n self . assertEqual ( entry [ <NUM_LIT:0> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . name , '<STR_LIT>' ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . oid , <NUM_LIT:5> ) \n self . assertEqual ( \n json . loads ( entry [ <NUM_LIT:2> ] . payload ) , \n { \n '<STR_LIT>' : <NUM_LIT:5> , \n '<STR_LIT>' : { '<STR_LIT>' : <NUM_LIT:1> , '<STR_LIT:name>' : '<STR_LIT>' } , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT:/>' , \n '<STR_LIT:time>' : entry [ <NUM_LIT:2> ] . timestamp , \n } , \n ) \n class Test_logged_in ( unittest . TestCase ) : \n def setUp ( self ) : \n self . request = testing . DummyRequest ( ) \n self . config = testing . setUp ( request = self . request ) \n def tearDown ( self ) : \n testing . tearDown ( ) \n def _callFUT ( self , event ) : \n from . . subscribers import logged_in \n return logged_in ( event ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it_noscribe ( self , mock_get_auditlog ) : \n mock_get_auditlog . side_effect = lambda c : None \n event = Dummy ( ) \n event . request = Dummy ( ) \n context = testing . DummyResource ( ) \n event . request . context = context \n self . assertEqual ( self . _callFUT ( event ) , None ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it_user_has_oid ( self , mock_get_auditlog ) : \n auditlog = _makeAuditLog ( ) \n mock_get_auditlog . side_effect = lambda c : auditlog \n event = Dummy ( ) \n event . request = Dummy ( ) \n context = testing . DummyResource ( ) \n event . request . context = context \n user = Dummy ( ) \n user . __oid__ = <NUM_LIT:5> \n event . user = user \n event . login = '<STR_LIT>' \n self . _callFUT ( event ) \n self . assertEqual ( len ( auditlog ) , <NUM_LIT:1> ) \n entries = list ( auditlog . entries ) \n entry = entries [ <NUM_LIT:0> ] \n self . assertEqual ( entry [ <NUM_LIT:0> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . name , '<STR_LIT>' ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . oid , None ) \n self . assertEqual ( \n json . loads ( entry [ <NUM_LIT:2> ] . payload ) , \n { \n '<STR_LIT>' : <NUM_LIT:5> , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:time>' : entry [ <NUM_LIT:2> ] . timestamp , \n } , \n ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it_user_has_no_oid ( self , mock_get_auditlog ) : \n auditlog = _makeAuditLog ( ) \n mock_get_auditlog . side_effect = lambda c : auditlog \n event = Dummy ( ) \n event . request = Dummy ( ) \n context = testing . DummyResource ( ) \n event . request . context = context \n user = Dummy ( ) \n event . user = user \n event . login = '<STR_LIT>' \n self . _callFUT ( event ) \n self . assertEqual ( len ( auditlog ) , <NUM_LIT:1> ) \n entries = list ( auditlog . entries ) \n entry = entries [ <NUM_LIT:0> ] \n self . assertEqual ( entry [ <NUM_LIT:0> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:1> ] , <NUM_LIT:0> ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . name , '<STR_LIT>' ) \n self . assertEqual ( entry [ <NUM_LIT:2> ] . oid , None ) \n self . assertEqual ( \n json . loads ( entry [ <NUM_LIT:2> ] . payload ) , \n { \n '<STR_LIT>' : None , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT:time>' : entry [ <NUM_LIT:2> ] . timestamp , \n } , \n ) \n class Test_root_added ( unittest . TestCase ) : \n def _callFUT ( self , event ) : \n from . . subscribers import root_added \n return root_added ( event ) \n @ mock . patch ( '<STR_LIT>' ) \n def test_it ( self , mock_set_auditlog ) : \n event = Dummy ( ) \n root = Dummy ( ) \n def is_set ( _root ) : \n self . assertEqual ( _root , root ) \n mock_set_auditlog . side_effect = is_set \n event . object = root \n self . _callFUT ( event ) \n class Dummy ( object ) : \n def __init__ ( self , kw = None ) : \n if kw : \n self . __dict__ . update ( kw ) \n class DummyContentRegistry ( object ) : \n def typeof ( self , content ) : \n return '<STR_LIT>' \n def _makeAuditLog ( ) : \n from substanced . audit import AuditLog \n auditlog = AuditLog ( ) \n return auditlog \n def _makeRegistry ( ) : \n registry = Dummy ( ) \n registry . content = DummyContentRegistry ( ) \n return registry \n def _makeEvent ( ) : \n event = Dummy ( ) \n event . moving = None \n event . duplicating = None \n event . parent = testing . DummyResource ( ) \n event . parent . __oid__ = <NUM_LIT:10> \n event . name = '<STR_LIT>' \n context = testing . DummyResource ( ) \n context . __oid__ = <NUM_LIT:5> \n context . __parent__ = event . parent \n event . registry = _makeRegistry ( ) \n event . object = context \n event . old_acl = '<STR_LIT>' \n event . new_acl = '<STR_LIT>' \n return <mask0> \n", "gt": "event"}
{"input": "\n import unittest \n from pyramid import testing \n class Test_root_factory ( unittest . TestCase ) : \n def setUp ( self ) : \n self . config = testing . setUp ( ) \n def tearDown ( self ) : \n testing . tearDown ( ) \n def _callFUT ( self , request , transaction , get_connection , evolve_packages ) : \n from . . import root_factory \n return root_factory ( request , transaction , get_connection , \n evolve_packages ) \n def _makeRequest ( self , app_root = None ) : \n request = Dummy ( ) \n request . registry = DummyRegistry ( ) \n request . registry . content = Dummy ( ) \n request . registry . content . create = lambda * arg : app_root \n return request \n def test_without_app_root ( self ) : \n txn = DummyTransaction ( ) \n root = { } \n gc = Dummy_get_connection ( root ) \n ep = DummyFunction ( True ) \n app_root = object ( ) \n request = self . _makeRequest ( app_root ) \n result = self . _callFUT ( request , txn , gc , ep ) \n self . assertEqual ( result , app_root ) \n self . assertTrue ( txn . committed ) \n self . assertTrue ( txn . savepointed ) \n self . assertTrue ( ep . called ) \n def test_with_app_root ( self ) : \n txn = DummyTransaction ( ) \n app_root = object ( ) \n root = { '<STR_LIT>' : app_root } \n gc = Dummy_get_connection ( root ) \n ep = DummyFunction ( True ) \n request = testing . DummyRequest ( ) \n result = self . _callFUT ( request , txn , gc , ep ) \n self . assertEqual ( result , app_root ) \n self . assertFalse ( txn . committed ) \n class Test_includeme ( unittest . TestCase ) : \n def test_it ( self ) : \n from . . import ( \n includeme , \n connection_opened , \n connection_will_close , \n ZODBConnectionOpened , \n ZODBConnectionWillClose , \n ) \n config = DummyConfig ( ) \n includeme ( config ) \n self . assertEqual ( \n config . subscriptions , \n [ ( connection_opened , ZODBConnectionOpened ) , \n ( connection_will_close , ZODBConnectionWillClose ) , \n ] \n ) \n class Test_connection_opened ( unittest . TestCase ) : \n def test_it ( self ) : \n from . . import connection_opened \n event = DummyEvent ( ) \n connection_opened ( event ) \n self . assertEqual ( event . request . _zodb_tx_counts , ( <NUM_LIT:0> , <NUM_LIT:0> ) ) \n class Test_connection_will_close ( unittest . TestCase ) : \n def _callFUT ( self , event , statsd_incr ) : \n from . . import connection_will_close \n return connection_will_close ( event , statsd_incr ) \n def test_no_tx_counts ( self ) : \n event = DummyEvent ( ) \n result = self . _callFUT ( event , None ) \n self . assertEqual ( result , None ) \n def test_with_postitive_tx_counts ( self ) : \n event = DummyEvent ( <NUM_LIT:5> , <NUM_LIT:5> ) \n event . request . _zodb_tx_counts = ( <NUM_LIT:1> , <NUM_LIT:1> ) \n L = [ ] \n def statsd_incr ( name , num , registry = None ) : \n L . append ( ( name , num ) ) \n self . _callFUT ( event , statsd_incr ) \n self . assertEqual ( \n L , \n [ ( '<STR_LIT>' , <NUM_LIT:4> ) , ( '<STR_LIT>' , <NUM_LIT:4> ) ] \n ) \n def test_with_zero_tx_counts ( self ) : \n event = DummyEvent ( <NUM_LIT:1> , <NUM_LIT:1> ) \n event . request . _zodb_tx_counts = ( <NUM_LIT:1> , <NUM_LIT:1> ) \n L = [ ] \n self . _callFUT ( event , None ) \n self . assertEqual ( \n L , \n [ ] \n ) \n class DummyTransaction ( object ) : \n committed = False \n savepointed = False \n def commit ( self ) : \n self . committed = True \n def savepoint ( self ) : \n self . savepointed = True \n class Dummy_get_connection ( object ) : \n def __init__ ( self , root ) : \n self . _root = root \n def root ( self ) : \n return self . _root \n def __call__ ( self , request ) : \n return self \n class DummyFunction ( object ) : \n called = False \n def __init__ ( self , result ) : \n self . result = result \n def __call__ ( self , * args , ** kw ) : \n self . called = True \n self . args = args \n self . kw = kw \n return self . result \n class Dummy ( object ) : \n pass \n class DummyRegistry ( object ) : \n def notify ( self , event ) : \n self . event = event \n class DummyConfig ( object ) : \n def __init__ ( self ) : \n self . subscriptions = [ ] \n def add_subscriber ( self , fn , event_type ) : \n self . subscriptions . append ( ( fn , event_type ) ) \n class DummyConnection ( object ) : \n def __init__ ( self , loads , stores ) : \n self . loads = loads \n self . stores = stores \n def getTransferCounts ( self ) : \n return ( self . loads , self . stores ) \n class DummyEvent ( object ) : \n def __init__ ( self , loads = <NUM_LIT:0> , stores = <NUM_LIT:0> ) : \n self . request = testing . DummyRequest ( ) \n self . conn = DummyConnection ( loads , <mask0> ) \n", "gt": "stores"}
{"input": "\n import pkg_resources \n import mimetypes \n import colander \n import deform . schema \n from pyramid . httpexceptions import HTTPFound \n from pyramid . response import Response \n from pyramid . security import NO_PERMISSION_REQUIRED \n from . . form import FormView \n from . . file import ( \n FilePropertiesSchema , \n FileUploadTempStore , \n file_upload_widget , \n file_name_node , \n USE_MAGIC , \n ) \n from . . interfaces import ( \n IFile , \n IFolder , \n ) \n from . . sdi import mgmt_view \n @ mgmt_view ( \n context = IFile , \n name = '<STR_LIT>' , \n permission = '<STR_LIT>' , \n tab_condition = False , \n http_cache = <NUM_LIT:0> , \n ) \n def view_file ( context , request ) : \n return context . get_response ( request = request ) \n @ mgmt_view ( \n context = IFile , \n name = '<STR_LIT>' , \n tab_title = '<STR_LIT>' , \n permission = '<STR_LIT>' \n ) \n def view_tab ( context , request ) : \n return HTTPFound ( location = request . sdiapi . mgmt_path ( context ) ) \n class AddFileSchema ( FilePropertiesSchema ) : \n file = colander . SchemaNode ( \n deform . schema . FileData ( ) , \n widget = file_upload_widget , \n missing = colander . null , \n ) \n @ colander . deferred \n def name_or_file ( node , kw ) : \n def _name_or_file ( node , struct ) : \n if not struct [ '<STR_LIT:file>' ] and not struct [ '<STR_LIT:name>' ] : \n raise colander . Invalid ( node , '<STR_LIT>' ) \n if not struct [ '<STR_LIT:name>' ] : \n filename = struct [ '<STR_LIT:file>' ] . get ( '<STR_LIT:filename>' ) \n if filename : \n name_node = file_name_node . bind ( \n context = kw [ '<STR_LIT>' ] , request = kw [ '<STR_LIT>' ] \n ) \n name_node . validator ( node [ '<STR_LIT:file>' ] , filename ) \n else : \n raise colander . Invalid ( \n node , \n '<STR_LIT>' \n ) \n return _name_or_file \n @ mgmt_view ( \n context = IFolder , \n name = '<STR_LIT>' , \n tab_title = '<STR_LIT>' , \n permission = '<STR_LIT>' , \n renderer = '<STR_LIT>' , \n addable_content = '<STR_LIT>' , \n tab_condition = False \n ) \n class AddFileView ( FormView ) : \n title = '<STR_LIT>' \n schema = AddFileSchema ( validator = name_or_file ) . clone ( ) \n schema [ '<STR_LIT:name>' ] . missing = colander . null \n schema [ '<STR_LIT>' ] . missing = colander . null \n buttons = ( '<STR_LIT>' , ) \n def _makeob ( self , stream , title , mimetype ) : \n return self . request . registry . content . create ( \n '<STR_LIT>' , \n stream = stream , \n mimetype = mimetype , \n title = title , \n ) \n def add_success ( self , appstruct ) : \n name = appstruct [ '<STR_LIT:name>' ] \n title = appstruct [ '<STR_LIT:title>' ] or None \n filedata = appstruct [ '<STR_LIT:file>' ] \n mimetype = appstruct [ '<STR_LIT>' ] or USE_MAGIC \n stream = None \n filename = None \n if filedata : \n filename = filedata [ '<STR_LIT:filename>' ] \n stream = filedata [ '<STR_LIT>' ] \n if stream : \n stream . seek ( <NUM_LIT:0> ) \n else : \n stream = None \n name = name or filename \n fileob = self . _makeob ( stream , title , mimetype ) \n self . context [ name ] = fileob \n tmpstore = FileUploadTempStore ( self . request ) \n tmpstore . clear ( ) \n return HTTPFound ( self . request . sdiapi . mgmt_path ( self . context ) ) \n onepixel = pkg_resources . resource_filename ( \n '<STR_LIT>' , '<STR_LIT>' ) \n @ mgmt_view ( \n name = '<STR_LIT>' , \n tab_condition = False , \n permission = NO_PERMISSION_REQUIRED \n ) \n def preview_image_upload ( request ) : \n uid = request . subpath [ <NUM_LIT:0> ] \n tempstore = FileUploadTempStore ( request ) \n filedata = tempstore . get ( uid , { } ) \n fp = filedata . get ( '<STR_LIT>' ) \n filename = '<STR_LIT>' \n if fp is not None : \n fp . seek ( <NUM_LIT:0> ) \n filename = filedata [ '<STR_LIT:filename>' ] \n mimetype = mimetypes . guess_type ( filename , strict = False ) [ <NUM_LIT:0> ] \n if not mimetype or not mimetype . startswith ( '<STR_LIT>' ) : \n mimetype = '<STR_LIT>' \n fp = open ( onepixel , '<STR_LIT:rb>' ) \n response = Response ( content_type = mimetype , app_iter = fp ) \n return <mask0> \n", "gt": "response"}
{"input": "\n import unittest \n from pyramid import testing \n class Test_principal_added ( unittest . TestCase ) : \n def _callFUT ( self , event ) : \n from . . subscribers import principal_added \n return principal_added ( event ) \n def test_event_wo_loading_attr ( self ) : \n event = testing . DummyResource ( ) \n event . object = testing . DummyResource ( ) \n self . assertRaises ( AttributeError , self . _callFUT , event ) \n def test_event_w_loading_True ( self ) : \n event = testing . DummyResource ( loading = True ) \n result = self . _callFUT ( event ) \n self . assertEqual ( result , None ) \n def test_wo_principals_service ( self ) : \n from zope . interface import directlyProvides \n from ... interfaces import IFolder \n event = testing . DummyResource ( loading = False ) \n root = testing . DummyResource ( ) \n directlyProvides ( root , IFolder ) \n event . object = root [ '<STR_LIT>' ] = testing . DummyResource ( ) \n self . assertRaises ( ValueError , self . _callFUT , event ) \n def test_user_not_in_groups ( self ) : \n from ... testing import make_site \n from ... interfaces import IUser \n site = make_site ( ) \n user = testing . DummyResource ( __provides__ = IUser ) \n site [ '<STR_LIT:user>' ] = user \n event = testing . DummyResource ( object = user , loading = False ) \n self . _callFUT ( event ) \n def test_user_in_groups ( self ) : \n from ... testing import make_site \n from ... interfaces import IUser \n site = make_site ( ) \n groups = site [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n groups [ '<STR_LIT:user>' ] = testing . DummyResource ( ) \n user = testing . DummyResource ( __provides__ = IUser ) \n site [ '<STR_LIT:user>' ] = user \n event = testing . DummyResource ( object = user , loading = False ) \n self . assertRaises ( ValueError , self . _callFUT , event ) \n def test_group_not_in_users ( self ) : \n from ... testing import make_site \n site = make_site ( ) \n group = testing . DummyResource ( ) \n site [ '<STR_LIT>' ] = group \n event = testing . DummyResource ( object = group , loading = False ) \n self . _callFUT ( event ) \n def test_group_in_users ( self ) : \n from ... testing import make_site \n site = make_site ( ) \n users = site [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n users [ '<STR_LIT>' ] = testing . DummyResource ( ) \n group = testing . DummyResource ( ) \n site [ '<STR_LIT>' ] = group \n event = testing . DummyResource ( object = group , loading = False ) \n self . assertRaises ( ValueError , self . _callFUT , event ) \n class Test_user_will_be_removed ( unittest . TestCase ) : \n def _callFUT ( self , event ) : \n from . . subscribers import user_will_be_removed \n return user_will_be_removed ( event ) \n def test_loading ( self ) : \n event = testing . DummyResource ( loading = True , moving = None ) \n result = self . _callFUT ( event ) \n self . assertEqual ( result , None ) \n def test_moving ( self ) : \n event = testing . DummyResource ( loading = False , moving = True ) \n result = self . _callFUT ( event ) \n self . assertEqual ( result , None ) \n def test_it ( self ) : \n from ... interfaces import IFolder \n parent = testing . DummyResource ( __provides__ = IFolder ) \n user = testing . DummyResource ( ) \n reset = testing . DummyResource ( ) \n def commit_suicide ( ) : \n reset . committed = True \n reset . commit_suicide = commit_suicide \n objectmap = DummyObjectMap ( ( reset , ) ) \n parent . __objectmap__ = objectmap \n parent [ '<STR_LIT:user>' ] = user \n event = testing . DummyResource ( object = user , loading = False , moving = None ) \n self . _callFUT ( event ) \n self . assertTrue ( reset . committed ) \n def test_it_moving ( self ) : \n event = testing . DummyResource ( object = None , loading = False ) \n event . moving = True \n self . assertEqual ( self . _callFUT ( event ) , None ) \n class Test_user_added ( unittest . TestCase ) : \n def _callFUT ( self , event ) : \n from . . subscribers import user_added \n return user_added ( event ) \n def test_loading ( self ) : \n event = testing . DummyResource ( loading = True ) \n result = self . _callFUT ( event ) \n self . assertEqual ( result , None ) \n def test_it_user_has_no_oid ( self ) : \n user = testing . DummyResource ( ) \n event = testing . DummyResource ( object = user , loading = False ) \n event . registry = DummyRegistry ( ) \n self . assertRaises ( AttributeError , self . _callFUT , event ) \n def test_it ( self ) : \n from pyramid . security import Allow \n user = testing . DummyResource ( ) \n user . __oid__ = <NUM_LIT:1> \n event = testing . DummyResource ( object = user , loading = False ) \n event . registry = DummyRegistry ( ) \n self . _callFUT ( event ) \n self . assertEqual ( \n user . __acl__ , \n [ ( Allow , <NUM_LIT:1> , ( '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ) ) ] ) \n class Test_acl_maybe_added ( unittest . TestCase ) : \n def _callFUT ( self , event ) : \n from . . subscribers import acl_maybe_added \n return acl_maybe_added ( event ) \n def test_moving ( self ) : \n event = DummyEvent ( moving = True , loading = False ) \n self . assertEqual ( self . _callFUT ( event ) , False ) \n def test_loading ( self ) : \n event = DummyEvent ( moving = None , loading = True ) \n self . assertEqual ( self . _callFUT ( event ) , False ) \n def test_objectmap_is_None ( self ) : \n event = DummyEvent ( moving = None , object = None , loading = False ) \n self . assertEqual ( self . _callFUT ( event ) , None ) \n def test_no_acls ( self ) : \n from substanced . interfaces import IFolder \n resource1 = testing . DummyResource ( __provides__ = IFolder ) \n resource2 = testing . DummyResource ( ) \n resource1 [ '<STR_LIT>' ] = resource2 \n objectmap = DummyObjectMap ( ) \n resource1 . __objectmap__ = objectmap \n event = DummyEvent ( moving = None , object = resource1 , loading = False ) \n self . _callFUT ( event ) \n self . assertEqual ( objectmap . connections , [ ] ) \n def test_with_acls ( self ) : \n from ... interfaces import PrincipalToACLBearing \n from substanced . interfaces import IFolder \n resource1 = testing . DummyResource ( __provides__ = IFolder ) \n resource2 = testing . DummyResource ( ) \n resource1 [ '<STR_LIT>' ] = resource2 \n resource1 . __acl__ = [ ( None , '<STR_LIT>' , None ) , ( None , <NUM_LIT:1> , None ) ] \n resource2 . __acl__ = [ ( None , '<STR_LIT>' , None ) , ( None , <NUM_LIT:2> , None ) ] \n objectmap = DummyObjectMap ( ) \n resource1 . __objectmap__ = objectmap \n event = DummyEvent ( moving = None , object = resource1 , loading = False ) \n self . _callFUT ( event ) \n self . assertEqual ( \n objectmap . connections , \n [ ( <NUM_LIT:2> , resource2 , PrincipalToACLBearing ) , \n ( <NUM_LIT:1> , resource1 , PrincipalToACLBearing ) ] \n ) \n class Test_acl_modified ( unittest . TestCase ) : \n def _callFUT ( self , event ) : \n from . . subscribers import acl_modified \n return acl_modified ( event ) \n def test_objectmap_is_None ( self ) : \n event = DummyEvent ( object = None ) \n self . assertEqual ( self . _callFUT ( event ) , None ) \n def test_gardenpath ( self ) : \n from ... interfaces import PrincipalToACLBearing \n resource = testing . DummyResource ( ) \n objectmap = DummyObjectMap ( ) \n resource . __objectmap__ = objectmap \n event = DummyEvent ( \n object = resource , \n new_acl = [ ( None , '<STR_LIT>' , None ) , ( None , <NUM_LIT:1> , None ) ] , \n old_acl = [ ( None , '<STR_LIT>' , None ) , ( None , <NUM_LIT:2> , None ) ] , \n ) \n self . _callFUT ( event ) \n self . assertEqual ( \n objectmap . connections , \n [ ( <NUM_LIT:1> , resource , PrincipalToACLBearing ) ] \n ) \n self . assertEqual ( \n objectmap . disconnections , \n [ ( <NUM_LIT:2> , resource , PrincipalToACLBearing ) ] \n ) \n class DummyObjectMap ( object ) : \n def __init__ ( self , result = ( ) ) : \n self . result = result \n self . connections = [ ] \n self . disconnections = [ ] \n def targets ( self , object , reftype ) : \n return self . result \n def connect ( self , source , target , reftype ) : \n self . connections . append ( ( source , target , reftype ) ) \n def disconnect ( self , source , target , reftype ) : \n self . disconnections . append ( ( source , target , reftype ) ) \n class DummyEvent ( object ) : \n def __init__ ( self , ** kw ) : \n self . __dict__ . update ( kw ) \n class DummyRegistry ( object ) : \n def subscribers ( self , * arg ) : \n <mask0> \n", "gt": "return"}
{"input": "\n from pyramid . httpexceptions import ( \n HTTPForbidden , \n HTTPFound \n ) \n from pyramid . renderers import get_renderer \n from pyramid . session import check_csrf_token \n from pyramid . security import ( \n remember , \n forget , \n Authenticated , \n NO_PERMISSION_REQUIRED , \n ) \n from ... util import get_oid \n from . . import mgmt_view \n from substanced . interfaces import IUserLocator \n from substanced . principal import DefaultUserLocator \n from substanced . event import LoggedIn \n @ mgmt_view ( \n name = '<STR_LIT>' , \n renderer = '<STR_LIT>' , \n tab_condition = False , \n permission = NO_PERMISSION_REQUIRED \n ) \n @ mgmt_view ( \n renderer = '<STR_LIT>' , \n context = HTTPForbidden , \n permission = NO_PERMISSION_REQUIRED , \n tab_condition = False \n ) \n @ mgmt_view ( \n renderer = '<STR_LIT>' , \n context = HTTPForbidden , \n permission = NO_PERMISSION_REQUIRED , \n effective_principals = Authenticated , \n tab_condition = False \n ) \n def login ( context , request ) : \n login_url = request . sdiapi . mgmt_path ( request . context , '<STR_LIT>' ) \n referrer = request . url \n if '<STR_LIT>' in referrer : \n return HTTPForbidden ( ) \n if login_url in referrer : \n referrer = request . sdiapi . mgmt_path ( request . virtual_root ) \n came_from = request . session . setdefault ( '<STR_LIT>' , referrer ) \n login = '<STR_LIT>' \n password = '<STR_LIT>' \n if '<STR_LIT>' in request . params : \n try : \n check_csrf_token ( request ) \n except : \n request . sdiapi . flash ( '<STR_LIT>' , '<STR_LIT>' ) \n else : \n login = request . params [ '<STR_LIT>' ] \n password = request . params [ '<STR_LIT:password>' ] \n adapter = request . registry . queryMultiAdapter ( \n ( context , request ) , \n IUserLocator \n ) \n if adapter is None : \n adapter = DefaultUserLocator ( context , request ) \n user = adapter . get_user_by_login ( login ) \n if user is not None and user . check_password ( password ) : \n request . session . pop ( '<STR_LIT>' , None ) \n headers = remember ( request , get_oid ( user ) ) \n request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n return HTTPFound ( location = came_from , headers = headers ) \n request . sdiapi . flash ( '<STR_LIT>' , '<STR_LIT>' ) \n template = get_renderer ( '<STR_LIT>' \n ) . implementation ( ) \n return dict ( \n url = request . sdiapi . mgmt_path ( request . virtual_root , '<STR_LIT>' ) , \n came_from = came_from , \n login = login , \n password = password , \n login_template = template , \n ) \n @ mgmt_view ( \n name = '<STR_LIT>' , \n tab_condition = False , \n permission = NO_PERMISSION_REQUIRED \n ) \n def logout ( request ) : \n headers = forget ( request ) \n return HTTPFound ( location = request . sdiapi . mgmt_path ( request . context ) , \n headers = <mask0> ) \n", "gt": "headers"}
{"input": "\n from venusian . tests . fixtures import categorydecorator \n from venusian . tests . fixtures import categorydecorator2 \n @ categorydecorator ( function = True ) \n def function ( request ) : \n return request \n @ categorydecorator2 ( function = True ) \n def function2 ( request ) : \n return <mask0> \n", "gt": "request"}
{"input": "\n import os \n import mimetypes \n mimetypes . add_type ( '<STR_LIT>' , '<STR_LIT>' ) \n mimetypes . add_type ( '<STR_LIT>' , '<STR_LIT>' ) \n from zope . structuredtext import stx2html \n from pyramid . response import Response \n from pyramid . httpexceptions import HTTPFound \n from pyramid . view import render_view_to_response \n from pyramid . view import view_config \n from virginia . models import File \n from virginia . models import Directory \n @ view_config ( context = File ) \n def file_view ( context , request ) : \n dirname , filename = os . path . split ( context . path ) \n name , ext = os . path . splitext ( filename ) \n result = render_view_to_response ( context , request , ext ) \n return result \n @ view_config ( context = Directory ) \n def directory_view ( context , request ) : \n path_info = request . environ [ '<STR_LIT>' ] \n if not path_info . endswith ( '<STR_LIT:/>' ) : \n response = HTTPFound ( location = path_info + '<STR_LIT:/>' ) \n return response \n defaults = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n for name in defaults : \n try : \n index = context [ name ] \n except KeyError : \n continue \n return file_view ( index , request ) \n response = Response ( '<STR_LIT>' % context . path ) \n response . content_type = '<STR_LIT>' \n return response \n @ view_config ( context = File , name = '<STR_LIT>' ) \n def structured_text_view ( context , request ) : \n \"\"\"<STR_LIT>\"\"\" \n result = stx2html ( context . source ) \n response = Response ( result ) \n response . content_type = '<STR_LIT>' \n return response \n @ view_config ( context = File , name = '<STR_LIT>' ) \n @ view_config ( context = File , name = '<STR_LIT>' ) \n @ view_config ( context = File , name = '<STR_LIT>' ) \n @ view_config ( context = File , name = '<STR_LIT>' ) \n def raw_view ( context , request ) : \n \"\"\"<STR_LIT>\"\"\" \n response = Response ( context . source ) \n dirname , filename = os . path . split ( context . path ) \n name , ext = os . path . splitext ( filename ) \n mt , encoding = mimetypes . guess_type ( filename ) \n response . content_type = mt or '<STR_LIT>' \n return <mask0> \n", "gt": "response"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import os \n import sys \n import unittest \n if not hasattr ( unittest . defaultTestLoader , '<STR_LIT>' ) : \n try : \n import unittest2 as unittest \n except ImportError : \n raise ImportError ( '<STR_LIT>' ) \n def additional_tests ( ) : \n setup_file = sys . modules [ '<STR_LIT:__main__>' ] . __file__ \n setup_dir = os . path . abspath ( os . path . dirname ( setup_file ) ) \n test_dir = os . path . join ( setup_dir , '<STR_LIT>' ) \n test_suite = unittest . defaultTestLoader . discover ( test_dir ) \n blacklist = [ ] \n if '<STR_LIT>' in __file__ : \n blacklist . append ( '<STR_LIT>' ) \n return exclude_tests ( test_suite , blacklist ) \n class SkipCase ( unittest . TestCase ) : \n def skeleton_run_test ( self ) : \n raise unittest . SkipTest ( \"<STR_LIT>\" ) \n def exclude_tests ( suite , blacklist ) : \n \"\"\"<STR_LIT>\"\"\" \n new_suite = unittest . TestSuite ( ) \n for test_group in suite . _tests : \n for test in test_group : \n if not hasattr ( test , '<STR_LIT>' ) : \n new_suite . addTest ( test ) \n continue \n for subtest in test . _tests : \n method = subtest . _testMethodName \n if method in blacklist : \n setattr ( test , \n method , \n getattr ( SkipCase ( ) , '<STR_LIT>' ) ) \n new_suite . addTest ( test ) \n return <mask0> \n", "gt": "new_suite"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import unicode_literals \n from __future__ import division \n from __future__ import absolute_import \n from future . utils import surrogateescape \n surrogateescape . register_surrogateescape ( ) \n __version__ = '<STR_LIT>' \n __all__ = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT:message>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n def message_from_string ( s , * args , ** kws ) : \n \"\"\"<STR_LIT>\"\"\" \n from future . backports . email . parser import Parser \n return Parser ( * args , ** kws ) . parsestr ( s ) \n def message_from_bytes ( s , * args , ** kws ) : \n \"\"\"<STR_LIT>\"\"\" \n from future . backports . email . parser import BytesParser \n return BytesParser ( * args , ** kws ) . parsebytes ( s ) \n def message_from_file ( fp , * args , ** kws ) : \n \"\"\"<STR_LIT>\"\"\" \n from future . backports . email . parser import Parser \n return Parser ( * args , ** kws ) . parse ( fp ) \n def message_from_binary_file ( fp , * args , ** kws ) : \n \"\"\"<STR_LIT>\"\"\" \n from future . backports . email . parser import BytesParser \n return BytesParser ( * args , ** kws ) . parse ( <mask0> ) \n", "gt": "fp"}
{"input": "\n '''<STR_LIT>''' \n _builtin_next = next \n _SENTINEL = object ( ) \n def newnext ( iterator , default = _SENTINEL ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n try : \n return iterator . __next__ ( ) \n except AttributeError : \n try : \n return iterator . next ( ) \n except AttributeError : \n raise TypeError ( \"<STR_LIT>\" . format ( \n iterator . __class__ . __name__ ) ) \n except StopIteration as e : \n if default is _SENTINEL : \n raise e \n else : \n return default \n <mask0> = [ '<STR_LIT>' ] \n", "gt": "__all__"}
{"input": "\n from __future__ import absolute_import \n from future . utils import PY2 \n from sys import * \n if PY2 : \n from __builtin__ import <mask0> \n", "gt": "intern"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from collections import Iterable \n from numbers import Integral \n import string \n from future . utils import istext , isbytes , PY3 , with_metaclass \n from future . types import no , issubset \n from future . types . newobject import newobject \n _builtin_bytes = bytes \n if PY3 : \n unicode = str \n class BaseNewBytes ( type ) : \n def __instancecheck__ ( cls , instance ) : \n if cls == newbytes : \n return isinstance ( instance , _builtin_bytes ) \n else : \n return issubclass ( instance . __class__ , cls ) \n class newbytes ( with_metaclass ( BaseNewBytes , _builtin_bytes ) ) : \n \"\"\"<STR_LIT>\"\"\" \n def __new__ ( cls , * args , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n encoding = None \n errors = None \n if len ( args ) == <NUM_LIT:0> : \n return super ( newbytes , cls ) . __new__ ( cls ) \n elif len ( args ) >= <NUM_LIT:2> : \n args = list ( args ) \n if len ( args ) == <NUM_LIT:3> : \n errors = args . pop ( ) \n encoding = args . pop ( ) \n if type ( args [ <NUM_LIT:0> ] ) == newbytes : \n return args [ <NUM_LIT:0> ] \n elif isinstance ( args [ <NUM_LIT:0> ] , _builtin_bytes ) : \n value = args [ <NUM_LIT:0> ] \n elif isinstance ( args [ <NUM_LIT:0> ] , unicode ) : \n try : \n if '<STR_LIT>' in kwargs : \n assert encoding is None \n encoding = kwargs [ '<STR_LIT>' ] \n if '<STR_LIT>' in kwargs : \n assert errors is None \n errors = kwargs [ '<STR_LIT>' ] \n except AssertionError : \n raise TypeError ( '<STR_LIT>' ) \n if encoding is None : \n raise TypeError ( '<STR_LIT>' ) \n newargs = [ encoding ] \n if errors is not None : \n newargs . append ( errors ) \n value = args [ <NUM_LIT:0> ] . encode ( * newargs ) \n elif isinstance ( args [ <NUM_LIT:0> ] , Iterable ) : \n if len ( args [ <NUM_LIT:0> ] ) == <NUM_LIT:0> : \n value = b'<STR_LIT>' \n else : \n try : \n values = [ chr ( x ) for x in args [ <NUM_LIT:0> ] ] \n value = b'<STR_LIT>' . join ( values ) \n except : \n raise ValueError ( '<STR_LIT>' ) \n elif isinstance ( args [ <NUM_LIT:0> ] , Integral ) : \n if args [ <NUM_LIT:0> ] < <NUM_LIT:0> : \n raise ValueError ( '<STR_LIT>' ) \n value = b'<STR_LIT:\\x00>' * args [ <NUM_LIT:0> ] \n else : \n value = args [ <NUM_LIT:0> ] \n return super ( newbytes , cls ) . __new__ ( cls , value ) \n def __repr__ ( self ) : \n return '<STR_LIT:b>' + super ( newbytes , self ) . __repr__ ( ) \n def __str__ ( self ) : \n return '<STR_LIT:b>' + \"<STR_LIT>\" . format ( super ( newbytes , self ) . __str__ ( ) ) \n def __getitem__ ( self , y ) : \n value = super ( newbytes , self ) . __getitem__ ( y ) \n if isinstance ( y , Integral ) : \n return ord ( value ) \n else : \n return newbytes ( value ) \n def __getslice__ ( self , * args ) : \n return self . __getitem__ ( slice ( * args ) ) \n def __contains__ ( self , key ) : \n if isinstance ( key , int ) : \n newbyteskey = newbytes ( [ key ] ) \n elif type ( key ) == newbytes : \n newbyteskey = key \n else : \n newbyteskey = newbytes ( key ) \n return issubset ( list ( newbyteskey ) , list ( self ) ) \n @ no ( unicode ) \n def __add__ ( self , other ) : \n return newbytes ( super ( newbytes , self ) . __add__ ( other ) ) \n @ no ( unicode ) \n def __radd__ ( self , left ) : \n return newbytes ( left ) + self \n @ no ( unicode ) \n def __mul__ ( self , other ) : \n return newbytes ( super ( newbytes , self ) . __mul__ ( other ) ) \n @ no ( unicode ) \n def __rmul__ ( self , other ) : \n return newbytes ( super ( newbytes , self ) . __rmul__ ( other ) ) \n def join ( self , iterable_of_bytes ) : \n errmsg = '<STR_LIT>' \n if isbytes ( iterable_of_bytes ) or istext ( iterable_of_bytes ) : \n raise TypeError ( errmsg . format ( <NUM_LIT:0> , type ( iterable_of_bytes ) ) ) \n for i , item in enumerate ( iterable_of_bytes ) : \n if istext ( item ) : \n raise TypeError ( errmsg . format ( i , type ( item ) ) ) \n return newbytes ( super ( newbytes , self ) . join ( iterable_of_bytes ) ) \n @ classmethod \n def fromhex ( cls , string ) : \n return cls ( string . replace ( '<STR_LIT:U+0020>' , '<STR_LIT>' ) . decode ( '<STR_LIT>' ) ) \n @ no ( unicode ) \n def find ( self , sub , * args ) : \n return super ( newbytes , self ) . find ( sub , * args ) \n @ no ( unicode ) \n def rfind ( self , sub , * args ) : \n return super ( newbytes , self ) . rfind ( sub , * args ) \n @ no ( unicode , ( <NUM_LIT:1> , <NUM_LIT:2> ) ) \n def replace ( self , old , new , * args ) : \n return newbytes ( super ( newbytes , self ) . replace ( old , new , * args ) ) \n def encode ( self , * args ) : \n raise AttributeError ( \"<STR_LIT>\" ) \n def decode ( self , encoding = '<STR_LIT:utf-8>' , errors = '<STR_LIT:strict>' ) : \n \"\"\"<STR_LIT>\"\"\" \n from future . types . newstr import newstr \n if errors == '<STR_LIT>' : \n from future . utils . surrogateescape import register_surrogateescape \n register_surrogateescape ( ) \n return newstr ( super ( newbytes , self ) . decode ( encoding , errors ) ) \n @ no ( unicode ) \n def startswith ( self , prefix , * args ) : \n return super ( newbytes , self ) . startswith ( prefix , * args ) \n @ no ( unicode ) \n def endswith ( self , prefix , * args ) : \n return super ( newbytes , self ) . endswith ( prefix , * args ) \n @ no ( unicode ) \n def split ( self , sep = None , maxsplit = - <NUM_LIT:1> ) : \n parts = super ( newbytes , self ) . split ( sep , maxsplit ) \n return [ newbytes ( part ) for part in parts ] \n def splitlines ( self , keepends = False ) : \n \"\"\"<STR_LIT>\"\"\" \n parts = super ( newbytes , self ) . splitlines ( keepends ) \n return [ newbytes ( part ) for part in parts ] \n @ no ( unicode ) \n def rsplit ( self , sep = None , maxsplit = - <NUM_LIT:1> ) : \n parts = super ( newbytes , self ) . rsplit ( sep , maxsplit ) \n return [ newbytes ( part ) for part in parts ] \n @ no ( unicode ) \n def partition ( self , sep ) : \n parts = super ( newbytes , self ) . partition ( sep ) \n return tuple ( newbytes ( part ) for part in parts ) \n @ no ( unicode ) \n def rpartition ( self , sep ) : \n parts = super ( newbytes , self ) . rpartition ( sep ) \n return tuple ( newbytes ( part ) for part in parts ) \n @ no ( unicode , ( <NUM_LIT:1> , ) ) \n def rindex ( self , sub , * args ) : \n '''<STR_LIT>''' \n pos = self . rfind ( sub , * args ) \n if pos == - <NUM_LIT:1> : \n raise ValueError ( '<STR_LIT>' ) \n @ no ( unicode ) \n def index ( self , sub , * args ) : \n '''<STR_LIT>''' \n if isinstance ( sub , int ) : \n if len ( args ) == <NUM_LIT:0> : \n start , end = <NUM_LIT:0> , len ( self ) \n elif len ( args ) == <NUM_LIT:1> : \n start = args [ <NUM_LIT:0> ] \n elif len ( args ) == <NUM_LIT:2> : \n start , end = args \n else : \n raise TypeError ( '<STR_LIT>' ) \n return list ( self ) [ start : end ] . index ( sub ) \n if not isinstance ( sub , bytes ) : \n try : \n sub = self . __class__ ( sub ) \n except ( TypeError , ValueError ) : \n raise TypeError ( \"<STR_LIT>\" ) \n try : \n return super ( newbytes , self ) . index ( sub , * args ) \n except ValueError : \n raise ValueError ( '<STR_LIT>' ) \n def __eq__ ( self , other ) : \n if isinstance ( other , ( _builtin_bytes , bytearray ) ) : \n return super ( newbytes , self ) . __eq__ ( other ) \n else : \n return False \n def __ne__ ( self , other ) : \n if isinstance ( other , _builtin_bytes ) : \n return super ( newbytes , self ) . __ne__ ( other ) \n else : \n return True \n unorderable_err = '<STR_LIT>' \n def __lt__ ( self , other ) : \n if not isbytes ( other ) : \n raise TypeError ( self . unorderable_err . format ( type ( other ) ) ) \n return super ( newbytes , self ) . __lt__ ( other ) \n def __le__ ( self , other ) : \n if not isbytes ( other ) : \n raise TypeError ( self . unorderable_err . format ( type ( other ) ) ) \n return super ( newbytes , self ) . __le__ ( other ) \n def __gt__ ( self , other ) : \n if not isbytes ( other ) : \n raise TypeError ( self . unorderable_err . format ( type ( other ) ) ) \n return super ( newbytes , self ) . __gt__ ( other ) \n def __ge__ ( self , other ) : \n if not isbytes ( other ) : \n raise TypeError ( self . unorderable_err . format ( type ( other ) ) ) \n return super ( newbytes , self ) . __ge__ ( other ) \n def __native__ ( self ) : \n return super ( newbytes , self ) . __str__ ( ) \n def __getattribute__ ( self , name ) : \n \"\"\"<STR_LIT>\"\"\" \n if name in [ '<STR_LIT>' , u'<STR_LIT>' ] : \n raise AttributeError ( \"<STR_LIT>\" ) \n return super ( newbytes , self ) . __getattribute__ ( name ) \n @ no ( unicode ) \n def rstrip ( self , bytes_to_strip = None ) : \n \"\"\"<STR_LIT>\"\"\" \n return newbytes ( super ( newbytes , self ) . rstrip ( bytes_to_strip ) ) \n @ no ( unicode ) \n def strip ( self , bytes_to_strip = None ) : \n \"\"\"<STR_LIT>\"\"\" \n return newbytes ( super ( newbytes , self ) . strip ( bytes_to_strip ) ) \n def lower ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return newbytes ( super ( newbytes , self ) . lower ( ) ) \n @ no ( unicode ) \n def upper ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return newbytes ( super ( newbytes , self ) . upper ( ) ) \n @ classmethod \n @ no ( unicode ) \n def maketrans ( cls , frm , to ) : \n \"\"\"<STR_LIT>\"\"\" \n return newbytes ( string . maketrans ( frm , to ) ) \n <mask0> = [ '<STR_LIT>' ] \n", "gt": "__all__"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from libpasteurize . fixes . fix_division import <mask0> \n", "gt": "FixDivision"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import unicode_literals \n from lib2to3 import fixer_base \n from lib2to3 . pygram import python_symbols as syms \n from lib2to3 . fixer_util import Name , Call , in_special_context \n from libfuturize . fixer_util import touch_import_top \n replaced_builtins = '''<STR_LIT>''' . split ( ) \n expression = '<STR_LIT:|>' . join ( [ \"<STR_LIT>\" . format ( name ) for name in replaced_builtins ] ) \n class FixFutureBuiltins ( fixer_base . BaseFix ) : \n BM_compatible = True \n run_order = <NUM_LIT:9> \n PATTERN = \"\"\"<STR_LIT>\"\"\" . format ( expression ) \n def transform ( self , node , results ) : \n name = results [ \"<STR_LIT:name>\" ] \n touch_import_top ( u'<STR_LIT>' , name . value , <mask0> ) \n", "gt": "node"}
{"input": "\n from __future__ import absolute_import \n import sys \n if sys . version_info [ <NUM_LIT:0> ] < <NUM_LIT:3> : \n from Tkinter import * \n else : \n raise <mask0> ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n", "gt": "ImportError"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import print_function , absolute_import \n import pprint \n from subprocess import Popen , PIPE \n import tempfile \n import os \n from future . tests . base import CodeHandler , unittest , skip26 \n class TestPasteurize ( CodeHandler ) : \n \"\"\"<STR_LIT>\"\"\" \n def setUp ( self ) : \n _ , self . textfilename = tempfile . mkstemp ( text = True ) \n super ( TestPasteurize , self ) . setUp ( ) \n def tearDown ( self ) : \n os . unlink ( self . textfilename ) \n @ skip26 \n def test_range_slice ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n code = '''<STR_LIT>''' \n self . unchanged ( code , from3 = True ) \n def test_print ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n code = '''<STR_LIT>''' \n self . unchanged ( code , from3 = True ) \n def test_division ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n code = '''<STR_LIT>''' \n self . unchanged ( code , from3 = True ) \n @ unittest . expectedFailure \n def test_exception_indentation ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n before = '''<STR_LIT>''' \n after = '''<STR_LIT>''' \n self . convert_check ( before , after , from3 = True ) \n @ unittest . expectedFailure \n def test_urllib_request ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n before = \"\"\"<STR_LIT>\"\"\" \n after = \"\"\"<STR_LIT>\"\"\" \n self . convert_check ( before , after , from3 = True ) \n def test_urllib_refactor2 ( self ) : \n before = \"\"\"<STR_LIT>\"\"\" \n after = \"\"\"<STR_LIT>\"\"\" \n def test_correct_exit_status ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n from libpasteurize . main import main \n retcode = main ( [ self . textfilename ] ) \n self . assertTrue ( isinstance ( retcode , int ) ) \n class TestFuturizeAnnotations ( CodeHandler ) : \n @ unittest . expectedFailure \n def test_return_annotations_alone ( self ) : \n before = \"<STR_LIT>\" \n after = \"\"\"<STR_LIT>\"\"\" \n self . convert_check ( before , after , from3 = True ) \n b = \"\"\"<STR_LIT>\"\"\" \n a = \"\"\"<STR_LIT>\"\"\" \n self . convert_check ( b , a , from3 = True ) \n @ unittest . expectedFailure \n def test_single_param_annotations ( self ) : \n b = \"<STR_LIT>\" \n a = \"\"\"<STR_LIT>\"\"\" \n self . convert_check ( b , a , from3 = True ) \n b = \"\"\"<STR_LIT>\"\"\" \n a = \"\"\"<STR_LIT>\"\"\" \n self . convert_check ( b , a , from3 = True ) \n def test_multiple_param_annotations ( self ) : \n b = \"<STR_LIT>\" \n a = \"<STR_LIT>\" \n self . convert_check ( b , a , from3 = True ) \n b = \"\"\"<STR_LIT>\"\"\" \n a = \"\"\"<STR_LIT>\"\"\" \n self . convert_check ( b , a , from3 = True ) \n def test_mixed_annotations ( self ) : \n b = \"<STR_LIT>\" \n a = \"<STR_LIT>\" \n self . convert_check ( b , a , from3 = True ) \n b = \"\"\"<STR_LIT>\"\"\" \n a = \"\"\"<STR_LIT>\"\"\" \n self . convert_check ( b , a , from3 = True ) \n b = \"<STR_LIT>\" \n a = \"<STR_LIT>\" \n self . convert_check ( b , a , from3 = True ) \n def test_functions_unchanged ( self ) : \n s = \"<STR_LIT>\" \n self . unchanged ( s , from3 = True ) \n s = \"\"\"<STR_LIT>\"\"\" \n self . unchanged ( s , from3 = True ) \n s = \"\"\"<STR_LIT>\"\"\" \n self . unchanged ( s , from3 = True ) \n if __name__ == '<STR_LIT:__main__>' : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import ast , copy \n from ast_utils import * \n class Inliner : \n def setup_inliner ( self , writer ) : \n self . writer = writer \n self . _with_inline = False \n self . _inline = [ ] \n self . _inline_ids = <NUM_LIT:0> \n self . _inline_breakout = False \n def inline_helper_remap_names ( self , remap ) : \n return \"<STR_LIT>\" % '<STR_LIT:U+002C>' . join ( remap . values ( ) ) \n def inline_helper_return_id ( self , return_id ) : \n return \"<STR_LIT>\" % return_id \n def inline_function ( self , node ) : \n name = self . visit ( node . func ) \n fnode = self . _global_functions [ name ] \n fnode = copy . deepcopy ( fnode ) \n finfo = inspect_function ( fnode ) \n remap = { } \n for n in finfo [ '<STR_LIT>' ] : \n if n . id not in finfo [ '<STR_LIT>' ] : continue \n if isinstance ( n . id , ast . Name ) : \n raise RuntimeError \n if n . id not in remap : \n new_name = n . id + '<STR_LIT>' % self . _inline_ids \n remap [ n . id ] = new_name \n self . _inline_ids += <NUM_LIT:1> \n n . id = remap [ n . id ] \n if remap : \n self . writer . write ( self . inline_helper_remap_names ( remap ) ) \n for n in remap : \n if n in finfo [ '<STR_LIT>' ] : \n self . _func_typedefs [ remap [ n ] ] = finfo [ '<STR_LIT>' ] [ n ] \n offset = len ( fnode . args . args ) - len ( fnode . args . defaults ) \n for i , ad in enumerate ( fnode . args . args ) : \n if i < len ( node . args ) : \n ac = self . visit ( node . args [ i ] ) \n else : \n assert fnode . args . defaults \n dindex = i - offset \n ac = self . visit ( fnode . args . defaults [ dindex ] ) \n ad = remap [ self . visit ( ad ) ] \n self . writer . write ( \"<STR_LIT>\" % ( ad , ac ) ) \n return_id = name + str ( self . _inline_ids ) \n self . _inline . append ( return_id ) \n self . writer . write ( self . inline_helper_return_id ( return_id ) ) \n if True : \n self . _inline_breakout = True \n self . writer . write ( '<STR_LIT>' ) \n self . writer . push ( ) \n for b in fnode . body : \n self . visit ( b ) \n if not len ( finfo [ '<STR_LIT>' ] ) : \n self . writer . write ( '<STR_LIT>' ) \n self . writer . pull ( ) \n else : \n for b in fnode . body : \n self . visit ( b ) \n if self . _inline . pop ( ) != return_id : \n raise RuntimeError \n for n in remap : \n gname = remap [ n ] \n for n in finfo [ '<STR_LIT>' ] : \n if n . id == gname : \n n . id = n \n return '<STR_LIT>' % <mask0> \n", "gt": "return_id"}
{"input": "\n import sys \n import ast \n import pythonjs \n class TransformSuperCalls ( ast . NodeVisitor ) : \n def __init__ ( self , node , class_names ) : \n self . _class_names = class_names \n self . visit ( node ) \n def visit_Call ( self , node ) : \n if isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id in self . _class_names : \n node . func . attr = '<STR_LIT>' + node . func . attr \n class CollectNames ( ast . NodeVisitor ) : \n def __init__ ( self ) : \n self . _names = [ ] \n def visit_Name ( self , node ) : \n self . _names . append ( node ) \n def collect_names ( node ) : \n a = CollectNames ( ) \n a . visit ( node ) \n return a . _names \n class DartGenerator ( pythonjs . JSGenerator ) : \n def __init__ ( self , requirejs = False , insert_runtime = False ) : \n pythonjs . JSGenerator . __init__ ( self , requirejs = False , insert_runtime = False ) \n self . _classes = dict ( ) \n self . _class_props = dict ( ) \n self . _raw_dict = False \n def visit_With ( self , node ) : \n s = [ ] \n for b in node . body : \n a = self . visit ( b ) \n a = a . replace ( '<STR_LIT>' , '<STR_LIT:\\n>' ) \n a = a . strip ( ) [ <NUM_LIT:1> : - <NUM_LIT:2> ] \n s . append ( a ) \n return '<STR_LIT:\\n>' . join ( s ) \n def _visit_subscript_ellipsis ( self , node ) : \n name = self . visit ( node . value ) \n return '<STR_LIT>' % name \n def visit_List ( self , node ) : \n return '<STR_LIT>' % '<STR_LIT:U+002CU+0020>' . join ( map ( self . visit , node . elts ) ) \n def visit_Dict ( self , node ) : \n a = [ ] \n for i in range ( len ( node . keys ) ) : \n k = self . visit ( node . keys [ i ] ) \n v = self . visit ( node . values [ i ] ) \n a . append ( '<STR_LIT>' % ( k , v ) ) \n b = '<STR_LIT:U+002C>' . join ( a ) \n if self . _raw_dict : \n return '<STR_LIT>' % b \n else : \n return '<STR_LIT>' % b \n def visit_ClassDef ( self , node ) : \n node . _parents = set ( ) \n out = [ ] \n extends = False \n props = set ( [ '<STR_LIT>' ] ) \n bases = set ( ) \n base_classes = set ( ) \n self . _classes [ node . name ] = node \n self . _class_props [ node . name ] = props \n for decor in node . decorator_list : \n if isinstance ( decor , ast . Call ) : \n props . update ( [ self . visit ( a ) for a in decor . args ] ) \n elif isinstance ( decor , ast . Attribute ) and isinstance ( decor . value , ast . Name ) and decor . value . id == '<STR_LIT>' : \n if decor . attr == '<STR_LIT>' : \n extends = True \n props . add ( '<STR_LIT>' ) \n for name_node in collect_names ( node ) : \n if name_node . id == '<STR_LIT>' : \n name_node . id = '<STR_LIT>' \n else : \n raise SyntaxError \n for base in node . bases : \n n = self . visit ( base ) \n if n == '<STR_LIT:object>' : \n continue \n node . _parents . add ( n ) \n bases . add ( n ) \n if n in self . _class_props : \n props . update ( self . _class_props [ n ] ) \n base_classes . add ( self . _classes [ n ] ) \n else : \n continue \n for p in self . _classes [ n ] . _parents : \n bases . add ( p ) \n props . update ( self . _class_props [ p ] ) \n base_classes . add ( self . _classes [ p ] ) \n if bases : \n if extends : \n assert len ( bases ) == <NUM_LIT:1> \n out . append ( '<STR_LIT>' % ( node . name , '<STR_LIT:U+002C>' . join ( bases ) ) ) \n else : \n out . append ( '<STR_LIT>' % ( node . name , '<STR_LIT:U+002CU+0020>' . join ( bases ) ) ) \n else : \n out . append ( '<STR_LIT>' % node . name ) \n self . push ( ) \n for p in props : \n out . append ( self . indent ( ) + '<STR_LIT>' % p ) \n method_names = set ( ) \n for b in node . body : \n if isinstance ( b , ast . With ) : \n out . append ( self . visit ( b ) ) \n elif isinstance ( b , ast . FunctionDef ) and len ( b . decorator_list ) : \n for name_node in collect_names ( b ) : \n if name_node . id == '<STR_LIT>' : \n name_node . id = '<STR_LIT>' \n b . args . args = b . args . args [ <NUM_LIT:1> : ] \n out . append ( self . visit ( b ) ) \n elif extends : \n if isinstance ( b , ast . FunctionDef ) : \n b . args . args = b . args . args [ <NUM_LIT:1> : ] \n if b . name == node . name : \n args = [ self . visit ( a ) for a in b . args . args ] \n args = '<STR_LIT:U+002C>' . join ( args ) \n out . append ( \n self . indent ( ) + '<STR_LIT>' % ( node . name , args , args ) \n ) \n b . name = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n b . name = '<STR_LIT>' \n b . _prefix = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n b . name = '<STR_LIT>' \n b . _prefix = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n b . name = '<STR_LIT>' \n b . _prefix = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n b . name = '<STR_LIT>' \n b . _prefix = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n b . name = '<STR_LIT>' \n b . _prefix = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n b . name = '<STR_LIT>' \n b . _prefix = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n b . name = '<STR_LIT>' \n b . _prefix = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n b . name = '<STR_LIT>' \n b . _prefix = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n b . name = '<STR_LIT>' \n b . _prefix = '<STR_LIT>' \n line = self . visit ( b ) \n out . append ( line ) \n elif isinstance ( b , ast . FunctionDef ) and b . name == node . name : \n args , kwargs = self . get_args_kwargs_from_funcdef ( b , skip_self = True ) \n kwargs_init = [ '<STR_LIT>' % ( x . split ( '<STR_LIT::>' ) [ <NUM_LIT:0> ] , x . split ( '<STR_LIT::>' ) [ <NUM_LIT:0> ] ) for x in kwargs ] \n b . _prefix = '<STR_LIT>' \n b . name = '<STR_LIT>' \n out . append ( self . visit ( b ) ) \n if args : \n args = '<STR_LIT:U+002C>' . join ( args ) \n if kwargs : \n out . append ( \n self . indent ( ) + '<STR_LIT>' % ( node . name , args , '<STR_LIT:U+002C>' . join ( kwargs ) , node . name , args , '<STR_LIT:U+002C>' . join ( kwargs_init ) ) \n ) \n else : \n out . append ( \n self . indent ( ) + '<STR_LIT>' % ( node . name , args , node . name , args ) \n ) \n elif kwargs : \n out . append ( \n self . indent ( ) + '<STR_LIT>' % ( node . name , '<STR_LIT:U+002C>' . join ( kwargs ) , node . name , '<STR_LIT:U+002C>' . join ( kwargs_init ) ) \n ) \n else : \n out . append ( \n self . indent ( ) + '<STR_LIT>' % ( node . name , node . name ) \n ) \n elif isinstance ( b , ast . FunctionDef ) : \n method_names . add ( b . name ) \n TransformSuperCalls ( b , bases ) \n operator = False \n if b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n elif b . name == '<STR_LIT>' : \n operator = '<STR_LIT>' \n args = [ self . visit ( a ) for a in b . args . args ] [ <NUM_LIT:1> : ] \n args = '<STR_LIT:U+002C>' . join ( args ) \n if operator and args : \n out . append ( self . indent ( ) + '<STR_LIT>' % ( operator , args , node . name , b . name , args ) ) \n elif operator : \n out . append ( self . indent ( ) + '<STR_LIT>' % ( operator , node . name , b . name ) ) \n elif args : \n out . append ( self . indent ( ) + '<STR_LIT>' % ( b . name , args , node . name , b . name , args ) ) \n else : \n out . append ( self . indent ( ) + '<STR_LIT>' % ( b . name , node . name , b . name ) ) \n b . _prefix = '<STR_LIT>' \n name = b . name \n b . name = '<STR_LIT>' % name \n out . append ( self . visit ( b ) ) \n b . name = name \n else : \n line = self . visit ( b ) \n if line . startswith ( '<STR_LIT>' ) : \n out . append ( self . indent ( ) + line ) \n else : \n out . append ( line ) \n if not extends and base_classes : \n for bnode in base_classes : \n for b in bnode . body : \n if isinstance ( b , ast . FunctionDef ) : \n if b . name == '<STR_LIT>' : continue \n if b . name in method_names : continue \n args = [ self . visit ( a ) for a in b . args . args ] [ <NUM_LIT:1> : ] \n args = '<STR_LIT:U+002C>' . join ( args ) \n if args : \n out . append ( self . indent ( ) + '<STR_LIT>' % ( b . name , args , bnode . name , b . name , args ) ) \n else : \n out . append ( self . indent ( ) + '<STR_LIT>' % ( b . name , bnode . name , b . name ) ) \n self . pull ( ) \n out . append ( '<STR_LIT:}>' ) \n return '<STR_LIT:\\n>' . join ( out ) \n def get_args_kwargs_from_funcdef ( self , node , skip_self = False ) : \n args = [ ] \n kwargs = [ ] \n if skip_self : nargs = node . args . args [ <NUM_LIT:1> : ] \n else : nargs = node . args . args \n offset = len ( nargs ) - len ( node . args . defaults ) \n for i , arg in enumerate ( nargs ) : \n a = arg . id \n dindex = i - offset \n if dindex >= <NUM_LIT:0> and node . args . defaults : \n default_value = self . visit ( node . args . defaults [ dindex ] ) \n kwargs . append ( '<STR_LIT>' % ( a , default_value ) ) \n else : \n args . append ( a ) \n return args , kwargs \n def _visit_for_prep_iter_helper ( self , node , out , iter_name ) : \n out . append ( \n self . indent ( ) + '<STR_LIT>' % ( iter_name , iter_name , iter_name ) \n ) \n def visit_Expr ( self , node ) : \n s = self . visit ( node . value ) \n if isinstance ( node . value , ast . Call ) and isinstance ( node . value . func , ast . Name ) and node . value . func . id == '<STR_LIT>' : \n if s . endswith ( '<STR_LIT:}>' ) and '<STR_LIT>' in s . split ( '<STR_LIT:U+0020>' ) : \n pass \n elif not s . endswith ( '<STR_LIT:;>' ) : \n s += '<STR_LIT:;>' \n elif not s . endswith ( '<STR_LIT:;>' ) : \n s += '<STR_LIT:;>' \n return s \n def visit_Print ( self , node ) : \n args = [ self . visit ( e ) for e in node . values ] \n if len ( args ) > <NUM_LIT:1> : \n s = '<STR_LIT>' % '<STR_LIT:U+002CU+0020>' . join ( args ) \n else : \n s = '<STR_LIT>' % '<STR_LIT:U+002CU+0020>' . join ( args ) \n return s \n def visit_Assign ( self , node ) : \n assert len ( node . targets ) == <NUM_LIT:1> \n target = node . targets [ <NUM_LIT:0> ] \n if isinstance ( target , ast . Tuple ) : \n elts = [ self . visit ( e ) for e in target . elts ] \n if self . indent ( ) : \n return '<STR_LIT>' % ( '<STR_LIT:U+002C>' . join ( elts ) , self . visit ( node . value ) ) \n else : \n return '<STR_LIT>' % ( '<STR_LIT:U+002C>' . join ( elts ) , self . visit ( node . value ) ) \n else : \n target = self . visit ( target ) \n value = self . visit ( node . value ) \n if self . indent ( ) : \n code = '<STR_LIT>' % ( target , value ) \n else : \n code = '<STR_LIT>' % ( target , value ) \n return code \n def _visit_function ( self , node ) : \n getter = False \n setter = False \n args_typedefs = { } \n for decor in node . decorator_list : \n if isinstance ( decor , ast . Name ) and decor . id == '<STR_LIT>' : \n getter = True \n elif isinstance ( decor , ast . Attribute ) and isinstance ( decor . value , ast . Name ) and decor . attr == '<STR_LIT>' : \n setter = True \n elif isinstance ( decor , ast . Call ) and isinstance ( decor . func , ast . Name ) and decor . func . id == '<STR_LIT>' : \n for key in decor . keywords : \n args_typedefs [ key . arg ] = key . value . id \n else : \n raise SyntaxError \n args = [ ] \n oargs = [ ] \n offset = len ( node . args . args ) - len ( node . args . defaults ) \n varargs = False \n varargs_name = None \n for i , arg in enumerate ( node . args . args ) : \n a = arg . id \n if a in args_typedefs : \n a = '<STR_LIT>' % ( args_typedefs [ a ] , a ) \n dindex = i - offset \n if a . startswith ( '<STR_LIT>' ) : \n varargs_name = a . split ( '<STR_LIT>' ) [ - <NUM_LIT:1> ] \n varargs = [ '<STR_LIT>' % n for n in range ( <NUM_LIT:16> ) ] \n args . append ( '<STR_LIT>' % '<STR_LIT:U+002C>' . join ( varargs ) ) \n elif dindex >= <NUM_LIT:0> and node . args . defaults : \n default_value = self . visit ( node . args . defaults [ dindex ] ) \n oargs . append ( '<STR_LIT>' % ( a , default_value ) ) \n else : \n args . append ( a ) \n if oargs : \n args . append ( '<STR_LIT>' % '<STR_LIT:U+002C>' . join ( oargs ) ) \n buffer = self . indent ( ) \n if hasattr ( node , '<STR_LIT>' ) : buffer += node . _prefix + '<STR_LIT:U+0020>' \n if getter : \n buffer += '<STR_LIT>' % node . name \n elif setter : \n buffer += '<STR_LIT>' % ( node . name , '<STR_LIT:U+002CU+0020>' . join ( args ) ) \n else : \n buffer += '<STR_LIT>' % ( node . name , '<STR_LIT:U+002CU+0020>' . join ( args ) ) \n self . push ( ) \n if varargs : \n buffer += '<STR_LIT>' % varargs_name \n for i , n in enumerate ( varargs ) : \n buffer += '<STR_LIT>' % ( n , varargs_name , n ) \n body = list ( ) \n for child in node . body : \n if isinstance ( child , ast . Str ) : \n continue \n else : \n body . append ( self . indent ( ) + self . visit ( child ) ) \n buffer += '<STR_LIT:\\n>' . join ( body ) \n self . pull ( ) \n buffer += '<STR_LIT>' % self . indent ( ) \n return buffer \n def visit_Is ( self , node ) : \n return '<STR_LIT>' \n def visit_IsNot ( self , node ) : \n return '<STR_LIT>' \n def visit_NotEq ( self , node ) : \n return '<STR_LIT>' \n def _visit_call_helper ( self , node ) : \n if node . args : \n args = [ self . visit ( e ) for e in node . args ] \n args = '<STR_LIT:U+002CU+0020>' . join ( [ e for e in args if e ] ) \n else : \n args = '<STR_LIT>' \n if isinstance ( node . func , ast . Name ) and node . func . id == '<STR_LIT>' and len ( node . args ) == <NUM_LIT:2> : \n func = '<STR_LIT>' \n else : \n func = self . visit ( node . func ) \n if node . keywords : \n kwargs = '<STR_LIT:U+002C>' . join ( [ '<STR_LIT>' % ( x . arg , self . visit ( x . value ) ) for x in node . keywords ] ) \n if args : \n return '<STR_LIT>' % ( func , '<STR_LIT:U+002C>' . join ( args ) , kwargs ) \n else : \n return '<STR_LIT>' % ( func , kwargs ) \n else : \n return '<STR_LIT>' % ( func , args ) \n def _visit_call_helper_var ( self , node ) : \n args = [ self . visit ( a ) for a in node . args ] \n if self . _function_stack : \n fnode = self . _function_stack [ - <NUM_LIT:1> ] \n rem = [ ] \n for arg in args : \n if arg in fnode . _local_vars : \n rem . append ( arg ) \n else : \n fnode . _local_vars . add ( arg ) \n for arg in rem : \n args . remove ( arg ) \n out = [ ] \n if args : \n out . append ( '<STR_LIT>' + '<STR_LIT:U+002C>' . join ( args ) ) \n if node . keywords : \n for key in node . keywords : \n out . append ( '<STR_LIT>' % ( key . value . id , key . arg ) ) \n return '<STR_LIT:;>' . join ( out ) \n def _visit_call_helper_list ( self , node ) : \n name = self . visit ( node . func ) \n if node . args : \n args = [ self . visit ( e ) for e in node . args ] \n args = '<STR_LIT:U+002CU+0020>' . join ( [ e for e in args if e ] ) \n else : \n args = '<STR_LIT>' \n return '<STR_LIT>' % ( name , args ) \n def _visit_call_helper_numpy_array ( self , node ) : \n simd = { \n '<STR_LIT>' : '<STR_LIT>' \n } \n arg_name = args = None \n direct = False \n if isinstance ( node . args [ <NUM_LIT:0> ] , ast . Name ) : \n arg_name = node . args [ <NUM_LIT:0> ] . id \n else : \n args = '<STR_LIT:U+002C>' . join ( [ self . visit ( a ) for a in node . args [ <NUM_LIT:0> ] . elts ] ) \n if len ( node . args [ <NUM_LIT:0> ] . elts ) == <NUM_LIT:4> : \n direct = True \n if node . keywords : \n for key in node . keywords : \n if key . arg == '<STR_LIT>' : \n if isinstance ( key . value , ast . Attribute ) and key . value . attr in simd : \n if arg_name : \n return '<STR_LIT>' % arg_name \n elif direct : \n return '<STR_LIT>' % ( simd [ key . value . attr ] , args ) \n else : \n return '<STR_LIT>' % args \n else : \n raise NotImplementedError ( '<STR_LIT>' ) \n def _visit_call_helper_instanceof ( self , node ) : \n args = map ( self . visit , node . args ) \n if len ( args ) == <NUM_LIT:2> : \n if args [ <NUM_LIT:1> ] == '<STR_LIT>' : \n args [ <NUM_LIT:1> ] = '<STR_LIT>' \n return '<STR_LIT>' % tuple ( args ) \n else : \n raise SyntaxError ( args ) \n def visit_ExceptHandler ( self , node ) : \n return '<STR_LIT:\\n>' . join ( [ self . visit ( n ) for n in node . body ] ) \n def visit_Compare ( self , node ) : \n specials = { \n '<STR_LIT:<>' : '<STR_LIT>' , \n '<STR_LIT:>>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n comp = [ ] \n if len ( node . ops ) == <NUM_LIT:0> : \n comp . append ( '<STR_LIT:(>' ) \n comp . append ( self . visit ( node . left ) ) \n comp . append ( '<STR_LIT:)>' ) \n else : \n if self . visit ( node . ops [ <NUM_LIT:0> ] ) in specials : \n pass \n else : \n comp . append ( '<STR_LIT:(>' ) \n comp . append ( self . visit ( node . left ) ) \n comp . append ( '<STR_LIT:)>' ) \n for i in range ( len ( node . ops ) ) : \n op = self . visit ( node . ops [ i ] ) \n if op in specials : \n comp . append ( specials [ op ] + '<STR_LIT>' % self . visit ( node . left ) ) \n else : \n comp . append ( op ) \n if isinstance ( node . comparators [ i ] , ast . BinOp ) : \n comp . append ( '<STR_LIT:(>' ) \n comp . append ( self . visit ( node . comparators [ i ] ) ) \n comp . append ( '<STR_LIT:)>' ) \n else : \n comp . append ( self . visit ( node . comparators [ i ] ) ) \n if op in specials : \n comp . append ( '<STR_LIT:)>' ) \n return '<STR_LIT:U+0020>' . join ( comp ) \n def main ( script ) : \n tree = ast . parse ( script ) \n return DartGenerator ( ) . visit ( tree ) \n def command ( ) : \n scripts = [ ] \n if len ( sys . argv ) > <NUM_LIT:1> : \n for arg in sys . argv [ <NUM_LIT:1> : ] : \n if arg . endswith ( '<STR_LIT>' ) : \n scripts . append ( arg ) \n if len ( scripts ) : \n a = [ ] \n for script in scripts : \n a . append ( open ( script , '<STR_LIT:rb>' ) . read ( ) ) \n data = '<STR_LIT:\\n>' . join ( a ) \n else : \n data = sys . stdin . read ( ) \n js = main ( data ) \n print ( js ) \n if __name__ == '<STR_LIT:__main__>' : \n <mask0> ( ) \n", "gt": "command"}
{"input": "\n '''<STR_LIT>''' \n from time import time \n from time import sleep \n import threading \n def main ( ) : \n if PYTHON == '<STR_LIT>' : \n pythonjs . configure ( direct_operator = '<STR_LIT:+>' ) \n pass \n starttime = time ( ) \n n = <NUM_LIT> \n seq = [ ] \n cache = [ ] \n w1 = threading . start_webworker ( worker , ( <NUM_LIT:0> , n , seq , cache ) ) \n sleep ( <NUM_LIT:1.0> ) \n testtime = time ( ) - starttime \n primes_per_sec = len ( seq ) * ( <NUM_LIT:1.0> / testtime ) \n print ( primes_per_sec ) \n print ( '<STR_LIT>' % testtime ) \n print ( '<STR_LIT>' ) \n with webworker : \n def worker ( start , end , seq , cache ) : \n print ( '<STR_LIT>' ) \n for i in range ( start , end ) : \n if i in cache : \n pass \n else : \n cache . append ( i ) \n if is_prime ( i ) : \n seq . append ( i ) \n print ( '<STR_LIT>' % i ) \n def is_prime ( n ) : \n hits = <NUM_LIT:0> \n for x in range ( <NUM_LIT:2> , n ) : \n for y in range ( <NUM_LIT:2> , n ) : \n if x * y == n : \n hits += <NUM_LIT:1> \n if hits > <NUM_LIT:1> : \n return False \n return <mask0> \n", "gt": "True"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n def main ( ) : \n d = { } \n if d : \n err1 = <NUM_LIT:1> \n else : \n err1 = <NUM_LIT:0> \n if { } : \n err2 = <NUM_LIT:1> \n else : \n err2 = <NUM_LIT:0> \n d [ '<STR_LIT:x>' ] = '<STR_LIT>' \n if d : \n err3 = <NUM_LIT:0> \n else : \n err3 = <NUM_LIT:1> \n TestError ( err1 == <NUM_LIT:0> ) \n TestError ( err2 == <NUM_LIT:0> ) \n TestError ( <mask0> == <NUM_LIT:0> ) \n", "gt": "err3"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n def main ( ) : \n a = False \n b = False \n if not a : \n b = True \n TestError ( b == True ) \n a = <NUM_LIT:0> \n b = False \n if not a : \n b = True \n TestError ( b == True ) \n a = <NUM_LIT:0.0> \n b = False \n if not a : \n b = True \n TestError ( b == True ) \n a = None \n b = False \n if not a : \n b = True \n TestError ( b == <mask0> ) \n", "gt": "True"}
{"input": "\n '''<STR_LIT>''' \n def main ( ) : \n a = range ( <NUM_LIT:10> ) \n TestError ( a [ <NUM_LIT:0> ] == <NUM_LIT:0> ) \n TestError ( a [ <NUM_LIT:1> ] == <NUM_LIT:1> ) \n TestError ( len ( a ) == <NUM_LIT:10> ) \n b = range ( <NUM_LIT:1> , <NUM_LIT:10> ) \n TestError ( b [ <NUM_LIT:0> ] == <NUM_LIT:1> ) \n TestError ( b [ <NUM_LIT:1> ] == <NUM_LIT:2> ) \n TestError ( len ( b ) == <NUM_LIT:9> ) \n c = <NUM_LIT:0> \n for i in range ( <NUM_LIT:10> ) : \n c += <NUM_LIT:1> \n TestError ( c == <NUM_LIT:10> ) \n d = <NUM_LIT:0> \n for i in range ( <NUM_LIT:1> , <NUM_LIT:10> ) : \n d += <NUM_LIT:1> \n TestError ( d == <NUM_LIT:9> ) \n e = <NUM_LIT:0> \n for i in range ( <NUM_LIT:1> , <NUM_LIT:8> + <NUM_LIT:2> ) : \n e += <NUM_LIT:1> \n TestError ( <mask0> == <NUM_LIT:9> ) \n", "gt": "e"}
{"input": "\n '''<STR_LIT>''' \n from time import time \n from time import sleep \n import threading \n def main ( ) : \n if PYTHON == '<STR_LIT>' : \n pythonjs . configure ( direct_operator = '<STR_LIT:+>' ) \n pass \n else : \n def l ( f , a ) : threading . _start_new_thread ( f , a ) \n threading . start_webworker = l \n seq = { } \n w1 = threading . start_webworker ( worker , ( seq , '<STR_LIT>' , '<STR_LIT:i>' ) ) \n w2 = threading . start_webworker ( worker , ( seq , '<STR_LIT>' , '<STR_LIT:p>' ) ) \n sleep ( <NUM_LIT:1.0> ) \n TestError ( '<STR_LIT:a>' in seq ) \n TestError ( '<STR_LIT:i>' in seq ) \n print ( '<STR_LIT>' ) \n print ( seq ) \n if PYTHON != '<STR_LIT>' : \n class webworker ( object ) : \n def __enter__ ( self , * args ) : pass \n def __exit__ ( self , * args ) : pass \n webworker = webworker ( ) \n with webworker : \n def worker ( seq , s , break_on ) : \n print ( '<STR_LIT>' ) \n for char in s : \n seq [ char ] = True \n if break_on in seq : \n break \n sleep ( <NUM_LIT:0.1> ) \n print ( '<STR_LIT>' ) \n print ( <mask0> ) \n", "gt": "seq"}
{"input": "\n from OpenGL . GL import * \n from OpenGL . GLU import * \n import pygame \n import os . path \n class Material ( object ) : \n def __init__ ( self ) : \n self . name = \"<STR_LIT>\" \n self . texture_fname = None \n self . texture_id = None \n class FaceGroup ( object ) : \n def __init__ ( self ) : \n self . tri_indices = [ ] \n self . material_name = \"<STR_LIT>\" \n class Model3D ( object ) : \n def __init__ ( self ) : \n self . vertices = [ ] \n self . tex_coords = [ ] \n self . normals = [ ] \n self . materials = { } \n self . face_groups = [ ] \n self . display_list_id = None \n def __del__ ( self ) : \n self . free_resources ( ) \n def free_resources ( self ) : \n if self . display_list_id is not None : \n glDeleteLists ( self . display_list_id , <NUM_LIT:1> ) \n self . display_list_id = None \n for material in self . materials . values ( ) : \n if material . texture_id is not None : \n glDeleteTextures ( material . texture_id ) \n self . materials . clear ( ) \n del self . vertices [ : ] \n del self . tex_coords [ : ] \n del self . normals [ : ] \n del self . face_groups [ : ] \n def read_obj ( self , fname ) : \n current_face_group = None \n file_in = open ( fname ) \n for line in file_in : \n words = line . split ( ) \n command = words [ <NUM_LIT:0> ] \n data = words [ <NUM_LIT:1> : ] \n if command == '<STR_LIT>' : \n model_path = os . path . split ( fname ) [ <NUM_LIT:0> ] \n mtllib_path = os . path . join ( model_path , data [ <NUM_LIT:0> ] ) \n self . read_mtllib ( mtllib_path ) \n elif command == '<STR_LIT:v>' : \n x , y , z = data \n vertex = ( float ( x ) , float ( y ) , float ( z ) ) \n self . vertices . append ( vertex ) \n elif command == '<STR_LIT>' : \n s , t = data \n tex_coord = ( float ( s ) , float ( t ) ) \n self . tex_coords . append ( tex_coord ) \n elif command == '<STR_LIT>' : \n x , y , z = data \n normal = ( float ( x ) , float ( y ) , float ( z ) ) \n self . normals . append ( normal ) \n elif command == '<STR_LIT>' : \n current_face_group = FaceGroup ( ) \n current_face_group . material_name = data [ <NUM_LIT:0> ] \n self . face_groups . append ( current_face_group ) \n elif command == '<STR_LIT:f>' : \n assert len ( data ) == <NUM_LIT:3> , \"<STR_LIT>\" \n for word in data : \n vi , ti , ni = word . split ( '<STR_LIT:/>' ) \n indices = ( int ( vi ) - <NUM_LIT:1> , int ( ti ) - <NUM_LIT:1> , int ( ni ) - <NUM_LIT:1> ) \n current_face_group . tri_indices . append ( indices ) \n for material in self . materials . values ( ) : \n model_path = os . path . split ( fname ) [ <NUM_LIT:0> ] \n texture_path = os . path . join ( model_path , material . texture_fname ) \n texture_surface = pygame . image . load ( texture_path ) \n texture_data = pygame . image . tostring ( texture_surface , '<STR_LIT>' , True ) \n material . texture_id = glGenTextures ( <NUM_LIT:1> ) \n glBindTexture ( GL_TEXTURE_2D , material . texture_id ) \n glTexParameteri ( GL_TEXTURE_2D , \n GL_TEXTURE_MAG_FILTER , \n GL_LINEAR ) \n glTexParameteri ( GL_TEXTURE_2D , \n GL_TEXTURE_MIN_FILTER , \n GL_LINEAR_MIPMAP_LINEAR ) \n glPixelStorei ( GL_UNPACK_ALIGNMENT , <NUM_LIT:1> ) \n width , height = texture_surface . get_rect ( ) . size \n gluBuild2DMipmaps ( GL_TEXTURE_2D , \n <NUM_LIT:3> , \n width , \n height , \n GL_RGB , \n GL_UNSIGNED_BYTE , \n texture_data ) \n def read_mtllib ( self , mtl_fname ) : \n file_mtllib = open ( mtl_fname ) \n for line in file_mtllib : \n words = line . split ( ) \n command = words [ <NUM_LIT:0> ] \n data = words [ <NUM_LIT:1> : ] \n if command == '<STR_LIT>' : \n material = Material ( ) \n material . name = data [ <NUM_LIT:0> ] \n self . materials [ data [ <NUM_LIT:0> ] ] = material \n elif command == '<STR_LIT>' : \n material . texture_fname = data [ <NUM_LIT:0> ] \n def draw ( self ) : \n vertices = self . vertices \n tex_coords = self . tex_coords \n normals = self . normals \n for face_group in self . face_groups : \n material = self . materials [ face_group . material_name ] \n glBindTexture ( GL_TEXTURE_2D , material . texture_id ) \n glBegin ( GL_TRIANGLES ) \n for vi , ti , ni in face_group . tri_indices : \n glTexCoord2fv ( tex_coords [ ti ] ) \n glNormal3fv ( normals [ ni ] ) \n glVertex3fv ( vertices [ vi ] ) \n glEnd ( ) \n def draw_quick ( self ) : \n if self . display_list_id is None : \n self . display_list_id = glGenLists ( <NUM_LIT:1> ) \n glNewList ( self . display_list_id , GL_COMPILE ) \n self . draw ( ) \n glEndList ( ) \n glCallList ( self . <mask0> ) \n", "gt": "display_list_id"}
{"input": "\n def saturate_color ( color ) : \n red , green , blue = color \n red = min ( red , <NUM_LIT:255> ) \n green = min ( green , <NUM_LIT:255> ) \n blue = min ( blue , <NUM_LIT:255> ) \n return red , green , <mask0> \n", "gt": "blue"}
{"input": "\n import pygame \n from pygame . locals import * \n from sys import exit \n from gameobjects . vector2 import Vector2 \n picture_file = '<STR_LIT>' \n pygame . init ( ) \n screen = pygame . display . set_mode ( ( <NUM_LIT> , <NUM_LIT> ) , <NUM_LIT:0> , <NUM_LIT:32> ) \n picture = pygame . image . load ( picture_file ) . convert ( ) \n picture_pos = Vector2 ( <NUM_LIT:0> , <NUM_LIT:0> ) \n scroll_speed = <NUM_LIT> \n clock = pygame . time . Clock ( ) \n joystick = None \n if pygame . joystick . get_count ( ) > <NUM_LIT:0> : \n joystick = pygame . joystick . Joystick ( <NUM_LIT:0> ) \n joystick . init ( ) \n if joystick is None : \n print ( \"<STR_LIT>\" ) \n pygame . quit ( ) \n exit ( ) \n while True : \n for event in pygame . event . get ( ) : \n if event . type == QUIT : \n pygame . quit ( ) \n exit ( ) \n scroll_direction = Vector2 ( * joystick . get_hat ( <NUM_LIT:0> ) ) \n scroll_direction . normalize ( ) \n screen . fill ( ( <NUM_LIT:255> , <NUM_LIT:255> , <NUM_LIT:255> ) ) \n screen . blit ( picture , ( - picture_pos . x , picture_pos . y ) ) \n time_passed = clock . tick ( ) \n time_passed_seconds = time_passed / <NUM_LIT> \n picture_pos += scroll_direction * scroll_speed * time_passed_seconds \n pygame . display . <mask0> ( ) \n", "gt": "update"}
{"input": "\n __all__ = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] \n <mask0> = \"<STR_LIT>\" \n", "gt": "__version__"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import os \n from setuptools import setup , find_packages \n THISDIR = os . path . dirname ( os . path . abspath ( __file__ ) ) \n os . chdir ( THISDIR ) \n VERSION = open ( \"<STR_LIT>\" ) . readline ( ) . strip ( ) \n HOMEPAGE = \"<STR_LIT>\" \n DOWNLOAD_BASEURL = \"<STR_LIT>\" \n DOWNLOAD_URL = DOWNLOAD_BASEURL + \"<STR_LIT>\" % VERSION \n setup ( \n name = \"<STR_LIT>\" , \n version = VERSION , \n description = ( \n \"<STR_LIT>\" \n ) , \n long_description = open ( \"<STR_LIT>\" ) . read ( ) , \n keywords = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) , \n author = \"<STR_LIT>\" , \n author_email = \"<STR_LIT>\" , \n url = HOMEPAGE , \n download_url = DOWNLOAD_URL , \n packages = find_packages ( ) , \n classifiers = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n <mask0> = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n ) \n", "gt": "install_requires"}
{"input": "\n from __future__ import unicode_literals \n from django . db import migrations , models \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ] \n operations = [ \n migrations . AddField ( \n model_name = '<STR_LIT:user>' , \n name = '<STR_LIT>' , \n field = models . BooleanField ( default = <mask0> ) , \n ) , \n ] \n", "gt": "False"}
{"input": "\n from __future__ import unicode_literals \n from django . db import models , migrations \n from django . conf import settings \n import utils . models \n class Migration ( migrations . Migration ) : \n dependencies = [ \n migrations . swappable_dependency ( settings . AUTH_USER_MODEL ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ] \n operations = [ \n migrations . CreateModel ( \n name = '<STR_LIT>' , \n fields = [ \n ( '<STR_LIT:id>' , models . AutoField ( verbose_name = '<STR_LIT>' , serialize = False , auto_created = True , primary_key = True ) ) , \n ( '<STR_LIT>' , models . IntegerField ( default = <NUM_LIT:0> ) ) , \n ( '<STR_LIT>' , models . IntegerField ( default = <NUM_LIT:0> ) ) , \n ( '<STR_LIT>' , models . IntegerField ( default = <NUM_LIT:0> ) ) , \n ( '<STR_LIT>' , utils . models . JsonField ( default = { } ) ) , \n ( '<STR_LIT>' , models . ForeignKey ( to = '<STR_LIT>' ) ) , \n ( '<STR_LIT:user>' , models . ForeignKey ( to = <mask0> . AUTH_USER_MODEL ) ) , \n ] , \n ) , \n ] \n", "gt": "settings"}
{"input": "\n import os \n import judger \n WA = <NUM_LIT:1> \n AC = <NUM_LIT:0> \n SPJ_ERROR = - <NUM_LIT:1> \n def file_exists ( path ) : \n return os . path . exists ( path ) \n def spj ( path , max_cpu_time , max_memory , in_path , user_out_path ) : \n if file_exists ( in_path ) and file_exists ( user_out_path ) : \n result = judger . run ( path = path , in_file = in_path , out_file = \"<STR_LIT>\" , \n max_cpu_time = max_cpu_time , max_memory = max_memory , \n args = [ in_path , user_out_path ] , env = [ \"<STR_LIT>\" + os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ] , \n use_sandbox = True , use_nobody = True ) \n if result [ \"<STR_LIT>\" ] == <NUM_LIT:0> and result [ \"<STR_LIT>\" ] in [ AC , WA , SPJ_ERROR ] : \n result [ \"<STR_LIT>\" ] = result [ \"<STR_LIT>\" ] \n else : \n result [ \"<STR_LIT>\" ] = SPJ_ERROR \n return result \n else : \n raise <mask0> ( \"<STR_LIT>\" ) \n", "gt": "ValueError"}
{"input": "\n from __future__ import unicode_literals \n from django . db import models , migrations \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ] \n operations = [ \n migrations . RemoveField ( \n model_name = '<STR_LIT>' , \n <mask0> = '<STR_LIT>' , \n ) , \n ] \n", "gt": "name"}
{"input": "\n from django . http import HttpResponse \n from utils . captcha import Captcha \n def show_captcha ( request ) : \n return HttpResponse ( Captcha ( request ) . display ( ) , <mask0> = \"<STR_LIT>\" ) \n", "gt": "content_type"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import time \n import numpy as np \n def _print_after_skip ( skip , it = None , dist = None , etime = None ) : \n if it is None : \n msg = \"<STR_LIT>\" . format ( i = \"<STR_LIT>\" , \n d = \"<STR_LIT>\" , \n t = \"<STR_LIT>\" ) \n print ( msg ) \n print ( \"<STR_LIT:->\" * len ( msg ) ) \n return \n if it % skip == <NUM_LIT:0> : \n if etime is None : \n print ( \"<STR_LIT>\" . format ( it = it , d = dist ) ) \n else : \n msg = \"<STR_LIT>\" \n print ( msg . format ( i = it , d = dist , t = etime ) ) \n return \n def compute_fixed_point ( T , v , error_tol = <NUM_LIT> , max_iter = <NUM_LIT:50> , verbose = <NUM_LIT:1> , \n print_skip = <NUM_LIT:5> , * args , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n iterate = <NUM_LIT:0> \n error = error_tol + <NUM_LIT:1> \n if verbose : \n start_time = time . time ( ) \n _print_after_skip ( print_skip , it = None ) \n while iterate < max_iter and error > error_tol : \n new_v = T ( v , * args , ** kwargs ) \n iterate += <NUM_LIT:1> \n error = np . max ( np . abs ( new_v - v ) ) \n if verbose : \n etime = time . time ( ) - start_time \n _print_after_skip ( print_skip , iterate , error , etime ) \n try : \n v [ : ] = new_v \n except TypeError : \n v = new_v \n return <mask0> \n", "gt": "v"}
{"input": "\n raise <mask0> ( \"<STR_LIT>\" ) \n", "gt": "ImportError"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import sys \n import os \n import unittest \n import numpy as np \n from scipy . linalg import LinAlgError \n from numpy . testing import assert_allclose \n from quantecon . lqcontrol import LQ \n from quantecon . robustlq import RBLQ \n class TestRBLQControl ( unittest . TestCase ) : \n def setUp ( self ) : \n a_0 = <NUM_LIT:100> \n a_1 = <NUM_LIT:0.5> \n rho = <NUM_LIT> \n sigma_d = <NUM_LIT> \n beta = <NUM_LIT> \n c = <NUM_LIT:2> \n gamma = <NUM_LIT> \n theta = <NUM_LIT> \n ac = ( a_0 - c ) / <NUM_LIT> \n R = np . array ( [ [ <NUM_LIT:0> , ac , <NUM_LIT:0> ] , \n [ ac , - a_1 , <NUM_LIT:0.5> ] , \n [ <NUM_LIT:0.> , <NUM_LIT:0.5> , <NUM_LIT:0> ] ] ) \n R = - R \n Q = gamma / <NUM_LIT:2> \n A = np . array ( [ [ <NUM_LIT:1.> , <NUM_LIT:0.> , <NUM_LIT:0.> ] , \n [ <NUM_LIT:0.> , <NUM_LIT:1.> , <NUM_LIT:0.> ] , \n [ <NUM_LIT:0.> , <NUM_LIT:0.> , rho ] ] ) \n B = np . array ( [ [ <NUM_LIT:0.> ] , \n [ <NUM_LIT:1.> ] , \n [ <NUM_LIT:0.> ] ] ) \n C = np . array ( [ [ <NUM_LIT:0.> ] , \n [ <NUM_LIT:0.> ] , \n [ sigma_d ] ] ) \n self . rblq_test = RBLQ ( Q , R , A , B , C , beta , theta ) \n self . lq_test = LQ ( Q , R , A , B , C , beta ) \n self . Fr , self . Kr , self . Pr = self . rblq_test . robust_rule ( ) \n def tearDown ( self ) : \n del self . rblq_test \n def test_robust_rule_vs_simple ( self ) : \n rblq = self . rblq_test \n Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n Fs , Ks , Ps = rblq . robust_rule_simple ( P_init = Pr , tol = <NUM_LIT> ) \n assert_allclose ( Fr , Fs , rtol = <NUM_LIT> ) \n assert_allclose ( Kr , Ks , rtol = <NUM_LIT> ) \n assert_allclose ( Pr , Ps , rtol = <NUM_LIT> ) \n def test_f2k_and_k2f ( self ) : \n rblq = self . rblq_test \n Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n K_f2k , P_f2k = rblq . F_to_K ( Fr ) \n F_k2f , P_k2f = rblq . K_to_F ( Kr ) \n assert_allclose ( K_f2k , Kr , rtol = <NUM_LIT> ) \n assert_allclose ( F_k2f , Fr , rtol = <NUM_LIT> ) \n assert_allclose ( P_f2k , P_k2f , rtol = <NUM_LIT> ) \n def test_evaluate_F ( self ) : \n rblq = self . rblq_test \n Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n Kf , Pf , df , Of , of = rblq . evaluate_F ( Fr ) \n assert_allclose ( Pf , Pr ) \n assert_allclose ( Kf , Kr ) \n if __name__ == '<STR_LIT:__main__>' : \n suite = unittest . TestLoader ( ) . loadTestsFromTestCase ( TestRBLQControl ) \n unittest . TextTestRunner ( verbosity = <NUM_LIT:2> , stream = sys . stderr ) . run ( <mask0> ) \n", "gt": "suite"}
{"input": "\n '''<STR_LIT>''' \n import tables as pt \n fileName = \"<STR_LIT>\" \n h5f = [ ] \n group = [ ] \n table = [ ] \n opened = False \n ctr = float ( <NUM_LIT:0.0> ) \n class AlphaDataModelClass ( pt . IsDescription ) : \n symbol = pt . StringCol ( <NUM_LIT:30> ) \n exchange = pt . StringCol ( <NUM_LIT:10> ) \n alphaValue = pt . Float32Col ( ) \n timestamp = pt . Time64Col ( ) \n def __init__ ( self ) : \n print \"<STR_LIT>\" \n def openFile ( newFileName ) : \n '''<STR_LIT>''' \n global fileName , h5f , group , table , opened , ctr \n ctr = float ( <NUM_LIT:0.0> ) \n if newFileName is None : \n print \"<STR_LIT>\" \n else : \n if ( len ( newFileName ) > <NUM_LIT:0> ) : \n fileName = str ( newFileName ) \n else : \n print \"<STR_LIT>\" \n if not opened : \n h5f = pt . openFile ( str ( fileName ) , mode = \"<STR_LIT:w>\" ) \n group = h5f . createGroup ( \"<STR_LIT:/>\" , '<STR_LIT>' ) \n table = h5f . createTable ( group , '<STR_LIT>' , AlphaDataModelClass ) \n opened = True \n else : \n print \"<STR_LIT>\" \n def addRow ( currSymbol , currExchange , currAlphaVal , currTS ) : \n '''<STR_LIT>''' \n global ctr \n if opened : \n ctr = ctr + <NUM_LIT:1> \n row = table . row \n row [ '<STR_LIT>' ] = currSymbol \n row [ '<STR_LIT>' ] = currExchange \n row [ '<STR_LIT>' ] = currAlphaVal \n row [ '<STR_LIT>' ] = currTS \n row . append ( ) \n if ( ctr == <NUM_LIT> ) : \n ctr = <NUM_LIT:0> \n table . flush ( ) \n else : \n print \"<STR_LIT>\" \n raise IOError \n def closeFile ( ) : \n '''<STR_LIT>''' \n table . flush ( ) \n h5f . close ( ) \n print str ( fileName ) + \"<STR_LIT>\" \n opened = <mask0> \n", "gt": "False"}
{"input": "\n '''<STR_LIT>''' \n import numpy as np \n import pickle as pkl \n import qstkutil . utils as utils \n import os \n import dircache \n import time \n import sys \n def main ( ) : \n print \"<STR_LIT>\" + str ( time . strftime ( \"<STR_LIT>\" ) ) \n try : \n rootdir = os . environ [ '<STR_LIT>' ] \n except KeyError : \n print \"<STR_LIT>\" \n fileExtensionToRemove = \"<STR_LIT>\" \n listOfInputPaths = list ( ) \n listOfInputPaths . append ( rootdir + \"<STR_LIT>\" ) \n listOfInputPaths . append ( rootdir + \"<STR_LIT>\" ) \n listOfInputPaths . append ( rootdir + \"<STR_LIT>\" ) \n listOfOutputPaths = list ( ) \n listOfOutputPaths . append ( rootdir + \"<STR_LIT>\" ) \n listOfOutputPaths . append ( rootdir + \"<STR_LIT>\" ) \n listOfOutputPaths . append ( rootdir + \"<STR_LIT>\" ) \n for path in listOfOutputPaths : \n if not ( os . access ( path , os . F_OK ) ) : \n os . makedirs ( path ) \n utils . clean_paths ( listOfOutputPaths ) \n if ( len ( listOfInputPaths ) != len ( listOfOutputPaths ) ) : \n print \"<STR_LIT>\" \n sys . exit ( \"<STR_LIT>\" ) \n path_ctr = - <NUM_LIT:1> ; \n use_cols = range ( <NUM_LIT:1> , <NUM_LIT:7> + <NUM_LIT:1> ) \n for path in listOfInputPaths : \n path_ctr = path_ctr + <NUM_LIT:1> ; \n stocks_at_this_path = dircache . listdir ( str ( path ) ) \n filtered_names = filter ( lambda x : ( str ( x ) . find ( str ( fileExtensionToRemove ) ) > - <NUM_LIT:1> ) , stocks_at_this_path ) \n filtered_names = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ <NUM_LIT:0> ] ) , filtered_names ) \n stock_ctr = - <NUM_LIT:1> \n for stock in filtered_names : \n stock_ctr = stock_ctr + <NUM_LIT:1> \n print \"<STR_LIT>\" + str ( path + stock ) \n stock_data = np . loadtxt ( path + stock + \"<STR_LIT>\" , np . float , None , \"<STR_LIT:U+002C>\" , None , <NUM_LIT:1> , use_cols ) \n stock_data_shape = stock_data . shape \n f = open ( listOfOutputPaths [ path_ctr ] + filtered_names [ stock_ctr ] + \"<STR_LIT>\" , \"<STR_LIT:wb>\" ) \n pkl . dump ( stock_data , f , - <NUM_LIT:1> ) \n f . close ( ) \n print \"<STR_LIT>\" + str ( time . strftime ( \"<STR_LIT>\" ) ) \n if __name__ == '<STR_LIT:__main__>' : \n <mask0> ( ) \n", "gt": "main"}
{"input": "\n '''<STR_LIT>''' \n import cPickle \n import sys \n from pandas import DataMatrix \n import datetime as dt \n import random \n import qstkutil . DataAccess as da \n import qstkutil . qsdateutil as du \n if __name__ == \"<STR_LIT:__main__>\" : \n print \"<STR_LIT>\" + sys . argv [ <NUM_LIT:1> ] + \"<STR_LIT>\" + sys . argv [ <NUM_LIT:2> ] \n symbols = list ( [ '<STR_LIT>' ] ) \n t = map ( int , sys . argv [ <NUM_LIT:1> ] . split ( '<STR_LIT:->' ) ) \n startday = dt . datetime ( t [ <NUM_LIT:2> ] , t [ <NUM_LIT:0> ] , t [ <NUM_LIT:1> ] ) \n t = map ( int , sys . argv [ <NUM_LIT:2> ] . split ( '<STR_LIT:->' ) ) \n endday = dt . datetime ( t [ <NUM_LIT:2> ] , t [ <NUM_LIT:0> ] , t [ <NUM_LIT:1> ] ) \n timeofday = dt . timedelta ( hours = <NUM_LIT:16> ) \n timestamps = du . getNYSEdays ( startday , endday , timeofday ) \n dataobj = da . DataAccess ( '<STR_LIT>' ) \n historic = dataobj . get_data ( timestamps , symbols , \"<STR_LIT>\" ) \n alloc_val = random . random ( ) \n alloc = DataMatrix ( index = [ historic . index [ <NUM_LIT:0> ] ] , data = [ alloc_val ] , columns = symbols ) \n for date in range ( <NUM_LIT:1> , len ( historic . index ) ) : \n alloc_val = <NUM_LIT:1> \n alloc = alloc . append ( DataMatrix ( index = [ historic . index [ date ] ] , data = [ alloc_val ] , columns = [ symbols [ <NUM_LIT:0> ] ] ) ) \n alloc [ '<STR_LIT>' ] = <NUM_LIT:1> - alloc [ symbols [ <NUM_LIT:0> ] ] \n output = open ( sys . argv [ <NUM_LIT:3> ] , \"<STR_LIT:wb>\" ) \n cPickle . dump ( alloc , <mask0> ) \n", "gt": "output"}
{"input": "\n '''<STR_LIT>''' \n import QSTK . qstkutil . DataAccess as da \n import tables as pt \n import numpy as np \n from itertools import izip \n import time \n import dircache \n def getStocks ( listOfPaths ) : \n listOfStocks = list ( ) \n print \"<STR_LIT>\" \n fileExtensionToRemove = \"<STR_LIT>\" \n for path in listOfPaths : \n stocksAtThisPath = list ( ) \n stocksAtThisPath = dircache . listdir ( str ( path ) ) \n stocksAtThisPath = filter ( lambda x : ( str ( x ) . find ( str ( fileExtensionToRemove ) ) > - <NUM_LIT:1> ) , stocksAtThisPath ) \n stocksAtThisPath = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ <NUM_LIT:0> ] ) , stocksAtThisPath ) \n for stock in stocksAtThisPath : \n listOfStocks . append ( stock ) \n return listOfStocks \n if __name__ == '<STR_LIT:__main__>' : \n print \"<STR_LIT>\" \n dataItemsList = [ ] \n dataItemsList . append ( '<STR_LIT>' ) \n listOfStocks = list ( ) \n listOfPaths = list ( ) \n listOfPaths . append ( \"<STR_LIT>\" ) \n listOfStocks = getStocks ( listOfPaths ) \n alpha = da . DataAccess ( True , listOfPaths , \"<STR_LIT>\" , \"<STR_LIT>\" , True , listOfStocks ) \n tslist = list ( alpha . getTimestampArray ( ) ) \n listOfTS = alpha . getTimestampArray ( ) \n for stock in [ \"<STR_LIT>\" ] : \n alphaList = alpha . getStockDataList ( stock , '<STR_LIT>' ) \n ctr = <NUM_LIT:0> \n for val in alphaList : \n print \"<STR_LIT>\" + str ( stock ) + \"<STR_LIT>\" + str ( val ) + \"<STR_LIT>\" + str ( listOfTS [ ctr ] ) \n ctr += <NUM_LIT:1> \n <mask0> \"<STR_LIT>\" \n", "gt": "print"}
{"input": "\n import unittest \n import collections_and_iterators \n \"\"\"<STR_LIT>\"\"\" \n class TestObjectMethods ( unittest . TestCase ) : \n def setUp ( self ) : \n self . singleLinkList = collections_and_iterators . SinglyLinkedList ( ) \n self . singleLinkListData = collections_and_iterators . SinglyLinkedList ( ) \n self . singleLinkListData . append ( \"<STR_LIT>\" ) \n self . singleLinkListData . append ( \"<STR_LIT>\" ) \n self . singleLinkListData . append ( \"<STR_LIT>\" ) \n self . doubleLinkList = collections_and_iterators . DoublyLinkedList ( ) \n self . doubleLinkListData = collections_and_iterators . DoublyLinkedList ( ) \n self . doubleLinkListData . append ( \"<STR_LIT>\" ) \n self . doubleLinkListData . append ( \"<STR_LIT>\" ) \n self . doubleLinkListData . append ( \"<STR_LIT>\" ) \n def test_empty_single_list ( self ) : \n self . assertEqual ( <NUM_LIT:0> , self . singleLinkList . size ) \n self . assertIsNone ( self . singleLinkList . head ) \n self . assertIsNone ( self . singleLinkList . cursor ) \n def test_contains_success ( self ) : \n self . assertTrue ( \"<STR_LIT>\" in self . singleLinkListData ) \n self . assertTrue ( \"<STR_LIT>\" in self . singleLinkListData ) \n self . assertTrue ( \"<STR_LIT>\" in self . singleLinkListData ) \n def test_contains_failure ( self ) : \n self . assertFalse ( \"<STR_LIT>\" in self . singleLinkListData ) \n self . assertFalse ( \"<STR_LIT>\" in self . singleLinkListData ) \n def test_append_success ( self ) : \n self . assertEqual ( \"<STR_LIT>\" , self . singleLinkListData [ <NUM_LIT:0> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . singleLinkListData [ <NUM_LIT:1> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . singleLinkListData [ <NUM_LIT:2> ] ) \n def test_append_failure ( self ) : \n with self . assertRaises ( IndexError ) : \n self . singleLinkListData [ <NUM_LIT:3> ] \n self . singleLinkListData . append ( \"<STR_LIT>\" ) \n self . assertEqual ( \"<STR_LIT>\" , self . singleLinkListData [ <NUM_LIT:3> ] ) \n def test_getitem_success ( self ) : \n self . assertEqual ( \"<STR_LIT>\" , self . singleLinkListData . __getitem__ ( <NUM_LIT:0> ) ) \n self . assertEqual ( \"<STR_LIT>\" , self . singleLinkListData . __getitem__ ( <NUM_LIT:1> ) ) \n self . assertEqual ( \"<STR_LIT>\" , self . singleLinkListData . __getitem__ ( <NUM_LIT:2> ) ) \n def test_getitem_failure ( self ) : \n with self . assertRaises ( IndexError ) : \n self . singleLinkListData . __getitem__ ( <NUM_LIT:3> ) \n self . singleLinkListData . __getitem__ ( - <NUM_LIT:3> ) \n def test_setitem_success ( self ) : \n self . assertEqual ( \"<STR_LIT>\" , self . singleLinkListData [ <NUM_LIT:0> ] ) \n self . singleLinkListData [ <NUM_LIT:0> ] = \"<STR_LIT>\" \n self . assertEqual ( \"<STR_LIT>\" , self . singleLinkListData [ <NUM_LIT:0> ] ) \n def test_setitem_failure ( self ) : \n with self . assertRaises ( IndexError ) : \n self . singleLinkListData [ <NUM_LIT:5> ] = \"<STR_LIT>\" \n self . singleLinkListData [ - <NUM_LIT:1> ] = \"<STR_LIT>\" \n def test_empty_double_list ( self ) : \n self . assertEqual ( <NUM_LIT:0> , self . doubleLinkList . size ) \n self . assertIsNone ( self . doubleLinkList . head ) \n self . assertIsNone ( self . doubleLinkList . cursor ) \n def test_insert_success ( self ) : \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:0> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:1> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:2> ] ) \n self . doubleLinkListData . insert ( \"<STR_LIT>\" , <NUM_LIT:0> ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:0> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:1> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:2> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:3> ] ) \n self . doubleLinkListData . insert ( \"<STR_LIT>\" , <NUM_LIT:2> ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:0> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:1> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:2> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:3> ] ) \n self . assertEqual ( \"<STR_LIT>\" , self . doubleLinkListData [ <NUM_LIT:4> ] ) \n def test_insert_fauilure ( self ) : \n pass \n if __name__ == '<STR_LIT:__main__>' : \n unittest . main ( <mask0> = <NUM_LIT:2> ) \n", "gt": "verbosity"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import re \n from itertools import chain \n from textwrap import dedent \n from jedi . evaluate . cache import memoize_default \n from jedi . parser import Parser \n from jedi . common import indent_block \n DOCSTRING_PARAM_PATTERNS = [ \n r'<STR_LIT>' , \n r'<STR_LIT>' , \n ] \n DOCSTRING_RETURN_PATTERNS = [ \n re . compile ( r'<STR_LIT>' , re . M ) , \n re . compile ( r'<STR_LIT>' , re . M ) , \n ] \n REST_ROLE_PATTERN = re . compile ( r'<STR_LIT>' ) \n @ memoize_default ( None , evaluator_is_first_arg = True ) \n def follow_param ( evaluator , param ) : \n func = param . parent_function \n param_str = _search_param_in_docstr ( func . raw_doc , str ( param . get_name ( ) ) ) \n return _evaluate_for_statement_string ( evaluator , param_str , param . get_parent_until ( ) ) \n def _search_param_in_docstr ( docstr , param_str ) : \n \"\"\"<STR_LIT>\"\"\" \n patterns = [ re . compile ( p % re . escape ( param_str ) ) \n for p in DOCSTRING_PARAM_PATTERNS ] \n for pattern in patterns : \n match = pattern . search ( docstr ) \n if match : \n return _strip_rst_role ( match . group ( <NUM_LIT:1> ) ) \n return None \n def _strip_rst_role ( type_str ) : \n \"\"\"<STR_LIT>\"\"\" \n match = REST_ROLE_PATTERN . match ( type_str ) \n if match : \n return match . group ( <NUM_LIT:1> ) \n else : \n return type_str \n def _evaluate_for_statement_string ( evaluator , string , module ) : \n code = dedent ( \"\"\"<STR_LIT>\"\"\" ) \n if string is None : \n return [ ] \n for element in re . findall ( '<STR_LIT>' , string ) : \n string = '<STR_LIT>' % element + string \n p = Parser ( code % indent_block ( string ) , no_docstr = True ) \n pseudo_cls = p . module . subscopes [ <NUM_LIT:0> ] \n try : \n stmt = pseudo_cls . statements [ - <NUM_LIT:1> ] \n except IndexError : \n return [ ] \n pseudo_cls . parent = module \n definitions = evaluator . eval_statement ( stmt ) \n it = ( evaluator . execute ( d ) for d in definitions ) \n return list ( chain . from_iterable ( it ) ) or definitions \n @ memoize_default ( None , evaluator_is_first_arg = True ) \n def find_return_types ( evaluator , func ) : \n def search_return_in_docstr ( code ) : \n for p in DOCSTRING_RETURN_PATTERNS : \n match = p . search ( code ) \n if match : \n return _strip_rst_role ( match . group ( <NUM_LIT:1> ) ) \n type_str = search_return_in_docstr ( func . raw_doc ) \n return _evaluate_for_statement_string ( evaluator , type_str , func . <mask0> ( ) ) \n", "gt": "get_parent_until"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from __future__ import absolute_import \n import unittest \n class Assertions ( unittest . TestCase ) : \n <mask0> \n", "gt": "pass"}
{"input": "\n from jedi import parser \n from jedi . _compatibility import u \n try : \n import unittest2 as unittest \n except ImportError : \n import unittest \n class TokenTest ( unittest . TestCase ) : \n def test_end_pos_one_line ( self ) : \n parsed = parser . Parser ( u ( '''<STR_LIT>''' ) ) \n tok = parsed . module . subscopes [ <NUM_LIT:0> ] . statements [ <NUM_LIT:0> ] . _token_list [ <NUM_LIT:2> ] \n self . assertEqual ( tok . end_pos , ( <NUM_LIT:3> , <NUM_LIT> ) ) \n def test_end_pos_multi_line ( self ) : \n parsed = parser . Parser ( u ( '''<STR_LIT>''' ) ) \n tok = parsed . module . subscopes [ <NUM_LIT:0> ] . statements [ <NUM_LIT:0> ] . _token_list [ <NUM_LIT:2> ] \n self . assertEqual ( tok . <mask0> , ( <NUM_LIT:4> , <NUM_LIT:11> ) ) \n", "gt": "end_pos"}
{"input": "\n from types import FunctionType \n from rdflib . graph import ConjunctiveGraph \n from rdflib . graph import Graph \n from rdflib . term import BNode \n from rdflib . term import Literal \n from rdflib . term import URIRef \n from rdflib . term import Variable \n from rdflib . namespace import NamespaceManager \n from rdfextras . sparql import _questChar \n from rdfextras . sparql import SPARQLError \n from rdflib . util import check_object \n from rdflib . util import check_subject \n __all__ = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n class SPARQLGraph ( object ) : \n \"\"\"<STR_LIT>\"\"\" \n SPARQL_DATASET = <NUM_LIT:0> \n NAMED_GRAPH = <NUM_LIT:1> \n __slots__ = ( \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ) \n def __init__ ( self , graph , graphVariable = None , dSCompliance = False ) : \n assert not graphVariable or graphVariable [ <NUM_LIT:0> ] != '<STR_LIT:?>' , repr ( graphVariable ) \n self . graphVariable = graphVariable \n self . DAWG_DATASET_COMPLIANCE = dSCompliance \n self . graphKind = None \n if graph is not None : \n self . graph = graph \n if isinstance ( graph , ConjunctiveGraph ) : \n self . graphKind = self . SPARQL_DATASET \n self . identifier = graph . default_context . identifier \n else : \n self . graphKind = self . NAMED_GRAPH \n self . identifier = graph . identifier \n def setupGraph ( self , store , graphKind = None ) : \n gKind = graphKind and graphKind or self . graphKind \n self . graph = gKind ( store , self . identifier ) \n def __reduce__ ( self ) : \n return ( SPARQLGraph , \n ( None , \n self . graphVariable , \n self . DAWG_DATASET_COMPLIANCE ) , \n self . __getstate__ ( ) ) \n def __getstate__ ( self ) : \n return ( self . graphVariable , \n self . DAWG_DATASET_COMPLIANCE , \n self . identifier ) \n def __setstate__ ( self , arg ) : \n gVar , flag , identifier = arg \n self . graphVariable = gVar \n self . DAWG_DATASET_COMPLIANCE = flag \n self . identifier = identifier \n def _clusterForward ( self , seed , Cluster ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n for ( p , o ) in self . graph . predicate_objects ( seed ) : \n if not ( seed , p , o ) in Cluster . graph : \n Cluster . add ( ( seed , p , o ) ) \n self . _clusterForward ( p , Cluster ) \n self . _clusterForward ( o , Cluster ) \n except : \n pass \n def clusterForward ( self , seed , Cluster = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if Cluster == None : \n Cluster = SPARQLGraph ( ) \n check_subject ( seed ) \n self . _clusterForward ( seed , Cluster ) \n return Cluster \n def _clusterBackward ( self , seed , Cluster ) : \n \"\"\"<STR_LIT>\"\"\" \n try : \n for ( s , p ) in self . graph . subject_predicates ( seed ) : \n if not ( s , p , seed ) in Cluster . graph : \n Cluster . add ( ( s , p , seed ) ) \n self . _clusterBackward ( s , Cluster ) \n self . _clusterBackward ( p , Cluster ) \n except : \n pass \n def clusterBackward ( self , seed , Cluster = None ) : \n \"\"\"<STR_LIT>\"\"\" \n if Cluster == None : \n Cluster = SPARQLGraph ( ) \n check_object ( seed ) \n self . _clusterBackward ( seed , Cluster ) \n return Cluster \n def cluster ( self , seed ) : \n \"\"\"<STR_LIT>\"\"\" \n raise \"<STR_LIT>\" \n return self . clusterBackward ( seed ) + self . clusterForward ( seed ) \n \"\"\"<STR_LIT>\"\"\" \n def _createResource ( v ) : \n \"\"\"<STR_LIT>\"\"\" \n if isinstance ( v , Literal ) or isinstance ( v , BNode ) or isinstance ( v , URIRef ) : \n return v \n else : \n return Literal ( v ) \n def _isResQuest ( r ) : \n \"\"\"<STR_LIT>\"\"\" \n if r and isinstance ( r , basestring ) and r [ <NUM_LIT:0> ] == _questChar : \n return True \n return False \n class GraphPattern : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , patterns = [ ] ) : \n \"\"\"<STR_LIT>\"\"\" \n self . patterns = [ ] \n self . constraints = [ ] \n self . unbounds = [ ] \n self . bnodes = { } \n if type ( patterns ) == list : \n self . addPatterns ( patterns ) \n elif type ( patterns ) == tuple : \n self . addPattern ( patterns ) \n else : \n raise SPARQLError ( \n \"<STR_LIT>\" ) \n def _generatePattern ( self , tupl ) : \n \"\"\"<STR_LIT>\"\"\" \n if type ( tupl ) != tuple : \n raise SPARQLError ( \n \"<STR_LIT>\" % type ( tupl ) ) \n if len ( tupl ) != <NUM_LIT:3> and len ( tupl ) != <NUM_LIT:4> : \n raise SPARQLError ( \n \"<STR_LIT>\" % len ( tupl ) ) \n if len ( tupl ) == <NUM_LIT:3> : \n ( s , p , o ) = tupl \n f = None \n else : \n ( s , p , o , f ) = tupl \n final = [ ] \n for c in ( s , p , o ) : \n if _isResQuest ( c ) : \n if not c in self . unbounds : \n self . unbounds . append ( c ) \n final . append ( c ) \n elif isinstance ( c , BNode ) : \n final . append ( c ) \n else : \n final . append ( _createResource ( c ) ) \n final . append ( f ) \n return tuple ( final ) \n def addPattern ( self , tupl ) : \n \"\"\"<STR_LIT>\"\"\" \n self . patterns . append ( self . _generatePattern ( tupl ) ) \n def insertPattern ( self , tupl ) : \n \"\"\"<STR_LIT>\"\"\" \n self . patterns . insert ( <NUM_LIT:0> , self . _generatePattern ( tupl ) ) \n def addPatterns ( self , lst ) : \n \"\"\"<STR_LIT>\"\"\" \n for l in lst : \n self . addPattern ( l ) \n def insertPatterns ( self , lst ) : \n \"\"\"<STR_LIT>\"\"\" \n for i in xrange ( len ( lst ) - <NUM_LIT:1> , - <NUM_LIT:1> , - <NUM_LIT:1> ) : \n self . insertPattern ( lst [ i ] ) \n def addConstraint ( self , func ) : \n \"\"\"<STR_LIT>\"\"\" \n if type ( func ) == FunctionType : \n self . constraints . append ( func ) \n else : \n raise SPARQLError ( \n \"<STR_LIT>\" % type ( func ) ) \n def addConstraints ( self , lst ) : \n \"\"\"<STR_LIT>\"\"\" \n for l in lst : \n self . addConstraint ( l ) \n def construct ( self , tripleStore , bindings ) : \n \"\"\"<STR_LIT>\"\"\" \n localBnodes = { } \n for c in self . bnodes : \n localBnodes [ c ] = BNode ( ) \n def bind ( st ) : \n if _isResQuest ( st ) : \n if st in bindings : \n return bindings [ st ] \n else : \n if isinstance ( self , GraphPattern ) : \n return st \n else : \n return None \n elif isinstance ( st , BNode ) : \n for c in self . bnodes : \n if self . bnodes [ c ] == st : \n return localBnodes [ c ] \n return st \n else : \n return st \n for pattern in self . patterns : \n ( s , p , o , f ) = pattern \n triplet = [ ] \n valid = True \n for res in ( s , p , o ) : \n val = bind ( res ) \n if val != None : \n triplet . append ( val ) \n else : \n valid = False \n break \n if valid : \n tripleStore . add ( tuple ( triplet ) ) \n def __add__ ( self , other ) : \n \"\"\"<STR_LIT>\"\"\" \n retval = GraphPattern ( ) \n retval += self \n retval += other \n return retval \n def __iadd__ ( self , other ) : \n \"\"\"<STR_LIT>\"\"\" \n self . patterns += other . patterns \n self . constraints += other . constraints \n for c in other . unbounds : \n if not c in self . unbounds : \n self . unbounds . append ( c ) \n for c in other . bnodes : \n if not c in self . bnodes : \n self . bnodes [ c ] = other . bnodes [ c ] \n return self \n def __str__ ( self ) : \n return self . __repr__ ( ) \n def isEmpty ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n return len ( self . patterns ) == <NUM_LIT:0> \n class BasicGraphPattern ( GraphPattern ) : \n \"\"\"<STR_LIT>\"\"\" \n def __init__ ( self , patterns = [ ] , prolog = None ) : \n \"\"\"<STR_LIT>\"\"\" \n GraphPattern . __init__ ( self , patterns ) \n self . prolog = prolog \n def canonicalTerm ( self , term ) : \n if isinstance ( term , URIRef ) : \n if self . prolog is not None : \n namespace_manager = NamespaceManager ( Graph ( ) ) \n for prefix , uri in self . prolog . prefixBindings . items ( ) : \n namespace_manager . bind ( prefix , uri , override = False ) \n try : \n prefix , uri , localName = namespace_manager . compute_qname ( term ) \n except : \n return term \n if prefix not in self . prolog . prefixBindings : \n return term \n else : \n return u'<STR_LIT::>' . join ( [ prefix , localName ] ) \n else : \n return term \n elif isinstance ( term , Literal ) : \n return term . n3 ( ) \n elif isinstance ( term , BNode ) : \n return term . n3 ( ) \n else : \n assert isinstance ( term , Variable ) \n return term . n3 ( ) \n def __repr__ ( self ) : \n if self . constraints : \n return \"<STR_LIT>\" % ( \n '<STR_LIT:U+002C>' . join ( [ '<STR_LIT:U+002C>' . join ( [ \n self . canonicalTerm ( pat [ <NUM_LIT:0> ] ) , \n self . canonicalTerm ( pat [ <NUM_LIT:1> ] ) , \n self . canonicalTerm ( pat [ <NUM_LIT:2> ] ) ] \n ) \n for pat in self . patterns ] ) ) \n else : \n return \"<STR_LIT>\" % ( \n '<STR_LIT:U+002C>' . join ( [ '<STR_LIT:(>' + '<STR_LIT:U+002C>' . join ( [ \n self . canonicalTerm ( s ) , \n self . canonicalTerm ( p ) , \n self . canonicalTerm ( o ) ] \n ) + '<STR_LIT:)>' \n for s , p , o , f in self . patterns ] ) ) \n retval = \"<STR_LIT>\" % self . patterns \n retval += \"<STR_LIT>\" % self . constraints \n retval += \"<STR_LIT>\" % self . unbounds \n return retval \n def _generatePattern ( self , tupl ) : \n \"\"\"<STR_LIT>\"\"\" \n if type ( tupl ) != tuple : \n raise SPARQLError ( \n \"<STR_LIT>\" % type ( tupl ) ) \n if len ( tupl ) != <NUM_LIT:3> and len ( tupl ) != <NUM_LIT:4> : \n raise SPARQLError ( \n \"<STR_LIT>\" % len ( tupl ) ) \n if len ( tupl ) == <NUM_LIT:3> : \n ( s , p , o ) = tupl \n f = None \n else : \n ( s , p , o , f ) = tupl \n final = [ ] \n for c in ( s , p , o ) : \n if isinstance ( c , Variable ) : \n if not c in self . unbounds : \n self . unbounds . append ( c ) \n final . append ( c ) \n elif isinstance ( c , BNode ) : \n final . append ( c ) \n else : \n final . append ( _createResource ( c ) ) \n final . append ( f ) \n return tuple ( final ) \n def fetchTerminalExpression ( self ) : \n yield self \n if __name__ == '<STR_LIT:__main__>' : \n from rdfextras . sparql . evaluate import Unbound \n v1 = Variable ( \"<STR_LIT:a>\" ) \n u1 = Unbound ( \"<STR_LIT:a>\" ) \n g = BasicGraphPattern ( \n [ ( \"<STR_LIT:a>\" , \"<STR_LIT>\" , <NUM_LIT> ) , ( \"<STR_LIT>\" , \"<STR_LIT>\" , <NUM_LIT> ) , ( v1 , \"<STR_LIT>\" , <NUM_LIT> ) , ( u1 , \"<STR_LIT>\" , <NUM_LIT> ) ] ) \n print <mask0> \n", "gt": "g"}
{"input": "\n from rdflib import ConjunctiveGraph , plugin \n from rdflib . store import Store \n from StringIO import StringIO \n import unittest \n \"\"\"<STR_LIT>\"\"\" \n test_data = \"\"\"<STR_LIT>\"\"\" \n test_query = \"\"\"<STR_LIT>\"\"\" \n correct = \"\"\"<STR_LIT>\"\"\" \n test_header_query = \"\"\"<STR_LIT>\"\"\" \n class JSON ( unittest . TestCase ) : \n def setUp ( self ) : \n self . graph = ConjunctiveGraph ( plugin . get ( '<STR_LIT>' , Store ) ( ) ) \n self . graph . parse ( StringIO ( test_data ) , format = \"<STR_LIT>\" ) \n def testComma ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n results = self . graph . query ( test_query ) \n result_json = results . serialize ( format = '<STR_LIT>' ) \n self . failUnless ( result_json . find ( correct ) > <NUM_LIT:0> ) \n def testHeader ( self ) : \n \"\"\"<STR_LIT>\"\"\" \n results = self . graph . query ( test_header_query ) \n result_json = results . serialize ( format = '<STR_LIT>' ) \n self . failUnless ( result_json . find ( '<STR_LIT>' ) == - <NUM_LIT:1> ) \n if __name__ == \"<STR_LIT:__main__>\" : \n unittest . <mask0> ( ) \n", "gt": "main"}
{"input": "\n import unittest \n from rdflib import plugin \n from rdflib . namespace import Namespace , RDF , RDFS \n from rdflib . term import URIRef \n from rdflib . store import Store \n from cStringIO import StringIO \n from rdflib import Graph \n import rdflib \n try : \n set \n except NameError : \n from sets import Set as set \n testGraph1N3 = \"\"\"<STR_LIT>\"\"\" \n sparqlQ1 = \"\"\"<STR_LIT>\"\"\" \n sparqlQ2 = \"\"\"<STR_LIT>\"\"\" \n sparqlQ3 = \"\"\"<STR_LIT>\"\"\" \n sparqlQ4 = \"\"\"<STR_LIT>\"\"\" \n class AdvancedTests ( unittest . TestCase ) : \n def setUp ( self ) : \n memStore = plugin . get ( '<STR_LIT>' , Store ) ( ) \n self . testGraph = Graph ( memStore ) \n self . testGraph . parse ( StringIO ( testGraph1N3 ) , format = '<STR_LIT>' ) \n def testNamedGraph ( self ) : \n OWL_NS = Namespace ( \"<STR_LIT>\" ) \n rt = self . testGraph . query ( sparqlQ4 ) \n self . assertEquals ( set ( rt ) , set ( ( x , ) for x in [ OWL_NS . DatatypeProperty , OWL_NS . ObjectProperty , OWL_NS . OntologyProperty , OWL_NS . Class , OWL_NS . Ontology , OWL_NS . AnnotationProperty , RDF . Property , RDFS . Class ] ) ) \n def testScopedBNodes ( self ) : \n rt = self . testGraph . query ( sparqlQ1 ) \n self . assertEquals ( list ( rt ) [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] , URIRef ( \"<STR_LIT>\" ) ) \n def testCollectionContentWithinAndWithout ( self ) : \n rt = self . testGraph . query ( sparqlQ3 ) \n self . assertEquals ( list ( rt ) [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] , URIRef ( \"<STR_LIT>\" ) ) \n def testCollectionAsObject ( self ) : \n rt = self . testGraph . query ( sparqlQ2 ) \n self . assertEquals ( list ( rt ) [ <NUM_LIT:0> ] [ <NUM_LIT:0> ] , URIRef ( \"<STR_LIT>\" ) ) \n self . assertEquals ( <NUM_LIT:1> , len ( rt ) ) \n if __name__ == '<STR_LIT:__main__>' : \n suite = unittest . makeSuite ( AdvancedTests ) \n unittest . TextTestRunner ( verbosity = <NUM_LIT:3> ) . run ( <mask0> ) \n", "gt": "suite"}
{"input": "\n from urllib2 import URLError \n try : \n from Ft . Lib import UriException \n except : \n from urllib2 import URLError as UriException \n import unittest \n from rdflib import ConjunctiveGraph , URIRef \n class SPARQLloadContextsTest ( unittest . TestCase ) : \n def test_dSet_parsed_as_URL_raises_Exception ( self ) : \n querystr = \"\"\"<STR_LIT>\"\"\" \n graph = ConjunctiveGraph ( ) \n graph . get_context ( URIRef ( \"<STR_LIT>\" ) \n ) . parse ( \"<STR_LIT>\" ) \n self . assertRaises ( ( URLError , UriException ) , \n graph . query , ( querystr ) , loadContexts = False ) \n def test_dSet_parsed_as_context_returns_results ( self ) : \n querystr = \"\"\"<STR_LIT>\"\"\" \n graph = ConjunctiveGraph ( ) \n graph . get_context ( URIRef ( '<STR_LIT>' ) \n ) . parse ( \"<STR_LIT>\" ) \n r = graph . query ( querystr , loadContexts = True ) \n self . assert_ ( len ( r . bindings ) is <mask0> <NUM_LIT:0> ) \n", "gt": "not"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from rdflib import Graph , RDF , RDFS , Literal \n from rdflib . namespace import FOAF \n if __name__ == '<STR_LIT:__main__>' : \n g = Graph ( ) \n bob = g . resource ( '<STR_LIT>' ) \n bob . set ( RDF . type , FOAF . Person ) \n bob . set ( FOAF . name , Literal ( \"<STR_LIT>\" ) ) \n bill = g . resource ( '<STR_LIT>' ) \n bill . add ( RDF . type , FOAF . Person ) \n bill . add ( RDF . type , FOAF . Agent ) \n bill . set ( RDFS . label , Literal ( \"<STR_LIT>\" ) ) \n bill . add ( FOAF . knows , bob ) \n print \"<STR_LIT>\" , bill . value ( FOAF . knows ) . value ( FOAF . name ) \n print \"<STR_LIT>\" , \n for friend in bill [ FOAF . knows ] : \n print friend [ FOAF . name ] . next ( ) , \"<STR_LIT:U+0020>\" \n print \"<STR_LIT>\" , \n for friend in bill [ FOAF . knows / FOAF . name ] : \n print friend \n bill [ RDFS . label ] = Literal ( \"<STR_LIT>\" ) \n print g . serialize ( <mask0> = '<STR_LIT>' ) \n", "gt": "format"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n from codecs import getreader \n from rdflib . py3compat import b \n from rdflib import ConjunctiveGraph \n from rdflib . plugins . parsers . ntriples import NTriplesParser \n from rdflib . plugins . parsers . ntriples import ParseError \n from rdflib . plugins . parsers . ntriples import r_tail \n from rdflib . plugins . parsers . ntriples import r_wspace \n from rdflib . plugins . parsers . ntriples import r_wspaces \n __all__ = [ '<STR_LIT>' ] \n class NQuadsParser ( NTriplesParser ) : \n def parse ( self , inputsource , sink , ** kwargs ) : \n \"\"\"<STR_LIT>\"\"\" \n assert sink . store . context_aware , ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n self . sink = ConjunctiveGraph ( store = sink . store , identifier = sink . identifier ) \n source = inputsource . getByteStream ( ) \n if not hasattr ( source , '<STR_LIT>' ) : \n raise ParseError ( \"<STR_LIT>\" ) \n source = getreader ( '<STR_LIT:utf-8>' ) ( source ) \n self . file = source \n self . buffer = '<STR_LIT>' \n while True : \n self . line = __line = self . readline ( ) \n if self . line is None : \n break \n try : \n self . parseline ( ) \n except ParseError , msg : \n raise ParseError ( \"<STR_LIT>\" % ( msg , __line ) ) \n return self . sink \n def parseline ( self ) : \n self . eat ( r_wspace ) \n if ( not self . line ) or self . line . startswith ( ( '<STR_LIT:#>' ) ) : \n return \n subject = self . subject ( ) \n self . eat ( r_wspace ) \n predicate = self . predicate ( ) \n self . eat ( r_wspace ) \n obj = self . object ( ) \n self . eat ( r_wspace ) \n context = self . uriref ( ) or self . nodeid ( ) or self . sink . identifier \n self . eat ( r_tail ) \n if self . line : \n raise ParseError ( \"<STR_LIT>\" ) \n self . sink . get_context ( context ) . add ( ( subject , predicate , <mask0> ) ) \n", "gt": "obj"}
{"input": "\n \"\"\"<STR_LIT>\"\"\" \n import codecs \n import csv \n from rdflib import Variable , BNode , URIRef , Literal , py3compat \n from rdflib . query import Result , ResultSerializer , ResultParser \n class CSVResultParser ( ResultParser ) : \n def __init__ ( self ) : \n self . delim = \"<STR_LIT:U+002C>\" \n def parse ( self , source ) : \n r = Result ( '<STR_LIT>' ) \n if isinstance ( source . read ( <NUM_LIT:0> ) , py3compat . bytestype ) : \n source = codecs . getreader ( '<STR_LIT:utf-8>' ) ( source ) \n reader = csv . reader ( source , delimiter = self . delim ) \n r . vars = [ Variable ( x ) for x in reader . next ( ) ] \n r . bindings = [ ] \n for row in reader : \n r . bindings . append ( self . parseRow ( row , r . vars ) ) \n return r \n def parseRow ( self , row , v ) : \n return dict ( ( var , val ) \n for var , val in zip ( v , [ self . convertTerm ( t ) \n for t in row ] ) if val is not None ) \n def convertTerm ( self , t ) : \n if t == \"<STR_LIT>\" : \n return None \n if t . startswith ( \"<STR_LIT>\" ) : \n return BNode ( t ) \n if t . startswith ( \"<STR_LIT>\" ) or t . startswith ( \"<STR_LIT>\" ) : \n return URIRef ( t ) \n return Literal ( t ) \n class CSVResultSerializer ( ResultSerializer ) : \n def __init__ ( self , result ) : \n ResultSerializer . __init__ ( self , result ) \n self . delim = \"<STR_LIT:U+002C>\" \n if result . type != \"<STR_LIT>\" : \n raise Exception ( \n \"<STR_LIT>\" ) \n def serialize ( self , stream , encoding = '<STR_LIT:utf-8>' ) : \n if py3compat . PY3 : \n import codecs \n stream = codecs . getwriter ( encoding ) ( stream ) \n out = csv . writer ( stream , delimiter = self . delim ) \n vs = [ self . serializeTerm ( v , encoding ) for v in self . result . vars ] \n out . writerow ( vs ) \n for row in self . result . bindings : \n out . writerow ( [ self . serializeTerm ( \n row . get ( v ) , encoding ) for v in self . result . vars ] ) \n def serializeTerm ( self , term , encoding ) : \n if term is None : \n return \"<STR_LIT>\" \n if not py3compat . PY3 : \n return term . encode ( encoding ) \n else : \n return <mask0> \n", "gt": "term"}
